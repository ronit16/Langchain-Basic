{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (0.0.38)\n",
      "Requirement already satisfied: faiss-cpu in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (1.8.0)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (0.1.59)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (0.1.52)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-community) (2.31.0)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (2.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain-community) (2.4)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ronit/Ronit/AI/myenv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.5.15 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from constants import gemini_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('/home/ronit/Ronit/Book/DSML.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science and Machine Learning\n",
      "Mathematical and Statistical Methods\n",
      "Dirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman\n",
      "8th May 2022To my wife and daughters: Lesley, Elise, and Jessica\n",
      "—DPK\n",
      "To Sarah, Soﬁa, and my parents\n",
      "—ZIB\n",
      "To my grandparents: Arno, Harry, Juta, and Maila\n",
      "—TT\n",
      "To Valerie\n",
      "—RVCONTENTS\n",
      "Preface xiii\n",
      "Notation xvii\n",
      "1 Importing, Summarizing, and Visualizing Data 1\n",
      "1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n",
      "1.2 Structuring Features According to Type . . . . . . . . . . . . . . . . . . 3\n",
      "1.3 Summary Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
      "1.4 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "1.5 Visualizing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.5.1 Plotting Qualitative Variables . . . . . . . . . . . . . . . . . . . . 9\n",
      "1.5.2 Plotting Quantitative Variables . . . . . . . . . . . . . . . . . . . 9\n",
      "1.5.3 Data Visualization in a Bivariate Setting . . . . . . . . . . . . . . 12\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "2 Statistical Learning 19\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "2.2 Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . . 20\n",
      "2.3 Training and Test Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "2.4 Tradeo \u000bs in Statistical Learning . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "2.5 Estimating Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n",
      "2.5.1 In-Sample Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n",
      "2.5.2 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "2.6 Modeling Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n",
      "2.7 Multivariate Normal Models . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "2.8 Normal Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n",
      "2.9 Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n",
      "3 Monte Carlo Methods 67\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n",
      "3.2 Monte Carlo Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.2.1 Generating Random Numbers . . . . . . . . . . . . . . . . . . . 68\n",
      "3.2.2 Simulating Random Variables . . . . . . . . . . . . . . . . . . . 69\n",
      "3.2.3 Simulating Random Vectors and Processes . . . . . . . . . . . . . 74\n",
      "3.2.4 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n",
      "3.2.5 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 78\n",
      "3.3 Monte Carlo Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n",
      "viiviii Contents\n",
      "3.3.1 Crude Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 85\n",
      "3.3.2 Bootstrap Method . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "3.3.3 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "3.4 Monte Carlo for Optimization . . . . . . . . . . . . . . . . . . . . . . . . 96\n",
      "3.4.1 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . 96\n",
      "3.4.2 Cross-Entropy Method . . . . . . . . . . . . . . . . . . . . . . . 100\n",
      "3.4.3 Splitting for Optimization . . . . . . . . . . . . . . . . . . . . . . 103\n",
      "3.4.4 Noisy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 105\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n",
      "4 Unsupervised Learning 121\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n",
      "4.2 Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . . 122\n",
      "4.3 Expectation–Maximization (EM) Algorithm . . . . . . . . . . . . . . . . 128\n",
      "4.4 Empirical Distribution and Density Estimation . . . . . . . . . . . . . . . 131\n",
      "4.5 Clustering via Mixture Models . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "4.5.1 Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "4.5.2 EM Algorithm for Mixture Models . . . . . . . . . . . . . . . . . 137\n",
      "4.6 Clustering via Vector Quantization . . . . . . . . . . . . . . . . . . . . . 142\n",
      "4.6.1 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n",
      "4.6.2 Clustering via Continuous Multiextremal Optimization . . . . . . 146\n",
      "4.7 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n",
      "4.8 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . 153\n",
      "4.8.1 Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . . 153\n",
      "4.8.2 PCA and Singular Value Decomposition (SVD) . . . . . . . . . . 155\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "5 Regression 167\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n",
      "5.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n",
      "5.3 Analysis via Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 171\n",
      "5.3.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . 171\n",
      "5.3.2 Model Selection and Prediction . . . . . . . . . . . . . . . . . . . 172\n",
      "5.3.3 Cross-Validation and Predictive Residual Sum of Squares . . . . . 173\n",
      "5.3.4 In-Sample Risk and Akaike Information Criterion . . . . . . . . . 175\n",
      "5.3.5 Categorical Features . . . . . . . . . . . . . . . . . . . . . . . . 177\n",
      "5.3.6 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n",
      "5.3.7 Coe \u000ecient of Determination . . . . . . . . . . . . . . . . . . . . 181\n",
      "5.4 Inference for Normal Linear Models . . . . . . . . . . . . . . . . . . . . 182\n",
      "5.4.1 Comparing Two Normal Linear Models . . . . . . . . . . . . . . 183\n",
      "5.4.2 Conﬁdence and Prediction Intervals . . . . . . . . . . . . . . . . 186\n",
      "5.5 Nonlinear Regression Models . . . . . . . . . . . . . . . . . . . . . . . . 188\n",
      "5.6 Linear Models in Python . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "5.6.1 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "5.6.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n",
      "5.6.3 Analysis of Variance (ANOV A) . . . . . . . . . . . . . . . . . . 195Contents ix\n",
      "5.6.4 Conﬁdence and Prediction Intervals . . . . . . . . . . . . . . . . 198\n",
      "5.6.5 Model Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n",
      "5.6.6 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n",
      "5.7 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 204\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n",
      "6 Regularization and Kernel Methods 215\n",
      "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n",
      "6.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "6.3 Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 222\n",
      "6.4 Construction of Reproducing Kernels . . . . . . . . . . . . . . . . . . . . 225\n",
      "6.4.1 Reproducing Kernels via Feature Mapping . . . . . . . . . . . . . 225\n",
      "6.4.2 Kernels from Characteristic Functions . . . . . . . . . . . . . . . 225\n",
      "6.4.3 Reproducing Kernels Using Orthonormal Features . . . . . . . . 227\n",
      "6.4.4 Kernels from Kernels . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "6.5 Representer Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n",
      "6.6 Smoothing Cubic Splines . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n",
      "6.7 Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . . 239\n",
      "6.8 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n",
      "7 Classiﬁcation 253\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n",
      "7.2 Classiﬁcation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n",
      "7.3 Classiﬁcation via Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . 259\n",
      "7.4 Linear and Quadratic Discriminant Analysis . . . . . . . . . . . . . . . . 261\n",
      "7.5 Logistic Regression and Softmax Classiﬁcation . . . . . . . . . . . . . . 268\n",
      "7.6 K-Nearest Neighbors Classiﬁcation . . . . . . . . . . . . . . . . . . . . . 270\n",
      "7.7 Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n",
      "7.8 Classiﬁcation with Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . 279\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n",
      "8 Decision Trees and Ensemble Methods 289\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n",
      "8.2 Top-Down Construction of Decision Trees . . . . . . . . . . . . . . . . . 291\n",
      "8.2.1 Regional Prediction Functions . . . . . . . . . . . . . . . . . . . 292\n",
      "8.2.2 Splitting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "8.2.3 Termination Criterion . . . . . . . . . . . . . . . . . . . . . . . . 294\n",
      "8.2.4 Basic Implementation . . . . . . . . . . . . . . . . . . . . . . . . 296\n",
      "8.3 Additional Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 300\n",
      "8.3.1 Binary Versus Non-Binary Trees . . . . . . . . . . . . . . . . . . 300\n",
      "8.3.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 300\n",
      "8.3.3 Alternative Splitting Rules . . . . . . . . . . . . . . . . . . . . . 300\n",
      "8.3.4 Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . . 301\n",
      "8.3.5 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n",
      "8.4 Controlling the Tree Shape . . . . . . . . . . . . . . . . . . . . . . . . . 302\n",
      "8.4.1 Cost-Complexity Pruning . . . . . . . . . . . . . . . . . . . . . . 305x Contents\n",
      "8.4.2 Advantages and Limitations of Decision Trees . . . . . . . . . . . 306\n",
      "8.5 Bootstrap Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n",
      "8.6 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\n",
      "8.7 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "9 Deep Learning 325\n",
      "9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\n",
      "9.2 Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.3 Back-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.4 Methods for Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n",
      "9.4.1 Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n",
      "9.4.2 Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . 337\n",
      "9.4.3 Limited-Memory BFGS Method . . . . . . . . . . . . . . . . . . 338\n",
      "9.4.4 Adaptive Gradient Methods . . . . . . . . . . . . . . . . . . . . . 340\n",
      "9.5 Examples in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n",
      "9.5.1 Simple Polynomial Regression . . . . . . . . . . . . . . . . . . . 342\n",
      "9.5.2 Image Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 346\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n",
      "A Linear Algebra and Functional Analysis 357\n",
      "A.1 Vector Spaces, Bases, and Matrices . . . . . . . . . . . . . . . . . . . . . 357\n",
      "A.2 Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n",
      "A.3 Complex Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . 363\n",
      "A.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "A.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . 365\n",
      "A.5.1 Left- and Right-Eigenvectors . . . . . . . . . . . . . . . . . . . . 366\n",
      "A.6 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n",
      "A.6.1 (P)LU Decomposition . . . . . . . . . . . . . . . . . . . . . . . 370\n",
      "A.6.2 Woodbury Identity . . . . . . . . . . . . . . . . . . . . . . . . . 372\n",
      "A.6.3 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . 375\n",
      "A.6.4 QR Decomposition and the Gram–Schmidt Procedure . . . . . . . 377\n",
      "A.6.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . 378\n",
      "A.6.6 Solving Structured Matrix Equations . . . . . . . . . . . . . . . . 381\n",
      "A.7 Functional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\n",
      "A.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "A.8.1 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . 394\n",
      "A.8.2 Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . 396\n",
      "B Multivariate Di \u000berentiation and Optimization 399\n",
      "B.1 Multivariate Di \u000berentiation . . . . . . . . . . . . . . . . . . . . . . . . . 399\n",
      "B.1.1 Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n",
      "B.1.2 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n",
      "B.2 Optimization Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n",
      "B.2.1 Convexity and Optimization . . . . . . . . . . . . . . . . . . . . 405\n",
      "B.2.2 Lagrangian Method . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "B.2.3 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409Contents xi\n",
      "B.3 Numerical Root-Finding and Minimization . . . . . . . . . . . . . . . . . 410\n",
      "B.3.1 Newton-Like Methods . . . . . . . . . . . . . . . . . . . . . . . 411\n",
      "B.3.2 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . 413\n",
      "B.3.3 Normal Approximation Method . . . . . . . . . . . . . . . . . . 415\n",
      "B.3.4 Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . 416\n",
      "B.4 Constrained Minimization via Penalty Functions . . . . . . . . . . . . . . 417\n",
      "C Probability and Statistics 423\n",
      "C.1 Random Experiments and Probability Spaces . . . . . . . . . . . . . . . 423\n",
      "C.2 Random Variables and Probability Distributions . . . . . . . . . . . . . . 424\n",
      "C.3 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428\n",
      "C.4 Joint Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n",
      "C.5 Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . . 430\n",
      "C.5.1 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 430\n",
      "C.5.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n",
      "C.5.3 Expectation and Covariance . . . . . . . . . . . . . . . . . . . . 431\n",
      "C.5.4 Conditional Density and Conditional Expectation . . . . . . . . . 433\n",
      "C.6 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . 433\n",
      "C.7 Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . 436\n",
      "C.8 Convergence of Random Variables . . . . . . . . . . . . . . . . . . . . . 441\n",
      "C.9 Law of Large Numbers and Central Limit Theorem . . . . . . . . . . . . 447\n",
      "C.10 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\n",
      "C.11 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "C.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456\n",
      "C.12.1 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . 457\n",
      "C.12.2 Maximum Likelihood Method . . . . . . . . . . . . . . . . . . . 458\n",
      "C.13 Conﬁdence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "C.14 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460\n",
      "D Python Primer 465\n",
      "D.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\n",
      "D.2 Python Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467\n",
      "D.3 Types and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n",
      "D.4 Functions and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n",
      "D.5 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\n",
      "D.6 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\n",
      "D.7 Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\n",
      "D.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\n",
      "D.9 Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\n",
      "D.10 NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480\n",
      "D.10.1 Creating and Shaping Arrays . . . . . . . . . . . . . . . . . . . . 480\n",
      "D.10.2 Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n",
      "D.10.3 Array Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n",
      "D.10.4 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 484\n",
      "D.11 Matplotlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "D.11.1 Creating a Basic Plot . . . . . . . . . . . . . . . . . . . . . . . . 485xii Contents\n",
      "D.12 Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "D.12.1 Series and DataFrame . . . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "D.12.2 Manipulating Data Frames . . . . . . . . . . . . . . . . . . . . . 489\n",
      "D.12.3 Extracting Information . . . . . . . . . . . . . . . . . . . . . . . 490\n",
      "D.12.4 Plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\n",
      "D.13 Scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\n",
      "D.13.1 Partitioning the Data . . . . . . . . . . . . . . . . . . . . . . . . 493\n",
      "D.13.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . . 493\n",
      "D.13.3 Fitting and Prediction . . . . . . . . . . . . . . . . . . . . . . . . 494\n",
      "D.13.4 Testing the Model . . . . . . . . . . . . . . . . . . . . . . . . . . 494\n",
      "D.14 System Calls, URL Access, and Speed-Up . . . . . . . . . . . . . . . . . 495\n",
      "Bibliography 497\n",
      "Index 505PREFACE\n",
      "In our present world of automation, cloud computing, algorithms, artiﬁcial intelligence,\n",
      "and big data, few topics are as relevant as data science andmachine learning . Their recent\n",
      "popularity lies not only in their applicability to real-life questions, but also in their natural\n",
      "blending of many di \u000berent disciplines, including mathematics, statistics, computer science,\n",
      "engineering, science, and ﬁnance.\n",
      "To someone starting to learn these topics, the multitude of computational techniques\n",
      "and mathematical ideas may seem overwhelming. Some may be satisﬁed with only learn-\n",
      "ing how to use o \u000b-the-shelf recipes to apply to practical situations. But what if the assump-\n",
      "tions of the black-box recipe are violated? Can we still trust the results? How should the\n",
      "algorithm be adapted? To be able to truly understand data science and machine learning it\n",
      "is important to appreciate the underlying mathematics and statistics, as well as the resulting\n",
      "algorithms.\n",
      "The purpose of this book is to provide an accessible, yet comprehensive, account of\n",
      "data science and machine learning. It is intended for anyone interested in gaining a better\n",
      "understanding of the mathematics and statistics that underpin the rich variety of ideas and\n",
      "machine learning algorithms in data science. Our viewpoint is that computer languages\n",
      "come and go, but the underlying key ideas and algorithms will remain forever and will\n",
      "form the basis for future developments.\n",
      "Before we turn to a description of the topics in this book, we would like to say a\n",
      "few words about its philosophy. This book resulted from various courses in data science\n",
      "and machine learning at the Universities of Queensland and New South Wales, Australia.\n",
      "When we taught these courses, we noticed that students were eager to learn not only how\n",
      "to apply algorithms but also to understand how these algorithms actually work. However,\n",
      "many existing textbooks assumed either too much background knowledge (e.g., measure\n",
      "theory and functional analysis) or too little (everything is a black box), and the information\n",
      "overload from often disjointed and contradictory internet sources made it more di \u000ecult for\n",
      "students to gradually build up their knowledge and understanding. We therefore wanted to\n",
      "write a book about data science and machine learning that can be read as a linear story,\n",
      "with a substantial “backstory” in the appendices. The main narrative starts very simply and\n",
      "builds up gradually to quite an advanced level. The backstory contains all the necessary\n",
      "xiiixiv Preface\n",
      "background, as well as additional information, from linear algebra and functional analysis\n",
      "(Appendix A), multivariate di \u000berentiation and optimization (Appendix B), and probability\n",
      "and statistics (Appendix C). Moreover, to make the abstract ideas come alive, we believe\n",
      "it is important that the reader sees actual implementations of the algorithms, directly trans-\n",
      "lated from the theory. After some deliberation we have chosen Python as our programming\n",
      "language. It is freely available and has been adopted as the programming language of\n",
      "choice for many practitioners in data science and machine learning. It has many useful\n",
      "packages for data manipulation (often ported from R) and has been designed to be easy to\n",
      "program. A gentle introduction to Python is given in Appendix D.\n",
      "To keep the book manageable in size we had to be selective in our choice of topics.\n",
      "Important ideas and connections between various concepts are highlighted via keywords\n",
      "keywords and page references (indicated by a +) in the margin. Key deﬁnitions and theorems are\n",
      "highlighted in boxes. Whenever feasible we provide proofs of theorems. Finally, we place\n",
      "great importance on notation . It is often the case that once a consistent and concise system\n",
      "of notation is in place, seemingly di \u000ecult ideas suddenly become obvious. We use di \u000ber-\n",
      "ent fonts to distinguish between di \u000berent types of objects. Vectors are denoted by letters in\n",
      "boldface italics, x;X, and matrices by uppercase letters in boldface roman font, A;K. We\n",
      "also distinguish between random vectors and their values by using upper and lower case\n",
      "letters, e.g., X(random vector) and x(its value or outcome). Sets are usually denoted by\n",
      "calligraphic letters G;H. The symbols for probability and expectation are PandE, respect-\n",
      "ively. Distributions are indicated by sans serif font, as in BinandGamma ; exceptions are\n",
      "the ubiquitous notations NandUfor the normal and uniform distributions. A summary of\n",
      "the most important symbols and abbreviations is given on Pages xvii–xxi. +xvii\n",
      "Data science provides the language and techniques necessary for understanding and\n",
      "dealing with data. It involves the design, collection, analysis, and interpretation of nu-\n",
      "merical data, with the aim of extracting patterns and other useful information. Machine\n",
      "learning, which is closely related to data science, deals with the design of algorithms and\n",
      "computer resources to learn from data. The organization of the book follows roughly the\n",
      "typical steps in a data science project: Gathering data to gain information about a research\n",
      "question; cleaning, summarization, and visualization of the data; modeling and analysis of\n",
      "the data; translating decisions about the model into decisions and predictions about the re-\n",
      "search question. As this is a mathematics and statistics oriented book, most emphasis will\n",
      "be on modeling and analysis.\n",
      "We start in Chapter 1 with the reading, structuring, summarization, and visualization\n",
      "of data using the data manipulation package pandas in Python. Although the material\n",
      "covered in this chapter requires no mathematical knowledge, it forms an obvious starting\n",
      "point for data science: to better understand the nature of the available data. In Chapter 2, we\n",
      "introduce the main ingredients of statistical learning . We distinguish between supervised\n",
      "andunsupervised learning techniques, and discuss how we can assess the predictive per-\n",
      "formance of (un)supervised learning methods. An important part of statistical learning is\n",
      "themodeling of data. We introduce various useful models in data science including linear,\n",
      "multivariate Gaussian, and Bayesian models. Many algorithms in machine learning and\n",
      "data science make use of Monte Carlo techniques, which is the topic of Chapter 3. Monte\n",
      "Carlo can be used for simulation, estimation, and optimization. Chapter 4 is concerned\n",
      "with unsupervised learning, where we discuss techniques such as density estimation, clus-\n",
      "tering, and principal component analysis. We then turn our attention to supervised learningPreface xv\n",
      "in Chapter 5, and explain the ideas behind a broad class of regression models. Therein, we\n",
      "also describe how Python’s statsmodels package can be used to deﬁne and analyze linear\n",
      "models. Chapter 6 builds upon the previous regression chapter by developing the power-\n",
      "ful concepts of kernel methods and regularization, which allow the fundamental ideas of\n",
      "Chapter 5 to be expanded in an elegant way, using the theory of reproducing kernel Hilbert\n",
      "spaces. In Chapter 7, we proceed with the classiﬁcation task, which also belongs to the\n",
      "supervised learning framework, and consider various methods for classiﬁcation, including\n",
      "Bayes classiﬁcation, linear and quadratic discriminant analysis, K-nearest neighbors, and\n",
      "support vector machines. In Chapter 8 we consider versatile methods for regression and\n",
      "classiﬁcation that make use of tree structures. Finally, in Chapter 9, we consider the work-\n",
      "ings of neural networks and deep learning, and show that these learning algorithms have a\n",
      "simple mathematical interpretation. An extensive range of exercises is provided at the end\n",
      "of each chapter.\n",
      "Python code and data sets for each chapter can be downloaded from the GitHub site:\n",
      "https://github.com/DSML-book\n",
      "Acknowledgments\n",
      "Some of the Python code for Chapters 1 and 5 was adapted from [73]. We thank Benoit\n",
      "Liquet for making this available, and Lauren Jones for translating the R code into Python.\n",
      "We thank all who through their comments, feedback, and suggestions have contributed\n",
      "to this book, including Qibin Duan, Luke Taylor, Rémi Mouzayek, Harry Goodman, Bryce\n",
      "Stansﬁeld, Ryan Tongs, Dillon Steyl, Bill Rudd, Nan Ye, Christian Hirsch, Chris van der\n",
      "Heide, Sarat Moka, Aapeli Vuorinen, Joshua Ross, Giang Nguyen, and the anonymous\n",
      "referees. David Grubbs deserves a special accollade for his professionalism and attention\n",
      "to detail in his role as Editor for this book.\n",
      "The book was test-run during the 2019 Summer School of the Australian Mathemat-\n",
      "ical Sciences Institute . More than 80 bright upper-undergraduate (Honours) students used\n",
      "the book for the course Mathematical Methods for Machine Learning , taught by Zdravko\n",
      "Botev. We are grateful for the valuable feedback that they provided.\n",
      "Our special thanks go out to Robert Salomone, Liam Berry, Robin Carrick, and Sam\n",
      "Daley, who commented in great detail on earlier versions of the entire book and wrote and\n",
      "improved our Python code. Their enthusiasm, perceptiveness, and kind assistance have\n",
      "been invaluable.\n",
      "Of course, none of this work would have been possible without the loving support,\n",
      "patience, and encouragement from our families, and we thank them with all our hearts.\n",
      "This book was ﬁnancially supported by the Australian Research Council Centre of\n",
      "Excellence for Mathematical &Statistical Frontiers , under grant number CE140100049.\n",
      "Dirk Kroese, Zdravko Botev,\n",
      "Thomas Taimre, and Radislav Vaisman\n",
      "Brisbane and SydneyxviNOTATION\n",
      "We could, of course, use any notation we want; do not laugh at notations;\n",
      "invent them, they are powerful. In fact, mathematics is, to a large extent, in-\n",
      "vention of better notations.\n",
      "Richard P. Feynman\n",
      "We have tried to use a notation system that is, in order of importance, simple, descript-\n",
      "ive, consistent, and compatible with historical choices. Achieving all of these goals all of\n",
      "the time would be impossible, but we hope that our notation helps to quickly recognize\n",
      "the type or “ﬂavor” of certain mathematical objects (vectors, matrices, random vectors,\n",
      "probability measures, etc.) and clarify intricate ideas.\n",
      "We make use of various typographical aids, and it will be beneﬁcial for the reader to\n",
      "be aware of some of these.\n",
      "Boldface font is used to indicate composite objects, such as column vectors x=\n",
      "[x1;:::; xn]>and matrices X=[xi j]. Note also the di \u000berence between the upright bold\n",
      "font for matrices and the slanted bold font for vectors.\n",
      "Random variables are generally speciﬁed with upper case roman letters X;Y;Zand their\n",
      "outcomes with lower case letters x;y;z. Random vectors are thus denoted in upper case\n",
      "slanted bold font: X=[X1;:::; Xn]>.\n",
      "Sets of vectors are generally written in calligraphic font, such as X, but the set of real\n",
      "numbers uses the common blackboard bold font R. Expectation and probability also use\n",
      "the latter font.\n",
      "Probability distributions use a sans serif font, such as BinandGamma . Exceptions to\n",
      "this rule are the “standard” notations NandUfor the normal and uniform distributions.\n",
      "We often omit brackets when it is clear what the argument is of a function or operator.\n",
      "For example, we prefer EX2toE[X2].\n",
      "xviixviii Notation\n",
      "We employ color to emphasize that certain words refer to a dataset ,function , or\n",
      "package in Python. All code is written in typewriter font. To be compatible with past\n",
      "notation choices, we introduced a special blue symbol Xfor the model (design) matrix of\n",
      "a linear model.\n",
      "Important notation such as T,g,g\u0003is often deﬁned in a mnemonic way, such as Tfor\n",
      "“training”, gfor “guess”, g\u0003for the “star” (that is, optimal) guess, and `for “loss”.\n",
      "We will occasionally use a Bayesian notation convention in which the same symbol is\n",
      "used to denote di \u000berent (conditional) probability densities. In particular, instead of writing\n",
      "fX(x) and fXjY(xjy) for the probability density function (pdf) of Xand the conditional pdf\n",
      "ofXgiven Y, we simply write f(x) and f(xjy). This particular style of notation can be of\n",
      "great descriptive value, despite its apparent ambiguity.\n",
      "General font/notation rules\n",
      "x scalar\n",
      "x vector\n",
      "X random vector\n",
      "X matrix\n",
      "X set\n",
      "bx estimate or approximation\n",
      "x\u0003optimal\n",
      "x average\n",
      "Common mathematical symbols\n",
      "8 for all\n",
      "9 there exists\n",
      "/ is proportional to\n",
      "? is perpendicular to\n",
      "\u0018 is distributed as\n",
      "iid\u0018,\u0018iid are independent and identically distributed as\n",
      "approx:\u0018 is approximately distributed as\n",
      "rf gradient of f\n",
      "r2f Hessian of f\n",
      "f2Cpfhas continuous derivatives of order p\n",
      "\u0019 is approximately\n",
      "' is asymptotically\n",
      "\u001c is much smaller than\n",
      "\b direct sumNotation xix\n",
      "\f elementwise product\n",
      "\\ intersection\n",
      "[ union\n",
      ":=,=: is deﬁned as\n",
      "a:s:\u0000! converges almost surely to\n",
      "d\u0000! converges in distribution to\n",
      "P\u0000! converges in probability to\n",
      "Lp\u0000! converges in Lp-norm to\n",
      "k\u0001k Euclidean norm\n",
      "dxe smallest integer larger than x\n",
      "bxc largest integer smaller than x\n",
      "x+ maxfx;0g\n",
      "Matrix/vector notation\n",
      "A>,x>transpose of matrix Aor vector x\n",
      "A\u00001inverse of matrix A\n",
      "A+pseudo-inverse of matrix A\n",
      "A\u0000>inverse of matrix A>or transpose of A\u00001\n",
      "A\u001f0 matrix Ais positive deﬁnite\n",
      "A\u00170 matrix Ais positive semideﬁnite\n",
      "dim(x) dimension of vector x\n",
      "det(A) determinant of matrix A\n",
      "jAj absolute value of the determinant of matrix A\n",
      "tr(A) trace of matrix A\n",
      "Reserved letters and words\n",
      "C set of complex numbers\n",
      "d di \u000berential symbol\n",
      "E expectation\n",
      "e the number 2 :71828:::\n",
      "f probability density (discrete or continuous)\n",
      "g prediction function\n",
      "1fAgor1Aindicator function of set A\n",
      "i the square root of \u00001\n",
      "` risk: expected lossxx Notation\n",
      "Loss loss function\n",
      "ln (natural) logarithm\n",
      "N set of natural numbers f0;1;:::g\n",
      "O big-O order symbol: f(x)=O(g(x)) ifjf(x)j6\u000bg(x) for some constant \u000bas\n",
      "x!a\n",
      "o little-o order symbol: f(x)=o(g(x)) if f(x)=g(x)!0 as x!a\n",
      "P probability measure\n",
      "\u0019 the number 3 :14159:::\n",
      "R set of real numbers (one-dimensional Euclidean space)\n",
      "Rnn-dimensional Euclidean space\n",
      "R+ positive real line: [0 ;1)\n",
      "\u001c deterministic training set\n",
      "T random training set\n",
      "X model (design) matrix\n",
      "Z set of integersf:::;\u00001;0;1;:::g\n",
      "Probability distributions\n",
      "Ber Bernoulli\n",
      "Beta beta\n",
      "Bin binomial\n",
      "Exp exponential\n",
      "Geom geometric\n",
      "Gamma gamma\n",
      "F Fisher–Snedecor F\n",
      "N normal or Gaussian\n",
      "Pareto Pareto\n",
      "Poi Poisson\n",
      "t Student’s t\n",
      "U uniform\n",
      "Abbreviations and acronyms\n",
      "cdf cumulative distribution function\n",
      "CMC crude Monte Carlo\n",
      "CE cross-entropy\n",
      "EM expectation–maximization\n",
      "GP Gaussian process\n",
      "KDE Kernel density estimate /estimatorNotation xxi\n",
      "KL Kullback–Leibler\n",
      "KKT Karush–Kuhn–Tucker\n",
      "iid independent and identically distributed\n",
      "MAP maximum a posteriori\n",
      "MCMC Markov chain Monte Carlo\n",
      "MLE maximum likelihood estimator /estimate\n",
      "OOB out-of-bag\n",
      "PCA principal component analysis\n",
      "pdf probability density function (discrete or continuous)\n",
      "SVD singular value decompositionxxiiCHAPTER1\n",
      "IMPORTING , SUMMARIZING ,AND\n",
      "VISUALIZING DATA\n",
      "This chapter describes where to ﬁnd useful data sets, how to load them into Python,\n",
      "and how to (re)structure the data. We also discuss various ways in which the data can\n",
      "be summarized via tables and ﬁgures. Which type of plots and numerical summaries\n",
      "are appropriate depends on the type of the variable(s) in play. Readers unfamiliar with\n",
      "Python are advised to read Appendix D ﬁrst.\n",
      "1.1 Introduction\n",
      "Data comes in many shapes and forms, but can generally be thought of as being the result\n",
      "of some random experiment — an experiment whose outcome cannot be determined in\n",
      "advance, but whose workings are still subject to analysis. Data from a random experiment\n",
      "are often stored in a table or spreadsheet. A statistical convention is to denote variables —\n",
      "often called features features — as columns and the individual items (or units) as rows . It is useful\n",
      "to think of three types of columns in such a spreadsheet:\n",
      "1. The ﬁrst column is usually an identiﬁer or index column, where each unit /row is\n",
      "given a unique name or ID.\n",
      "2. Certain columns (features) can correspond to the design of the experiment, specify-\n",
      "ing, for example, to which experimental group the unit belongs. Often the entries in\n",
      "these columns are deterministic ; that is, they stay the same if the experiment were to\n",
      "be repeated.\n",
      "3. Other columns represent the observed measurements of the experiment. Usually,\n",
      "these measurements exhibit variability ; that is, they would change if the experiment\n",
      "were to be repeated.\n",
      "There are many data sets available from the Internet and in software packages. A well-\n",
      "known repository of data sets is the Machine Learning Repository maintained by the Uni-\n",
      "versity of California at Irvine (UCI), found at https://archive.ics.uci.edu/ .\n",
      "12 1.1. Introduction\n",
      "These data sets are typically stored in a CSV (comma separated values) format, which\n",
      "can be easily read into Python. For example, to access the abalone data set from this web-\n",
      "site with Python, download the ﬁle to your working directory, import the pandas package\n",
      "via\n",
      "import pandas as pd\n",
      "and read in the data as follows:\n",
      "abalone = pd.read_csv( 'abalone.data ',header = None)\n",
      "It is important to add header = None , as this lets Python know that the ﬁrst line of the\n",
      "CSV does not contain the names of the features, as it assumes so by default. The data set\n",
      "was originally used to predict the age of abalone from physical measurements, such as\n",
      "shell weight and diameter.\n",
      "Another useful repository of over 1000 data sets from various packages in the R pro-\n",
      "gramming language, collected by Vincent Arel-Bundock, can be found at:\n",
      "https://vincentarelbundock.github.io/Rdatasets/datasets.html .\n",
      "For example, to read Fisher’s famous iris data set from R’s datasets package into Py-\n",
      "thon, type:\n",
      "urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\n",
      "dataname = 'datasets/iris.csv '\n",
      "iris = pd.read_csv(urlprefix + dataname)\n",
      "Theiris data set contains four physical measurements (sepal /petal length /width) on\n",
      "50 specimens (each) of 3 species of iris: setosa, versicolor, and virginica. Note that in this\n",
      "case the headers are included. The output of read_csv is aDataFrame object, which is\n",
      "pandas ’s implementation of a spreadsheet; see Section D.12.1. The DataFrame method +487\n",
      "head gives the ﬁrst few rows of the DataFrame , including the feature names. The number\n",
      "of rows can be passed as an argument and is 5 by default. For the iris DataFrame , we\n",
      "have:\n",
      "iris.head()\n",
      "Unnamed: 0 Sepal.Length ... Petal.Width Species\n",
      "0 1 5.1 ... 0.2 setosa\n",
      "1 2 4.9 ... 0.2 setosa\n",
      "2 3 4.7 ... 0.2 setosa\n",
      "3 4 4.6 ... 0.2 setosa\n",
      "4 5 5.0 ... 0.2 setosa\n",
      "[5 rows x 6 columns]\n",
      "The names of the features can be obtained via the columns attribute of the DataFrame\n",
      "object, as in iris.columns . Note that the ﬁrst column is a duplicate index column, whose\n",
      "name (assigned by pandas ) is'Unnamed: 0 '. We can drop this column and reassign the\n",
      "iris object as follows:\n",
      "iris = iris.drop( 'Unnamed: 0 ',1)Chapter 1. Importing, Summarizing, and Visualizing Data 3\n",
      "The data for each feature (corresponding to its speciﬁc name) can be accessed by using\n",
      "Python’s slicing notation []. For example, the object iris['Sepal.Length'] contains\n",
      "the 150 sepal lengths.\n",
      "The ﬁrst three rows of the abalone data set from the UCI repository can be found as\n",
      "follows:\n",
      "abalone.head(3)\n",
      "0 1 2 3 4 5 6 7 8\n",
      "0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15\n",
      "1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7\n",
      "2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9\n",
      "Here, the missing headers have been assigned according to the order of the natural\n",
      "numbers. The names should correspond to Sex, Length, Diameter, Height, Whole weight,\n",
      "Shucked weight, Viscera weight, Shell weight, and Rings, as described in the ﬁle with the\n",
      "name abalone.names on the UCI website. We can manually add the names of the features\n",
      "to the DataFrame by reassigning the columns attribute, as in:\n",
      "abalone.columns = [ 'Sex','Length ','Diameter ','Height ',\n",
      "'Whole weight ','Shucked weight ','Viscera weight ','Shell weight ',\n",
      "'Rings ']\n",
      "1.2 Structuring Features According to Type\n",
      "We can generally classify features as either quantitative or qualitative. Quantitative Quantitative features\n",
      "possess “numerical quantity”, such as height, age, number of births, etc., and can either be\n",
      "continuous ordiscrete . Continuous quantitative features take values in a continuous range\n",
      "of possible values, such as height, voltage, or crop yield; such features capture the idea\n",
      "that measurements can always be made more precisely. Discrete quantitative features have\n",
      "a countable number of possibilities, such as a count.\n",
      "In contrast, qualitative qualitative features do not have a numerical meaning, but their possible\n",
      "values can be divided into a ﬁxed number of categories, such as fM,Fgfor gender orfblue,\n",
      "black, brown, green gfor eye color. For this reason such features are also called categorical categorical .\n",
      "A simple rule of thumb is: if it does not make sense to average the data, it is categorical.\n",
      "For example, it does not make sense to average eye colors. Of course it is still possible to\n",
      "represent categorical data with numbers, such as 1 =blue, 2 =black, 3 =brown, but such\n",
      "numbers carry no quantitative meaning. Categorical features are often called factors factors .\n",
      "When manipulating, summarizing, and displaying data, it is important to correctly spe-\n",
      "cify the type of the variables (features). We illustrate this using the nutrition_elderly\n",
      "data set from [73], which contains the results of a study involving nutritional measure-\n",
      "ments of thirteen features (columns) for 226 elderly individuals (rows). The data set can be\n",
      "obtained from:\n",
      "http://www.biostatisticien.eu/springeR/nutrition_elderly.xls .\n",
      "Excel ﬁles can be read directly into pandas via the read_excel method:4 1.2. Structuring Features According to Type\n",
      "xls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls '\n",
      "nutri = pd.read_excel(xls)\n",
      "This creates a DataFrame object nutri . The ﬁrst three rows are as follows:\n",
      "pd.set_option( 'display.max_columns ', 8) # to fit display\n",
      "nutri.head(3)\n",
      "gender situation tea ... cooked_fruit_veg chocol fat\n",
      "0 2 1 0 ... 4 5 6\n",
      "1 2 1 1 ... 5 1 4\n",
      "2 2 1 0 ... 2 5 4\n",
      "[3 rows x 13 columns]\n",
      "You can check the type (or structure) of the variables via the info method of nutri .\n",
      "nutri.info()\n",
      "<class 'pandas.core.frame.DataFrame '>\n",
      "RangeIndex: 226 entries , 0 to 225\n",
      "Data columns (total 13 columns):\n",
      "gender 226 non-null int64\n",
      "situation 226 non-null int64\n",
      "tea 226 non-null int64\n",
      "coffee 226 non-null int64\n",
      "height 226 non-null int64\n",
      "weight 226 non-null int64\n",
      "age 226 non-null int64\n",
      "meat 226 non-null int64\n",
      "fish 226 non-null int64\n",
      "raw_fruit 226 non-null int64\n",
      "cooked_fruit_veg 226 non-null int64\n",
      "chocol 226 non-null int64\n",
      "fat 226 non-null int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 23.0 KB\n",
      "All 13 features in nutri are (at the moment) interpreted by Python as quantitative\n",
      "variables, indeed as integers, simply because they have been entered as whole numbers.\n",
      "The meaning of these numbers becomes clear when we consider the description of the\n",
      "features, given in Table 1.2. Table 1.1 shows how the variable types should be classiﬁed.\n",
      "Table 1.1: The feature types for the data frame nutri .\n",
      "Qualitative gender ,situation ,fat\n",
      "meat ,fish ,raw_fruit ,cooked_fruit_veg ,chocol\n",
      "Discrete quantitative tea,coffee\n",
      "Continuous quantitative height ,weight ,age\n",
      "Note that the categories of the qualitative features in the second row of Table 1.1, meat ,\n",
      ". . . ,chocol have a natural order. Such qualitative features are sometimes called ordinal , inChapter 1. Importing, Summarizing, and Visualizing Data 5\n",
      "Table 1.2: Description of the variables in the nutritional study [73].\n",
      "Feature Description Unit orCoding\n",
      "gender Gender 1 =Male; 2 =Female\n",
      "situation Family status1=Single\n",
      "2=Living with spouse\n",
      "3=Living with family\n",
      "4=Living with someone else\n",
      "tea Daily consumption of tea Number of cups\n",
      "coffee Daily consumption of co \u000bee Number of cups\n",
      "height Height cm\n",
      "weight Weight (actually: mass) kg\n",
      "age Age at date of interview Years\n",
      "meat Consumption of meat0=Never\n",
      "1=Less than once a week\n",
      "2=Once a week\n",
      "3=2–3 times a week\n",
      "4=4–6 times a week\n",
      "5=Every day\n",
      "fish Consumption of ﬁsh As in meat\n",
      "raw_fruit Consumption of raw fruits As in meat\n",
      "cooked_fruit_vegConsumption of cookedAs in meatfruits and vegetables\n",
      "chocol Consumption of chocolate As in meat\n",
      "fat1=Butter\n",
      "2=Margarine\n",
      "3=Peanut oil\n",
      "Type of fat used 4 =Sunﬂower oil\n",
      "for cooking 5 =Olive oil\n",
      "6=Mix of vegetable oils (e.g., Isio4)\n",
      "7=Colza oil\n",
      "8=Duck or goose fat\n",
      "contrast to qualitative features without order, which are called nominal . We will not make\n",
      "such a distinction in this book.\n",
      "We can modify the Python value and type for each categorical feature, using the\n",
      "replace andastype methods. For categorical features, such as gender , we can replace\n",
      "the value 1 with 'Male 'and 2 with 'Female ', and change the type to 'category 'as\n",
      "follows.\n",
      "DICT = {1: 'Male ', 2: 'Female '} # dictionary specifies replacement\n",
      "nutri[ 'gender '] = nutri[ 'gender '].replace(DICT).astype( 'category ')\n",
      "The structure of the other categorical-type features can be changed in a similar way.\n",
      "Continuous features such as height should have type float :\n",
      "nutri[ 'height '] = nutri[ 'height '].astype(float)6 1.3. Summary Tables\n",
      "We can repeat this for the other variables (see Exercise 2) and save this modiﬁed data\n",
      "frame as a CSV ﬁle, by using the pandas method to_csv .\n",
      "nutri.to_csv( 'nutri.csv ',index=False)\n",
      "1.3 Summary Tables\n",
      "It is often useful to summarize a large spreadsheet of data in a more condensed form. A\n",
      "table of counts or a table of frequencies makes it easier to gain insight into the underlying\n",
      "distribution of a variable, especially if the data are qualitative. Such tables can be obtained\n",
      "with the methods describe andvalue_counts .\n",
      "As a ﬁrst example, we load the nutri DataFrame , which we restructured and saved\n",
      "(see previous section) as 'nutri.csv ', and then construct a summary for the feature\n",
      "(column) 'fat'.\n",
      "nutri = pd.read_csv( 'nutri.csv ')\n",
      "nutri[ 'fat'].describe()\n",
      "count 226\n",
      "unique 8\n",
      "top sunflower\n",
      "freq 68\n",
      "Name: fat, dtype: object\n",
      "We see that there are 8 di \u000berent types of fat used and that sunﬂower has the highest\n",
      "count, with 68 out of 226 individuals using this type of cooking fat. The method\n",
      "value_counts gives the counts for the di \u000berent fat types.\n",
      "nutri[ 'fat'].value_counts()\n",
      "sunflower 68\n",
      "peanut 48\n",
      "olive 40\n",
      "margarine 27\n",
      "Isio4 23\n",
      "butter 15\n",
      "duck 4\n",
      "colza 1\n",
      "Name: fat, dtype: int64\n",
      "Column labels are also attributes of a DataFrame , and nutri.fat , for example, is\n",
      "exactly the same object as nutri[ 'fat'].Chapter 1. Importing, Summarizing, and Visualizing Data 7\n",
      "It is also possible to use crosstab tocross tabulate between two or more variables,cross tabulategiving a contingency table :\n",
      "pd.crosstab(nutri.gender , nutri.situation)\n",
      "situation Couple Family Single\n",
      "gender\n",
      "Female 56 7 78\n",
      "Male 63 2 20\n",
      "We see, for example, that the proportion of single men is substantially smaller than the\n",
      "proportion of single women in the data set of elderly people. To add row and column totals\n",
      "to a table, use margins=True .\n",
      "pd.crosstab(nutri.gender , nutri.situation , margins=True)\n",
      "situation Couple Family Single All\n",
      "gender\n",
      "Female 56 7 78 141\n",
      "Male 63 2 20 85\n",
      "All 119 9 98 226\n",
      "1.4 Summary Statistics\n",
      "In the following, x=[x1;:::; xn]>is a column vector of nnumbers. For our nutri data,\n",
      "the vector xcould, for example, correspond to the heights of the n=226 individuals.\n",
      "Thesample mean sample mean ofx, denoted by x, is simply the average of the data values:\n",
      "x=1\n",
      "nnX\n",
      "i=1xi:\n",
      "Using the mean method in Python for the nutri data, we have, for instance:\n",
      "nutri[ 'height '].mean()\n",
      "163.96017699115043\n",
      "Thep-sample quantile sample quantile (0<p<1) of xis a value xsuch that at least a fraction pof the\n",
      "data is less than or equal to xand at least a fraction 1 \u0000pof the data is greater than or equal\n",
      "tox. The sample median sample median is the sample 0 :5-quantile. The p-sample quantile is also called\n",
      "the 100\u0002p percentile . The 25, 50, and 75 sample percentiles are called the ﬁrst, second,\n",
      "and third quartiles quartiles of the data. For the nutri data they are obtained as follows.\n",
      "nutri[ 'height '].quantile(q=[0.25,0.5,0.75])\n",
      "0.25 157.0\n",
      "0.50 163.0\n",
      "0.75 170.08 1.5. Visualizing Data\n",
      "The sample mean and median give information about the location of the data, while the\n",
      "distance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of\n",
      "thedispersion (spread) of the data. Other measures for dispersion are the sample range ,sample rangemax ixi\u0000min ixi, the sample variancesample variance\n",
      "s2=1\n",
      "n\u00001nX\n",
      "i=1(xi\u0000x)2; (1.1)\n",
      "and the sample standard deviation s =p\n",
      "s2. For the nutri data, the range (in cm) is:sample\n",
      "standard\n",
      "deviation\n",
      "+457nutri[ 'height '].max() - nutri[ 'height '].min()\n",
      "48.0\n",
      "The variance (in cm2) is:\n",
      "round(nutri[ 'height '].var(), 2) # round to two decimal places\n",
      "81.06\n",
      "And the standard deviation can be found via:\n",
      "round(nutri[ 'height '].std(), 2)\n",
      "9.0\n",
      "We already encountered the describe method in the previous section for summarizing\n",
      "qualitative features, via the most frequent count and the number of unique elements. When\n",
      "applied to a quantitative feature, it returns instead the minimum, maximum, mean, and the\n",
      "three quartiles. For example, the 'height 'feature in the nutri data has the following\n",
      "summary statistics.\n",
      "nutri[ 'height '].describe()\n",
      "count 226.000000\n",
      "mean 163.960177\n",
      "std 9.003368\n",
      "min 140.000000\n",
      "25\\% 157.000000\n",
      "50\\% 163.000000\n",
      "75\\% 170.000000\n",
      "max 188.000000\n",
      "Name: height , dtype: float64\n",
      "1.5 Visualizing Data\n",
      "In this section we describe various methods for visualizing data. The main point we would\n",
      "like to make is that the way in which variables are visualized should always be adapted to\n",
      "the variable types; for example, qualitative data should be plotted di \u000berently from quantit-\n",
      "ative data.Chapter 1. Importing, Summarizing, and Visualizing Data 9\n",
      "For the rest of this section, it is assumed that matplotlib.pyplot ,pandas , and\n",
      "numpy , have been imported in the Python code as follows.\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "1.5.1 Plotting Qualitative Variables\n",
      "Suppose we wish to display graphically how many elderly people are living by themselves,\n",
      "as a couple, with family, or other. Recall that the data are given in the situation column\n",
      "of our nutri data. Assuming that we already restructured the data , as in Section 1.2, we + 3\n",
      "can make a barplot of the number of people in each category via the plt.bar function ofbarplotthe standard matplotlib plotting library. The inputs are the x-axis positions, heights, and\n",
      "widths of each bar respectively.\n",
      "width = 0.35 # the width of the bars\n",
      "x = [0, 0.8, 1.6] # the bar positions on x-axis\n",
      "situation_counts=nutri[ 'situation '].value_counts()\n",
      "plt.bar(x, situation_counts , width , edgecolor = 'black ')\n",
      "plt.xticks(x, situation_counts.index)\n",
      "plt.show()\n",
      "Couple Single Family0255075100125\n",
      "Figure 1.1: Barplot for the qualitative variable 'situation '.\n",
      "1.5.2 Plotting Quantitative Variables\n",
      "We now present a few useful methods for visualizing quantitative data, again using the\n",
      "nutri data set. We will ﬁrst focus on continuous features (e.g., 'age') and then add some\n",
      "speciﬁc graphs related to discrete features (e.g., 'tea'). The aim is to describe the variab-\n",
      "ility present in a single feature. This typically involves a central tendency, where observa-\n",
      "tions tend to gather around, with fewer observations further away. The main aspects of the\n",
      "distribution are the location (or center) of the variability, the spread of the variability (how\n",
      "far the values extend from the center), and the shape of the variability; e.g., whether or not\n",
      "values are spread symmetrically on either side of the center.10 1.5. Visualizing Data\n",
      "1.5.2.1 Boxplot\n",
      "Aboxplot can be viewed as a graphical representation of the ﬁve-number summary ofboxplotthe data consisting of the minimum, maximum, and the ﬁrst, second, and third quartiles.\n",
      "Figure 1.2 gives a boxplot for the 'age'feature of the nutri data.\n",
      "plt.boxplot(nutri[ 'age'],widths=width ,vert=False)\n",
      "plt.xlabel( 'age')\n",
      "plt.show()\n",
      "Thewidths parameter determines the width of the boxplot, which is by default plotted\n",
      "vertically. Setting vert=False plots the boxplot horizontally, as in Figure 1.2.\n",
      "65\n",
      " 70\n",
      " 75\n",
      " 80\n",
      " 85\n",
      " 90\n",
      "age\n",
      "1\n",
      "Figure 1.2: Boxplot for 'age'.\n",
      "The box is drawn from the ﬁrst quartile ( Q1) to the third quartile ( Q3). The vertical line\n",
      "inside the box signiﬁes the location of the median. So-called “whiskers” extend to either\n",
      "side of the box. The size of the box is called the interquartile range : IQR =Q3\u0000Q1. The\n",
      "left whisker extends to the largest of (a) the minimum of the data and (b) Q1\u00001:5 IQR.\n",
      "Similarly, the right whisker extends to the smallest of (a) the maximum of the data and\n",
      "(b)Q3+1:5 IQR. Any data point outside the whiskers is indicated by a small hollow dot,\n",
      "indicating a suspicious or deviant point (outlier). Note that a boxplot may also be used for\n",
      "discrete quantitative features.\n",
      "1.5.2.2 Histogram\n",
      "Ahistogram is a common graphical representation of the distribution of a quantitativehistogramfeature. We start by breaking the range of the values into a number of bins orclasses .\n",
      "We tally the counts of the values falling in each bin and then make the plot by drawing\n",
      "rectangles whose bases are the bin intervals and whose heights are the counts. In Python\n",
      "we can use the function plt.hist . For example, Figure 1.3 shows a histogram of the 226\n",
      "ages in nutri , constructed via the following Python code.\n",
      "weights = np.ones_like(nutri.age)/nutri.age.count()\n",
      "plt.hist(nutri.age,bins=9,weights=weights ,facecolor= 'cyan ',\n",
      "edgecolor= 'black ', linewidth=1)\n",
      "plt.xlabel( 'age')\n",
      "plt.ylabel( 'Proportion of Total ')\n",
      "plt.show()Chapter 1. Importing, Summarizing, and Visualizing Data 11\n",
      "Here 9 bins were used. Rather than using raw counts (the default), the vertical axis\n",
      "here gives the percentage in each class, deﬁned bycount\n",
      "total. This is achieved by choosing the\n",
      "“weights” parameter to be equal to the vector with entries 1 =266, with length 226. Various\n",
      "plotting parameters have also been changed.\n",
      "65\n",
      " 70\n",
      " 75\n",
      " 80\n",
      " 85\n",
      " 90\n",
      "age\n",
      "0.00\n",
      "0.05\n",
      "0.10\n",
      "0.15\n",
      "0.20Proportion of Total\n",
      "Figure 1.3: Histogram of 'age'.\n",
      "Histograms can also be used for discrete features, although it may be necessary to\n",
      "explicitly specify the bins and placement of the ticks on the axes.\n",
      "1.5.2.3 Empirical Cumulative Distribution Function\n",
      "The empirical cumulative distribution function , denoted by Fn, is a step function whichempirical\n",
      "cumulative\n",
      "distribution\n",
      "functionjumps an amount k=nat observation values, where kis the number of tied observations\n",
      "at that value. For observations x1;:::; xn,Fn(x) is the fraction of observations less than or\n",
      "equal to x, i.e.,\n",
      "Fn(x)=number of xi6x\n",
      "n=1\n",
      "nnX\n",
      "i=11fxi6xg; (1.2)\n",
      "where 1denotes the indicator indicator function; that is, 1fxi6xgis equal to 1 when xi6xand 0\n",
      "otherwise. To produce a plot of the empirical cumulative distribution function we can use\n",
      "theplt.step function. The result for the age data is shown in Figure 1.4. The empirical\n",
      "cumulative distribution function for a discrete quantitative variable is obtained in the same\n",
      "way.\n",
      "x = np.sort(nutri.age)\n",
      "y = np.linspace(0,1,len(nutri.age))\n",
      "plt.xlabel( 'age')\n",
      "plt.ylabel( 'Fn(x) ')\n",
      "plt.step(x,y)\n",
      "plt.xlim(x.min(),x.max())\n",
      "plt.show()12 1.5. Visualizing Data\n",
      "65\n",
      " 70\n",
      " 75\n",
      " 80\n",
      " 85\n",
      " 90\n",
      "age\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0Fn(x)\n",
      "Figure 1.4: Plot of the empirical distribution function for the continuous quantitative fea-\n",
      "ture'age'.\n",
      "1.5.3 Data Visualization in a Bivariate Setting\n",
      "In this section, we present a few useful visual aids to explore relationships between two\n",
      "features. The graphical representation will depend on the type of the two features.\n",
      "1.5.3.1 Two-way Plots for Two Categorical Variables\n",
      "Comparing barplots for two categorical variables involves introducing subplots to the ﬁg-\n",
      "ure. Figure 1.5 visualizes the contingency table of Section 1.3, which cross-tabulates the\n",
      "family status (situation) with the gender of the elderly people. It simply shows two barplots\n",
      "next to each other in the same ﬁgure.\n",
      "Couple\n",
      " Family\n",
      " Single\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80Counts\n",
      "Male\n",
      "Female\n",
      "Figure 1.5: Barplot for two categorical variables.Chapter 1. Importing, Summarizing, and Visualizing Data 13\n",
      "The ﬁgure was made using the seaborn package, which was speciﬁcally designed to\n",
      "simplify statistical visualization tasks.\n",
      "import seaborn as sns\n",
      "sns.countplot(x= 'situation ', hue = 'gender ', data=nutri ,\n",
      "hue_order = [ 'Male ','Female '], palette = [ 'SkyBlue ','Pink '],\n",
      "saturation = 1, edgecolor= 'black ')\n",
      "plt.legend(loc= 'upper center ')\n",
      "plt.xlabel( '')\n",
      "plt.ylabel( 'Counts ')\n",
      "plt.show()\n",
      "1.5.3.2 Plots for Two Quantitative Variables\n",
      "We can visualize patterns between two quantitative features using a scatterplot scatterplot . This can be\n",
      "done with plt.scatter . The following code produces a scatterplot of 'weight 'against\n",
      "'height 'for the nutri data.\n",
      "plt.scatter(nutri.height , nutri.weight , s=12, marker= 'o')\n",
      "plt.xlabel( 'height ')\n",
      "plt.ylabel( 'weight ')\n",
      "plt.show()\n",
      "140\n",
      " 150\n",
      " 160\n",
      " 170\n",
      " 180\n",
      " 190\n",
      "height\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90weight\n",
      "Figure 1.6: Scatterplot of 'weight 'against 'height '.\n",
      "The next Python code illustrates that it is possible to produce highly sophisticated scat-\n",
      "ter plots, such as in Figure 1.7. The ﬁgure shows the birth weights (mass) of babies whose\n",
      "mothers smoked (blue triangles) or not (red circles). In addition, straight lines were ﬁtted to\n",
      "the two groups, suggesting that birth weight decreases with age when the mother smokes,\n",
      "but increases when the mother does not smoke! The question is whether these trends are\n",
      "statistically signiﬁcant or due to chance. We will revisit this data set later on in the book. +19914 1.5. Visualizing Data\n",
      "urlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\n",
      "dataname = 'MASS/birthwt.csv '\n",
      "bwt = pd.read_csv(urlprefix + dataname)\n",
      "bwt = bwt.drop( 'Unnamed: 0 ',1) #drop unnamed column\n",
      "styles = {0: [ 'o','red'], 1: [ '^','blue ']}\n",
      "for k in styles:\n",
      "grp = bwt[bwt.smoke==k]\n",
      "m,b = np.polyfit(grp.age, grp.bwt, 1) # fit a straight line\n",
      "plt.scatter(grp.age, grp.bwt, c=styles[k][1], s=15, linewidth=0,\n",
      "marker = styles[k][0])\n",
      "plt.plot(grp.age, m*grp.age + b, '-', color=styles[k][1])\n",
      "plt.xlabel( 'age')\n",
      "plt.ylabel( 'birth weight (g) ')\n",
      "plt.legend([ 'non-smokers ','smokers '],prop={ 'size ':8},\n",
      "loc=(0.5,0.8))\n",
      "plt.show()\n",
      "10\n",
      " 15\n",
      " 20\n",
      " 25\n",
      " 30\n",
      " 35\n",
      " 40\n",
      " 45\n",
      " 50\n",
      "age\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000birth weight (g)\n",
      "non-smokers\n",
      "smokers\n",
      "Figure 1.7: Birth weight against age for smoking and non-smoking mothers.\n",
      "1.5.3.3 Plots for One Qualitative and One Quantitative Variable\n",
      "In this setting, it is interesting to draw boxplots of the quantitative feature for each level\n",
      "of the categorical feature. Assuming the variables are structured correctly, the function\n",
      "plt.boxplot can be used to produce Figure 1.8, using the following code:\n",
      "males = nutri[nutri.gender == 'Male ']\n",
      "females = nutri[nutri.gender == 'Female ']\n",
      "plt.boxplot([males.coffee ,females.coffee],notch=True ,widths\n",
      "=(0.5,0.5))\n",
      "plt.xlabel( 'gender ')\n",
      "plt.ylabel( 'coffee ')\n",
      "plt.xticks([1,2],[ 'Male ','Female '])\n",
      "plt.show()Chapter 1. Importing, Summarizing, and Visualizing Data 15\n",
      "Male\n",
      " Female\n",
      "gender\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5coffee\n",
      "Figure 1.8: Boxplots of a quantitative feature 'coffee 'as a function of the levels of a\n",
      "categorical feature 'gender '. Note that we used a di \u000berent, “notched”, style boxplot this\n",
      "time.\n",
      "Further Reading\n",
      "The focus in this book is on the mathematical and statistical analysis of data, and for the\n",
      "rest of the book we assume that the data is available in a suitable form for analysis. How-\n",
      "ever, a large part of practical data science involves the cleaning of data; that is, putting\n",
      "it into a form that is amenable to analysis with standard software packages. Standard Py-\n",
      "thon modules such as numpy andpandas can be used to reformat rows, rename columns,\n",
      "remove faulty outliers, merge rows, and so on. McKinney, the creator of pandas , gives\n",
      "many practical case studies in [84]. E \u000bective data visualization techniques are beautifully\n",
      "illustrated in [65].\n",
      "Exercises\n",
      "Before you attempt these exercises, make sure you have up-to-date versions of the relevant\n",
      "Python packages, speciﬁcally matplotlib ,pandas , and seaborn . An easy way to ensure\n",
      "this is to update packages via the Anaconda Navigator, as explained in Appendix D.\n",
      "1. Visit the UCI Repository https://archive.ics.uci.edu/ . Read the description of\n",
      "the data and download the Mushroom data set agaricus-lepiota.data . Using pandas ,\n",
      "read the data into a DataFrame called mushroom , via read_csv .\n",
      "(a) How many features are in this data set?\n",
      "(b) What are the initial names and types of the features?\n",
      "(c) Rename the ﬁrst feature (index 0) to 'edibility 'and the sixth feature (index 5) to\n",
      "'odor '[Hint: the column names in pandas are immutable; so individual columns\n",
      "cannot be modiﬁed directly. However it is possible to assign the entire column names\n",
      "list via mushroom.columns = newcols . ]16 Exercises\n",
      "(d) The 6th column lists the various odors of the mushrooms: encoded as 'a','c', . . . .\n",
      "Replace these with the names 'almond ','creosote ', etc. (categories correspond-\n",
      "ing to each letter can be found on the website). Also replace the 'edibility 'cat-\n",
      "egories 'e'and'p'with 'edible 'and'poisonous '.\n",
      "(e) Make a contingency table cross-tabulating 'edibility 'and'odor '.\n",
      "(f) Which mushroom odors should be avoided, when gathering mushrooms for consump-\n",
      "tion?\n",
      "(g) What proportion of odorless mushroom samples were safe to eat?\n",
      "2. Change the type and value of variables in the nutri data set according to Table 1.2 and\n",
      "save the data as a CSV ﬁle. The modiﬁed data should have eight categorical features, three\n",
      "ﬂoats, and two integer features.\n",
      "3. It frequently happens that a table with data needs to be restructured before the data can\n",
      "be analyzed using standard statistical software. As an example, consider the test scores in\n",
      "Table 1.3 of 5 students before and after specialized tuition.\n",
      "Table 1.3: Student scores.\n",
      "Student Before After\n",
      "1 75 85\n",
      "2 30 50\n",
      "3 100 100\n",
      "4 50 52\n",
      "5 60 65\n",
      "This is not in the standard format described in Section 1.1. In particular, the student scores\n",
      "are divided over two columns, whereas the standard format requires that they are collected\n",
      "in one column, e.g., labelled 'Score '. Reformat (by hand) the table in standard format,\n",
      "using three features:\n",
      "'Score ', taking continuous values,\n",
      "'Time ', taking values 'Before 'and'After ',\n",
      "'Student ', taking values from 1 to 5.\n",
      "Useful methods for reshaping tables in pandas aremelt ,stack , and unstack .\n",
      "4. Create a similar barplot as in Figure 1.5, but now plot the corresponding proportions of\n",
      "males and females in each of the three situation categories. That is, the heights of the bars\n",
      "should sum up to 1 for both barplots with the same 'gender' value. [Hint: seaborn does\n",
      "not have this functionality built in, instead you need to ﬁrst create a contingency table and\n",
      "usematplotlib.pyplot to produce the ﬁgure.]\n",
      "5. The iris data set, mentioned in Section 1.1, contains various features, including +2\n",
      "'Petal.Length 'and'Sepal.Length ', of three species of iris: setosa, versicolor, and\n",
      "virginica.Chapter 1. Importing, Summarizing, and Visualizing Data 17\n",
      "(a) Load the data set into a pandas DataFrame object.\n",
      "(b) Using matplotlib.pyplot , produce boxplots of 'Petal.Length 'for each the\n",
      "three species, in one ﬁgure.\n",
      "(c) Make a histogram with 20 bins for 'Petal.Length '.\n",
      "(d) Produce a similar scatterplot for 'Sepal.Length 'against 'Petal.Length 'to that\n",
      "of the left plot in Figure 1.9. Note that the points should be colored according to the\n",
      "'Species' feature as per the legend in the right plot of the ﬁgure.\n",
      "(e) Using the kdeplot method of the seaborn package, reproduce the right plot of\n",
      "Figure 1.9, where kernel density plots for 'Petal.Length 'are given. +131\n",
      "1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      "Petal.Length\n",
      "5\n",
      "6\n",
      "7\n",
      "8Sepal.Length\n",
      "2\n",
      " 4\n",
      " 6\n",
      " 8\n",
      "Petal.Length\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "2.5Density\n",
      "setosa\n",
      "versicolor\n",
      "virginica\n",
      "Figure 1.9: Left: scatterplot of 'Sepal.Length 'against 'Petal.Length '. Right: kernel\n",
      "density estimates of 'Petal.Length 'for the three species of iris.\n",
      "6. Import the data set EuStockMarkets from the same website as the iris data set above.\n",
      "The data set contains the daily closing prices of four European stock indices during the\n",
      "1990s, for 260 working days per year.\n",
      "(a) Create a vector of times (working days) for the stock prices, between 1991.496 and\n",
      "1998.646 with increments of 1 /260.\n",
      "(b) Reproduce Figure 1.10. [Hint: Use a dictionary to map column names (stock indices)\n",
      "to colors.]18 Exercises\n",
      "1991\n",
      " 1992\n",
      " 1993\n",
      " 1994\n",
      " 1995\n",
      " 1996\n",
      " 1997\n",
      " 1998\n",
      " 1999\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "DAX\n",
      "SMI\n",
      "CAC\n",
      "FTSE\n",
      "Figure 1.10: Closing stock indices for various European stock markets.\n",
      "7. Consider the KASANDR data set from the UCI Machine Learning Repository, which can\n",
      "be downloaded from\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/00385/de\n",
      ".tar.bz2 .\n",
      "This archive ﬁle has a size of 900Mb, so it may take a while to download. Uncompressing\n",
      "the ﬁle (e.g., via 7-Zip) yields a directory decontaining two large CSV ﬁles: test_de.csv\n",
      "andtrain_de.csv , with sizes 372Mb and 3Gb, respectively. Such large data ﬁles can still\n",
      "be processed e \u000eciently in pandas , provided there is enough memory. The ﬁles contain\n",
      "records of user information from Kelkoo web logs in Germany as well as meta-data on\n",
      "users, o \u000bers, and merchants. The data sets have 7 attributes and 1919561 and 15844717\n",
      "rows, respectively. The data sets are anonymized via hex strings.\n",
      "(a) Load train_de.csv into a pandas DataFrame object de, using\n",
      "read_csv( 'train_de.csv ', delimiter = '\\t').\n",
      "If not enough memory is available, load test_de.csv instead. Note that entries are\n",
      "separated here by tabs, not commas. Time how long it takes for the ﬁle to load, using\n",
      "thetime package. (It took 38 seconds for train_de.csv to load on one of our\n",
      "computers.)\n",
      "(b) How many unique users and merchants are in this data set?\n",
      "8. Visualizing data involving more than two features requires careful design, which is often\n",
      "more of an art than a science.\n",
      "(a) Go to Vincent Arel-Bundocks’s website (URL given in Section 1.1) and read the\n",
      "Orange data set into a pandas DataFrame object called orange . Remove its ﬁrst\n",
      "(unnamed) column.\n",
      "(b) The data set contains the circumferences of 5 orange trees at various stages in their\n",
      "development. Find the names of the features.\n",
      "(c) In Python, import seaborn and visualize the growth curves (circumference against\n",
      "age) of the trees, using the regplot andFacetGrid methods.CHAPTER2\n",
      "STATISTICAL LEARNING\n",
      "The purpose of this chapter is to introduce the reader to some common concepts\n",
      "and themes in statistical learning. We discuss the di \u000berence between supervised and\n",
      "unsupervised learning, and how we can assess the predictive performance of supervised\n",
      "learning. We also examine the central role that the linear and Gaussian properties play\n",
      "in the modeling of data. We conclude with a section on Bayesian learning. The required\n",
      "probability and statistics background is given in Appendix C.\n",
      "2.1 Introduction\n",
      "Although structuring and visualizing data are important aspects of data science, the main\n",
      "challenge lies in the mathematical analysis of the data. When the goal is to interpret the\n",
      "model and quantify the uncertainty in the data, this analysis is usually referred to as stat-\n",
      "istical learning . In contrast, when the emphasis is on making predictions using large-scalestatistical\n",
      "learning data, then it is common to speak about machine learning ordata mining .\n",
      "machine\n",
      "learning\n",
      "data miningThere are two major goals for modeling data: 1) to accurately predict some future\n",
      "quantity of interest, given some observed data, and 2) to discover unusual or interesting\n",
      "patterns in the data. To achieve these goals, one must rely on knowledge from three im-\n",
      "portant pillars of the mathematical sciences.\n",
      "Function approximation. Building a mathematical model for data usually means under-\n",
      "standing how one data variable depends on another data variable. The most natural\n",
      "way to represent the relationship between variables is via a mathematical function or\n",
      "map. We usually assume that this mathematical function is not completely known,\n",
      "but can be approximated well given enough computing power and data. Thus, data\n",
      "scientists have to understand how best to approximate and represent functions using\n",
      "the least amount of computer processing and memory.\n",
      "Optimization. Given a class of mathematical models, we wish to ﬁnd the best possible\n",
      "model in that class. This requires some kind of e \u000ecient search or optimization pro-\n",
      "cedure. The optimization step can be viewed as a process of ﬁtting or calibrating\n",
      "a function to observed data. This step usually requires knowledge of optimization\n",
      "algorithms and e \u000ecient computer coding or programming.\n",
      "1920 2.2. Supervised and Unsupervised Learning\n",
      "Probability and Statistics. In general, the data used to ﬁt the model is viewed as a realiz-\n",
      "ation of a random process or numerical vector, whose probability law determines the\n",
      "accuracy with which we can predict future observations. Thus, in order to quantify\n",
      "the uncertainty inherent in making predictions about the future, and the sources of er-\n",
      "ror in the model, data scientists need a ﬁrm grasp of probability theory and statistical\n",
      "inference.\n",
      "2.2 Supervised and Unsupervised Learning\n",
      "Given an input or feature feature vector x, one of the main goals of machine learning is to predict\n",
      "an output or response response variable y. For example, xcould be a digitized signature and ya\n",
      "binary variable that indicates whether the signature is genuine or false. Another example is\n",
      "where xrepresents the weight and smoking habits of an expecting mother and ythe birth\n",
      "weight of the baby. The data science attempt at this prediction is encoded in a mathematical\n",
      "function g, called the prediction function prediction\n",
      "function, which takes as an input xand outputs a guess g(x)\n",
      "fory(denoted by by, for example). In a sense, gencompasses all the information about the\n",
      "relationship between the variables xandy, excluding the e \u000bects of chance and randomness\n",
      "in nature.\n",
      "Inregression problems, the response variable ycan take any real value. In contrast,regressionwhen ycan only lie in a ﬁnite set, say y2f0;:::; c\u00001g, then predicting yis conceptually\n",
      "the same as classifying the input xinto one of ccategories, and so prediction becomes a\n",
      "classiﬁcation classification problem.\n",
      "We can measure the accuracy of a prediction bywith respect to a given response yby\n",
      "using some loss function loss function Loss( y;by). In a regression setting the usual choice is the squared-\n",
      "error loss ( y\u0000by)2. In the case of classiﬁcation, the zero–one (also written 0–1) loss function\n",
      "Loss( y;by)=1fy,bygis often used, which incurs a loss of 1 whenever the predicted class\n",
      "byis not equal to the class y. Later on in this book, we will encounter various other useful\n",
      "loss functions, such as the cross-entropy and hinge loss functions (see, e.g., Chapter 7).\n",
      "The word error is often used as a measure of distance between a “true” object yand\n",
      "some approximation bythereof. If yis real-valued, the absolute error jy\u0000byjand the\n",
      "squared error ( y\u0000by)2are both well-established error concepts, as are the norm ky\u0000byk\n",
      "and squared norm ky\u0000byk2for vectors. The squared error ( y\u0000by)2is just one example\n",
      "of a loss function.\n",
      "It is unlikely that any mathematical function gwill be able to make accurate predictions\n",
      "for all possible pairs ( x;y) one may encounter in Nature. One reason for this is that, even\n",
      "with the same input x, the output ymay be di \u000berent, depending on chance circumstances\n",
      "or randomness. For this reason, we adopt a probabilistic approach and assume that each\n",
      "pair ( x;y) is the outcome of a random pair ( X;Y) that has some joint probability density\n",
      "f(x;y). We then assess the predictive performance via the expected loss, usually called the\n",
      "risk risk , for g:\n",
      "`(g)=ELoss( Y;g(X)): (2.1)\n",
      "For example, in the classiﬁcation case with zero–one loss function the risk is equal to the\n",
      "probability of incorrect classiﬁcation: `(g)=P[Y,g(X)]. In this context, the predictionChapter 2. Statistical Learning 21\n",
      "function gis called a classiﬁer classifier . Given the distribution of ( X;Y) and any loss function, we\n",
      "can in principle ﬁnd the best possible g\u0003:=argmingELoss( Y;g(X)) that yields the smallest\n",
      "risk`\u0003:=`(g\u0003). We will see in Chapter 7 that in the classiﬁcation case with y2f0;:::; c\u00001g +253\n",
      "and`(g)=P[Y,g(X)], we have\n",
      "g\u0003(x)=argmax\n",
      "y2f0;:::;c\u00001gf(yjx);\n",
      "where f(yjx)=P[Y=yjX=x] is the conditional probability of Y=ygiven X=x.\n",
      "As already mentioned, for regression the most widely-used loss function is the squared-\n",
      "error loss. In this setting, the optimal prediction function g\u0003is often called the regression\n",
      "function . The following theorem speciﬁes its exact form.regression\n",
      "function\n",
      "Theorem 2.1: Optimal Prediction Function for Squared-Error Loss\n",
      "For the squared-error loss Loss( y;by)=(y\u0000by)2, the optimal prediction function g\u0003is\n",
      "equal to the conditional expectation of Ygiven X=x:\n",
      "g\u0003(x)=E[YjX=x]:\n",
      "Proof: Letg\u0003(x)=E[YjX=x]. For any function g, the squared-error risk satisﬁes\n",
      "E(Y\u0000g(X))2=E[(Y\u0000g\u0003(X)+g\u0003(X)\u0000g(X))2]\n",
      "=E(Y\u0000g\u0003(X))2+2E[(Y\u0000g\u0003(X))(g\u0003(X)\u0000g(X))]+E(g\u0003(X)\u0000g(X))2\n",
      ">E(Y\u0000g\u0003(X))2+2E[(Y\u0000g\u0003(X))(g\u0003(X)\u0000g(X))]\n",
      "=E(Y\u0000g\u0003(X))2+2Ef(g\u0003(X)\u0000g(X))E[Y\u0000g\u0003(X)jX]g:\n",
      "In the last equation we used the tower property. By the deﬁnition of the conditional expect- +433\n",
      "ation, we have E[Y\u0000g\u0003(X)jX]=0. It follows that E(Y\u0000g(X))2>E(Y\u0000g\u0003(X))2, showing\n",
      "thatg\u0003yields the smallest squared-error risk. \u0003\n",
      "One consequence of Theorem 2.1 is that, conditional on X=x, the (random) response\n",
      "Ycan be written as\n",
      "Y=g\u0003(x)+\"(x); (2.2)\n",
      "where\"(x) can be viewed as the random deviation of the response from its conditional\n",
      "mean at x. This random deviation satisﬁes E\"(x)=0. Further, the conditional variance of\n",
      "the response Yatxcan be written as Var\"(x)=v2(x) for some unknown positive function\n",
      "v. Note that, in general, the probability distribution of \"(x) is unspeciﬁed.\n",
      "Since, the optimal prediction function g\u0003depends on the typically unknown joint distri-\n",
      "bution of ( X;Y), it is not available in practice. Instead, all that we have available is a ﬁnite\n",
      "number of (usually) independent realizations from the joint density f(x;y). We denote this\n",
      "sample byT=f(X1;Y1);:::; (Xn;Yn)gand call it the training set training set (Tis a mnemonic for\n",
      "training) with nexamples. It will be important to distinguish between a random training\n",
      "setTand its (deterministic) outcome f(x1;y1);:::; (xn;yn)g. We will use the notation \u001cfor\n",
      "the latter. We will also add the subscript nin\u001cnwhen we wish to emphasize the size of the\n",
      "training set.\n",
      "Our goal is thus to “learn” the unknown g\u0003using the nexamples in the training set T.\n",
      "Let us denote by gTthe best (by some criterion) approximation for g\u0003that we can construct22 2.2. Supervised and Unsupervised Learning\n",
      "fromT. Note that gTis a random function. A particular outcome is denoted by g\u001c. It is\n",
      "often useful to think of a teacher–learner metaphor, whereby the function gTis alearner learner\n",
      "who learns the unknown functional relationship g\u0003:x7!yfrom the training data T. We\n",
      "can imagine a “teacher” who provides nexamples of the true relationship between the\n",
      "output Yiand the input Xifori=1;:::; n, and thus “trains” the learner gTto predict the\n",
      "output of a new input X, for which the correct output Yis not provided by the teacher (is\n",
      "unknown).\n",
      "The above setting is called supervised learning supervised\n",
      "learning, because one tries to learn the functional\n",
      "relationship between the feature vector xand response yin the presence of a teacher who\n",
      "provides nexamples. It is common to speak of “explaining” or predicting yon the basis of\n",
      "x, where xis a vector of explanatory variables explanatory\n",
      "variables.\n",
      "An example of supervised learning is email spam detection. The goal is to train the\n",
      "learner gTto accurately predict whether any future email, as represented by the feature\n",
      "vector x, is spam or not. The training data consists of the feature vectors of a number\n",
      "of di\u000berent email examples as well as the corresponding labels (spam or not spam). For\n",
      "instance, a feature vector could consist of the number of times sales-pitch words like “free”,\n",
      "“sale”, or “miss out” occur within a given email.\n",
      "As seen from the above discussion, most questions of interest in supervised learning\n",
      "can be answered if we know the conditional pdf f(yjx), because we can then in principle\n",
      "work out the function value g\u0003(x).\n",
      "In contrast, unsupervised learning unsupervised\n",
      "learningmakes no distinction between response and explan-\n",
      "atory variables, and the objective is simply to learn the structure of the unknown distribu-\n",
      "tion of the data. In other words, we need to learn f(x). In this case the guess g(x) is an\n",
      "approximation of f(x) and the risk is of the form\n",
      "`(g)=ELoss( f(X);g(X)):\n",
      "An example of unsupervised learning is when we wish to analyze the purchasing be-\n",
      "haviors of the customers of a grocery shop that has a total of, say, a hundred items on sale.\n",
      "A feature vector here could be a binary vector x2f0;1g100representing the items bought\n",
      "by a customer on a visit to the shop (a 1 in the k-th position if a customer bought item\n",
      "k2f1;:::; 100gand a 0 otherwise). Based on a training set \u001c=fx1;:::; xng, we wish to\n",
      "ﬁnd any interesting or unusual purchasing patterns. In general, it is di \u000ecult to know if an\n",
      "unsupervised learner is doing a good job, because there is no teacher to provide examples\n",
      "of accurate predictions.\n",
      "The main methodologies for unsupervised learning include clustering ,principal com-\n",
      "ponent analysis , and kernel density estimation , which will be discussed in Chapter 4. +121\n",
      "In the next three sections we will focus on supervised learning. The main super-\n",
      "vised learning methodologies are regression andclassiﬁcation , to be discussed in detail in\n",
      "Chapters 5 and 7. More advanced supervised learning techniques, including reproducing +167\n",
      "+253 kernel Hilbert spaces ,tree methods , and deep learning , will be discussed in Chapters 6, 8,\n",
      "and 9.Chapter 2. Statistical Learning 23\n",
      "2.3 Training and Test Loss\n",
      "Given an arbitrary prediction function g, it is typically not possible to compute its risk `(g)\n",
      "in (2.1). However, using the training sample T, we can approximate `(g) via the empirical\n",
      "(sample average) risk\n",
      "`T(g)=1\n",
      "nnX\n",
      "i=1Loss( Yi;g(Xi)); (2.3)\n",
      "which we call the training loss training loss . The training loss is thus an unbiased estimator of the risk\n",
      "(the expected loss) for a prediction function g, based on the training data.\n",
      "To approximate the optimal prediction function g\u0003(the minimizer of the risk `(g)) we\n",
      "ﬁrst select a suitable collection of approximating functions Gand then take our learner to\n",
      "be the function in Gthat minimizes the training loss; that is,\n",
      "gG\n",
      "T=argmin\n",
      "g2G`T(g): (2.4)\n",
      "For example, the simplest and most useful Gis the set of linear functions of x; that is, the\n",
      "set of all functions g:x7!\f>xfor some real-valued vector \f.\n",
      "We suppress the superscript Gwhen it is clear which function class is used. Note that\n",
      "minimizing the training loss over all possible functions g(rather than over all g2G) does\n",
      "not lead to a meaningful optimization problem, as any function gfor which g(Xi)=Yifor\n",
      "alligives minimal training loss. In particular, for a squared-error loss, the training loss will\n",
      "be 0. Unfortunately, such functions have a poor ability to predict new (that is, independent\n",
      "fromT) pairs of data. This poor generalization performance is called overﬁtting overfitting .\n",
      "By choosing ga function that predicts the training data exactly (and is, for example,\n",
      "0 otherwise), the squared-error training loss is zero. Minimizing the training loss is\n",
      "not the ultimate goal!\n",
      "The prediction accuracy of new pairs of data is measured by the generalization risk generalization\n",
      "riskof\n",
      "the learner. For a ﬁxed training set \u001cit is deﬁned as\n",
      "`(gG\n",
      "\u001c)=ELoss( Y;gG\n",
      "\u001c(X)); (2.5)\n",
      "where ( X;Y) is distributed according to f(x;y). In the discrete case the generalization risk\n",
      "is therefore: `(gG\n",
      "\u001c)=P\n",
      "x;yLoss( y;gG\n",
      "\u001c(x))f(x;y) (replace the sum with an integral for the\n",
      "continuous case). The situation is illustrated in Figure 2.1, where the distribution of ( X;Y)\n",
      "is indicated by the red dots. The training set (points in the shaded regions) determines a\n",
      "ﬁxed prediction function shown as a straight line. Three possible outcomes of ( X;Y) are\n",
      "shown (black dots). The amount of loss for each point is shown as the length of the dashed\n",
      "lines. The generalization risk is the average loss over all possible pairs ( x;y), weighted by\n",
      "the corresponding f(x;y).24 2.3. Training and Test Loss\n",
      "xx xyy\n",
      "y\n",
      "Figure 2.1: The generalization risk for a ﬁxed training set is the weighted-average loss over\n",
      "all possible pairs ( x;y).\n",
      "For a random training setT, the generalization risk is thus a random variable that\n",
      "depends onT(andG). If we average the generalization risk over all possible instances of\n",
      "T, we obtain the expected generalization risk expected\n",
      "generalization\n",
      "risk:\n",
      "E`(gG\n",
      "T)=ELoss( Y;gG\n",
      "T(X)); (2.6)\n",
      "where ( X;Y) in the expectation above is independent of T. In the discrete case, we have\n",
      "E`(gG\n",
      "T)=P\n",
      "x;y;x1;y1;:::;xn;ynLoss( y;gG\n",
      "\u001c(x))f(x;y)f(x1;y1)\u0001\u0001\u0001f(xn;yn). Figure 2.2 gives an il-\n",
      "lustration.\n",
      "y\n",
      "xy\n",
      "y\n",
      "x x\n",
      "Figure 2.2: The expected generalization risk is the weighted-average loss over all possible\n",
      "pairs ( x;y) and over all training sets.\n",
      "For any outcome \u001cof the training data, we can estimate the generalization risk without\n",
      "bias by taking the sample average\n",
      "`T0(gG\n",
      "\u001c) :=1\n",
      "n0n0X\n",
      "i=1Loss( Y0\n",
      "i;gG\n",
      "\u001c(X0\n",
      "i)); (2.7)\n",
      "wheref(X0\n",
      "1;Y0\n",
      "1);:::; (X0\n",
      "n0;Y0\n",
      "n0)g=:T0is a so-called test sample test sample . The test sample is com-\n",
      "pletely separate from T, but is drawn in the same way as T; that is, via independent draws\n",
      "from f(x;y), for some sample size n0. We call the estimator (2.7) the test loss test loss . For a ran-\n",
      "dom training setTwe can deﬁne `T0(gG\n",
      "T) similarly. It is then crucial to assume that Tis\n",
      "independent ofT0. Table 2.1 summarizes the main deﬁnitions and notation for supervised\n",
      "learning.Chapter 2. Statistical Learning 25\n",
      "Table 2.1: Summary of deﬁnitions for supervised learning.\n",
      "x Fixed explanatory (feature) vector.\n",
      "X Random explanatory (feature) vector.\n",
      "y Fixed (real-valued) response.\n",
      "Y Random response.\n",
      "f(x;y) Joint pdf of XandY, evaluated at ( x;y).\n",
      "f(yjx) Conditional pdf of Ygiven X=x, evaluated at y.\n",
      "\u001cor\u001cn Fixed training data f(xi;yi);i=1;:::; ng.\n",
      "TorTn Random training data f(Xi;Yi);i=1;:::; ng.\n",
      "X Matrix of explanatory variables, with nrows x>\n",
      "i;i=1;:::; n\n",
      "and dim( x) feature columns; one of the features may be the\n",
      "constant 1.\n",
      "y Vector of response variables ( y1;:::; yn)>.\n",
      "g Prediction (guess) function.\n",
      "Loss( y;by) Loss incurred when predicting response ywithby.\n",
      "`(g) Risk for prediction function g; that is,ELoss( Y;g(X)).\n",
      "g\u0003Optimal prediction function; that is, argming`(g).\n",
      "gGOptimal prediction function in function class G; that is,\n",
      "argming2G`(g).\n",
      "`\u001c(g) Training loss for prediction function g; that is, the sample av-\n",
      "erage estimate of `(g) based on a ﬁxed training sample \u001c.\n",
      "`T(g) The same as `\u001c(g), but now for a random training sample T.\n",
      "gG\n",
      "\u001corg\u001c The learner : argming2G`\u001c(g). That is, the optimal prediction\n",
      "function based on a ﬁxed training set \u001cand function class G.\n",
      "We suppress the superscript Gif the function class is implicit.\n",
      "gG\n",
      "TorgT The learner, where we have replaced \u001cwith a random training\n",
      "setT.\n",
      "To compare the predictive performance of various learners in the function class G, as\n",
      "measured by the test loss, we can use the same ﬁxed training set \u001cand test set \u001c0for all\n",
      "learners. When there is an abundance of data, the “overall” data set is usually (randomly)\n",
      "divided into a training and test set, as depicted in Figure 2.3. We then use the training data\n",
      "to construct various learners gG1\u001c;gG2\u001c;:::, and use the test data to select the best (with the\n",
      "smallest test loss) among these learners. In this context the test set is called the validation\n",
      "set validation set . Once the best learner has been chosen, a third “test” set can be used to assess the\n",
      "predictive performance of the best learner. The training, validation, and test sets can again\n",
      "be obtained from the overall data set via a random allocation. When the overall data set\n",
      "is of modest size, it is customary to perform the validation phase (model selection) on the\n",
      "training set only, using cross-validation. This is the topic of Section 2.5.2. + 3726 2.3. Training and Test Loss\n",
      "\u0001\u0002\u0003\u0004\u0005\u0004\u0005\u0006\n",
      "\u0001\t\n",
      "\u0003\u0005\u0003\t\u000e\u0002\u000f \u0010\f\u000e\u0007 \n",
      "\u0003\u0005\u0003\t\u000e\u0002\u000f \u0010\f\u000e\u0007\n",
      "\u0001\u0002\u0003\u0004\u0005\u0004\u0005\u0006\n",
      "\u0004\u0012\u0003\t\u0004\u000e\u0005\n",
      "\u0001\t\n",
      "Figure 2.3: Statistical learning algorithms often require the data to be divided into training\n",
      "and test data. If the latter is used for model selection, a third set is needed for testing the\n",
      "performance of the selected model.\n",
      "We next consider a concrete example that illustrates the concepts introduced so far.\n",
      "Example 2.1 (Polynomial Regression) In what follows, it will appear that we have ar-\n",
      "bitrarily replaced the symbols x;g;Gwith u;h;H, respectively. The reason for this switch\n",
      "of notation will become clear at the end of the example.\n",
      "The data (depicted as dots) in Figure 2.4 are n=100 points ( ui;yi);i=1;:::; ndrawn\n",
      "from iid random points ( Ui;Yi);i=1;:::; n, where thefUigare uniformly distributed on\n",
      "the interval (0 ;1) and, given Ui=ui, the random variable Yihas a normal distribution with\n",
      "expectation 10\u0000140ui+400u2\n",
      "i\u0000250u3\n",
      "iand variance `\u0003=25. This is an example of a\n",
      "polynomial regression model polynomial\n",
      "regression\n",
      "model. Using a squared-error loss, the optimal prediction function\n",
      "h\u0003(u)=E[YjU=u] is thus\n",
      "h\u0003(u)=10\u0000140u+400u2\u0000250u3;\n",
      "which is depicted by the dashed curve in Figure 2.4.\n",
      "0.0\n",
      " 0.2\n",
      " 0.4\n",
      " 0.6\n",
      " 0.8\n",
      " 1.0\n",
      "u\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40h*(u)\n",
      "data points\n",
      "true\n",
      "Figure 2.4: Training data and the optimal polynomial prediction function h\u0003.Chapter 2. Statistical Learning 27\n",
      "To obtain a good estimate of h\u0003(u) based on the training set \u001c=f(ui;yi);i=1;:::; ng,\n",
      "we minimize the outcome of the training loss (2.3):\n",
      "`\u001c(h)=1\n",
      "nnX\n",
      "i=1(yi\u0000h(ui))2; (2.8)\n",
      "over a suitable set Hof candidate functions. Let us take the set Hpof polynomial functions\n",
      "inuof order p\u00001:\n",
      "h(u) :=\f1+\f2u+\f3u2+\u0001\u0001\u0001+\fpup\u00001(2.9)\n",
      "forp=1;2;:::and parameter vector \f=[\f1;\f2;:::;\f p]>. This function class contains the\n",
      "best possible h\u0003(u)=E[YjU=u] for p>4. Note that optimization over Hpis a parametric\n",
      "optimization problem, in that we need to ﬁnd the best \f. Optimization of (2.8) over Hpis\n",
      "not straightforward, unless we notice that (2.9) is a linear function in\f. In particular, if we\n",
      "map each feature uto a feature vector x=[1;u;u2;:::; up\u00001]>, then the right-hand side of\n",
      "(2.9) can be written as the function\n",
      "g(x)=x>\f;\n",
      "which is linear in x(as well as\f). The optimal h\u0003(u) inHpforp>4 then corresponds\n",
      "to the function g\u0003(x)=x>\f\u0003in the setGpof linear functions from RptoR, where\f\u0003=\n",
      "[10;\u0000140;400;\u0000250;0;:::; 0]>. Thus, instead of working with the set Hpof polynomial\n",
      "functions we may prefer to work with the set Gpof linear functions. This brings us to a\n",
      "very important idea in statistical learning:\n",
      "Expand the feature space to obtain a linear prediction function.\n",
      "Let us now reformulate the learning problem in terms of the new explanatory (feature)\n",
      "variables xi=[1;ui;u2\n",
      "i;:::; up\u00001\n",
      "i]>,i=1;:::; n. It will be convenient to arrange these\n",
      "feature vectors into a matrix Xwith rows x>\n",
      "1;:::; x>\n",
      "n:\n",
      "X=2666666666666666641u1u2\n",
      "1\u0001\u0001\u0001 up\u00001\n",
      "1\n",
      "1u2u2\n",
      "2\u0001\u0001\u0001 up\u00001\n",
      "2:::::::::::::::\n",
      "1unu2\n",
      "n\u0001\u0001\u0001 up\u00001\n",
      "n377777777777777775: (2.10)\n",
      "Collecting the responses fyiginto a column vector y, the training loss (2.3) can now be\n",
      "written compactly as\n",
      "1\n",
      "nky\u0000X\fk2: (2.11)\n",
      "To ﬁnd the optimal learner (2.4) in the class Gpwe need to ﬁnd the minimizer of (2.11):\n",
      "b\f=argmin\n",
      "\fky\u0000X\fk2; (2.12)\n",
      "which is called the ordinary least-squares ordinary\n",
      "least -squaressolution. As is illustrated in Figure 2.5, to ﬁnd b\f,\n",
      "we choose Xb\fto be equal to the orthogonal projection of yonto the linear space spanned\n",
      "by the columns of the matrix X; that is, Xb\f=Py, where Pis the projection matrix projection\n",
      "matrix.28 2.3. Training and Test Loss\n",
      "Span( X)XβX/hatwideβy\n",
      "Figure 2.5: Xb\fis the orthogonal projection of yonto the linear space spanned by the\n",
      "columns of the matrix X.\n",
      "According to Theorem A.4, the projection matrix is given by +364\n",
      "P=X X+; (2.13)\n",
      "where the p\u0002nmatrix X+in (2.13) is the pseudo-inverse ofX. IfXhappens to be of full +362\n",
      "pseudo -inverse column rank (so that none of the columns can be expressed as a linear combination of the\n",
      "+358 other columns), then X+=(X>X)\u00001X>.\n",
      "In any case, from Xb\f=PyandPX=X, we can see that b\fsatisﬁes the normal\n",
      "equations normal\n",
      "equations:\n",
      "X>X\f=X>Py=(PX)>y=X>y: (2.14)\n",
      "This is a set of linear equations, which can be solved very fast and whose solution can be\n",
      "written explicitly as:\n",
      "b\f=X+y: (2.15)\n",
      "Figure 2.6 shows the trained learners for various values of p:\n",
      "hHp\n",
      "\u001c(u)=gGp\n",
      "\u001c(x)=x>b\f\n",
      "0.0\n",
      " 0.2\n",
      " 0.4\n",
      " 0.6\n",
      " 0.8\n",
      " 1.0\n",
      "u\n",
      "10\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40hp(u)\n",
      "data points\n",
      "true\n",
      "p= 2, underfit\n",
      "p= 4, correct\n",
      "p= 16, overfit\n",
      "Figure 2.6: Training data with ﬁtted curves for p=2;4, and 16. The true cubic polynomial\n",
      "curve for p=4 is also plotted (dashed line).Chapter 2. Statistical Learning 29\n",
      "We see that for p=16 the ﬁtted curve lies closer to the data points, but is further away\n",
      "from the dashed true polynomial curve, indicating that we overﬁt. The choice p=4 (the\n",
      "true cubic polynomial) is much better than p=16, or indeed p=2 (straight line).\n",
      "Each function class Gpgives a di \u000berent learner gGp\n",
      "\u001c,p=1;2;:::. To assess which is\n",
      "better, we should not simply take the one that gives the smallest training loss. We can\n",
      "always get a zero training loss by taking p=n, because for any set of npoints there exists\n",
      "a polynomial of degree n\u00001 that interpolates all points!\n",
      "Instead, we assess the predictive performance of the learners using the test loss (2.7),\n",
      "computed from a test data set. If we collect all n0test feature vectors in a matrix X0and\n",
      "the corresponding test responses in a vector y0, then, similar to (2.11), the test loss can be\n",
      "written compactly as\n",
      "`\u001c0(gGp\n",
      "\u001c)=1\n",
      "n0ky0\u0000X0b\fk2;\n",
      "whereb\fis given by (2.15), using the training data.\n",
      "Figure 2.7 shows a plot of the test loss against the number of parameters in the vector\n",
      "\f; that is, p. The graph has a characteristic “bath-tub” shape and is at its lowest for p=4,\n",
      "correctly identifying the polynomial order 3 for the true model. Note that the test loss, as\n",
      "an estimate for the generalization risk (2.7), becomes numerically unreliable after p=16\n",
      "(the graph goes down, where it should go up). The reader may check that the graph for\n",
      "the training loss exhibits a similar numerical instability for large p, and in fact fails to\n",
      "numerically decrease to 0 for large p, contrary to what it should do in theory. The numerical\n",
      "problems arise from the fact that for large pthe columns of the (Vandermonde) matrix X\n",
      "are of vastly di \u000berent magnitudes and so ﬂoating point errors quickly become very large.\n",
      "Finally, observe that the lower bound for the test loss is here around 21, which corres-\n",
      "ponds to an estimate of the minimal (squared-error) risk `\u0003=25.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "Number of parameters p\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160Test loss\n",
      "Figure 2.7: Test loss as function of the number of parameters pof the model.\n",
      "This script shows how the training data were generated and plotted in Python:30 2.3. Training and Test Loss\n",
      "polyreg1.py\n",
      "import numpy as np\n",
      "from numpy.random import rand , randn\n",
      "from numpy.linalg import norm , solve\n",
      "import matplotlib.pyplot as plt\n",
      "def generate_data(beta , sig, n):\n",
      "u = np.random.rand(n, 1)\n",
      "y = (u ** np.arange(0, 4)) @ beta + sig * np.random.randn(n, 1)\n",
      "return u, y\n",
      "np.random.seed(12)\n",
      "beta = np.array([[10, -140, 400, -250]]).T\n",
      "n = 100\n",
      "sig = 5\n",
      "u, y = generate_data(beta , sig, n)\n",
      "xx = np.arange(np.min(u), np.max(u)+5e-3, 5e-3)\n",
      "yy = np.polyval(np.flip(beta), xx)\n",
      "plt.plot(u, y, '.', markersize=8)\n",
      "plt.plot(xx, yy, '--',linewidth=3)\n",
      "plt.xlabel(r '$u$')\n",
      "plt.ylabel(r '$h^*(u)$ ')\n",
      "plt.legend([ 'data points ','true '])\n",
      "plt.show()\n",
      "The following code, which imports the code above, ﬁts polynomial models with p=\n",
      "1;:::; K=18 parameters to the training data and plots a selection of ﬁtted curves, as\n",
      "shown in Figure 2.6.\n",
      "polyreg2.py\n",
      "from polyreg1 import *\n",
      "max_p = 18\n",
      "p_range = np.arange(1, max_p + 1, 1)\n",
      "X = np.ones((n, 1))\n",
      "betahat , trainloss = {}, {}\n",
      "for p in p_range: # p is the number of parameters\n",
      "if p > 1:\n",
      "X = np.hstack((X, u**(p-1))) # add column to matrix\n",
      "betahat[p] = solve(X.T @ X, X.T @ y)\n",
      "trainloss[p] = (norm(y - X @ betahat[p])**2/n)\n",
      "p = [2, 4, 16] # select three curves\n",
      "#replot the points and true line and store in the list \"plots\"\n",
      "plots = [plt.plot(u, y, 'k.', markersize=8)[0],\n",
      "plt.plot(xx, yy, 'k--',linewidth=3)[0]]\n",
      "# add the three curves\n",
      "for i in p:\n",
      "yy = np.polyval(np.flip(betahat[i]), xx)\n",
      "plots.append(plt.plot(xx, yy)[0])Chapter 2. Statistical Learning 31\n",
      "plt.xlabel(r '$u$')\n",
      "plt.ylabel(r '$h^{\\mathcal{H}_p}_{\\tau}(u)$ ')\n",
      "plt.legend(plots ,( 'data points ','true ','$p=2$, underfit ',\n",
      "'$p=4$, correct ','$p=16$, overfit '))\n",
      "plt.savefig( 'polyfitpy.pdf ',format= 'pdf')\n",
      "plt.show()\n",
      "The last code snippet which imports the previous code, generates the test data and plots the\n",
      "graph of the test loss, as shown in Figure 2.7.\n",
      "polyreg3.py\n",
      "from polyreg2 import *\n",
      "# generate test data\n",
      "u_test , y_test = generate_data(beta , sig, n)\n",
      "MSE = []\n",
      "X_test = np.ones((n, 1))\n",
      "for p in p_range:\n",
      "if p > 1:\n",
      "X_test = np.hstack((X_test , u_test**(p-1)))\n",
      "y_hat = X_test @ betahat[p] # predictions\n",
      "MSE.append(np.sum((y_test - y_hat)**2/n))\n",
      "plt.plot(p_range , MSE, 'b', p_range , MSE, 'bo')\n",
      "plt.xticks(ticks=p_range)\n",
      "plt.xlabel( 'Number of parameters $p$ ')\n",
      "plt.ylabel( 'Test loss ')\n",
      "2.4 Tradeoffs in Statistical Learning\n",
      "The art of machine learning in the supervised case is to make the generalization risk (2.5)\n",
      "or expected generalization risk (2.6) as small as possible, while using as few computational\n",
      "resources as possible. In pursuing this goal, a suitable class Gof prediction functions has\n",
      "to be chosen. This choice is driven by various factors, such as\n",
      "the complexity of the class (e.g., is it rich enough to adequately approximate, or even\n",
      "contain, the optimal prediction function g\u0003?),\n",
      "the ease of training the learner via the optimization program (2.4),\n",
      "how accurately the training loss (2.3) estimates the risk (2.1) within class G,\n",
      "the feature types (categorical, continuous, etc.).\n",
      "As a result, the choice of a suitable function class Gusually involves a tradeo \u000bbetween\n",
      "conﬂicting factors. For example, a learner from a simple class Gcan be trained very32 2.4. Tradeoffs in Statistical Learning\n",
      "quickly, but may not approximate g\u0003very well, whereas a learner from a rich class G\n",
      "that contains g\u0003may require a lot of computing resources to train.\n",
      "To better understand the relation between model complexity, computational simplicity,\n",
      "and estimation accuracy, it is useful to decompose the generalization risk into several parts,\n",
      "so that the tradeo \u000bs between these parts can be studied. We will consider two such decom-\n",
      "positions: the approximation–estimation tradeo \u000band the bias–variance tradeo \u000b.\n",
      "We can decompose the generalization risk (2.5) into the following three components:\n",
      "`(gG\n",
      "\u001c)=`\u0003|{z}\n",
      "irreducible risk+`(gG)\u0000`\u0003\n",
      "|     {z     }\n",
      "approximation error+`(gG\n",
      "\u001c)\u0000`(gG)|          {z          }\n",
      "statistical error; (2.16)\n",
      "where`\u0003:=`(g\u0003) is the irreducible risk irreducible risk andgG:=argming2G`(g) is the best learner within\n",
      "classG. No learner can predict a new response with a smaller risk than `\u0003.\n",
      "The second component is the approximation error approximation\n",
      "error; it measures the di \u000berence between\n",
      "the irreducible risk and the best possible risk that can be obtained by selecting the best\n",
      "prediction function in the selected class of functions G. Determining a suitable class Gand\n",
      "minimizing `(g) over this class is purely a problem of numerical and functional analysis,\n",
      "as the training data \u001care not present. For a ﬁxed Gthat does not contain the optimal g\u0003, the\n",
      "approximation error cannot be made arbitrarily small and may be the dominant component\n",
      "in the generalization risk. The only way to reduce the approximation error is by expanding\n",
      "the classGto include a larger set of possible functions.\n",
      "The third component is the statistical (estimation) error statistical\n",
      "(estimation )\n",
      "error. It depends on the training\n",
      "set\u001cand, in particular, on how well the learner gG\n",
      "\u001cestimates the best possible prediction\n",
      "function, gG, within classG. For any sensible estimator this error should decay to zero (in\n",
      "probability or expectation) as the training size tends to inﬁnity. +441\n",
      "The approximation–estimation tradeo \u000b approximation –\n",
      "estimation\n",
      "tradeoffpits two competing demands against each\n",
      "other. The ﬁrst is that the class Ghas to be simple enough so that the statistical error is\n",
      "not too large. The second is that the class Ghas to be rich enough to ensure a small approx-\n",
      "imation error. Thus, there is a tradeo \u000bbetween the approximation and estimation errors.\n",
      "For the special case of the squared-error loss, the generalization risk is equal to `(gG\n",
      "\u001c)=\n",
      "E(Y\u0000gG\n",
      "\u001c(X))2; that is, the expected squared error1between the predicted value gG\n",
      "\u001c(X)\n",
      "and the response Y. Recall that in this case the optimal prediction function is given by\n",
      "g\u0003(x)=E[YjX=x]. The decomposition (2.16) can now be interpreted as follows.\n",
      "1. The ﬁrst component, `\u0003=E(Y\u0000g\u0003(X))2, is the irreducible error , as no prediction\n",
      "function will yield a smaller expected squared error.\n",
      "2. The second component, the approximation error `(gG)\u0000`(g\u0003), is equal to E(gG(X)\u0000\n",
      "g\u0003(X))2. We leave the proof (which is similar to that of Theorem 2.1) as an exercise;\n",
      "see Exercise 2. Thus, the approximation error (deﬁned as a risk di \u000berence) can here\n",
      "be interpreted as the expected squared error between the optimal predicted value and\n",
      "the optimal predicted value within the class G.\n",
      "3. For the third component, the statistical error, `(gG\n",
      "\u001c)\u0000`(gG) there is no direct inter-\n",
      "pretation as an expected squared error unlessGis the class of linear functions; that\n",
      "is,g(x)=x>\ffor some vector \f. In this case we can write (see Exercise 3) the\n",
      "statistical error as `(gG\n",
      "\u001c)\u0000`(gG)=E(gG\n",
      "\u001c(X)\u0000gG(X))2.\n",
      "1Colloquially called mean squared error .Chapter 2. Statistical Learning 33\n",
      "Thus, when using a squared-error loss, the generalization risk for a linear class Gcan\n",
      "be decomposed as:\n",
      "`(gG\n",
      "\u001c)=E(gG\n",
      "\u001c(X)\u0000Y)2=`\u0003+E(gG(X)\u0000g\u0003(X))2\n",
      "|                 {z                 }\n",
      "approximation error+E(gG\n",
      "\u001c(X)\u0000gG(X))2\n",
      "|                  {z                  }\n",
      "statistical error: (2.17)\n",
      "Note that in this decomposition the statistical error is the only term that depends on the\n",
      "training set.\n",
      "Example 2.2 (Polynomial Regression (cont.)) We continue Example 2.1. Here G=\n",
      "Gpis the class of linear functions of x=[1;u;u2;:::; up\u00001]>, and g\u0003(x)=x>\f\u0003. Condi-\n",
      "tional on X=xwe have that Y=g\u0003(x)+\"(x), with\"(x)\u0018N(0;`\u0003), where`\u0003=E(Y\u0000\n",
      "g\u0003(X))2=25 is the irreducible error. We wish to understand how the approximation and\n",
      "statistical errors behave as we change the complexity parameter p.\n",
      "First, we consider the approximation error. Any function g2G pcan be written as\n",
      "g(x)=h(u)=\f1+\f2u+\u0001\u0001\u0001+\fpup\u00001=[1;u;:::; up\u00001]\f;\n",
      "and so g(X) is distributed as [1 ;U;:::; Up\u00001]\f, where U\u0018U(0;1). Similarly, g\u0003(X) is\n",
      "distributed as [1 ;U;U2;U3]\f\u0003. It follows that an expression for the approximation error\n",
      "is:R1\n",
      "0\u0010\n",
      "[1;u;:::; up\u00001]\f\u0000[1;u;u2;u3]\f\u0003\u00112du:To minimize this error, we set the gradient\n",
      "with respect to \fto zero and obtain the plinear equations +399\n",
      "R1\n",
      "0\u0010\n",
      "[1;u;:::; up\u00001]\f\u0000[1;u;u2;u3]\f\u0003\u0011\n",
      "du=0;\n",
      "R1\n",
      "0\u0010\n",
      "[1;u;:::; up\u00001]\f\u0000[1;u;u2;u3]\f\u0003\u0011\n",
      "udu=0;\n",
      ":::\n",
      "R1\n",
      "0\u0010\n",
      "[1;u;:::; up\u00001]\f\u0000[1;u;u2;u3]\f\u0003\u0011\n",
      "up\u00001du=0:\n",
      "Let\n",
      "Hp=Z1\n",
      "0[1;u;:::; up\u00001]>[1;u;:::; up\u00001] du\n",
      "be the p\u0002p Hilbert matrix Hilbert matrix , which has ( i;j)-th entry given byR1\n",
      "0ui+j\u00002du=1=(i+j\u00001).\n",
      "Then, the above system of linear equations can be written as Hp\f=eH\f\u0003, where eHis the\n",
      "p\u00024 upper left sub-block of Hepandep=maxfp;4g. The solution, which we denote by \fp,\n",
      "is:\n",
      "\fp=8>>>>>>>><>>>>>>>>:65\n",
      "6; p=1;\n",
      "[\u000020\n",
      "3;35]>; p=2;\n",
      "[\u00005\n",
      "2;10;25]>; p=3;\n",
      "[10;\u0000140;400;\u0000250;0;:::; 0]>;p>4:(2.18)\n",
      "Hence, the approximation error E\u0010\n",
      "gGp(X)\u0000g\u0003(X)\u00112is given by\n",
      "Z1\n",
      "0\u0010\n",
      "[1;u;:::; up\u00001]\fp\u0000[1;u;u2;u3]\f\u0003\u00112du=8>>>>>>>><>>>>>>>>:32225\n",
      "252\u0019127:9;p=1;\n",
      "1625\n",
      "63\u001925:8; p=2;\n",
      "625\n",
      "28\u001922:3; p=3;\n",
      "0; p>4:(2.19)34 2.4. Tradeoffs in Statistical Learning\n",
      "Notice how the approximation error becomes smaller as pincreases. In this particular\n",
      "example the approximation error is in fact zero for p>4. In general, as the class of ap-\n",
      "proximating functions Gbecomes more complex, the approximation error goes down.\n",
      "Next, we illustrate the typical behavior of the statistical error. Since g\u001c(x)=x>b\f, the\n",
      "statistical error can be written as\n",
      "Z1\n",
      "0\u0010\n",
      "[1;:::; up\u00001](b\f\u0000\fp)\u00112du=(b\f\u0000\fp)>Hp(b\f\u0000\fp): (2.20)\n",
      "Figure 2.8 illustrates the decomposition (2.17) of the generalization risk for the same train-\n",
      "ing set that was used to compute the test loss in Figure 2.7. Recall that test loss gives an\n",
      "estimate of the generalization risk, using independent test data. Comparing the two ﬁgures,\n",
      "we see that in this case the two match closely. The global minimum of the statistical error is\n",
      "approximately 0 :28, with minimizer p=4. Since the approximation error is monotonically\n",
      "decreasing to zero, p=4 is also the global minimizer of the generalization risk.\n",
      "0 2 4 6 8 10 12 14 16 18050100150approximation error\n",
      "statistical error\n",
      "irreducible error\n",
      "generalization risk\n",
      "Figure 2.8: The generalization risk for a particular training set is the sum of the irreducible\n",
      "error, the approximation error, and the statistical error. The approximation error decreases\n",
      "to zero as pincreases, whereas the statistical error has a tendency to increase after p=4.\n",
      "Note that the statistical error depends on the estimate b\f, which in its turn depends on\n",
      "the training set \u001c. We can obtain a better understanding of the statistical error by consid-\n",
      "ering its expected behavior; that is, averaged over many training sets. This is explored in\n",
      "Exercise 11.\n",
      "Using again a squared-error loss, a second decomposition (for general G) starts from\n",
      "`(gG\n",
      "\u001c)=`\u0003+`(gG\n",
      "\u001c)\u0000`(g\u0003);\n",
      "where the statistical error and approximation error are combined. Using similar reasoning\n",
      "as in the proof of Theorem 2.1, we have\n",
      "`(gG\n",
      "\u001c)=E(gG\n",
      "\u001c(X)\u0000Y)2=`\u0003+E\u0010\n",
      "gG\n",
      "\u001c(X)\u0000g\u0003(X)\u00112=`\u0003+ED2(X;\u001c);Chapter 2. Statistical Learning 35\n",
      "where D(x;\u001c) :=gG\n",
      "\u001c(x)\u0000g\u0003(x). Now consider the random variable D(x;T) for a random\n",
      "training setT. The expectation of its square is:\n",
      "E\u0010\n",
      "gG\n",
      "T(x)\u0000g\u0003(x)\u00112=ED2(x;T)=(ED(x;T))2+VarD(x;T)\n",
      "=(EgG\n",
      "T(x)\u0000g\u0003(x))2\n",
      "|                 {z                 }\n",
      "pointwise squared bias+VargG\n",
      "T(x)|     {z     }\n",
      "pointwise variance: (2.21)\n",
      "If we view the learner gG\n",
      "T(x) as a function of a random training set, then the pointwise\n",
      "squared bias pointwise\n",
      "squared biasterm is a measure for how close gG\n",
      "T(x) is on average to the true g\u0003(x),\n",
      "whereas the pointwise variance term measures the deviation of gG\n",
      "T(x) from its expectedpointwise\n",
      "variance valueEgG\n",
      "T(x). The squared bias can be reduced by making the class of functions Gmore\n",
      "complex. However, decreasing the bias by increasing the complexity often leads to an in-\n",
      "crease in the variance term. We are thus seeking learners that provide an optimal balance\n",
      "between the bias and variance, as expressed via a minimal generalization risk. This is called\n",
      "thebias–variance tradeo \u000b bias–variance\n",
      "tradeoff.\n",
      "Note that the expected generalization risk (2.6) can be written as `\u0003+ED2(X;T), where\n",
      "XandTare independent. It therefore decomposes as\n",
      "E`(gG\n",
      "T)=`\u0003+E(E[gG\n",
      "T(X)jX]\u0000g\u0003(X))2\n",
      "|                           {z                           }\n",
      "expected squared bias+E[Var[gG\n",
      "T(X)jX]]|                 {z                 }\n",
      "expected variance: (2.22)\n",
      "2.5 Estimating Risk\n",
      "The most straightforward way to quantify the generalization risk (2.5) is to estimate it via\n",
      "the test loss (2.7). However, the generalization risk depends inherently on the training set,\n",
      "and so di \u000berent training sets may yield signiﬁcantly di \u000berent estimates. Moreover, when\n",
      "there is a limited amount of data available, reserving a substantial proportion of the data\n",
      "for testing rather than training may be uneconomical. In this section we consider di \u000berent\n",
      "methods for estimating risk measures which aim to circumvent these di \u000eculties.\n",
      "2.5.1 In-Sample Risk\n",
      "We mentioned that, due to the phenomenon of overﬁtting, the training loss of the learner,\n",
      "`\u001c(g\u001c) (for simplicity, here we omit Gfrom gG\n",
      "\u001c), is not a good estimate of the generalization\n",
      "risk`(g\u001c) of the learner. One reason for this is that we use the same data for both training\n",
      "the model and assessing its risk. How should we then estimate the generalization risk or\n",
      "expected generalization risk?\n",
      "To simplify the analysis, suppose that we wish to estimate the average accuracy of the\n",
      "predictions of the learner g\u001cat the nfeature vectors x1;:::; xn(these are part of the training\n",
      "set\u001c). In other words, we wish to estimate the in-sample risk in-sample risk of the learner g\u001c:\n",
      "`in(g\u001c)=1\n",
      "nnX\n",
      "i=1ELoss( Y0\n",
      "i;g\u001c(xi)); (2.23)\n",
      "where each response Y0\n",
      "iis drawn from f(yjxi), independently. Even in this simpliﬁed set-\n",
      "ting, the training loss of the learner will be a poor estimate of the in-sample risk. Instead, the36 2.5. Estimating Risk\n",
      "proper way to assess the prediction accuracy of the learner at the feature vectors x1;:::; xn,\n",
      "is to draw new response values Y0\n",
      "i\u0018f(yjxi);i=1;:::; n, that are independent from the\n",
      "responses y1;:::; ynin the training data, and then estimate the in-sample risk of g\u001cvia\n",
      "1\n",
      "nnX\n",
      "i=1Loss( Y0\n",
      "i;g\u001c(xi)):\n",
      "For a ﬁxed training set \u001c, we can compare the training loss of the learner with the\n",
      "in-sample risk. Their di \u000berence,\n",
      "op\u001c=`in(g\u001c)\u0000`\u001c(g\u001c);\n",
      "is called the optimism (of the training loss), because it measures how much the training\n",
      "loss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is\n",
      "simpler to work with the expected optimism expected\n",
      "optimism:\n",
      "E[opTjX1=x1;:::; Xn=xn]=:EXopT;\n",
      "where the expectation is taken over a random training set T, conditional on Xi=xi;\n",
      "i=1;:::; n. For ease of notation, we have abbreviated the expected optimism to EXopT,\n",
      "whereEXdenotes the expectation operator conditional on Xi=xi;i=1;:::; n. As in Ex-\n",
      "ample 2.1, the feature vectors are stored as the rows of an n\u0002pmatrix X. It turns out that the\n",
      "expected optimism for various loss functions can be expressed in terms of the (conditional)\n",
      "covariance between the observed and predicted response.\n",
      "Theorem 2.2: Expected Optimism\n",
      "For the squared-error loss and 0–1 loss with 0–1 response, the expected optimism is\n",
      "EXopT=2\n",
      "nnX\n",
      "i=1CovX(gT(xi);Yi): (2.24)\n",
      "Proof: In what follows, all expectations are taken conditional on X1=x1;:::; Xn=xn.\n",
      "LetYibe the response for xiand let bYi=gT(xi) be the predicted value. Note that the latter\n",
      "depends on Y1;:::; Yn. Also, let Y0\n",
      "ibe an independent copy of Yifor the same xi, as in\n",
      "(2.23). In particular, Y0\n",
      "ihas the same distribution as Yiand is statistically independent of\n",
      "allfYjg, including Yi, and therefore is also independent of bYi. We have\n",
      "EXopT=1\n",
      "nnX\n",
      "i=1EXh\n",
      "(Y0\n",
      "i\u0000bYi)2\u0000(Yi\u0000bYi)2i\n",
      "=2\n",
      "nnX\n",
      "i=1EXh\n",
      "(Yi\u0000Y0\n",
      "i)bYii\n",
      "=2\n",
      "nnX\n",
      "i=1\u0010\n",
      "EX[YibYi]\u0000EXYiEXbYi\u0011\n",
      "=2\n",
      "nnX\n",
      "i=1CovX(bYi;Yi):\n",
      "The proof for the 0–1 loss with 0–1 response is left as Exercise 4. \u0003\n",
      "In summary, the expected optimism indicates how much, on average, the training loss\n",
      "deviates from the expected in-sample risk. Since the covariance of independent random\n",
      "variables is zero, the expected optimism is zero if the learner gTis statistically independent\n",
      "from the responses Y1;:::; Yn.Chapter 2. Statistical Learning 37\n",
      "Example 2.3 (Polynomial Regression (cont.)) We continue Example 2.2, where the\n",
      "components of the response vector Y=[Y1;:::; Yn]>are independent and normally distrib-\n",
      "uted with variance `\u0003=25 (the irreducible error) and expectations EXYi=g\u0003(xi)=x>\n",
      "i\f\u0003,\n",
      "i=1;:::; n. Using the formula (2.15) for the least-squares estimator b\f, the expected op-\n",
      "timism (2.24) is\n",
      "2\n",
      "nnX\n",
      "i=1CovX\u0010\n",
      "x>\n",
      "ib\f;Yi\u0011\n",
      "=2\n",
      "ntr\u0010\n",
      "CovX\u0010\n",
      "Xb\f;Y\u0011\u0011\n",
      "=2\n",
      "ntr\u0000CovX\u0000XX+Y;Y\u0001\u0001\n",
      "=2tr(XX+CovX(Y;Y))\n",
      "n=2`\u0003tr(XX+)\n",
      "n=2`\u0003p\n",
      "n:\n",
      "In the last equation we used the cyclic property of the trace (Theorem A.1): tr( XX+)= +359\n",
      "tr(X+X)=tr(Ip), assuming that rank( X)=p. Therefore, an estimate for the in-sample risk\n",
      "(2.23) is:\n",
      "b`in(g\u001c)=`\u001c(g\u001c)+2`\u0003p=n; (2.25)\n",
      "where we have assumed that the irreducible risk `\u0003is known. Figure 2.9 shows that this\n",
      "estimate is very close to the test loss from Figure 2.7. Hence, instead of computing the test\n",
      "loss to assess the best model complexity p, we could simply have minimized the training\n",
      "loss plus the correction term 2 `\u0003p=n. In practice, `\u0003also has to be estimated somehow.\n",
      "2 4 6 8 10 12 14 16 18050100150\n",
      "Figure 2.9: In-sample risk estimate b`in(g\u001c) as a function of the number of parameters pof\n",
      "the model. The test loss is superimposed as a blue dashed curve.\n",
      "2.5.2 Cross-Validation\n",
      "In general, for complex function classes G, it is very di \u000ecult to derive simple formulas of\n",
      "the approximation and statistical errors, let alone for the generalization risk or expected\n",
      "generalization risk. As we saw, when there is an abundance of data, the easiest way to\n",
      "assess the generalization risk for a given training set \u001cis to obtain a test set \u001c0and evaluate\n",
      "the test loss (2.7). When a su \u000eciently large test set is not available but computational + 24\n",
      "resources are cheap, one can instead gain direct knowledge of the expected generalization\n",
      "risk via a computationally intensive method called cross-validation cross -validation .38 2.5. Estimating Risk\n",
      "The idea is to make multiple identical copies of the data set, and to partition each copy\n",
      "into di \u000berent training and test sets, as illustrated in Figure 2.10. Here, there are four copies\n",
      "of the data set (consisting of response and explanatory variables). Each copy is divided into\n",
      "a test set (colored blue) and training set (colored pink). For each of these sets, we estimate\n",
      "the model parameters using only training data and then predict the responses for the test\n",
      "set. The average loss between the predicted and observed responses is then a measure for\n",
      "the predictive power of the model.\n",
      "\u0001\u0002\u0003\u0004\u0005\u0006\u0003\u0005\n",
      "\t\n",
      "\n",
      "\u0001\u0002\u0003\u0004\u0005\u0006\u0003\u0005 \u0001\u0002\u0003\u0004\u0005\u0006\u0003\u0005 \u0001\u0002\u0003\u0004\u0005\u0006\u0003\u0005\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 2.10: An illustration of four-fold cross-validation, representing four copies of the\n",
      "same data set. The data in each copy is partitioned into a training set (pink) and a test\n",
      "set (blue). The darker columns represent the response variable and the lighter ones the\n",
      "explanatory variables.\n",
      "In particular, suppose we partition a data set Tof size nintoK folds folds C1;:::;CKof sizes\n",
      "n1;:::; nK(hence, n1+\u0001\u0001\u0001+nK=n). Typically nk\u0019n=K,k=1;:::; K.\n",
      "Let`Ckbe the test loss when using Ckas test data and all remaining data, denoted T\u0000k,\n",
      "as training data. Each `Ckis an unbiased estimator of the generalization risk for training set\n",
      "T\u0000k; that is, for `(gT\u0000k).\n",
      "TheK-fold cross-validation K-fold\n",
      "cross -validationloss is the weighted average of these risk estimators:\n",
      "CV K=KX\n",
      "k=1nk\n",
      "n`Ck(gT\u0000k)\n",
      "=1\n",
      "nKX\n",
      "k=1X\n",
      "i2CkLoss( gT\u0000k(xi);yi)\n",
      "=1\n",
      "nnX\n",
      "i=1Loss( gT\u0000\u0014(i)(xi);yi);\n",
      "where the function \u0014:f1;:::; ng 7! f 1;:::; Kgindicates to which of the Kfolds each\n",
      "of the nobservations belongs. As the average is taken over varying training sets fT\u0000kg, it\n",
      "estimates the expected generalization risk E`(gT), rather than the generalization risk `(g\u001c)\n",
      "for the particular training set \u001c.\n",
      "Example 2.4 (Polynomial Regression (cont.)) For the polynomial regression ex-\n",
      "ample, we can calculate a K-fold cross-validation loss with a nonrandom partitioning of the\n",
      "training set using the following code, which imports the previous code for the polynomial\n",
      "regression example. We omit the full plotting code.Chapter 2. Statistical Learning 39\n",
      "polyregCV.py\n",
      "from polyreg3 import *\n",
      "K_vals = [5, 10, 100] # number of folds\n",
      "cv = np.zeros((len(K_vals), max_p)) # cv loss\n",
      "X = np.ones((n, 1))\n",
      "for p in p_range:\n",
      "if p > 1:\n",
      "X = np.hstack((X, u**(p-1)))\n",
      "j = 0\n",
      "for K in K_vals:\n",
      "loss = []\n",
      "for k in range(1, K+1):\n",
      "# integer indices of test samples\n",
      "test_ind = ((n/K)*(k-1) + np.arange(1,n/K+1) -1).astype( 'int')\n",
      "train_ind = np.setdiff1d(np.arange(n), test_ind)\n",
      "X_train , y_train = X[train_ind , :], y[train_ind , :]\n",
      "X_test , y_test = X[test_ind , :], y[test_ind]\n",
      "# fit model and evaluate test loss\n",
      "betahat = solve(X_train.T @ X_train , X_train.T @ y_train)\n",
      "loss.append(norm(y_test - X_test @ betahat) ** 2)\n",
      "cv[j, p-1] = sum(loss)/n\n",
      "j += 1\n",
      "# basic plotting\n",
      "plt.plot(p_range , cv[0, :], 'k-.')\n",
      "plt.plot(p_range , cv[1, :], 'r')\n",
      "plt.plot(p_range , cv[2, :], 'b--')\n",
      "plt.show()\n",
      "2\n",
      " 4\n",
      " 6\n",
      " 8\n",
      " 10\n",
      " 12\n",
      " 14\n",
      " 16\n",
      " 18\n",
      "Number of parameters p\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300K-fold cross-validation loss\n",
      "K=5\n",
      "K=10\n",
      "K=100\n",
      "Figure 2.11: K-fold cross-validation for the polynomial regression example.40 2.6. Modeling Data\n",
      "Figure 2.11 shows the cross-validation loss for K2f5;10;100g. The case K=100 cor-\n",
      "responds to the leave-one-out cross-validation , which can be computed more e \u000ecientlyleave -one-out\n",
      "cross -validation using the formula in Theorem 5.1.\n",
      "+174\n",
      "2.6 Modeling Data\n",
      "The ﬁrst step in any data analysis is to model model the data in one form or another. For example,\n",
      "in an unsupervised learning setting with data represented by a vector x=[x1;:::; xp]>, a\n",
      "very general model is to assume that xis the outcome of a random vector X=[X1;:::; Xp]>\n",
      "with some unknown pdf f. The model can then be reﬁned by assuming a speciﬁc form of\n",
      "f.\n",
      "When given a sequence of such data vectors x1;:::; xn, one of the simplest models is to\n",
      "assume that the corresponding random vectors X1;:::; Xnareindependent and identically\n",
      "distributed (iid) . We write +431\n",
      "X1;:::; Xniid\u0018forX1;:::; Xniid\u0018Dist;\n",
      "to indicate that the random vectors form an iid sample from a sampling pdf for sampling\n",
      "distribution Dist. This model formalizes the notion that the knowledge about one variable\n",
      "does not provide extra information about another variable. The main theoretical use of\n",
      "independent data models is that the joint density of the random vectors X1;:::; Xnis simply\n",
      "theproduct of the marginal ones; see Theorem C.1. Speciﬁcally, +431\n",
      "fX1;:::;Xn(x1;:::; xn)=f(x1)\u0001\u0001\u0001f(xn):\n",
      "In most models of this kind, our approximation or model for the sampling distribution is\n",
      "speciﬁed up to a small number of parameters. That is, g(x) is of the form g(xj\f) which\n",
      "is known up to some parameter vector \f. Examples for the one-dimensional case ( p=1)\n",
      "include the N(\u0016;\u001b2);Bin(n;p), and Exp(\u0015) distributions. See Tables C.1 and C.2 for other +427\n",
      "common sampling distributions.\n",
      "Typically, the parameters are unknown and must be estimated from the data. In a non-\n",
      "parametric setting the whole sampling distribution would be unknown. To visualize the\n",
      "underlying sampling distribution from outcomes x1;:::; xnone can use graphical repres-\n",
      "entations such as histograms, density plots, and empirical cumulative distribution func-\n",
      "tions, as discussed in Chapter 1. +11\n",
      "If the order in which the data were collected (or their labeling) is not informative or\n",
      "relevant, then the joint pdf of X1;:::; Xnsatisﬁes the symmetry:\n",
      "fX1;:::;Xn(x1;:::; xn)=fX\u00191;:::;X\u0019n(x\u00191;:::; x\u0019n) (2.26)\n",
      "for any permutation \u00191;:::;\u0019 nof the integers 1 ;:::; n. We say that the inﬁnite sequence\n",
      "X1;X2;:::isexchangeable exchangeable if this permutational invariance (2.26) holds for any ﬁnite subset\n",
      "of the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to\n",
      "assume that the random vectors X1;:::; Xnare a subset of an exchangeable sequence and\n",
      "thus satisfy (2.26). Note that while iid random variables are exchangeable, the converse is\n",
      "not necessarily true. Thus, the assumption of an exchangeable sequence of random vectors\n",
      "is weaker than the assumption of iid random vectors.Chapter 2. Statistical Learning 41\n",
      "Figure 2.12 illustrates the modeling tradeo \u000bs. The keywords within the triangle repres-\n",
      "ent various modeling paradigms. A few keywords have been highlighted, symbolizing their\n",
      "importance in modeling. The speciﬁc meaning of the keywords does not concern us here,\n",
      "but the point is there are many models to choose from, depending on what assumptions are\n",
      "made about the data.\n",
      "Figure 2.12: Illustration of the modeling dilemma. Complex models are more generally\n",
      "applicable, but may be di \u000ecult to analyze. Simple models may be highly tractable, but\n",
      "may not describe the data accurately. The triangular shape signiﬁes that there are a great\n",
      "many speciﬁc models but not so many generic ones.\n",
      "On the one hand, models that make few assumptions are more widely applicable, but at\n",
      "the same time may not be very mathematically tractable or provide insight into the nature\n",
      "of the data. On the other hand, very speciﬁc models may be easy to handle and interpret, but\n",
      "may not match the data very well. This tradeo \u000bbetween the tractability and applicability of\n",
      "the model is very similar to the approximation–estimation tradeo \u000bdescribed in Section 2.4.\n",
      "In the typical unsupervised setting we have a training set \u001c=fx1;:::; xngthat is viewed\n",
      "as the outcome of niid random variables X1;:::; Xnfrom some unknown pdf f. The ob-\n",
      "jective is then to learn or estimate ffrom the ﬁnite training data. To put the learning in\n",
      "a similar framework as for supervised learning discussed in the preceding Sections 2.3–\n",
      "2.5, we begin by specifying a class of probability density functions Gp:=fg(\u0001j\u0012);\u00122\u0002g,\n",
      "where\u0012is a parameter in some subset \u0002ofRp. We now seek the best ginGpto minimize\n",
      "some risk. Note that Gpmay not necessarily contain the true feven for very large p.\n",
      "We stress that our notation g(x) has a di \u000berent meaning in the supervised and unsu-\n",
      "pervised case. In the supervised case, gis interpreted as a prediction function for a\n",
      "response y; in the unsupervised setting, gis an approximation of a density f.\n",
      "For each xwe measure the discrepancy between the true model f(x) and the hypothes-\n",
      "ized model g(xj\u0012) using the loss function\n",
      "Loss( f(x);g(xj\u0012))=lnf(x)\n",
      "g(xj\u0012)=lnf(x)\u0000lng(xj\u0012):42 2.6. Modeling Data\n",
      "The expected value of this loss (that is, the risk) is thus\n",
      "`(g)=Elnf(X)\n",
      "g(Xj\u0012)=Z\n",
      "f(x) lnf(x)\n",
      "g(xj\u0012)dx: (2.27)\n",
      "The integral in (2.27) provides a fundamental way to measure the distance between two\n",
      "densities and is called the Kullback–Leibler (KL) divergence2Kullback –\n",
      "Leibler\n",
      "divergencebetween fandg(\u0001j\u0012). Note\n",
      "that the KL divergence is not symmetric in fandg(\u0001j\u0012). Moreover, it is always greater\n",
      "than or equal to 0 (see Exercise 15) and equal to 0 when f=g(\u0001j\u0012).\n",
      "Using similar notation as for the supervised learning setting in Table 2.1, deﬁne gGpas\n",
      "the global minimizer of the risk in the class Gp; that is, gGp=argming2Gp`(g). If we deﬁne\n",
      "\u0012\u0003=argmin\n",
      "\u0012ELoss( f(X);g(Xj\u0012))=argmin\n",
      "\u0012Z\u0000lnf(x)\u0000lng(xj\u0012)\u0001f(x) dx\n",
      "=argmax\n",
      "\u0012Z\n",
      "f(x) lng(xj\u0012) dx=argmax\n",
      "\u0012Elng(Xj\u0012);\n",
      "then gGp=g(\u0001j\u0012\u0003) and learning gGpis equivalent to learning (or estimating) \u0012\u0003. To learn\u0012\u0003\n",
      "from a training set \u001c=fx1;:::; xngwe then minimize the training loss,\n",
      "1\n",
      "nnX\n",
      "i=1Loss( f(xi);g(xij\u0012))=\u00001\n",
      "nnX\n",
      "i=1lng(xij\u0012)+1\n",
      "nnX\n",
      "i=1lnf(xi);\n",
      "giving:\n",
      "b\u0012n:=argmax\n",
      "\u00121\n",
      "nnX\n",
      "i=1lng(xij\u0012): (2.28)\n",
      "As the logarithm is an increasing function, this is equivalent to\n",
      "b\u0012n:=argmax\n",
      "\u0012nY\n",
      "i=1g(xij\u0012);\n",
      "whereQn\n",
      "i=1g(xij\u0012) is the likelihood of the data; that is, the joint density of the fXigeval-\n",
      "uated at the points fxig. We therefore have recovered the classical maximum likelihood\n",
      "estimate of\u0012\u0003.maximum\n",
      "likelihood\n",
      "estimate\n",
      "+458When the risk `(g(\u0001j\u0012)) is convex in \u0012over a convex set \u0002, we can ﬁnd the maximum\n",
      "likelihood estimator by setting the gradient of the training loss to zero; that is, we solve\n",
      "\u00001\n",
      "nnX\n",
      "i=1S(xij\u0012)=0;\n",
      "where S(xj\u0012) :=@lng(xj\u0012)\n",
      "@\u0012is the gradient of ln g(xj\u0012) with respect to \u0012and is often called\n",
      "thescore score .\n",
      "Example 2.5 (Exponential Model) Suppose we have the training data \u001cn=fx1;:::; xng,\n",
      "which is modeled as a realization of npositive iid random variables: X1;:::; Xn\u0018iidf(x).\n",
      "We select the class of approximating functions Gto be the parametric class fg:g(xj\u0012)=\n",
      "2Sometimes called cross-entropy distance.Chapter 2. Statistical Learning 43\n",
      "\u0012exp(\u0000x\u0012);x>0;\u0012 > 0g. In other words, we look for the best gGwithin the family of\n",
      "exponential distributions with unknown parameter \u0012>0. The likelihood of the data is\n",
      "nY\n",
      "i=1g(xij\u0012)=nY\n",
      "i=1\u0012exp(\u0000\u0012xi)=exp(\u0000\u0012nxn+nln\u0012)\n",
      "and the score is S(xj\u0012)=\u0000x+\u0012\u00001. Thus, maximizing the likelihood with respect to \u0012is the\n",
      "same as maximizing \u0000\u0012nxn+nln\u0012or solving\u0000Pn\n",
      "i=1S(xij\u0012)=n=xn\u0000\u0012\u00001=0. In other\n",
      "words, the solution to (2.28) is the maximum likelihood estimate b\u0012n=1=xn.\n",
      "In a supervised setting, where the data is represented by a vector xof explanatory\n",
      "variables and a response y, the general model is that ( x;y) is an outcome of ( X;Y)\u0018f\n",
      "for some unknown f. And for a training sequence ( x1;y1);:::; (xn;yn) the default model\n",
      "assumption is that ( X1;Y1);:::; (Xn;Yn)\u0018iidf. As explained in Section 2.2, the analysis\n",
      "primarily involves the conditional pdf f(yjx) and in particular (when using the squared-\n",
      "error loss) the conditional expectation g\u0003(x)=E[YjX=x]. The resulting representation\n",
      "(2.2) allows us to then write the response at X=xas a function of the feature xplus an\n",
      "error term: Y=g\u0003(x)+\"(x).\n",
      "This leads to the simplest and most important model for supervised learning, where we\n",
      "choose a linear classGof prediction or guess functions and assume that it is rich enough\n",
      "to contain the true g\u0003. If we further assume that, conditional on X=x, the error term \"\n",
      "does not depend on x, that is,E\"=0 andVar\"=\u001b2, then we obtain the following model.\n",
      "Deﬁnition 2.1: Linear Model\n",
      "In alinear model linear model the response Ydepends on a p-dimensional explanatory variable\n",
      "x=[x1;:::; xp]>via the linear relationship\n",
      "Y=x>\f+\"; (2.29)\n",
      "whereE\"=0 andVar\"=\u001b2.\n",
      "Note that (2.29) is a model for a single pair ( x;Y). The model for the training set\n",
      "f(xi;Yi)gis simply that each Yisatisﬁes (2.29) (with x=xi) and that thefYigare independ-\n",
      "ent. Gathering all responses in the vector Y=[Y1;:::; Yn]>, we can write\n",
      "Y=X\f+\"; (2.30)\n",
      "where\"=[\"1;:::;\" n]>is a vector of iid copies of \"andXis the so-called model matrix model matrix ,\n",
      "with rows x>\n",
      "1;:::; x>\n",
      "n. Linear models are fundamental building blocks of statistical learning\n",
      "algorithms. For this reason, a large part of Chapter 5 is devoted to linear regression models. +167\n",
      "Example 2.6 (Polynomial Regression (cont.)) For our running Example 2.1, we see + 26\n",
      "that the data is described by a linear model of the form (2.30), with model matrix Xgiven\n",
      "in (2.10).44 2.7. Multivariate Normal Models\n",
      "Before we discuss a few other models in the following sections, we would like to em-\n",
      "phasize a number of points about modeling.\n",
      "Any model for data is likely to be wrong . For example, real data (as opposed to\n",
      "computer-generated data) are often assumed to come from a normal distribution,\n",
      "which is never exactly true. However, an important advantage of using a normal\n",
      "distribution is that it has many nice mathematical properties, as we will see in Sec-\n",
      "tion 2.7.\n",
      "Most data models depend on a number of unknown parameters, which need to be\n",
      "estimated from the observed data.\n",
      "Any model for real-life data needs to be checked for suitability. An important cri-\n",
      "terion is that data simulated from the model should resemble the observed data, at\n",
      "least for a certain choice of model parameters.\n",
      "Here are some guidelines for choosing a model. Think of the data as a spreadsheet or\n",
      "data frame, as in Chapter 1, where rows represent the data units and the columns the data\n",
      "features (variables, groups).\n",
      "First establish the type of the features (quantitative, qualitative, discrete, continuous,\n",
      "etc.).\n",
      "Assess whether the data can be assumed to be independent across rows or columns.\n",
      "Decide on the level of generality of the model. For example, should we use a simple\n",
      "model with a few unknown parameters or a more generic model that has a large\n",
      "number of parameters? Simple speciﬁc models are easier to ﬁt to the data (low es-\n",
      "timation error) than more general models, but the ﬁt itself may not be accurate (high\n",
      "approximation error). The tradeo \u000bs discussed in Section 2.4 play an important role\n",
      "here.\n",
      "Decide on using a classical (frequentist) or Bayesian model. Section 2.9 gives a short\n",
      "introduction to Bayesian learning. +47\n",
      "2.7 Multivariate Normal Models\n",
      "A standard model for numerical observations x1;:::; xn(forming, e.g., a column in a\n",
      "spreadsheet or data frame) is that they are the outcomes of iid normal random variables\n",
      "X1;:::; Xniid\u0018N(\u0016;\u001b2):\n",
      "It is helpful to view a normally distributed random variable as a simple transformation\n",
      "of a standard normal random variable. To wit, if Zhas a standard normal distribution, then\n",
      "X=\u0016+\u001bZhas aN(\u0016;\u001b2) distribution. The generalization to ndimensions is discussed\n",
      "in Appendix C.7. We summarize the main points: Let Z1;:::; Zniid\u0018N(0;1). The pdf of +436\n",
      "Z=[Z1;:::; Zn]>(that is, the joint pdf of Z1;:::; Zn) is given by\n",
      "fZ(z)=nY\n",
      "i=11p\n",
      "2\u0019e\u00001\n",
      "2z2\n",
      "i=(2\u0019)\u0000n\n",
      "2e\u00001\n",
      "2z>z;z2Rn: (2.31)Chapter 2. Statistical Learning 45\n",
      "We write Z\u0018N(0;In) and say that Zhas a standard normal distribution in Rn. Let\n",
      "X=\u0016+BZ (2.32)\n",
      "for some m\u0002nmatrix Bandm-dimensional vector \u0016. Then Xhas expectation vector \u0016and\n",
      "covariance matrix \u0006=BB>; see (C.20) and (C.21). This leads to the following deﬁnition. +434\n",
      "Deﬁnition 2.2: Multivariate Normal Distribution\n",
      "Anm-dimensional random vector Xthat can be written in the form (2.32) for some\n",
      "m-dimensional vector \u0016andm\u0002nmatrix B, with Z\u0018N(0;In), is said to have a\n",
      "multivariate normal multivariate\n",
      "normalormultivariate Gaussian distribution with mean vector \u0016and\n",
      "covariance matrix \u0006=BB>. We write X\u0018N(\u0016;\u0006).\n",
      "Them-dimensional density of a multivariate normal distribution has a very similar form\n",
      "to the density of the one-dimensional normal distribution and is given in the next theorem.\n",
      "We leave the proof as an exercise; see Exercise 5. + 59\n",
      "Theorem 2.3: Density of a Multivariate Random Vector\n",
      "LetX\u0018N(\u0016;\u0006), where the m\u0002mcovariance matrix \u0006is invertible. Then Xhas pdf\n",
      "fX(x)=1p(2\u0019)mj\u0006je\u00001\n",
      "2(x\u0000\u0016)>\u0006\u00001(x\u0000\u0016);x2Rm: (2.33)\n",
      "Figure 2.13 shows the pdfs of two bivariate (that is, two-dimensional) normal distribu-\n",
      "tions. In both cases the mean vector is \u0016=[0;0]>and the variances (the diagonal elements\n",
      "of\u0006) are 1. The correlation coe \u000ecients (or, equivalently here, the covariances) are respect-\n",
      "ively%=0 and%=0:8.\n",
      "00.1\n",
      "20.2\n",
      "0\n",
      "-22 0 -2\n",
      "00.1\n",
      "20.2\n",
      "0\n",
      "-22 0 -2\n",
      "Figure 2.13: Pdfs of bivariate normal distributions with means zero, variances 1, and cor-\n",
      "relation coe \u000ecients 0 (left) and 0 :8 (right).46 2.8. Normal Linear Models\n",
      "The main reason why the multivariate normal distribution plays an important role in\n",
      "data science and machine learning is that it satisﬁes the following properties, the details\n",
      "and proofs of which can be found in Appendix C.7: +436\n",
      "1. A\u000ene combinations are normal.\n",
      "2. Marginal distributions are normal.\n",
      "3. Conditional distributions are normal.\n",
      "2.8 Normal Linear Models\n",
      "Normal linear models combine the simplicity of the linear model with the tractability of\n",
      "the Gaussian distribution. They are the principal model for traditional statistics, and include\n",
      "the classic linear regression and analysis of variance models.\n",
      "Deﬁnition 2.3: Normal Linear Model\n",
      "In a normal linear model normal linear\n",
      "modelthe response Ydepends on a p-dimensional explanatory\n",
      "variable x=[x1;:::; xp]>, via the linear relationship\n",
      "Y=x>\f+\"; (2.34)\n",
      "where\"\u0018N(0;\u001b2).\n",
      "Thus, a normal linear model is a linear model (in the sense of Deﬁnition 2.1) with\n",
      "normal error terms. Similar to (2.30), the corresponding normal linear model for the whole\n",
      "training setf(xi;Yi)ghas the form\n",
      "Y=X\f+\"; (2.35)\n",
      "where Xis the model matrix comprised of rows x>\n",
      "1;:::; x>\n",
      "nand\"\u0018N(0;\u001b2In). Con-\n",
      "sequently, Ycan be written as Y=X\f+\u001bZ, where Z\u0018N(0;In), so that Y\u0018N(X\f;\u001b2In).\n",
      "It follows from (2.33) that its joint density is given by +45\n",
      "g(yj\f;\u001b2;X)=(2\u0019\u001b2)\u0000n\n",
      "2e\u00001\n",
      "2\u001b2jjy\u0000X\fjj2: (2.36)\n",
      "Estimation of the parameter \fcan be performed via the least-squares method, as discussed\n",
      "in Example 2.1. An estimate can also be obtained via the maximum likelihood method.\n",
      "This simply means ﬁnding the parameters \u001b2and\fthat maximize the likelihood of the\n",
      "outcome y, given by the right-hand side of (2.36). It is clear that for every value of \u001b2\n",
      "the likelihood is maximal when ky\u0000X\fk2is minimal. As a consequence, the maximum\n",
      "likelihood estimate for \fis the same as the least-squares estimate (2.15). We leave it as an\n",
      "exercise (see Exercise 18) to show that the maximum likelihood estimate of \u001b2is equal to +63\n",
      "c\u001b2=ky\u0000Xb\fk2\n",
      "n; (2.37)\n",
      "whereb\fis the maximum likelihood estimate (least squares estimate in this case) of \f.Chapter 2. Statistical Learning 47\n",
      "2.9 Bayesian Learning\n",
      "In Bayesian unsupervised learning, we seek to approximate the unknown joint density\n",
      "f(x1;:::; xn) of the training data Tn=fX1;:::; Xngvia a joint pdf of the form\n",
      "Z0BBBBB@nY\n",
      "i=1g(xij\u0012)1CCCCCAw(\u0012) d\u0012; (2.38)\n",
      "where g(\u0001j\u0012) belongs to a family of parametric densities Gp:=fg(\u0001j\u0012);\u00122\u0002g(viewed\n",
      "as a family of pdfs conditional on a parameter \u0012in some set \u0002\u001aRp) and w(\u0012) is a pdf\n",
      "that belongs to a (possibly di \u000berent) family of densities W p. Note how the joint pdf (2.38)\n",
      "satisﬁes the permutational invariance (2.26) and can thus be useful as a model for training\n",
      "data which is part of an exchangeable sequence of random variables.\n",
      "Following standard practice in a Bayesian context, instead of writing fX(x) and\n",
      "fXjY(xjy) for the pdf of Xand the conditional pdf of Xgiven Y, one simply writes\n",
      "f(x) and f(xjy). IfYis a di \u000berent random variable, its pdf (at y) is thus denoted by\n",
      "f(y).\n",
      "Thus, we will use the same symbol gfor di \u000berent (conditional) approximating probab-\n",
      "ility densities and ffor the di \u000berent (conditional) true and unknown probability densities.\n",
      "Using Bayesian notation, we can write g(\u001cj\u0012)=Qn\n",
      "i=1g(xij\u0012) and thus the approximating\n",
      "joint pdf (2.38) can then be written asR\n",
      "g(\u001cj\u0012)w(\u0012) d\u0012and the true unknown joint pdf as\n",
      "f(\u001c)=f(x1;:::; xn).\n",
      "OnceGpandW pare speciﬁed, selecting an approximating function g(x) of the form\n",
      "g(x)=Z\n",
      "g(xj\u0012)w(\u0012) d\u0012\n",
      "is equivalent to selecting a suitable wfromW p. Similar to (2.27), we can use the Kullback–\n",
      "Leibler risk to measure the discrepancy between the proposed approximation (2.38) and the\n",
      "true f(\u001c):\n",
      "`(g)=Elnf(T)R\n",
      "g(Tj\u0012)w(\u0012) d\u0012=Z\n",
      "f(\u001c) lnf(\u001c)R\n",
      "g(\u001cj\u0012)w(\u0012) d\u0012d\u001c: (2.39)\n",
      "The main di \u000berence with (2.27) is that since the training data is not necessarily iid (it may\n",
      "be exchangeable, for example), the expectation must be with respect to the joint density of + 40\n",
      "T, not with respect to the marginal f(x) (as in the iid case).\n",
      "Minimizing the training loss is equivalent to maximizing the likelihood of the training\n",
      "data\u001c; that is, solving the optimization problem\n",
      "max\n",
      "w2W pZ\n",
      "g(\u001cj\u0012)w(\u0012) d\u0012;\n",
      "where the maximization is over an appropriate class W pof density functions that is be-\n",
      "lieved to result in the smallest KL risk.48 2.9. Bayesian Learning\n",
      "Suppose that we have a rough guess, denoted w0(\u0012), for the best w2W pthat min-\n",
      "imizes the Kullback–Leibler risk. We can always increase the resulting likelihood L0:=R\n",
      "g(\u001cj\u0012)w0(\u0012) d\u0012by instead using the density w1(\u0012) :=w0(\u0012)g(\u001cj\u0012)=L0, giving a likeli-\n",
      "hood L1:=R\n",
      "g(\u001cj\u0012)w1(\u0012) d\u0012. To see this, write L0andL1as expectations with respect to\n",
      "w0. In particular, we can write\n",
      "L0=Ew0g(\u001cj\u0012) and L1=Ew1g(\u001cj\u0012)=Ew0g2(\u001cj\u0012)=L0:\n",
      "It follows that\n",
      "L1\u0000L0=1\n",
      "L0Ew0h\n",
      "g2(\u001cj\u0012)\u0000L2\n",
      "0i\n",
      "=1\n",
      "L0Varw0[g(\u001cj\u0012)]>0: (2.40)\n",
      "We may thus expect to obtain better predictions using w1instead of w0, because w1has\n",
      "taken into account the observed data \u001cand increased the likelihood of the model. In fact,\n",
      "if we iterate this process (see Exercise 20) and create a sequence of densities w1;w2;:::\n",
      "such that wt(\u0012)_wt\u00001(\u0012)g(\u001cj\u0012), then wt(\u0012) concentrates more and more of its probability\n",
      "mass at the maximum likelihood estimator b\u0012(see (2.28)) and in the limit equals a (degen-\n",
      "erate) point-mass pdf at b\u0012. In other words, in the limit we recover the maximum likelihood\n",
      "method: g\u001c(x)=g(xjb\u0012). Thus, unless the class of densities W pis restricted to be non-\n",
      "degenerate, maximizing the likelihood as much as possible leads to a degenerate choice\n",
      "forw(\u0012).\n",
      "In many situations, the maximum likelihood estimate g(\u001cjb\u0012) is either not an ap-\n",
      "propriate approximation to f(\u001c) (see Example 2.9), or simply fails to exist (see Exer-\n",
      "cise 10 in Chapter 4). In such cases, given an initial non-degenerate guess w0(\u0012)=g(\u0012), +161\n",
      "one can obtain a more appropriate and non-degenerate approximation to f(\u001c) by taking\n",
      "w(\u0012)=w1(\u0012)_g(\u001cj\u0012)g(\u0012) in (2.38), giving the following Bayesian learner of f(x):\n",
      "g\u001c(x) :=Z\n",
      "g(xj\u0012)g(\u001cj\u0012)g(\u0012)R\n",
      "g(\u001cj#)g(#) d#d\u0012; (2.41)\n",
      "whereR\n",
      "g(\u001cj#)g(#) d#=g(\u001c). Using Bayes’ formula for probability densities, +430\n",
      "g(\u0012j\u001c)=g(\u001cj\u0012)g(\u0012)\n",
      "g(\u001c); (2.42)\n",
      "we can write w1(\u0012)=g(\u0012j\u001c). With this notation, we have the following deﬁnitions.\n",
      "Deﬁnition 2.4: Prior, Likelihood, and Posterior\n",
      "Let\u001candGp:=fg(\u0001j\u0012);\u00122\u0002gbe the training set and family of approximating\n",
      "functions.\n",
      "A pdf g(\u0012) that reﬂects our a priori beliefs about \u0012is called the prior prior pdf.\n",
      "The conditional pdf g(\u001cj\u0012) is called the likelihood likelihood .\n",
      "Inference about \u0012is given by the posterior posterior pdfg(\u0012j\u001c), which is proportional\n",
      "to the product of the prior and the likelihood:\n",
      "g(\u0012j\u001c)/g(\u001cj\u0012)g(\u0012):Chapter 2. Statistical Learning 49\n",
      "Remark 2.1 (Early Stopping) Bayes iteration is an example of an “early stopping”\n",
      "heuristic for maximum likelihood optimization, where we exit after only one step. As ob-\n",
      "served above, if we keep iterating, we obtain the maximum likelihood estimate (MLE). In\n",
      "a sense the Bayes rule provides a regularization of the MLE. Regularization is discussed in\n",
      "more detail in Chapter 6; see also Example 2.9. The early stopping rule is also of beneﬁt\n",
      "in regularization; see Exercise 20 in Chapter 6.\n",
      "On the one hand, the initial guess g(\u0012) conveys the a priori (prior to training the\n",
      "Bayesian learner) information about the optimal density in W pthat minimizes the KL risk.\n",
      "Using this prior g(\u0012), the Bayesian approximation to f(x) is the prior predictive density prior predictive\n",
      "density:\n",
      "g(x)=Z\n",
      "g(xj\u0012)g(\u0012) d\u0012:\n",
      "On the other hand, the posterior pdf conveys improved knowledge about this optimal dens-\n",
      "ity inW pafter training with \u001c. Using the posterior g(\u0012j\u001c), the Bayesian learner of f(x) is\n",
      "theposterior predictive density posterior\n",
      "predictive\n",
      "density:\n",
      "g\u001c(x)=g(xj\u001c)=Z\n",
      "g(xj\u0012)g(\u0012j\u001c) d\u0012;\n",
      "where we have assumed that g(xj\u0012;\u001c)=g(xj\u0012); that is, the likelihood depends on \u001conly\n",
      "through the parameter \u0012.\n",
      "The choice of the prior is typically governed by two considerations:\n",
      "1. the prior should be simple enough to facilitate the computation or simulation of the\n",
      "posterior pdf;\n",
      "2. the prior should be general enough to model ignorance of the parameter of interest.\n",
      "Priors that do not convey much knowledge of the parameter are said to be uninformat-\n",
      "ive. The uniform or ﬂatprior in Example 2.9 (to follow) is frequently used.uninformative\n",
      "prior\n",
      "For the purpose of analytical and numerical computations, we can view \u0012as a ran-\n",
      "dom vector with prior density g(\u0012), which after training is updated to the posterior\n",
      "density g(\u0012j\u001c).\n",
      "The above thinking allows us to write g(xj\u001c)_R\n",
      "g(xj\u0012)g(\u001cj\u0012)g(\u0012) d\u0012, for example,\n",
      "thus ignoring any constants that do not depend on the argument of the densities.\n",
      "Example 2.7 (Normal Model) Suppose that the training data T=fX1;:::; Xngis\n",
      "modeled using the likelihood g(xj\u0012) that is the pdf of\n",
      "Xj\u0012\u0018N(\u0016;\u001b2);\n",
      "where\u0012:=[\u0016;\u001b2]>. Next, we need to specify the prior distribution of \u0012to complete\n",
      "the model. We can specify prior distributions for \u0016and\u001b2separately and then take their\n",
      "product to obtain the prior for vector \u0012(assuming independence). A possible prior distri-\n",
      "bution for\u0016is\n",
      "\u0016\u0018N(\u0017;\u001e2): (2.43)50 2.9. Bayesian Learning\n",
      "It is typical to refer to any parameters of the prior density as hyperparameters hyperparamet -\n",
      "ersof the\n",
      "Bayesian model. Instead of giving directly a prior for \u001b2(or\u001b), it turns out to be con-\n",
      "venient to give the following prior distribution to 1 =\u001b2:\n",
      "1\n",
      "\u001b2\u0018Gamma (\u000b;\f): (2.44)\n",
      "The smaller \u000band\fare, the less informative is the prior. Under this prior, \u001b2is said to have\n",
      "aninverse gamma inverse gamma3distribution. If 1 =Z\u0018Gamma (\u000b;\f), then the pdf of Zis proportional\n",
      "to exp (\u0000\f=z)=z\u000b+1(Exercise 19). The Bayesian posterior is then given by: +63\n",
      "g(\u0016;\u001b2j\u001c)_g(\u0016)\u0002g(\u001b2)\u0002g(\u001cj\u0016;\u001b2)\n",
      "_exp(\n",
      "\u0000(\u0016\u0000\u0017)2\n",
      "2\u001e2)\n",
      "\u0002expn\n",
      "\u0000\f=\u001b2o\n",
      "(\u001b2)\u000b+1\u0002expn\n",
      "\u0000P\n",
      "i(xi\u0000\u0016)2=(2\u001b2)o\n",
      "(\u001b2)n=2\n",
      "_(\u001b2)\u0000n=2\u0000\u000b\u00001exp(\n",
      "\u0000(\u0016\u0000\u0017)2\n",
      "2\u001e2\u0000\f\n",
      "\u001b2\u0000(\u0016\u0000xn)2+S2\n",
      "n\n",
      "2\u001b2=n)\n",
      ";\n",
      "where S2\n",
      "n:=1\n",
      "nP\n",
      "ix2\n",
      "i\u0000x2\n",
      "n=1\n",
      "nP\n",
      "i(xi\u0000xn)2is the (scaled) sample variance. All inference\n",
      "about (\u0016;\u001b2) is then represented by the posterior pdf. To facilitate computations it is helpful\n",
      "to ﬁnd out if the posterior belongs to a recognizable family of distributions. For example,\n",
      "the conditional pdf of \u0016given\u001b2and\u001cis\n",
      "g(\u0016j\u001b2;\u001c)_exp(\n",
      "\u0000(\u0016\u0000\u0017)2\n",
      "2\u001e2\u0000(\u0016\u0000xn)2\n",
      "2\u001b2=n)\n",
      ";\n",
      "which after simpliﬁcation can be recognized as the pdf of\n",
      "(\u0016j\u001b2;\u001c)\u0018N\u0010\n",
      " n\u001b2=n\u0011\n",
      "; (2.45)\n",
      "n:=ne we have deﬁned the weight parameter: \n",
      "\u001b2.\u0010\n",
      "1\n",
      "\u001e2+n\n",
      "\u001b2\u0011\n",
      ":We can then see that the\n",
      "n)\u0017is a weighted linear combination of the prior\n",
      "n!1 and thus theple average xn. Further, as n!1 , the weight \n",
      "posterior mean approaches the maximum likelihood estimate xn.\n",
      "It is sometimes possible to use a prior g(\u0012) that is not a bona ﬁde probability density, in the\n",
      "sense thatR\n",
      "g(\u0012) d\u0012=1, as long as the resulting posterior g(\u0012j\u001c)_g(\u001cj\u0012)g(\u0012) is a proper\n",
      "pdf. Such a prior is called an improper prior improper prior .\n",
      "Example 2.8 (Normal Model (cont.)) An example of an improper prior is obtained\n",
      "from (2.43) when we let \u001e!1 (the larger \u001eis, the more uninformative is the prior).\n",
      "Then, g(\u0016)_1 is a ﬂat prior, butR\n",
      "g(\u0016) d\u0016=1, making it an improper prior. Neverthe-\n",
      "less, the posterior is a proper density, and in particular the conditional posterior of ( \u0016j\u001b2;\u001c)\n",
      "simpliﬁes to\n",
      "(\u0016j\u001b2;\u001c)\u0018N\u0010\n",
      "xn;\u001b2=n\u0011\n",
      ";\n",
      "3Reciprocal gamma distribution would have been a better name.Chapter 2. Statistical Learning 51\n",
      "ngoes to 1 as \u001e!1 . The improper prior g(\u0016)_1 also\n",
      "allows us to simplify the posterior marginal for \u001b2:\n",
      "g(\u001b2j\u001c)=Z\n",
      "g(\u0016;\u001b2j\u001c) d\u0016_(\u001b2)\u0000(n\u00001)=2\u0000\u000b\u00001exp(\n",
      "\u0000\f+nS2\n",
      "n=2\n",
      "\u001b2)\n",
      ";\n",
      "which we recognize as the density corresponding to\n",
      "1\n",
      "\u001b2\f\f\f\f\u001c\u0018Gamma \n",
      "\u000b+n\u00001\n",
      "2; \f+n\n",
      "2S2\n",
      "n!\n",
      ":\n",
      "In addition to g(\u0016)_1, we can also use an improper prior for \u001b2. If we take the limit \u000b!0\n",
      "and\f!0 in (2.44), then we also obtain the improper prior g(\u001b2)_1=\u001b2(or equivalently\n",
      "g(1=\u001b2)_1=\u001b2). In this case, the posterior marginal density for \u001b2implies that:\n",
      "nS2\n",
      "n\n",
      "\u001b2\f\f\f\f\u001c\u0018\u001f2\n",
      "n\u00001\n",
      "and the posterior marginal density for \u0016implies that:\n",
      "\u0016\u0000xn\n",
      "Sn=p\n",
      "n\u00001\f\f\f\f\u001c\u0018tn\u00001: (2.46)\n",
      "In general, deriving a simple formula for the posterior density of \u0012is either impossible\n",
      "or too tedious. Instead, the Monte Carlo methods in Chapter 3 can be used to simulate\n",
      "(approximately) from the posterior for the purposes of inference and prediction.\n",
      "One way in which a distributional result such as (2.46) can be useful is in the construc-\n",
      "tion of a 95% credible interval credible\n",
      "intervalIfor the parameter \u0016; that is, an interval Isuch that the\n",
      "probabilityP[\u00162Ij\u001c] is equal to 0 :95. For example, the symmetric 95% credible interval\n",
      "is\n",
      "I=\"\n",
      "xn\u0000Snp\n",
      ";xn+Snp\n",
      "#\u00001\n",
      ";\n",
      "is the 0:975-quantile of the tn\u00001distribution. Note that the credible interval is\n",
      "not a random object and that the parameter \u0016is interpreted as a random variable with a\n",
      "distribution. This is unlike the case of classical conﬁdence intervals, where the parameter\n",
      "is nonrandom, but the interval is (the outcome of) a random object. +459\n",
      "As a generalization of the 95% Bayesian credible interval we can deﬁne a 1 \u0000\u000bcredible\n",
      "region credible region , which is any setRsatisfying\n",
      "P[\u00122Rj\u001c]=Z\n",
      "\u00122Rg(\u0012j\u001c) d\u0012>1\u0000\u000b: (2.47)52 2.9. Bayesian Learning\n",
      "Example 2.9 (Bayesian Regularization of Maximum Likelihood) Consider model-\n",
      "ing the number of deaths during birth in a maternity ward. Suppose that the hospital data\n",
      "consists of\u001c=fx1;:::; xng, with xi=1 if the i-th baby has died during birth and xi=0\n",
      "otherwise, for i=1;:::; n. A possible Bayesian model for the data is \u0012\u0018U(0;1) (uniform\n",
      "prior) with ( X1;:::; Xnj\u0012)iid\u0018Ber(\u0012). The likelihood is therefore\n",
      "g(\u001cj\u0012)=nY\n",
      "i=1\u0012xi(1\u0000\u0012)1\u0000xi=\u0012s(1\u0000\u0012)n\u0000s;\n",
      "where s=x1+\u0001\u0001\u0001+xnis the total number of deaths. Since g(\u0012)=1, the posterior pdf is\n",
      "g(\u0012j\u001c)_\u0012s(1\u0000\u0012)n\u0000s; \u00122[0;1];\n",
      "which is the pdf of the Beta (s+1;n\u0000s+1) distribution. The normalization constant is\n",
      "(n+1)\u0010n\n",
      "s\u0011\n",
      ". The posterior pdf is shown in Figure 2.14 for ( s;n)=(0;100). It is not di \u000ecult\n",
      "Figure 2.14: Posterior pdf for \u0012, with n=100 and s=0.\n",
      "to see that the maximum a posteriori maximum a\n",
      "posteriori(MAP) estimate of \u0012(the mode or maximizer of the\n",
      "posterior density) is\n",
      "argmax\n",
      "\u0012g(\u0012j\u001c)=s\n",
      "n;\n",
      "which agrees with the maximum likelihood estimate. Figure 2.14 also shows that the left\n",
      "one-sided 95% credible interval for \u0012is [0;0:0292], where 0 :0292 is the 0.95 quantile\n",
      "(rounded) of the Beta (1;101) distribution.\n",
      "Observe that when ( s;n)=(0;100) the maximum likelihood estimate b\u0012=0 infers that\n",
      "deaths at birth are not possible. We know that this inference is wrong — the probability of\n",
      "death can never be zero, it is simply (and fortunately) too small to be inferred accurately\n",
      "from a sample size of n=100. In contrast to the maximum likelihood estimate, the pos-\n",
      "terior mean E[\u0012j\u001c]=(s+1)=(n+2) is not zero for ( s;n)=(0;100) and provides the more\n",
      "reasonable point estimate of 0 :0098 for the probability of death.Chapter 2. Statistical Learning 53\n",
      "In addition, while computing a Bayesian credible interval poses no conceptual di \u000e-\n",
      "culties, it is not simple to derive a conﬁdence interval for the maximum likelihood estimate\n",
      "ofb\u0012, because the likelihood as a function of \u0012is not di \u000berentiable at \u0012=0. As a result of\n",
      "this lack of smoothness, the usual conﬁdence intervals based on the normal approximation\n",
      "cannot be used.\n",
      "We now return to the unsupervised learning setting of Section 2.6, but consider this\n",
      "from a Bayesian perspective. Recall from (2.39) that the Kullback–Leibler risk for an ap-\n",
      "proximating function gis\n",
      "`(g)=Z\n",
      "f(\u001c0\n",
      "n)[lnf(\u001c0\n",
      "n)\u0000lng(\u001c0\n",
      "n)] d\u001c0\n",
      "n;\n",
      "where\u001c0\n",
      "ndenotes the test data. SinceR\n",
      "f(\u001c0\n",
      "n) lnf(\u001c0\n",
      "n) d\u001c0\n",
      "nplays no role in minimizing the\n",
      "risk, we consider instead the cross-entropy risk , deﬁned as +122\n",
      "`(g)=\u0000Z\n",
      "f(\u001c0\n",
      "n) lng(\u001c0\n",
      "n) d\u001c0\n",
      "n:\n",
      "Note that the smallest possible cross-entropy risk is `\u0003\n",
      "n=\u0000R\n",
      "f(\u001c0\n",
      "n) lnf(\u001c0\n",
      "n) d\u001c0\n",
      "n. The expec-\n",
      "ted generalization risk of the Bayesian learner can then be decomposed as\n",
      "E`(gTn)=`\u0003\n",
      "n+Z\n",
      "f(\u001c0\n",
      "n) lnf(\u001c0\n",
      "n)\n",
      "Eg(\u001c0\n",
      "njTn)d\u001c0\n",
      "n\n",
      "|                            {z                            }\n",
      "“bias” component+EZ\n",
      "f(\u001c0\n",
      "n) lnEg(\u001c0\n",
      "njTn)\n",
      "g(\u001c0\n",
      "njTn)d\u001c0\n",
      "n\n",
      "|                               {z                               }\n",
      "“variance” component;\n",
      "where gTn(\u001c0\n",
      "n)=g(\u001c0\n",
      "njTn)=R\n",
      "g(\u001c0\n",
      "nj\u0012)g(\u0012jTn) d\u0012is the posterior predictive density after\n",
      "observingTn.\n",
      "Assuming that the sets TnandT0\n",
      "nare comprised of 2 niid random variables with density\n",
      "f, we can show (Exercise 23) that the expected generalization risk simpliﬁes to\n",
      "E`(gTn)=Elng(Tn)\u0000Elng(T2n); (2.48)\n",
      "where g(\u001cn) and g(\u001c2n) are the prior predictive densities of \u001cnand\u001c2n, respectively.\n",
      "Let\u0012n=argmax\u0012g(\u0012jTn) be the MAP estimator of \u0012\u0003:=argmax\u0012Elng(Xj\u0012). As-\n",
      "suming that \u0012nconverges to \u0012\u0003(with probability one) and1\n",
      "nElng(Tnj\u0012n)=Elng(Xj\u0012\u0003)+\n",
      "O(1=n), we can use the following large-sample approximation of the expected generaliza-\n",
      "tion risk.\n",
      "Theorem 2.4: Approximating the Bayesian Cross-Entropy Risk\n",
      "Forn!1 , the expected cross-entropy generalization risk satisﬁes:\n",
      "E`(gTn)'\u0000Elng(Tn)\u0000p\n",
      "2lnn; (2.49)\n",
      "where (with pthe dimension of the parameter vector \u0012and\u0012nthe MAP estimator):\n",
      "Elng(Tn)'Elng(Tnj\u0012n)\u0000p\n",
      "2lnn: (2.50)54 2.9. Bayesian Learning\n",
      "Proof: To show (2.50), we apply Theorem C.21 to lnR\n",
      "e\u0000nrn(\u0012)g(\u0012) d\u0012, where +452\n",
      "rn(\u0012) :=\u00001\n",
      "nlng(Tnj\u0012)=\u00001\n",
      "nnX\n",
      "i=1lng(Xij\u0012)a:s:\u0000!\u0000Elng(Xj\u0012)=:r(\u0012)<1:\n",
      "This gives (with probability one)\n",
      "lnZ\n",
      "g(Tnj\u0012)g(\u0012) d\u0012'\u0000nr(\u0012\u0003)\u0000p\n",
      "2ln(n):\n",
      "Taking expectations on both sides and using nr(\u0012\u0003)=nE[rn(\u0012n)]+O(1), we deduce (2.50).\n",
      "To demonstrate (2.49), we derive the asymptotic approximation of Elng(T2n) by repeating\n",
      "the argument for (2.50), but replacing nwith 2 n, where necessary. Thus, we obtain:\n",
      "Elng(T2n)'\u00002nr(\u0012\u0003)\u0000p\n",
      "2ln(2n):\n",
      "Then, (2.49) follows from the identity (2.48). \u0003\n",
      "The results of Theorem 2.4 have two major implications for model selection and assess-\n",
      "ment. First, (2.49) suggests that \u0000lng(Tn) can be used as a crude (leading-order) asymp-\n",
      "totic approximation to the expected generalization risk for large nand ﬁxed p. In this\n",
      "context, the prior predictive density g(Tn) is usually called the model evidence model evidence ormarginal\n",
      "likelihood for the classGp. Since the integralR\n",
      "g(Tnj\u0012)g(\u0012) d\u0012is rarely available in closed\n",
      "form, the exact computation of the model evidence is typically not feasible and may require\n",
      "Monte Carlo estimation methods. +78\n",
      "Second, when the model evidence is di \u000ecult to compute via Monte Carlo methods or\n",
      "otherwise, (2.50) suggests that we can use the following large-sample approximation:\n",
      "\u00002Elng(Tn)'\u00002 lng(Tnj\u0012n)+pln(n): (2.51)\n",
      "The asymptotic approximation on the right-hand side of (2.51) is called the Bayesian in-\n",
      "formation criterion Bayesian\n",
      "information\n",
      "criterion(BIC). We prefer the class Gpwith the smallest BIC. The BIC is typic-\n",
      "ally used when the model evidence is di \u000ecult to compute and nis su\u000eciently larger than\n",
      "p. For a ﬁxed p, and as nbecomes larger and larger, the BIC becomes a more and more\n",
      "accurate estimator of \u00002Elng(Tn). Note that the BIC approximation is valid even when the\n",
      "true density f<Gp. The BIC provides an alternative to the Akaike information criterion\n",
      "(AIC) for model selection. However, while the BIC approximation does not assume that +126\n",
      "the true model fbelongs to the parametric class under consideration, the AIC assumes\n",
      "that f2G p. Thus, the AIC is merely a heuristic approximation based on the asymptotic\n",
      "approximations in Theorem 4.1.\n",
      "Although the above Bayesian theory has been presented in an unsupervised learn-\n",
      "ing setting, it can be readily extended to the supervised case. We only need to relabel\n",
      "the training setTn. In particular, when (as is typical for regression models) the train-\n",
      "ing responses Y1;:::; Ynare considered as random variables but the corresponding fea-\n",
      "ture vectors x1;:::; xnare viewed as being ﬁxed, then Tnis the collection of random re-\n",
      "sponsesfY1;:::; Yng. Alternatively, we can simply identify Tnwith the response vector\n",
      "Y=[Y1;:::; Yn]>. We will adopt this notation in the next example.Chapter 2. Statistical Learning 55\n",
      "Example 2.10 (Polynomial Regression (cont.)) Consider Example 2.2 once again, but\n",
      "now in a Bayesian framework, where the prior knowledge on ( \u001b2;\f) is speciﬁed by\n",
      "g(\u001b2)=1=\u001b2and\fj\u001b2\u0018N(0;\u001b2D), and Dis a (matrix) hyperparameter. Let \u0006:=\n",
      "(X>X+D\u00001)\u00001. Then the posterior can be written as:\n",
      "g(\f;\u001b2jy)=exp\u0010\n",
      "\u0000ky\u0000X\fk2\n",
      "2\u001b2\u0011\n",
      "(2\u0019\u001b2)n=2\u0002exp\u0010\n",
      "\u0000\f>D\u00001\f\n",
      "2\u001b2\u0011\n",
      "(2\u0019\u001b2)p=2jDj1=2\u00021\n",
      "\u001b2,\n",
      "g(y)\n",
      "=(\u001b2)\u0000(n+p)=2\u00001\n",
      "(2\u0019)(n+p)=2jDj1=2exp \n",
      "\u0000k\u0006\u00001=2(\f\u0000\f)k2\n",
      "2\u001b2\u0000(n+p+2)\u001b2\n",
      "2\u001b2!,\n",
      "g(y);\n",
      "where\f:=\u0006X>yand\u001b2:=y>(I\u0000X\u0006X>)y=(n+p+2) are the MAP estimates of \fand\n",
      "\u001b2, and g(y) is the model evidence for Gp:\n",
      "g(y)=\"\n",
      "g(\f;\u001b2;y) d\fd\u001b2\n",
      "=j\u0006j1=2\n",
      "(2\u0019)n=2jDj1=2Z1\n",
      "0exp\u0012\n",
      "\u0000(n+p+2)\u001b2\n",
      "2\u001b2\u0013\n",
      "(\u001b2)n=2+1d\u001b2\n",
      "=j\u0006j1=2\u0000(n=2)\n",
      "jDj1=2(\u0019(n+p+2)\u001b2)n=2:\n",
      "Therefore, based on (2.49), we have\n",
      "2E`(gTn)'\u00002 lng(y)=nlnh\n",
      "\u0019(n+p+2)\u001b2i\n",
      "\u00002 ln\u0000(n=2)+lnjDj\u0000lnj\u0006j:\n",
      "On the other hand, the minus of the log-likelihood of Ycan be written as\n",
      "\u0000lng(yj\f;\u001b2)=ky\u0000X\fk2\n",
      "2\u001b2+n\n",
      "2ln(2\u0019\u001b2)\n",
      "=k\u0006\u00001=2(\f\u0000\f)k2\n",
      "2\u001b2+(n+p+2)\u001b2\n",
      "2\u001b2+n\n",
      "2ln(2\u0019\u001b2):\n",
      "Therefore, the BIC approximation (2.51) is\n",
      "\u00002 lng(yj\f;\u001b2)+(p+1) ln( n)=n[ln(2\u0019\u001b2)+1]+(p+1) ln( n)+(p+2); (2.52)\n",
      "where the extra ln( n) term in ( p+1) ln( n) is due to the inclusion of \u001b2in\u0012=(\u001b2;\f).\n",
      "Figure 2.15 shows the model evidence and its BIC approximation, where we used a hyper-\n",
      "parameter D=104\u0002Ipfor the prior density of \f. We can see that both approximations\n",
      "exhibit a pronounced minimum at p=4, thus identifying the true polynomial regression\n",
      "model. Compare the overall qualitative shape of the cross-entropy risk estimate with the\n",
      "shape of the square-error risk estimate in Figure 2.11.56 2.9. Bayesian Learning\n",
      "123456789 1 0600650700750800\n",
      "Figure 2.15: The BIC and marginal likelihood used for model selection.\n",
      "It is possible to give the model complexity parameter pa Bayesian treatment, in which\n",
      "we deﬁne a prior density on the set of all models under consideration. For example, let\n",
      "g(p);p=1;:::; mbe a prior density on mcandidate models. Treating the model com-\n",
      "plexity index pas an additional parameter to \u00122Rp, and applying Bayes’ formula, the\n",
      "posterior for ( \u0012;p) can be written as:\n",
      "g(\u0012;pj\u001c)=g(\u0012jp;\u001c)\u0002g(pj\u001c)\n",
      "=g(\u001cj\u0012;p)g(\u0012jp)\n",
      "g(\u001cjp)|               {z               }\n",
      "posterior of \u0012given model p\u0002g(\u001cjp)g(p)\n",
      "g(\u001c)|        {z        }\n",
      "posterior of model p:\n",
      "The model evidence for a ﬁxed pis now interpreted as the prior predictive density of \u001c,\n",
      "conditional on the model p:\n",
      "g(\u001cjp)=Z\n",
      "g(\u001cj\u0012;p)g(\u0012jp) d\u0012;\n",
      "and the quantity g(\u001c)=Pm\n",
      "p=1g(\u001cjp)g(p) is interpreted as the marginal likelihood of all the\n",
      "mcandidate models. Finally, a simple method for model selection is to pick the index bp\n",
      "with the largest posterior probability:\n",
      "bp=argmax\n",
      "pg(pj\u001c)=argmax\n",
      "pg(\u001cjp)g(p):\n",
      "Example 2.11 (Polynomial Regression (cont.)) Let us revisit Example 2.10 by giving\n",
      "the parameter p=1;:::; m, with m=10, a Bayesian treatment. Recall that we used the\n",
      "notation\u001c=yin that example. We assume that the prior g(p)=1=mis ﬂat and uninform-\n",
      "ative so that the posterior is given by\n",
      "g(pjy)_g(yjp)=j\u0006j1=2\u0000(n=2)\n",
      "jDj1=2(\u0019(n+p+2)\u001b2)n=2;Chapter 2. Statistical Learning 57\n",
      "where all quantities in g(yjp) are computed using the ﬁrst pcolumns of X. Figure 2.16\n",
      "shows the resulting posterior density g(pjy). The ﬁgure also shows the posterior density\n",
      "bg(yjp)\u000eP10\n",
      "p=1bg(yjp);where\n",
      "bg(yjp) :=exp \n",
      "\u0000n[ln(2\u0019\u001b2)+1]+(p+1) ln( n)+(p+2)\n",
      "2!\n",
      "is derived from the BIC approximation (2.52). In both cases, there is a clear maximum at\n",
      "p=4, suggesting that a third-degree polynomial is the most appropriate model for the\n",
      "data.\n",
      "1 2 3 4 5 6 7 8 9 1000.20.40.60.81\n",
      "Figure 2.16: Posterior probabilities for each polynomial model of degree p\u00001.\n",
      "Suppose that we wish to compare two models, say model p=1 and model p=2.\n",
      "Instead of computing the posterior g(pj\u001c) explicitly, we can compare the posterior odds\n",
      "ratio:\n",
      "g(p=1j\u001c)\n",
      "g(p=2j\u001c)=g(p=1)\n",
      "g(p=2)\u0002g(\u001cjp=1)\n",
      "g(\u001cjp=2)|        {z        }\n",
      "Bayes factor B1j2:\n",
      "This gives rise to the Bayes factor Bayes factor Bijj, whose value signiﬁes the strength of the evidence\n",
      "in favor of model iover model j. In particular Bijj>1 means that the evidence in favor for\n",
      "model iis larger.\n",
      "Example 2.12 (Savage–Dickey Ratio) Suppose that we have two models. Model p=\n",
      "2 has a likelihood g(\u001cj\u0016;\u0017;p=2), depending on two parameters. Model p=1 has the\n",
      "same functional form for the likelihood but now \u0017is ﬁxed to some (known) \u00170; that\n",
      "is,g(\u001cj\u0016;p=1)=g(\u001cj\u0016;\u0017=\u00170;p=2). We also assume that the prior information on \u001658 Exercises\n",
      "for model 1 is the same as that for model 2, conditioned on \u0017=\u00170. That is, we assume\n",
      "g(\u0016jp=1)=g(\u0016j\u0017=\u00170;p=2). As model 2 contains model 1 as a special case, the latter\n",
      "is said to be nested inside model 2. We can formally write (see also Exercise 26):\n",
      "g(\u001cjp=1)=Z\n",
      "g(\u001cj\u0016;p=1)g(\u0016jp=1) d\u0016\n",
      "=Z\n",
      "g(\u001cj\u0016;\u0017=\u00170;p=2)g(\u0016j\u0017=\u00170;p=2) d\u0016\n",
      "=g(\u001cj\u0017=\u00170;p=2)=g(\u001c;\u0017=\u00170jp=2)\n",
      "g(\u0017=\u00170jp=2):\n",
      "Hence, the Bayes factor simpliﬁes to\n",
      "B1j2=g(\u001cjp=1)\n",
      "g(\u001cjp=2)=g(\u001c;\u0017=\u00170jp=2)\n",
      "g(\u0017=\u00170jp=2)\u001e\n",
      "g(\u001cjp=2)=g(\u0017=\u00170j\u001c;p=2)\n",
      "g(\u0017=\u00170jp=2):\n",
      "In other words, B1j2is the ratio of the posterior density to the prior density of \u0017, evaluated at\n",
      "\u0017=\u00170and both under the unrestricted model p=2. This ratio of posterior to prior densities\n",
      "is called the Savage–Dickey density ratio Savage –Dickey\n",
      "density ratio.\n",
      "Whether to use a classical (frequentist) or Bayesian model is largely a question of con-\n",
      "venience. Classical inference is useful because it comes with a huge repository of ready-\n",
      "to-use results, and requires no (subjective) prior information on the parameters. Bayesian\n",
      "models are useful because the whole theory is based on the elegant Bayes’ formula, and\n",
      "uncertainty in the inference (e.g., conﬁdence intervals) can be quantiﬁed much more nat-\n",
      "urally (e.g., credible intervals). A usual practice is to “Bayesify” a classical model, simply\n",
      "by adding some prior information on the parameters.\n",
      "Further Reading\n",
      "A popular textbook on statistical learning is [55]. Accessible treatments of mathematical\n",
      "statistics can be found, for example, in [69], [74], and [124]. More advanced treatments\n",
      "are given in [10], [25], and [78]. A good overview of modern-day statistical inference\n",
      "is given in [36]. Classical references on pattern classiﬁcation and machine learning are\n",
      "[12] and [35]. For advanced learning theory including information theory and Rademacher\n",
      "complexity, we refer to [28] and [109]. An applied reference for Bayesian inference is [46].\n",
      "For a survey of numerical techniques relevant to computational statistics, see [90].\n",
      "Exercises\n",
      "1. Suppose that the loss function is the piecewise linear function\n",
      "Loss( y;by)=\u000b(by\u0000y)++\f(y\u0000by)+; \u000b;\f> 0;\n",
      "where c+is equal to cifc>0, and zero otherwise. Show that the minimizer of the risk\n",
      "`(g)=ELoss( Y;g(X)) satisﬁes\n",
      "P[Y<g\u0003(x)jX=x]=\f\n",
      "\u000b+\f:\n",
      "In other words, g\u0003(x) is the\f=(\u000b+\f) quantile of Y, conditional on X=x.Chapter 2. Statistical Learning 59\n",
      "2. Show that, for the squared-error loss, the approximation error `(gG)\u0000`(g\u0003) in (2.16), is\n",
      "equal toE(gG(X)\u0000g\u0003(X))2. [Hint: expand `(gG)=E(Y\u0000g\u0003(X)+g\u0003(X)\u0000gG(X))2.]\n",
      "3. SupposeGis the class of linear functions. A linear function evaluated at a feature xcan\n",
      "be described as g(x)=\f>xfor some parameter vector \fof appropriate dimension. Denote\n",
      "gG(x)=x>\fGandgG\n",
      "\u001c(x)=x>b\f. Show that\n",
      "E\u0010\n",
      "gG\n",
      "\u001c(X)\u0000g\u0003(X)\u00112=E\u0010\n",
      "X>b\f\u0000X>\fG\u00112+E\u0010\n",
      "X>\fG\u0000g\u0003(X)\u00112:\n",
      "Hence, deduce that the statistical error in (2.16) is `(gG\n",
      "\u001c)\u0000`(gG)=E(gG\n",
      "\u001c(X)\u0000gG(X))2.\n",
      "4. Show that formula (2.24) holds for the 0–1 loss with 0–1 response.\n",
      "5. Let Xbe an n-dimensional normal random vector with mean vector \u0016and covariance\n",
      "matrix \u0006, where the determinant of \u0006is non-zero. Show that Xhas joint probability density\n",
      "fX(x)=1p(2\u0019)nj\u0006je\u00001\n",
      "2(x\u0000\u0016)>\u0006\u00001(x\u0000\u0016);x2Rn:\n",
      "6. Letb\f=A+y. Using the deﬁning properties of the pseudo-inverse, show that for any +362\n",
      "\f2Rp,\n",
      "kAb\f\u0000yk6kA\f\u0000yk:\n",
      "7. Suppose that in the polynomial regression Example 2.1 we select the linear class of\n",
      "functionsGpwith p>4. Then, g\u00032G pand the approximation error is zero, because\n",
      "gGp(x)=g\u0003(x)=x>\f, where\f=[10;\u0000140;400;\u0000250;0;:::; 0]>2Rp. Use the tower\n",
      "property to show that the learner g\u001c(x)=x>b\fwithb\f=X+y, assuming rank( X)>4, is +433\n",
      "unbiased unbiased :\n",
      "EgT(x)=g\u0003(x):\n",
      "8. (Exercise 7 continued.) Observe that the learner gTcan be written as a linear combina-\n",
      "tion of the response variable: gT(x)=x>X+Y. Prove that for any learner of the form x>Ay,\n",
      "where A2Rp\u0002nis some matrix and that satisﬁes EX[x>AY]=g\u0003(x), we have\n",
      "VarX[x>X+Y]6VarX[x>AY];\n",
      "where the equality is achieved for A=X+. This is called the Gauss–Markov inequality Gauss–Markov\n",
      "inequality.\n",
      "Hence, using the Gauss–Markov inequality deduce that for the unconditional variance:\n",
      "VargT(x)6Var[x>AY]:\n",
      "Deduce that A=X+also minimizes the expected generalization risk.\n",
      "9. Consider again the polynomial regression Example 2.1. Use the fact that EXb\f=X+h\u0003(u),\n",
      "where h\u0003(u)=E[YjU=u]=[h\u0003(u1);:::; h\u0003(un)]>, to show that the expected in-sample\n",
      "risk is:\n",
      "EX`in(gT)=`\u0003+kh\u0003(u)k2\u0000kXX+h\u0003(u)k2\n",
      "n+`\u0003p\n",
      "n:\n",
      "Also, use Theorem C.2 to show that the expected statistical error is: +432\n",
      "EX(b\f\u0000\f)>Hp(b\f\u0000\f)=`\u0003tr(X+(X+)>Hp)+(X+h\u0003(u)\u0000\f)>Hp(X+h\u0003(u)\u0000\f):60 Exercises\n",
      "10. Consider the setting of the polynomial regression in Example 2.2. Use Theorem C.19\n",
      "to prove that +451\n",
      "pn(b\fn\u0000\fp)d\u0000!N\u0010\n",
      "0; `\u0003H\u00001\n",
      "p+H\u00001\n",
      "pMpH\u00001\n",
      "p\u0011\n",
      "; (2.53)\n",
      "where Mp:=E[XX>(g\u0003(X)\u0000gGp(X))2] is the matrix with ( i;j)-th entry:\n",
      "Z1\n",
      "0ui+j\u00002(hHp(u)\u0000h\u0003(u))2du;\n",
      "andH\u00001\n",
      "pis the p\u0002p inverse Hilbert matrix inverse Hilbert\n",
      "matrixwith ( i;j)-th entry:\n",
      "(\u00001)i+j(i+j\u00001) p+i\u00001\n",
      "p\u0000j! p+j\u00001\n",
      "p\u0000i! i+j\u00002\n",
      "i\u00001!2\n",
      ":\n",
      "Observe that Mp=0forp>4, so that the matrix Mpterm is due to choosing a restrictive\n",
      "classGpthat does not contain the true prediction function.\n",
      "11. In Example 2.2 we saw that the statistical error can be expressed (see (2.20)) as\n",
      "Z1\n",
      "0\u0010\n",
      "[1;:::; up\u00001](b\f\u0000\fp)\u00112du=(b\f\u0000\fp)>Hp(b\f\u0000\fp):\n",
      "By Exercise 10 the random vector Zn:=pn(b\fn\u0000\fp) has asymptotically a multivariate\n",
      "normal distribution with mean vector 0and covariance matrix V:=`\u0003H\u00001\n",
      "p+H\u00001\n",
      "pMpH\u00001\n",
      "p.\n",
      "Use Theorem C.2 to show that the expected statistical error is asymptotically +432\n",
      "E(b\f\u0000\fp)>Hp(b\f\u0000\fp)'`\u0003p\n",
      "n+tr(MpH\u00001\n",
      "p)\n",
      "n;n!1: (2.54)\n",
      "Plot this large-sample approximation of the expected statistical error and compare it with\n",
      "the outcome of the statistical error.\n",
      "We note a subtle technical detail: In general, convergence in distribution does not imply\n",
      "convergence in Lp-norm (see Example C.6), and so here we have implicitly assumed that +444\n",
      "kZnkd\u0000!Dist:)kZnkL2\u0000!constant : =lim n\"1EkZnk.\n",
      "12. Consider again Example 2.2. The result in (2.53) suggests that Eb\f!\fpasn!1 ,\n",
      "where\fpis the solution in the class Gpgiven in (2.18). Thus, the large-sample approxim-\n",
      "ation of the pointwise bias of the learner gGp\n",
      "T(x)=x>b\fatx=[1;:::; up\u00001]>is\n",
      "EgGp\n",
      "T(x)\u0000g\u0003(x)'[1;:::; up\u00001]\fp\u0000[1;u;u2;u3]\f\u0003;n!1:\n",
      "Use Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared\n",
      "bias of the learner for p2f1;2;3g. Note how the bias is larger near the endpoints u=0\n",
      "andu=1. Explain why the areas under the curves correspond to the approximation errors.Chapter 2. Statistical Learning 61\n",
      "0 0.2 0.4 0.6 0.8 1050100150200250\n",
      "Figure 2.17: The large-sample pointwise squared bias of the learner for p=1;2;3. The\n",
      "bias is zero for p>4.\n",
      "13. For our running Example 2.2 we can use (2.53) to derive a large-sample approximation\n",
      "of the pointwise variance of the learner gT(x)=x>b\fn. In particular, show that for large n\n",
      "VargT(x)'`\u0003x>H\u00001\n",
      "px\n",
      "n+x>H\u00001\n",
      "pMpH\u00001\n",
      "px\n",
      "n;n!1: (2.55)\n",
      "Figure 2.18 shows this (large-sample) variance of the learner for di \u000berent values of the\n",
      "predictor uand model index p. Observe that the variance ultimately increases in pand that\n",
      "it is smaller at u=1=2 than closer to the endpoints u=0 or u=1. Since the bias is also\n",
      "9\n",
      "7\n",
      "5\n",
      "1\n",
      "2\n",
      "0.05\n",
      "3\n",
      "34\n",
      "0.5 10.95\n",
      "Figure 2.18: The pointwise variance of the learner for various pairs of pandu.\n",
      "larger near the endpoints, we deduce that the pointwise mean squared error (2.21) is larger\n",
      "near the endpoints of the interval [0 ;1] than near its middle. In other words, the error is\n",
      "much smaller in the center of the data cloud than near its periphery.62 Exercises\n",
      "14. Let h:x7!Rbe a convex function and let Xbe a random variable. Use the subgradi-\n",
      "ent deﬁnition of convexity to prove Jensen’s inequality : +405\n",
      "Jensen ’s\n",
      "inequality Eh(X)>h(EX): (2.56)\n",
      "15. Using Jensen’s inequality, show that the Kullback–Leibler divergence between prob-\n",
      "ability densities fandgis always positive; that is,\n",
      "Elnf(X)\n",
      "g(X)>0;\n",
      "where X\u0018f.\n",
      "16. The purpose of this exercise is to prove the following Vapnik–Chernovenkis bound Vapnik –\n",
      "Chernovenkis\n",
      "bound: for\n",
      "anyﬁnite classG(containing only a ﬁnite number jGjof possible functions) and a general\n",
      "bounded loss function, l6Loss6u, the expected statistical error is bounded from above\n",
      "according to:\n",
      "E`(gG\n",
      "Tn)\u0000`(gG)6(u\u0000l)p2 ln(2jGj)pn: (2.57)\n",
      "Note how this bound conveniently does not depend on the distribution of the training set\n",
      "Tn(which is typically unknown), but only on the complexity (i.e., cardinality) of the class\n",
      "G. We can break up the proof of (2.57) into the following four parts:\n",
      "(a) For a general function class G, training setT, risk function `, and training loss `T,\n",
      "we have, by deﬁnition, `(gG)6`(g) and`T(gG\n",
      "T)6`T(g) for all g2G. Show that\n",
      "`(gG\n",
      "T)\u0000`(gG)6sup\n",
      "g2Gj`T(g)\u0000`(g)j+`T(gG)\u0000`(gG);\n",
      "where we used the notation sup (supremum) for the least upper bound. Since\n",
      "E`T(g)=E`(g), we obtain, after taking expectations on both sides of the inequal-\n",
      "ity above:\n",
      "E`(gG\n",
      "T)\u0000`(gG)6Esup\n",
      "g2Gj`T(g)\u0000`(g)j:\n",
      "(b) If Xis a zero-mean random variable taking values in the interval [ l;u], then the fol-\n",
      "lowing Hoe\u000bding’s inequality Hoeffding ’s\n",
      "inequalitystates that the moment generating function satisﬁes\n",
      "EetX6exp t2(u\u0000l)2\n",
      "8!\n",
      ";t2R: (2.58)\n",
      "Prove this result by using the fact that the line segment joining points ( l;exp(tl)) and\n",
      "(u;exp(tu)) bounds the convex function x7!exp(tx) for x2[l;u]; that is:\n",
      "etx6etlu\u0000x\n",
      "u\u0000l+etux\u0000l\n",
      "u\u0000l;x2[l;u]:\n",
      "(c) Let Z1;:::; Znbe (possibly dependent and non-identically distributed) zero-mean ran-\n",
      "dom variables with moment generating functions that satisfy Eexp(tZk)6exp(t2\u00112=2)\n",
      "for all kand some parameter \u0011. Use Jensen’s inequality (2.56) to prove that for any +429Chapter 2. Statistical Learning 63\n",
      "t>0,\n",
      "Emax\n",
      "kZk=1\n",
      "tEln max\n",
      "ketZk61\n",
      "tlnn+t\u00112\n",
      "2:\n",
      "From this derive that\n",
      "Emax\n",
      "kZk6\u0011p\n",
      "2 lnn:\n",
      "Finally, show that this last inequality implies that\n",
      "Emax\n",
      "kjZkj6\u0011p\n",
      "2 ln(2 n): (2.59)\n",
      "(d) Returning to the objective of this exercise, denote the elements of Gbyg1;:::; gjGj,\n",
      "and let Zk=`Tn(gk)\u0000`(gk). By part (a) it is su \u000ecient to bound Emax kjZkj. Show that\n",
      "thefZkgsatisfy the conditions of (c) with \u0011=(u\u0000l)=pn. For this you will need to\n",
      "apply part (b) to the random variable Loss( g(X);Y)\u0000`(g), where ( X;Y) is a generic\n",
      "data point. Now complete the proof of (2.57).\n",
      "17. Consider the problem in Exercise 16a above. Show that\n",
      "j`T(gG\n",
      "T)\u0000`(gG)j62 sup\n",
      "g2Gj`T(g)\u0000`(g)j+`T(gG)\u0000`(gG):\n",
      "From this, conclude:\n",
      "Ej`T(gG\n",
      "T)\u0000`(gG)j62Esup\n",
      "g2Gj`T(g)\u0000`(g)j:\n",
      "The last bound allows us to assess how close the training loss `T(gG\n",
      "T) is to the optimal risk\n",
      "`(gG) within classG.\n",
      "18. Show that for the normal linear model Y\u0018N(X\f;\u001b2In), the maximum likelihood es-\n",
      "timator of\u001b2is identical to the method of moments estimator (2.37).\n",
      "19. Let X\u0018Gamma (\u000b;\u0015). Show that the pdf of Z=1=Xis equal to\n",
      "\u0015\u000b(z)\u0000\u000b\u00001e\u0000\u0015(z)\u00001\n",
      "\u0000(\u000b);z>0:\n",
      "20. Consider the sequence w0;w1;:::, where w0=g(\u0012) is a non-degenerate initial guess\n",
      "andwt(\u0012)_wt\u00001(\u0012)g(\u001cj\u0012);t>1. We assume that g(\u001cj\u0012) is not the constant function (with\n",
      "respect to\u0012) and that the maximum likelihood value\n",
      "g(\u001cjb\u0012)=max\n",
      "\u0012g(\u001cj\u0012)<1\n",
      "exists (is bounded). Let\n",
      "lt:=Z\n",
      "g(\u001cj\u0012)wt(\u0012) d\u0012:\n",
      "Show thatfltgis a strictly increasing and bounded sequence. Hence, conclude that its limit\n",
      "isg(\u001cjb\u0012).64 Exercises\n",
      "21. Consider the Bayesian model for \u001c=fx1;:::; xngwith likelihood g(\u001cj\u0016) such that\n",
      "(X1;:::; Xnj\u0016)\u0018iidN(\u0016;1) and prior pdf g(\u0016) such that \u0016\u0018N(\u0017;1) for some hyperpara-\n",
      "meter\u0017. Deﬁne a sequence of densities wt(\u0016);t>2 via wt(\u0016)_wt\u00001(\u0016)g(\u001cj\u0016), start-\n",
      "ing with w1(\u0016)=g(\u0016). Let atand btdenote the mean and precision4of\u0016under the\n",
      "posterior gt(\u0016j\u001c)_g(\u001cj\u0016)wt(\u0016). Show that gt(\u0016j\u001c) is a normal density with precision\n",
      "t:=n=(bt\u00001+n).eand mean at=(1\u0000\n",
      "Hence, deduce that gt(\u0016j\u001c) converges to a degenerate density with a point-mass at xn.\n",
      "22. Consider again Example 2.8, where we have a normal model with improper prior\n",
      "g(\u0012)=g(\u0016;\u001b2)_1=\u001b2. Show that the prior predictive pdf is an improper density g(x)_1,\n",
      "but that the posterior predictive density is\n",
      "g(xj\u001c)_ \n",
      "1+(x\u0000xn)2\n",
      "(n+1)S2\n",
      "n!\u0000n=2\n",
      ":\n",
      "Deduce thatX\u0000xn\n",
      "Snp(n+1)=(n\u00001)\u0018tn\u00001.\n",
      "23. Assuming that X1;:::; Xniid\u0018f, show that (2.48) holds and that `\u0003\n",
      "n=\u0000nElnf(X).\n",
      "24. Suppose that \u001c=fx1;:::; xngare observations of iid continuous and strictly positive\n",
      "random variables, and that there are two possible models for their pdf. The ﬁrst model\n",
      "p=1 is\n",
      "g(xj\u0012;p=1)=\u0012exp(\u0000\u0012x)\n",
      "and the second p=2 is\n",
      "g(xj\u0012;p=2)= 2\u0012\n",
      "\u0019!1=2\n",
      "exp \n",
      "\u0000\u0012x2\n",
      "2!\n",
      ":\n",
      "For both models, assume that the prior for \u0012is a gamma density\n",
      "g(\u0012)=bt\n",
      "\u0000(t)\u0012t\u00001exp(\u0000b\u0012);\n",
      "with the same hyperparameters bandt. Find a formula for the Bayes factor, g(\u001cjp=\n",
      "1)=g(\u001cjp=2), for comparing these models.\n",
      "25. Suppose that we have a total of mpossible models with prior probabilities g(p);p=\n",
      "1;:::; m. Show that the posterior probability of model g(pj\u001c) can be expressed in terms of\n",
      "all the p(p\u00001) Bayes factors:\n",
      "g(p=ij\u001c)=0BBBBBB@1+X\n",
      "j,ig(p=j)\n",
      "g(p=i)Bjji1CCCCCCA\u00001\n",
      ":\n",
      "4The precision is the reciprocal of the variance.Chapter 2. Statistical Learning 65\n",
      "26. Given the data \u001c=fx1;:::; xng, suppose that we use the likelihood ( Xj\u0012)\u0018N(\u0016;\u001b2)\n",
      "with parameter \u0012=(\u0016;\u001b2)>and wish to compare the following two nested models.\n",
      "(a) Model p=1, where\u001b2=\u001b2\n",
      "0is known and this is incorporated via the prior\n",
      "g(\u0012jp=1)=g(\u0016j\u001b2;p=1)g(\u001b2jp=1)=1p\n",
      "2\u0019\u001be\u0000(\u0016\u0000x0)2\n",
      "2\u001b2\u0002\u000e(\u001b2\u0000\u001b2\n",
      "0):\n",
      "(b) Model p=2, where both mean and variance are unknown with prior\n",
      "g(\u0012jp=2)=g(\u0016j\u001b2)g(\u001b2)=1p\n",
      "2\u0019\u001be\u0000(\u0016\u0000x0)2\n",
      "2\u001b2\u0002bt(\u001b2)\u0000t\u00001e\u0000b=\u001b2\n",
      "\u0000(t):\n",
      "Show that the prior g(\u0012jp=1) can be viewed as the limit of the prior g(\u0012jp=2) when\n",
      "t!1 andb=t\u001b2\n",
      "0. Hence, conclude that\n",
      "g(\u001cjp=1)=lim\n",
      "t!1\n",
      "b=t\u001b2\n",
      "0g(\u001cjp=2)\n",
      "and use this result to calculate B1j2. Check that the formula for B1j2agrees with the Savage–\n",
      "Dickey density ratio:\n",
      "g(\u001cjp=1)\n",
      "g(\u001cjp=2)=g(\u001b2=\u001b2\n",
      "0j\u001c)\n",
      "g(\u001b2=\u001b2\n",
      "0);\n",
      "where g(\u001b2j\u001c) and g(\u001b2) are the posterior and prior, respectively, under model p=2.66CHAPTER3\n",
      "MONTE CARLO METHODS\n",
      "Many algorithms in machine learning and data science make use of Monte Carlo\n",
      "techniques. This chapter gives an introduction to the three main uses of Monte Carlo\n",
      "simulation: to (1) simulate random objects and processes in order to observe their beha-\n",
      "vior, (2) estimate numerical quantities by repeated sampling, and (3) solve complicated\n",
      "optimization problems through randomized algorithms.\n",
      "3.1 Introduction\n",
      "Brieﬂy put, Monte Carlo simulation Monte Carlo\n",
      "simulationis the generation of random data by means of a com-\n",
      "puter. These data could arise from simple models, such as those described in Chapter 2,\n",
      "or from very complicated models describing real-life systems, such as the positions of\n",
      "vehicles on a complex road network, or the evolution of security prices in the stock mar-\n",
      "ket. In many cases, Monte Carlo simulation simply involves random sampling from certain\n",
      "probability distributions. The idea is to repeat the random experiment that is described by\n",
      "the model many times to obtain a large quantity of data that can be used to answer questions\n",
      "about the model. The three main uses of Monte Carlo simulation are:\n",
      "Sampling. Here the objective is to gather information about a random object by observing\n",
      "many realizations of it. For instance, this could be a random process that mimics the\n",
      "behavior of some real-life system such as a production line or telecommunications\n",
      "network. Another usage is found in Bayesian statistics, where Markov chains are\n",
      "often used to sample from a posterior distribution. + 48\n",
      "Estimation. In this case the emphasis is on estimating certain numerical quantities related\n",
      "to a simulation model. An example is the evaluation of multidimensional integrals\n",
      "via Monte Carlo techniques. This is achieved by writing the integral as the expecta-\n",
      "tion of a random variable, which is then approximated by the sample mean. Appeal-\n",
      "ing to the Law of Large Numbers guarantees that this approximation will eventually +448\n",
      "converge when the sample size becomes large.\n",
      "Optimization. Monte Carlo simulation is a powerful tool for the optimization of complic-\n",
      "ated objective functions. In many applications these functions are deterministic and\n",
      "6768 3.2. Monte Carlo Sampling\n",
      "randomness is introduced artiﬁcially in order to more e \u000eciently search the domain of\n",
      "the objective function. Monte Carlo techniques are also used to optimize noisy func-\n",
      "tions, where the function itself is random; for example, when the objective function\n",
      "is the output of a Monte Carlo simulation.\n",
      "The Monte Carlo method dramatically changed the way in which statistics is used in\n",
      "today’s analysis of data. The ever-increasing complexity of data requires radically di \u000berent\n",
      "statistical models and analysis techniques from those that were used 20 to 100 years ago.\n",
      "By using Monte Carlo techniques, the data analyst is no longer restricted to using basic\n",
      "(and often inappropriate) models to describe data. Now, any probabilistic model that can\n",
      "be simulated on a computer can serve as the basis for statistical analysis. This Monte Carlo\n",
      "revolution has had an impact on both Bayesian and frequentist statistics. In particular, in\n",
      "frequentist statistics, Monte Carlo methods are often referred to as resampling techniques.\n",
      "An important example is the well-known bootstrap method [37], where statistical quantit-\n",
      "ies such as conﬁdence intervals and P-values for statistical tests can simply be determined\n",
      "by simulation without the need of a sophisticated analysis of the underlying probability\n",
      "distributions; see, for example, [69] for basic applications. The impact on Bayesian statist-\n",
      "ics has been even more profound, through the use of Markov chain Monte Carlo (MCMC)\n",
      "techniques [87, 48]. MCMC samplers construct a Markov process which converges in dis-\n",
      "tribution to a desired (often high-dimensional) density. This convergence in distribution\n",
      "justiﬁes using a ﬁnite run of the Markov process as an approximate random realization\n",
      "from the target density. The MCMC approach has rapidly gained popularity as a versat-\n",
      "ile heuristic approximation, partly due to its simple computer implementation and inbuilt\n",
      "mechanism to tradeo \u000bbetween computational cost and accuracy; namely, the longer one\n",
      "runs the Markov process, the better the approximation. Nowadays, MCMC methods are\n",
      "indispensable for analyzing posterior distributions for inference and model selection; see\n",
      "also [50, 99].\n",
      "The following three sections elaborate on these three uses of Monte Carlo simulation\n",
      "in turn.\n",
      "3.2 Monte Carlo Sampling\n",
      "In this section we describe a variety of Monte Carlo sampling methods, from the building\n",
      "block of simulating uniform random numbers to MCMC samplers.\n",
      "3.2.1 Generating Random Numbers\n",
      "At the heart of any Monte Carlo method is a random number generator : a procedure thatrandom number\n",
      "generator produces a stream of uniform random numbers on the interval (0,1). Since such numbers\n",
      "are usually produced via deterministic algorithms, they are not truly random. However, for\n",
      "most applications all that is required is that such pseudo-random numbers are statistically\n",
      "indistinguishable from genuine random numbers U1;U2;:::that are uniformly distributed\n",
      "on the interval (0,1) and are independent of each other; we write U1;U2;:::\u0018iidU(0;1).\n",
      "For example, in Python the rand method of the numpy.random module is widely used for\n",
      "this purpose.Chapter 3. Monte Carlo Methods 69\n",
      "Most random number generators at present are based on linear recurrence relations.\n",
      "One of the most important random number generators is the multiple-recursive generatormultiple -\n",
      "recursive\n",
      "generator(MRG) of order k , which generates a sequence of integers Xk;Xk+1;:::via the linear recur-\n",
      "rence\n",
      "Xt=(a1Xt\u00001+\u0001\u0001\u0001+akXt\u0000k) mod m;t=k;k+1;::: (3.1)\n",
      "for some modulus m andmultipliersfai;i=1;:::; kg. Here “mod” refers to the modulo op-modulus\n",
      "multipliers eration: nmod mis the remainder when nis divided by m. The recurrence is initialized by\n",
      "specifying k“seeds”, X0;:::; Xk\u00001. To yield fast algorithms, all but a few of the multipliers\n",
      "should be 0. When mis a large integer, one can obtain a stream of pseudo-random numbers\n",
      "Uk;Uk+1;:::between 0 and 1 from the sequence Xk;Xk+1;:::, simply by setting Ut=Xt=m.\n",
      "It is also possible to set a small modulus, in particular m=2. The output function for such\n",
      "modulo 2 generators is then typically of the formmodulo 2\n",
      "generators\n",
      "Ut=wX\n",
      "i=1Xtw+i\u000012\u0000i\n",
      "for some w6k, e.g., w=32 or 64. Examples of modulo 2 generators are the feedback shift\n",
      "register generators, the most popular of which are the Mersenne twisters ; see, for example,feedback shift\n",
      "register\n",
      "Mersenne\n",
      "twisters[79] and [83]. MRGs with excellent statistical properties can be implemented e \u000eciently\n",
      "by combining several simpler MRGs and carefully choosing their respective moduli and\n",
      "multipliers. One of the most successful is L’Ecuyer’s MRG32k3a generator; see [77]. From\n",
      "now on, we assume that the reader has a sound random number generator available.\n",
      "3.2.2 Simulating Random Variables\n",
      "Simulating a random variable Xfrom an arbitrary (that is, not necessarily uniform) distri-\n",
      "bution invariably involves the following two steps:\n",
      "1. Simulate uniform random numbers U1;:::; Ukon (0;1) for some k=1;2;:::.\n",
      "2. Return X=g(U1;:::; Uk), where gis some real-valued function.\n",
      "The construction of suitable functions gis as much of an art as a science. Many\n",
      "simulation methods may be found, for example, in [71] and the accompanying website\n",
      "www.montecarlohandbook.org . Two of the most useful general procedures for gen-\n",
      "erating random variables are the inverse-transform method and the acceptance–rejection\n",
      "method. Before we discuss these, we show one possible way to simulate standard normal\n",
      "random variables. In Python we can generate standard normal random variables via the\n",
      "randn method of the numpy.random module.\n",
      "Example 3.1 (Simulating Standard Normal Random Variables) IfXandYare in-\n",
      "dependent standard normally distributed random variables (that is, X;Y\u0018iidN(0;1)), then\n",
      "their joint pdf is\n",
      "f(x;y)=1\n",
      "2\u0019e\u00001\n",
      "2(x2+y2);(x;y)2R2;\n",
      "which is a radially symmetric function. In Example C.2 we see that, in polar coordin- +435\n",
      "ates, the angle \u0002that the random vector [ X;Y]>makes with the positive x-axis is U(0;2\u0019)70 3.2. Monte Carlo Sampling\n",
      "distributed (as would be expected from the radial symmetry) and the radius Rhas pdf\n",
      "fR(r)=re\u0000r2=2;r>0. Moreover, Rand\u0002are independent. We will see shortly, in Ex-\n",
      "ample 3.4, that Rhas the same distribution asp\n",
      "\u00002 lnUwith U\u0018U(0;1). So, to sim- +72\n",
      "ulate X;Y\u0018iidN(0;1), the idea is to ﬁrst simulate Rand\u0002independently and then return\n",
      "X=Rcos(\u0002) and Y=Rsin(\u0002) as a pair of independent standard normal random variables.\n",
      "This leads to the Box–Muller approach for generating standard normal random variables.\n",
      "Algorithm 3.2.1: Normal Random Variable Simulation: Box–Muller Approach\n",
      "output: Independent standard normal random variables XandY.\n",
      "1Simulate two independent random variables, U1andU2, from U(0;1).\n",
      "2X (\u00002 lnU1)1=2cos(2\u0019U2)\n",
      "3Y (\u00002 lnU1)1=2sin(2\u0019U2)\n",
      "4return X;Y\n",
      "Once a standard normal number generator is available, simulation from any n-\n",
      "dimensional normal distribution N(\u0016;\u0006) is relatively straightforward. The ﬁrst step is to\n",
      "ﬁnd an n\u0002nmatrix Bthat decomposes \u0006into the matrix product BB>. In fact there exist\n",
      "many such decompositions. One of the more important ones is the Cholesky decomposition ,Cholesky\n",
      "decomposition which is a special case of the LU decomposition; see Section A.6.1 for more information\n",
      "+370 on such decompositions. In Python, the function cholesky ofnumpy.linalg can be used\n",
      "to produce such a matrix B.\n",
      "Once the Cholesky factorization is determined, it is easy to simulate X\u0018N(\u0016;\u0006) as,\n",
      "by deﬁnition, it is the a \u000ene transformation \u0016+BZof an n-dimensional standard normal\n",
      "random vector.\n",
      "Algorithm 3.2.2: Normal Random Vector Simulation\n",
      "input:\u0016;\u0006\n",
      "output: X\u0018N(\u0016;\u0006)\n",
      "1Determine the Cholesky factorization \u0006=BB>.\n",
      "2Simulate Z=[Z1;:::; Zn]>by drawing Z1;:::; Zn\u0018iidN(0;1).\n",
      "3X \u0016+BZ\n",
      "4return X\n",
      "Example 3.2 (Simulating from a Bivariate Normal Distribution) The Python code\n",
      "below draws N=1000 iid samples from the two bivariate ( n=2) normal pdfs in Fig-\n",
      "ure 2.13. The resulting point clouds are given in Figure 3.1. +45\n",
      "bvnormal.py\n",
      "import numpy as np\n",
      "from numpy.random import randn\n",
      "import matplotlib.pyplot as plt\n",
      "N = 1000\n",
      "r = 0.0 #change to 0.8 for other plot\n",
      "Sigma = np.array([[1, r], [r, 1]])Chapter 3. Monte Carlo Methods 71\n",
      "B = np.linalg.cholesky(Sigma)\n",
      "x = B @ randn(2,N)\n",
      "plt.scatter([x[0,:]],[x[1,:]], alpha =0.4, s = 4)\n",
      "2\n",
      " 0\n",
      " 2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      " 0\n",
      " 2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Figure 3.1: 1000 realizations of bivariate normal distributions with means zero, variances\n",
      "1, and correlation coe \u000ecients 0 (left) and 0 :8 (right).\n",
      "In some cases, the covariance matrix \u0006has special structure which can be exploited to\n",
      "create even faster generation algorithms, as illustrated in the following example.\n",
      "Example 3.3 (Simulating Normal Vectors in O(n2)Time) Suppose that the random\n",
      "vector X=[X1;:::; Xn]>represents the values at times t0+k\u000e,k=0;:::; n\u00001 of a zero-\n",
      "mean Gaussian process (X(t);t>0) that is weakly stationary , meaning that Cov(X(s);X(t)) +239\n",
      "depends only on t\u0000s. Then clearly the covariance matrix of X, say An, is a symmetric Toep-\n",
      "litz matrix. Suppose for simplicity that VarX(t)=1. Then the covariance matrix is in fact +381\n",
      "a correlation matrix, and will have the following structure:\n",
      "An:=266666666666666666666666641 a1::: an\u00002an\u00001\n",
      "a1 1::: an\u00002\n",
      ":::::::::::::::\n",
      "an\u00002::::::a1\n",
      "an\u00001an\u00002\u0001\u0001\u0001 a1 137777777777777777777777775:\n",
      "Using the Levinson–Durbin algorithm we can compute a lower diagonal matrix Lnand\n",
      "a diagonal matrix DninO(n2) time such that LnAnL>\n",
      "n=Dn; see Theorem A.14. If we +385\n",
      "simulate Zn\u0018N(0;In), then the solution Xof the linear system:\n",
      "LnX=D1=2\n",
      "nZn\n",
      "has the desired distribution N(0;An). The linear system is solved in O(n2) time via forward\n",
      "substitution.72 3.2. Monte Carlo Sampling\n",
      "3.2.2.1 Inverse-Transform Method\n",
      "LetXbe a random variable with cumulative distribution function (cdf) F. Let F\u00001denote\n",
      "the inverse1ofFandU\u0018U(0;1). Then,\n",
      "P[F\u00001(U)6x]=P[U6F(x)]=F(x): (3.2)\n",
      "This leads to the following method to simulate a random variable Xwith cdf F:\n",
      "Algorithm 3.2.3: Inverse-Transform Method\n",
      "input: Cumulative distribution function F.\n",
      "output: Random variable Xdistributed according to F.\n",
      "1Generate UfromU(0;1).\n",
      "2X F\u00001(U)\n",
      "3return X\n",
      "The inverse-transform method works both for continuous and discrete distribu-\n",
      "tions. After importing numpy asnp, simulating numbers 0 ;:::; k\u00001 according to\n",
      "probabilities p0;:::; pk\u00001can be done via np.min(np.where(np.cumsum(p) >\n",
      "np.random.rand())) , where pis the vector of the probabilities.\n",
      "Example 3.4 (Example 3.1 (cont.)) One remaining issue in Example 3.1 was how to\n",
      "simulate the radius Rwhen we only know its density fR(r)=re\u0000r2=2;r>0. We can use the\n",
      "inverse-transform method for this, but ﬁrst we need to determine its cdf. The cdf of Ris,\n",
      "by integration of the pdf,\n",
      "FR(r)=1\u0000e\u00001\n",
      "2r2;r>0;\n",
      "and its inverse is found by solving u=FR(r) in terms of r, giving\n",
      "F\u00001\n",
      "R(u)=p\n",
      "\u00002 ln(1\u0000u);u2(0;1):\n",
      "Thus Rhas the same distribution asp\u00002 ln(1\u0000U), with U\u0018U(0;1). Since 1\u0000Ualso has\n",
      "aU(0;1) distribution, Rhas also the same distribution asp\n",
      "\u00002 lnU.\n",
      "3.2.2.2 Acceptance–Rejection Method\n",
      "The acceptance–rejection method is used to sample from a “di \u000ecult” probability density\n",
      "function (pdf) f(x) by generating instead from an “easy” pdf g(x) satisfying f(x)6C g(x)\n",
      "for some constant C>1 (for example, via the inverse-transform method), and then ac-\n",
      "cepting or rejecting the drawn sample with a certain probability. Algorithm 3.2.4 gives the\n",
      "pseudo-code.\n",
      "The idea of the algorithm is to generate uniformly a point ( X;Y) under the graph of the\n",
      "function Cg, by ﬁrst drawing X\u0018gand then Y\u0018U(0;Cg(X)). If this point lies under the\n",
      "graph of f, then we accept Xas a sample from f; otherwise, we try again. The e \u000eciency\n",
      "of the acceptance–rejection method is usually expressed in terms of the probability of\n",
      "acceptance, which is 1 =C.\n",
      "1Every cdf has a unique inverse function deﬁned by F\u00001(u)=inffx:F(x)>ug. If, for each u, the\n",
      "equation F(x)=uhas a unique solution x, this deﬁnition coincides with the usual interpretation of the\n",
      "inverse function.Chapter 3. Monte Carlo Methods 73\n",
      "Algorithm 3.2.4: Acceptance–Rejection Method\n",
      "input: Pdfgand constant Csuch that Cg(x)>f(x) for all x.\n",
      "output: Random variable Xdistributed according to pdf f.\n",
      "1found false\n",
      "2while not found do\n",
      "3 Generate Xfrom g.\n",
      "4 Generate UfromU(0;1) independently of X.\n",
      "5 Y UCg (X)\n",
      "6 ifY6f(X)then found true\n",
      "7return X\n",
      "Example 3.5 (Simulating Gamma Random Variables) Simulating random variables\n",
      "from a Gamma (\u000b;\u0015) distribution is generally done via the acceptance–rejection method.\n",
      "Consider, for example, the Gamma distribution with \u000b=1:3 and\u0015=5:6. Its pdf, +427\n",
      "f(x)=\u0015\u000bx\u000b\u00001e\u0000\u0015x\n",
      "\u0000(\u000b);x>0;\n",
      "where \u0000is the gamma function \u0000(\u000b) :=R1\n",
      "0e\u0000xx\u000b\u00001dx,\u000b>0, is depicted by the blue solid\n",
      "curve in Figure 3.2.\n",
      "0 0.5 1 1.5 2012345\n",
      "Figure 3.2: The pdf gof the Exp(4) distribution multiplied by C=1:2 dominates the pdf f\n",
      "of the Gamma (1:3;5:6) distribution.\n",
      "This pdf happens to lie completely under the graph of Cg(x), where C=1:2 and\n",
      "g(x)=4 exp(\u00004x);x>0 is the pdf of the exponential distribution Exp(4). Hence, we\n",
      "can simulate from this particular Gamma distribution by accepting or rejecting a sample\n",
      "from the Exp(4) distribution according to Step 6 of Algorithm 3.2.4. Simulating from the +427\n",
      "Exp(4) distribution can be done via the inverse-transform method: simulate U\u0018U(0;1)\n",
      "and return X=\u0000ln(U)=4. The following Python code implements Algorithm 3.2.4 for this\n",
      "example.74 3.2. Monte Carlo Sampling\n",
      "accrejgamma.py\n",
      "from math import exp, gamma , log\n",
      "from numpy.random import rand\n",
      "alpha = 1.3\n",
      "lam = 5.6\n",
      "f = lambda x: lam**alpha * x**(alpha -1) * exp(-lam*x)/gamma(alpha)\n",
      "g = lambda x: 4*exp(-4*x)\n",
      "C = 1.2\n",
      "found = False\n",
      "while not found:\n",
      "x = - log(rand())/4\n",
      "if C*g(x)*rand() <= f(x):\n",
      "found = True\n",
      "print(x)\n",
      "3.2.3 Simulating Random Vectors and Processes\n",
      "Techniques for generating random vectors and processes are as diverse as the class of\n",
      "random processes themselves; see, for example, [71]. We highlight a few general scenarios.\n",
      "When X1;:::; Xnareindependent random variables with pdfs fi,i=1;:::; n, so that\n",
      "their joint pdf is f(x)=f1(x1)\u0001\u0001\u0001fn(xn), the random vector X=[X1;:::; Xn]>can be +431\n",
      "simply simulated by drawing each component Xi\u0018fiindividually — for example, via the\n",
      "inverse-transform method or acceptance–rejection.\n",
      "Fordependent components X1;:::; Xn, we can, as a consequence of the product rule of\n",
      "probability, represent the joint pdf f(x) as +433\n",
      "f(x)=f(x1;:::; xn)=f1(x1)f2(x2jx1)\u0001\u0001\u0001fn(xnjx1;:::; xn\u00001); (3.3)\n",
      "where f1(x1) is the marginal pdf of X1and fk(xkjx1;:::; xk\u00001) is the conditional pdf of Xk\n",
      "given X1=x1;X2=x2;:::; Xk\u00001=xk\u00001. Provided the conditional pdfs are known, one can\n",
      "generate Xby ﬁrst generating X1, then, given X1=x1, generate X2from f2(x2jx1), and so\n",
      "on, until generating Xnfrom fn(xnjx1;:::; xn\u00001).\n",
      "The latter method is particularly applicable for generating Markov chains. Recall from\n",
      "Section C.10 that a Markov chain is a stochastic process fXt;t=0;1;2;:::gthat satisﬁes +453\n",
      "Markov chain theMarkov property ; meaning that for all tandsthe conditional distribution of Xt+sgiven\n",
      "Xu;u6t, is the same as that of Xt+sgiven only Xt. As a result, each conditional density\n",
      "ft(xtjx1;:::; xt\u00001) can be written as a one-step transition density q t(xtjxt\u00001); that is, the\n",
      "probability density to go from state xt\u00001to state xtin one step. In many cases of interest\n",
      "the chain is time-homogeneous , meaning that the transition density qtdoes not depend on\n",
      "t. Such Markov chains can be generated sequentially , as given in Algorithm 3.2.5.Chapter 3. Monte Carlo Methods 75\n",
      "Algorithm 3.2.5: Simulate a Markov Chain\n",
      "input: Number of steps N, initial pdf f0, transition density q.\n",
      "1Draw X0from the initial pdf f0.\n",
      "2fort=1toNdo\n",
      "3 Draw Xtfrom the distribution corresponding to the density q(\u0001jXt\u00001)\n",
      "4return X0;:::; XN\n",
      "Example 3.6 (Markov Chain Simulation) For time-homogeneous Markov chains\n",
      "with a discrete state space, we can visualize the one-step transitions by means of a trans-\n",
      "ition graph transition\n",
      "graph, where arrows indicate possible transitions between states and the labels de-\n",
      "scribe the corresponding probabilities. Figure 3.3 shows (on the left) the transition graph\n",
      "of the Markov chain fXt;t=0;1;2;:::gwith state spacef1;2;3;4gand one-step transition\n",
      "matrix\n",
      "P=2666666666666640 0:2 0:5 0:3\n",
      "0:5 0 0:5 0\n",
      "0:3 0:7 0 0\n",
      "0:1 0 0 0 :9377777777777775:\n",
      "1 2\n",
      "3 40.2\n",
      "0.70.5\n",
      "0.90.50.3\n",
      "0.50.3\n",
      "0.1\n",
      "0 20 40 60 80 1001234\n",
      "Figure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.\n",
      "In the same ﬁgure (on the right) a typical outcome (path) of the Markov chain is\n",
      "shown. The path was simulated using the Python program below. In this implementation\n",
      "the Markov chain always starts in state 1. We will revisit Markov chains, and in particular\n",
      "Markov chains with continuous state spaces, in Section 3.2.5. + 78\n",
      "MCsim.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "n = 101\n",
      "P = np.array([[0, 0.2, 0.5, 0.3],\n",
      "[0.5, 0, 0.5, 0],\n",
      "[0.3, 0.7, 0, 0],\n",
      "[0.1, 0, 0, 0.9]])\n",
      "x = np.array(np.ones(n, dtype=int))\n",
      "x[0] = 0\n",
      "for t in range(0,n-1):76 3.2. Monte Carlo Sampling\n",
      "x[t+1] = np.min(np.where(np.cumsum(P[x[t],:]) >\n",
      "np.random.rand()))\n",
      "x = x + 1 #add 1 to all elements of the vector x\n",
      "plt.plot(np.array(range(0,n)),x, 'o')\n",
      "plt.plot(np.array(range(0,n)),x, '--')\n",
      "plt.show()\n",
      "3.2.4 Resampling\n",
      "The idea behind resampling is very simple: an iid sample \u001c:=fx1;:::; xngfrom someresamplingunknown cdf Frepresents our best knowledge of Fif we make no further a priori as-\n",
      "sumptions about it. If it is not possible to simulate more samples from F, the best way to\n",
      "“repeat” the experiment is to resample from the original data by drawing from the empir-\n",
      "ical cdf Fn; see (1.2). That is, we draw each xiwith equal probability and repeat this N +11\n",
      "times, according to Algorithm 3.2.6 below. As we draw here “with replacement”, multiple\n",
      "instances of the original data points may occur in the resampled data.\n",
      "Algorithm 3.2.6: Sampling from an Empirical Cdf.\n",
      "input: Original iid sample x1;:::; xnand sample size N.\n",
      "output: Iid sample X\u0003\n",
      "1;:::; X\u0003\n",
      "Nfrom the empirical cdf.\n",
      "1fort=1toNdo\n",
      "2 Draw U\u0018U(0;1)\n",
      "3 SetI dnUe\n",
      "4 SetX\u0003\n",
      "t xI\n",
      "5return X\u0003\n",
      "1;:::; X\u0003\n",
      "N\n",
      "In Step 3,dnUereturns the ceiling ofnU; that is, it is the smallest integer larger than\n",
      "or equal to nU. Consequently, Iis drawn uniformly at random from the set of indices\n",
      "f1;:::; ng.\n",
      "By sampling from the empirical cdf we can thus (approximately) repeat the experiment\n",
      "that gave us the original data as many times as we like. This is useful if we want to assess\n",
      "the properties of certain statistics obtained from the data. For example, suppose that the\n",
      "original data \u001cgave the statistic t(\u001c). By resampling we can gain information about the\n",
      "distribution of the corresponding random variable t(T).\n",
      "Example 3.7 (Quotient of Uniforms) LetU1;:::; Un;V1;:::; Vnbe iidU(0;1) random\n",
      "variables and deﬁne Xi=Ui=Vi,i=1;:::; n. Suppose we wish to investigate the distribu-\n",
      "tion of the sample median eXand sample mean Xof the (random) data T:=fX1;:::; Xng.\n",
      "Since we know the model for Texactly, we can generate a large number, Nsay, of inde-\n",
      "pendent copies of it, and for each of these copies evaluate the sample medians eX1;:::;eXN\n",
      "and sample means X1;:::; XN. For n=100 and N=1000 the empirical cdfs might look\n",
      "like the left and right curves in Figure 3.4, respectively. Contrary to what you might have\n",
      "expected, the distributions of the sample median and sample mean do not match at all. The\n",
      "sample median is quite concentrated around 1, whereas the distribution of the sample mean\n",
      "is much more spread out.Chapter 3. Monte Carlo Methods 77\n",
      "0 1 2 3 4 5 6 700.51\n",
      "Figure 3.4: Empirical cdfs of the medians of the resampled data (left curve) and sample\n",
      "means (right curve) of the resampled data.\n",
      "Instead of sampling completely new data, we could also reuse the original data by\n",
      "resampling them via Algorithm 3.2.6. This gives independent copies eX\u0003\n",
      "1;:::;eX\u0003\n",
      "Nand\n",
      "X\u0003\n",
      "1;:::; X\u0003\n",
      "N, for which we can again plot the empirical cdf. The results will be similar\n",
      "to the previous case. In fact, in Figure 3.4 the cdf of the resampled sample medians and\n",
      "sample means are plotted. The corresponding Python code is given below. The essential\n",
      "point of this example is that resampling of data can greatly add to the understanding of the\n",
      "probabilistic properties of certain measurements on the data, even if the underlying model\n",
      "is not known . See Exercise 12 for a further investigation of this example. +116\n",
      "quotunif.py\n",
      "import numpy as np\n",
      "from numpy.random import rand , choice\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.distributions.empirical_distribution import ECDF\n",
      "n = 100\n",
      "N = 1000\n",
      "x = rand(n)/rand(n) # data\n",
      "med = np.zeros(N)\n",
      "ave = np.zeros(N)\n",
      "for i in range(0,N):\n",
      "s = choice(x, n, replace=True) # resampled data\n",
      "med[i] = np.median(s)\n",
      "ave[i] = np.mean(s)\n",
      "med_cdf = ECDF(med)\n",
      "ave_cdf = ECDF(ave)\n",
      "plt.plot(med_cdf.x, med_cdf.y)\n",
      "plt.plot(ave_cdf.x, ave_cdf.y)\n",
      "plt.show()78 3.2. Monte Carlo Sampling\n",
      "3.2.5 Markov Chain Monte Carlo\n",
      "Markov chain Monte Carlo (MCMC) is a Monte Carlo sampling technique for (approxim-Markov chain\n",
      "Monte Carlo ately) generating samples from an arbitrary distribution — often referred to as the target\n",
      "target distribution. The basic idea is to run a Markov chain long enough such that its limiting\n",
      "distribution is close to the target distribution. Often such a Markov chain is constructed to\n",
      "be reversible, so that the detailed balance equations (C.43) can be used. Depending on the +455\n",
      "starting position of the Markov chain, the initial random variables in the Markov chain may\n",
      "have a distribution that is signiﬁcantly di \u000berent from the target (limiting) distribution. The\n",
      "random variables that are generated during this burn-in period burn -in period are often discarded. The\n",
      "remaining random variables form an approximate anddependent sample from the target\n",
      "distribution.\n",
      "In the next two sections we discuss two popular MCMC samplers: the Metropolis–\n",
      "Hastings sampler and the Gibbs sampler.\n",
      "3.2.5.1 Metropolis–Hastings Sampler\n",
      "The Metropolis–Hastings sampler [87] is similar to the acceptance–rejection method in +72\n",
      "that it simulates a trial state, which is then accepted or rejected according to some random\n",
      "mechanism. Speciﬁcally, suppose we wish to sample from a target pdf f(x), where xtakes\n",
      "values in some d-dimensional set. The aim is to construct a Markov chain fXt;t=0;1;:::g\n",
      "in such a way that its limiting pdf is f. Suppose the Markov chain is in state xat time t. A\n",
      "transition of the Markov chain from state xis carried out in two phases. First a proposal proposal\n",
      "state Yis drawn from a transition density q(\u0001jx). This state is accepted as the new state,\n",
      "with acceptance probability acceptance\n",
      "probability\n",
      "\u000b(x;y)=min(f(y)q(xjy)\n",
      "f(x)q(yjx);1)\n",
      "; (3.4)\n",
      "or rejected otherwise. In the latter case the chain remains in state x. The algorithm just\n",
      "described can be summarized as follows.\n",
      "Algorithm 3.2.7: Metropolis–Hastings Sampler\n",
      "input: Initial state X0, sample size N, target pdf f(x), proposal function q(yjx).\n",
      "output: X1;:::; XN(dependent), approximately distributed according to f(x).\n",
      "1fort=0toN\u00001do\n",
      "2 Draw Y\u0018q(yjXt) // draw a proposal\n",
      "3\u000b \u000b(Xt;Y) // acceptance probability as in (3.4)\n",
      "4 Draw U\u0018U(0;1)\n",
      "5 ifU6\u000bthen Xt+1 Y\n",
      "6 elseXt+1 Xt\n",
      "7return X1;:::; XN\n",
      "The fact that the limiting distribution of the Metropolis–Hastings Markov chain is equal\n",
      "to the target distribution (under general conditions) is a consequence of the following result.Chapter 3. Monte Carlo Methods 79\n",
      "Theorem 3.1: Local Balance for the Metropolis–Hastings Sampler\n",
      "The transition density of the Metropolis–Hastings Markov chain satisﬁes +455 the de-\n",
      "tailed balance equations.\n",
      "Proof: We prove the theorem for the discrete case only. Because a transition of the\n",
      "Metropolis–Hastings Markov chain consists of two steps, the one-step transition probabil-\n",
      "ity to go from xtoyis not q(yjx) but\n",
      "eq(yjx)=8>><>>:q(yjx)\u000b(x;y); ify,x;\n",
      "1\u0000P\n",
      "z,xq(zjx)\u000b(x;z);ify=x:(3.5)\n",
      "We thus need to show that\n",
      "f(x)eq(yjx)=f(y)eq(xjy) for all x;y: (3.6)\n",
      "With the acceptance probability as in (3.4), we need to check (3.6) for three cases:\n",
      "(a)x=y,\n",
      "(b)x,yandf(y)q(xjy)6f(x)q(yjx), and\n",
      "(c)x,yandf(y)q(xjy)>f(x)q(yjx).\n",
      "Case (a) holds trivially. For case (b), \u000b(x;y)=f(y)q(xjy)=(f(x)q(yjx)) and\u000b(y;x)=1.\n",
      "Consequently,\n",
      "eq(yjx)=f(y)q(xjy)=f(x) and eq(xjy)=q(xjy);\n",
      "so that (3.6) holds. Similarly, for case (c) we have \u000b(x;y)=1 and\u000b(y;x)=f(x)q(yjx)=\n",
      "(f(y)q(xjy)). It follows that,\n",
      "eq(yjx)=q(yjx) and eq(xjy)=f(x)q(yjx)=f(y);\n",
      "so that (3.6) holds again. \u0003\n",
      "Thus if the Metropolis–Hastings Markov chain is ergodic, then its limiting pdf is f(x). +454\n",
      "A fortunate property of the algorithm, which is important in many applications, is that in\n",
      "order to evaluate the acceptance probability \u000b(x;y) in (3.4), one only needs to know the\n",
      "target pdf f(x)up to a constant ; that is f(x)=cf(x) for some known function f(x) but\n",
      "unknown constant c.\n",
      "The e \u000eciency of the algorithm depends of course on the choice of the proposal trans-\n",
      "ition density q(yjx). Ideally, we would like q(yjx) to be “close” to the target f(y), irre-\n",
      "spective of x. We discuss two common approaches.\n",
      "1. Choose the proposal transition density q(yjx) independent of x; that is, q(yjx)=\n",
      "g(y) for some pdf g(y). An MCMC sampler of this type is called an independence\n",
      "sampler independence\n",
      "sampler. The acceptance probability is thus\n",
      "\u000b(x;y)=min(f(y)g(x)\n",
      "f(x)g(y);1)\n",
      ":80 3.2. Monte Carlo Sampling\n",
      "2. If the proposal transition density is symmetric (that is, q(yjx)=q(xjy)), then the\n",
      "acceptance probability has the simple form\n",
      "\u000b(x;y)=min(f(y)\n",
      "f(x);1)\n",
      "; (3.7)\n",
      "and the MCMC algorithm is called a random walk sampler . A typical example israndom walk\n",
      "sampler when, for a given current state x, the proposal state Yis of the form Y=x+Z,\n",
      "where Zis generated from some spherically symmetric distribution, such as N(0;I).\n",
      "We now give an example illustrating the second approach.\n",
      "Example 3.8 (Random Walk Sampler) Consider the two-dimensional pdf\n",
      "f(x1;x2)=ce\u00001\n",
      "4p\n",
      "x2\n",
      "1+x2\n",
      "2\u0012\n",
      "sin\u0012\n",
      "2q\n",
      "x2\n",
      "1+x2\n",
      "2\u0013\n",
      "+1\u0013\n",
      ";\u00002\u0019<x1<2\u0019;\u00002\u0019<x2<2\u0019;(3.8)\n",
      "where cis an unknown normalization constant. The graph of this pdf (unnormalized) is\n",
      "depicted in the left panel of Figure 3.5.\n",
      "-6 -4 -2 0 2 4 6-6-4-20246\n",
      "Figure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random\n",
      "walk sampler are approximately distributed according to the target pdf.\n",
      "The following Python program implements a random walk sampler to (approximately)\n",
      "draw N=104dependent samples from the pdf f. At each step, given a current state x,\n",
      "a proposal Yis drawn from the N(x;I) distribution. That is, Y=x+Z, with Zbivariate\n",
      "standard normal. We see in the right panel of Figure 3.5 that the sampler works correctly.\n",
      "The starting point for the Markov chain is chosen as (0 ;0). Note that the normalization\n",
      "constant cis never required to be speciﬁed in the program.\n",
      "rwsamp.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import pi, exp, sqrt , sin\n",
      "from numpy.random import rand , randnChapter 3. Monte Carlo Methods 81\n",
      "N = 10000\n",
      "a = lambda x: -2*pi < x\n",
      "b = lambda x: x < 2*pi\n",
      "f = lambda x1, x2: exp(-sqrt(x1**2+x2**2)/4)*(\n",
      "sin(2*sqrt(x1**2+x2**2))+1)*a(x1)*b(x1)*a(x2)*b(x2)\n",
      "xx = np.zeros((N,2))\n",
      "x = np.zeros((1,2))\n",
      "for i in range(1,N):\n",
      "y = x + randn(1,2)\n",
      "alpha = np.amin((f(y[0][0],y[0][1])/f(x[0][0],x[0][1]) ,1))\n",
      "r = rand() < alpha\n",
      "x = r*y + (1-r)*x\n",
      "xx[i,:] = x\n",
      "plt.scatter(xx[:,0], xx[:,1], alpha =0.4,s =2)\n",
      "plt.axis( 'equal ')\n",
      "plt.show()\n",
      "3.2.5.2 Gibbs Sampler\n",
      "The Gibbs sampler Gibbs sampler [48] uses a somewhat di \u000berent methodology from the Metropolis–\n",
      "Hastings algorithm and is particularly useful for generating n-dimensional random vectors.\n",
      "The key idea of the Gibbs sampler is to update the components of the random vector\n",
      "one at a time, by sampling them from conditional pdfs. Thus, Gibbs sampling can be\n",
      "advantageous if it is easier to sample from the conditional distributions than from the joint\n",
      "distribution.\n",
      "Speciﬁcally, suppose that we wish to sample a random vector X=[X1;:::; Xn]>ac-\n",
      "cording to a target pdf f(x). Let f(xijx1;:::; xi\u00001;xi+1;:::; xn) represent the conditional\n",
      "pdf2of the i-th component, Xi, given the other components x1;:::; xi\u00001;xi+1;:::; xn. The\n",
      "Gibbs sampling algorithm is as follows.\n",
      "Algorithm 3.2.8: Gibbs Sampler\n",
      "input: Initial point X0, sample size N, and target pdf f.\n",
      "output: X1;:::; XNapproximately distributed according to f.\n",
      "1fort=0toN\u00001do\n",
      "2 Draw Y1from the conditional pdf f(y1jXt;2;:::; Xt;n).\n",
      "3 fori=2tondo\n",
      "4 Draw Yifrom the conditional pdf f(yijY1;:::; Yi\u00001;Xt;i+1;:::; Xt;n).\n",
      "5 Xt+1 Y\n",
      "6return X1;:::; XN\n",
      "There exist many variants of the Gibbs sampler, depending on the steps required to\n",
      "update XttoXt+1— called the cycle of the Gibbs algorithm. In the algorithm above, thecycle\n",
      "2In this section we employ a Bayesian notation style, using the same letter ffor di \u000berent (conditional)\n",
      "densities.82 3.2. Monte Carlo Sampling\n",
      "cycle consists of Steps 2–5, in which the components are updated in a ﬁxed order 1 !2!\n",
      "\u0001\u0001\u0001! n. For this reason Algorithm 3.2.8 is also called the systematic Gibbs sampler .systematic\n",
      "Gibbs sampler In the random-order Gibbs sampler , the order in which the components are updated\n",
      "random -order\n",
      "Gibbs samplerin each cycle is a random permutation of f1;:::; ng(see Exercise 9). Other modiﬁcations\n",
      "+115are to update the components in blocks (i.e., several at the same time), or to update only\n",
      "a random selection of components. The variant where in each cycle only a single random\n",
      "component is updated is called the random Gibbs sampler . In the reversible Gibbs samplerrandom Gibbs\n",
      "sampler\n",
      "reversible\n",
      "Gibbs samplera cycle consists of the coordinate-wise updating 1 !2!\u0001\u0001\u0001! n\u00001!n!n\u00001!\n",
      "\u0001\u0001\u0001! 2!1. In all cases, except for the systematic Gibbs sampler, the resulting Markov\n",
      "chainfXt;t=1;2;:::gisreversible and hence its limiting distribution is precisely f(x).+454Unfortunately, the systematic Gibbs Markov chain is not reversible and so the detailed\n",
      "balance equations are not satisﬁed. However, a similar result holds, due to Hammersley and\n",
      "Cli\u000bord, under the so-called positivity condition : if at a point x=(x1;:::; xn) all marginal\n",
      "densities f(xi)>0;i=1;:::; n, then the joint density f(x)>0.\n",
      "Theorem 3.2: Hammersley–Cli \u000bord Balance for the Gibbs Sampler\n",
      "Letq1!n(yjx) denote the transition density of the systematic Gibbs sampler, and let\n",
      "qn!1(xjy) be the transition density of the reverse move, in the order n!n\u00001!\n",
      "\u0001\u0001\u0001! 1. Then, if the positivity condition holds,\n",
      "f(x)q1!n(yjx)=f(y)qn!1(xjy): (3.9)\n",
      "Proof: For the forward move we have:\n",
      "q1!n(yjx)=f(y1jx2;:::; xn)f(y2jy1;x3;:::; xn)\u0001\u0001\u0001f(ynjy1;:::; yn\u00001);\n",
      "and for the reverse move:\n",
      "qn!1(xjy)=f(xnjy1;:::; yn\u00001)f(xn\u00001jy1;:::; yn\u00002;xn)\u0001\u0001\u0001f(x1jx2;:::; xn):\n",
      "Consequently,\n",
      "q1!n(yjx)\n",
      "qn!1(xjy)=nY\n",
      "i=1f(yijy1;:::; yi\u00001;xi+1;:::; xn)\n",
      "f(xijy1;:::; yi\u00001;xi+1;:::; xn)\n",
      "=nY\n",
      "i=1f(y1;:::; yi;xi+1;:::; xn)\n",
      "f(y1;:::; yi\u00001;xi;:::; xn)\n",
      "=f(y)Qn\u00001\n",
      "i=1f(y1;:::; yi;xi+1;:::; xn)\n",
      "f(x)Qn\n",
      "j=2f(y1;:::; yj\u00001;xj;:::; xn)\n",
      "=f(y)Qn\u00001\n",
      "i=1f(y1;:::; yi;xi+1;:::; xn)\n",
      "f(x)Qn\u00001\n",
      "j=1f(y1;:::; yj;xj+1;:::; xn)=f(y)\n",
      "f(x):\n",
      "The result follows by rearranging the last identity. The positivity condition ensures that we\n",
      "do not divide by 0 along the line. \u0003\n",
      "Intuitively, the long-run proportion of transitions x!yfor the “forward move” chain\n",
      "is equal to the long-run proportion of transitions y!xfor the “reverse move” chain.Chapter 3. Monte Carlo Methods 83\n",
      "To verify that the Markov chain X0;X1;:::for the systematic Gibbs sampler indeed has\n",
      "limiting pdf f(x), we need to check that the global balance equations (C.42) hold. By +454\n",
      "integrating (in the continuous case) both sides in (3.9) with respect to x, we see that indeed\n",
      "Z\n",
      "f(x)q1!n(yjx) dx=f(y):\n",
      "Example 3.9 (Gibbs Sampler for the Bayesian Normal Model) Gibbs samplers are\n",
      "often applied in Bayesian statistics, to sample from the posterior pdf. Consider for instance\n",
      "the Bayesian normal model + 50\n",
      "f(\u0016;\u001b2)=1=\u001b2\n",
      "(xj\u0016;\u001b2)\u0018N(\u00161;\u001b2I):\n",
      "Here the prior for ( \u0016;\u001b2) isimproper . improper prior That is, it is not a pdf in itself, but by obstinately\n",
      "applying Bayes’ formula it does yield a proper posterior pdf. In some sense this prior\n",
      "conveys the least amount of information about \u0016and\u001b2. Following the same procedure as\n",
      "in Example 2.8, we ﬁnd the posterior pdf:\n",
      "f(\u0016;\u001b2jx)/\u0010\n",
      "\u001b2\u0011\u0000n=2\u00001exp(\n",
      "\u00001\n",
      "2P\n",
      "i(xi\u0000\u0016)2\n",
      "\u001b2)\n",
      ": (3.10)\n",
      "Note that\u0016and\u001b2here are the “variables” and xis a ﬁxed data vector. To simulate samples\n",
      "\u0016and\u001b2from (3.10) using the Gibbs sampler, we need the distributions of both ( \u0016j\u001b2;x)\n",
      "and (\u001b2j\u0016;x). To ﬁnd f(\u0016j\u001b2;x), view the right-hand side of (3.10) as a function of \u0016\n",
      "only, regarding \u001b2as a constant. This gives\n",
      "f(\u0016j\u001b2;x)/exp(\n",
      "\u0000n\u00162\u00002\u0016P\n",
      "ixi\n",
      "2\u001b2)\n",
      "=exp(\n",
      "\u0000\u00162\u00002\u0016x\n",
      "2(\u001b2=n))\n",
      "/exp(\n",
      "\u00001\n",
      "2(\u0016\u0000x)2\n",
      "\u001b2=n)\n",
      ": (3.11)\n",
      "This shows that ( \u0016j\u001b2;x) has a normal distribution with mean xand variance \u001b2=n.\n",
      "Similarly, to ﬁnd f(\u001b2j\u0016;x), view the right-hand side of (3.10) as a function of \u001b2,\n",
      "regarding\u0016as a constant. This gives\n",
      "f(\u001b2j\u0016;x)/(\u001b2)\u0000n=2\u00001exp8>><>>:\u00001\n",
      "2nX\n",
      "i=1(xi\u0000\u0016)2=\u001b29>>=>>;; (3.12)\n",
      "showing that ( \u001b2j\u0016;x) has an inverse-gamma distribution with parameters n=2 and +427Pn\n",
      "i=1(xi\u0000\u0016)2=2. The Gibbs sampler thus involves the repeated simulation of\n",
      "(\u0016j\u001b2;x)\u0018N\u0010\n",
      "x; \u001b2=n\u0011\n",
      "and (\u001b2j\u0016;x)\u0018InvGamma0BBBBB@n=2;nX\n",
      "i=1(xi\u0000\u0016)2=21CCCCCA:\n",
      "Simulating X\u0018InvGamma (\u000b;\u0015) is achieved by ﬁrst generating Z\u0018Gamma (\u000b;\u0015) and\n",
      "then returning X=1=Z.84 3.2. Monte Carlo Sampling\n",
      "In our parameterization of the Gamma (\u000b;\u0015) distribution, \u0015is the rate parameter.\n",
      "Many software packages instead use the scale parameter c=1=\u0015. Be aware of this\n",
      "when simulating Gamma random variables.\n",
      "The Python script below deﬁnes a small data set of size n=10 (which was randomly\n",
      "simulated from a standard normal distribution), and implements the systematic Gibbs\n",
      "sampler to simulate from the posterior distribution, using N=105samples.\n",
      "gibbsamp.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "x = np.array([[-0.9472, 0.5401, -0.2166, 1.1890, 1.3170,\n",
      "-0.4056, -0.4449, 1.3284, 0.8338, 0.6044]])\n",
      "n=x.size\n",
      "sample_mean = np.mean(x)\n",
      "sample_var = np.var(x)\n",
      "sig2 = np.var(x)\n",
      "mu=sample_mean\n",
      "N=10**5\n",
      "gibbs_sample = np.array(np.zeros((N, 2)))\n",
      "for k in range(N):\n",
      "mu=sample_mean + np.sqrt(sig2/n)*np.random.randn()\n",
      "V=np.sum((x-mu)**2)/2\n",
      "sig2 = 1/np.random.gamma(n/2, 1/V)\n",
      "gibbs_sample[k,:]= np.array([mu, sig2])\n",
      "plt.scatter(gibbs_sample[:,0], gibbs_sample[:,1],alpha =0.1,s =1)\n",
      "plt.plot(np.mean(x), np.var(x), 'wo')\n",
      "plt.show()\n",
      "-1 -0.5 0 0.5 1 1.5 2\n",
      "700.511.5^f(7jx)\n",
      "Figure 3.6: Left: approximate draws from the posterior pdf f(\u0016;\u001b2jx) obtained via the\n",
      "Gibbs sampler. Right: estimate of the posterior pdf f(\u0016jx).Chapter 3. Monte Carlo Methods 85\n",
      "The left panel of Figure 3.6 shows the ( \u0016;\u001b2) points generated by the Gibbs sampler.\n",
      "Also shown, via the white circle, is the point ( x;s2), where x=0:3798 is the sample mean\n",
      "ands2=0:6810 the sample variance. This posterior point cloud visualizes the considerable\n",
      "uncertainty in the estimates. By projecting the ( \u0016;\u001b2) points onto the \u0016-axis — that is,\n",
      "by ignoring the \u001b2values — one obtains (approximate) samples from the posterior pdf\n",
      "of\u0016; that is, f(\u0016jx). The right panel of Figure 3.6 shows a kernel density estimate (see\n",
      "Section 4.4) of this pdf. The corresponding 0 :025 and 0:975 sample quantiles were found +134\n",
      "to be\u00000:2054 and 0:9662, respectively, giving the 95% credible interval ( \u00000:2054;0:9662)\n",
      "for\u0016, which contains the true expectation 0. Similarly, an estimated 95% credible interval\n",
      "for\u001b2is (0:3218;2:2485), which contains the true variance 1.\n",
      "3.3 Monte Carlo Estimation\n",
      "In this section we describe how Monte Carlo simulation can be used to estimate complic-\n",
      "ated integrals, probabilities, and expectations. A number of variance reduction techniques\n",
      "are introduced as well, including the recent cross-entropy method.\n",
      "3.3.1 Crude Monte Carlo\n",
      "The most common setting for Monte Carlo estimation is the following: Suppose we wish to\n",
      "compute the expectation \u0016=EYof some (say continuous) random variable Ywith pdf f,\n",
      "but the integral EY=R\n",
      "y f(y) dyis di\u000ecult to evaluate. For example, if Yis a complicated\n",
      "function of other random variables, it would be di \u000ecult to obtain an exact expression for\n",
      "f(y). The idea of crude Monte Carlo — sometimes abbreviated as CMC — is to approx-crude Monte\n",
      "Carlo imate\u0016by simulating many independent copies Y1;:::; YNofYand then take their sample\n",
      "mean Yas an estimator of \u0016. All that is needed is an algorithm to simulate such copies.\n",
      "By the Law of Large Numbers, Yconverges to \u0016asN!1 , provided the expectation +448\n",
      "ofYexists. Moreover, by the Central Limit Theorem, Yapproximately has a N(\u0016;\u001b2=N) +449\n",
      "distribution for large N, provided that the variance \u001b2=VarY<1. This enables the con-\n",
      "struction of an approximate (1 \u0000\u000b)conﬁdence interval for\u0016:confidence\n",
      "interval \n",
      "Y\u0000z1\u0000\u000b=2Sp\n",
      "N;Y+z1\u0000\u000b=2Sp\n",
      "N!\n",
      "; (3.13)\n",
      "-quantile of themple standard deviation of the fYigandz\n",
      "N(0;1) distribution; see also Section C.13. Instead of specifying the conﬁdence interval, +459\n",
      "one often reports only the sample mean and the estimated standard error :S=p\n",
      "N, or theestimated\n",
      "standard error estimated relative error :S=(Yp\n",
      "N). The basic estimation procedure for independent data\n",
      "estimated\n",
      "relative erroris summarized in Algorithm 3.3.1 below.\n",
      "It is often the case that the output Yis a function of some underlying random vector or\n",
      "stochastic process; that is, Y=H(X), where His a real-valued function and Xis a random\n",
      "vector or process. The beauty of Monte Carlo for estimation is that (3.13) holds regardless\n",
      "of the dimension of X.86 3.3. Monte Carlo Estimation\n",
      "Algorithm 3.3.1: Crude Monte Carlo for Independent Data\n",
      "input: Simulation algorithm for Y\u0018f, sample size N, conﬁdence level 1 \u0000\u000b.\n",
      "output: Point estimate and approximate (1 \u0000\u000b) conﬁdence interval for \u0016=EY.\n",
      "1Simulate Y1;:::; YNiid\u0018f.\n",
      "2Y 1\n",
      "NPN\n",
      "i=1Yi\n",
      "3S2 1\n",
      "N\u00001PN\n",
      "i=1(Yi\u0000Y)2\n",
      "4return Yand the interval (3.13) .\n",
      "Example 3.10 (Monte Carlo Integration) InMonte Carlo integration , simulation isMonte Carlo\n",
      "integration used to evaluate complicated integrals. Consider, for example, the integral\n",
      "\u0016=Z1\n",
      "\u00001Z1\n",
      "\u00001Z1\n",
      "\u00001p\n",
      "jx1+x2+x3je\u0000(x2\n",
      "1+x2\n",
      "2+x2\n",
      "3)=2dx1dx2dx3:\n",
      "Deﬁning Y=jX1+X2+X3j1=2(2\u0019)3=2, with X1;X2;X3iid\u0018N(0;1), we can write \u0016=EY.\n",
      "Using the following Python program, with a sample size of N=106, we obtained an\n",
      "estimate Y=17:031 with an approximate 95% conﬁdence interval (17 :017;17:046).\n",
      "mcint.py\n",
      "import numpy as np\n",
      "from numpy import pi\n",
      "c = (2*pi)**(3/2)\n",
      "H = lambda x: c*np.sqrt(np.abs(np.sum(x,axis=1)))\n",
      "N = 10**6\n",
      "z = 1.96\n",
      "x = np.random.randn(N,3)\n",
      "y = H(x)\n",
      "mY = np.mean(y)\n",
      "sY = np.std(y)\n",
      "RE = sY/mY/np.sqrt(N)\n",
      "print( 'Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}) '.format(\n",
      "mY, mY*(1-z*RE), mY*(1+z*RE)))\n",
      "Estimate = 17.031, CI = (17.017 ,17.046)\n",
      "Example 3.11 (Example 2.1 (cont.)) We return to the bias–variance tradeo \u000bin Ex-\n",
      "ample 2.1. Figure 2.7 gives estimates of the (squared-error) generalization risk (2.5) as +26\n",
      "+29\n",
      "+23a function of the number of parameters in the model. But how accurate are these estim-\n",
      "ates? Because we know in this case the exact model for the data, we can use Monte Carlo\n",
      "simulation to estimate the generalization risk (for a ﬁxed training set) and the expected\n",
      "generalization risk (averaged over all training sets) precisely. All we need to do is repeat\n",
      "the data generation, ﬁtting, and validation steps many times and then take averages of the\n",
      "results. The following Python code repeats 100 times:\n",
      "1. Simulate the training set of size n=100.\n",
      "2. Fit models up to size k=8.Chapter 3. Monte Carlo Methods 87\n",
      "3. Estimate the test loss using a test set with the same sample size n=100.\n",
      "Figure 3.7 shows that there is some variation in the test losses, due to the randomness in\n",
      "both the training and test sets. To obtain an accurate estimate of the expected generalization\n",
      "risk (2.6), take the average of the test losses. We see that for k68 the estimate in Figure 2.7\n",
      "is close to the true expected generalization risk.\n",
      "1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      "Number of parameters p\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200Test loss\n",
      "Figure 3.7: Independent estimates of the test loss show some variability.\n",
      "CMCtestloss.py\n",
      "import numpy as np, matplotlib.pyplot as plt\n",
      "from numpy.random import rand , randn\n",
      "from numpy.linalg import solve\n",
      "def generate_data(beta , sig, n):\n",
      "u = rand(n, 1)\n",
      "y = (u ** np.arange(0, 4)) @ beta + sig * randn(n, 1)\n",
      "return u, y\n",
      "beta = np.array([[10, -140, 400, -250]]).T\n",
      "n = 100\n",
      "sig = 5\n",
      "betahat = {}\n",
      "plt.figure(figsize=[6,5])\n",
      "totMSE = np.zeros(8)\n",
      "max_p = 8\n",
      "p_range = np.arange(1, max_p + 1, 1)\n",
      "for N in range(0,100):88 3.3. Monte Carlo Estimation\n",
      "u, y = generate_data(beta , sig, n) #training data\n",
      "X = np.ones((n, 1))\n",
      "for p in p_range:\n",
      "if p > 1:\n",
      "X = np.hstack((X, u**(p-1)))\n",
      "betahat[p] = solve(X.T @ X, X.T @ y)\n",
      "u_test , y_test = generate_data(beta , sig, n) #test data\n",
      "MSE = []\n",
      "X_test = np.ones((n, 1))\n",
      "for p in p_range:\n",
      "if p > 1:\n",
      "X_test = np.hstack((X_test , u_test**(p-1)))\n",
      "y_hat = X_test @ betahat[p] # predictions\n",
      "MSE.append(np.sum((y_test - y_hat)**2/n))\n",
      "totMSE = totMSE + np.array(MSE)\n",
      "plt.plot(p_range , MSE, 'C0',alpha=0.1)\n",
      "plt.plot(p_range ,totMSE/N, 'r-o')\n",
      "plt.xticks(ticks=p_range)\n",
      "plt.xlabel( 'Number of parameters $p$ ')\n",
      "plt.ylabel( 'Test loss ')\n",
      "plt.tight_layout()\n",
      "plt.savefig( 'MSErepeatpy.pdf ',format= 'pdf')\n",
      "plt.show()\n",
      "3.3.2 Bootstrap Method\n",
      "The bootstrap method [37] combines CMC estimation with the resampling procedure of\n",
      "Section 3.2.4. The idea is as follows: Suppose we wish to estimate a number \u0016via some +76\n",
      "estimator Y=H(T), whereT:=fX1;:::; Xngis an iid sample from some unknown cdf\n",
      "F. It is assumed that Ydoes not depend on the order of the fXig. To assess the quality (for\n",
      "example, accuracy) of the estimator Y, one could draw independent replications T1;:::;TN\n",
      "ofTand ﬁnd sample estimates for quantities such as the variance VarY, the biasEY\u0000\u0016,\n",
      "and the mean squared error E(Y\u0000\u0016)2. However, it may be too time-consuming or simply\n",
      "not feasible to obtain such replications. An alternative is to resample the original data.\n",
      "To reiterate, given an outcome \u001c=fx1;:::; xngofT, we simulate an iid sample T\u0003:=\n",
      "fX\u0003\n",
      "1;:::; X\u0003\n",
      "ngfrom the empirical cdf Fn, via Algorithm 3.2.6 (hence the resampling size is +76\n",
      "N=nhere).\n",
      "The rationale is that the empirical cdf Fnis close to the actual cdf Fand gets closer as\n",
      "ngets larger. Hence, any quantities depending on F, such asEFg(Y), where gis a function,\n",
      "can be approximated by EFng(Y). The latter is usually still di \u000ecult to evaluate, but it can\n",
      "be simply estimated via CMC as\n",
      "1\n",
      "KKX\n",
      "i=1g(Y\u0003\n",
      "i);\n",
      "where Y\u0003\n",
      "1;:::; Y\u0003\n",
      "Kare independent random variables, each distributed as Y\u0003=H(T\u0003). This\n",
      "seemingly self-referent procedure is called bootstrapping — alluding to Baron von Mün-bootstrappingChapter 3. Monte Carlo Methods 89\n",
      "chausen, who pulled himself out of a swamp by his own bootstraps. As an example, the\n",
      "bootstrap estimate of the expectation of Yis\n",
      "cEY=Y\u0003=1\n",
      "KKX\n",
      "i=1Y\u0003\n",
      "i;\n",
      "which is simply the sample mean of fY\u0003\n",
      "ig. Similarly, the bootstrap estimate for VarYis the\n",
      "sample variance\n",
      "[VarY=1\n",
      "K\u00001KX\n",
      "i=1(Y\u0003\n",
      "i\u0000Y\u0003)2: (3.14)\n",
      "Bootstrap estimators for the bias and MSE are Y\u0003\u0000Yand1\n",
      "KPK\n",
      "i=1(Y\u0003\n",
      "i\u0000Y)2, respectively.\n",
      "Note that for these estimators the unknown quantity \u0016is replaced with its original estimator\n",
      "Y. Conﬁdence intervals can be constructed in the same fashion. We mention two variants:\n",
      "thenormal method and the percentile method . In the normal method, a 1 \u0000\u000bconﬁdencenormal method\n",
      "percentile\n",
      "methodinterval for \u0016is given by\n",
      "(Y\u0006z1\u0000\u000b=2S\u0003);\n",
      "where S\u0003is the bootstrap estimate of the standard deviation of Y; that is, the square root\n",
      "of (3.14). In the percentile method, the upper and lower bounds of the 1 \u0000\u000bconﬁdence\n",
      "interval for \u0016are given by the 1 \u0000\u000b=2 and\u000b=2 quantiles of Y, which in turn are estimated\n",
      "via the corresponding sample quantiles of the bootstrap sample fY\u0003\n",
      "ig.\n",
      "The following example illustrates the usefulness of the bootstrap method for ratio es-\n",
      "timation and also introduces the renewal reward process model for data.\n",
      "Example 3.12 (Bootstrapping the Ratio Estimator) A common scenario in stochastic\n",
      "simulation is that the output of the simulation consists of independent pairs of data\n",
      "(C1;R1);(C2;R2);:::, where each Cis interpreted as the length of a period of time — a so-\n",
      "called cycle — and Ris the reward obtained during that cycle. Such a collection of random\n",
      "variablesf(Ci;Ri)gis called a renewal reward process renewal\n",
      "reward process. Typically, the reward Ridepends on\n",
      "the cycle length Ci. Let Atbe the average reward earned by time t; that is, At=PNt\n",
      "i=1Ri=t,\n",
      "where Nt=maxfn:C1+\u0001\u0001\u0001+Cn6tgcounts the number of complete cycles at time t. It\n",
      "can be shown, see Exercise 20, that if the expectations of the cycle length and reward are +118\n",
      "ﬁnite, then Atconverges to the constant ER=EC. This ratio can thus be interpreted as the\n",
      "long-run average reward long -run\n",
      "average reward.\n",
      "Estimation of the ratio ER=ECfrom data ( C1;R1);:::; (Cn;Rn) is easy: take the ratio\n",
      "estimatorratio estimator\n",
      "A=R\n",
      "C:\n",
      "However, this estimator Ais not unbiased and it is not obvious how to derive conﬁdence\n",
      "intervals. Fortunately, the bootstrap method can come to the rescue: simply resample the\n",
      "pairsf(Ci;Ri)g, obtain ratio estimators A\u0003\n",
      "1;:::; A\u0003\n",
      "K, and from these compute quantities of\n",
      "interest such as conﬁdence intervals.\n",
      "As a concrete example, let us return to the Markov chain in Example 3.6. Recall that + 75\n",
      "the chain starts at state 1 at time 0. After a certain amount of time T1, the process returns\n",
      "to state 1. The time steps 0 ;:::; T1\u00001 form a natural “cycle” for this process, as from\n",
      "time T1onwards the process behaves probabilistically exactly the same as when it started,90 3.3. Monte Carlo Estimation\n",
      "independently ofX0;:::; XT1\u00001. Thus, if we deﬁne T0=0, and let Tibe the i-th time that\n",
      "the chain returns to state 1, then we can break up the time interval into independent cycles\n",
      "of lengths Ci=Ti\u0000Ti\u00001,i=1;2;:::. Now suppose that during the i-th cycle a reward\n",
      "Ri=Ti\u00001X\n",
      "t=Ti\u00001%t\u0000Ti\u00001r(Xt)\n",
      "is received, where r(i) is some ﬁxed reward for visiting state i2f1;2;3;4gand%2(0;1)\n",
      "is a discounting factor. Clearly, f(Ci;Ri)gis a renewal reward process. Figure 3.8 shows the\n",
      "outcomes of 1000 pairs ( C;R), using r(1)=4;r(2)=3;r(3)=10;r(4)=1, and%=0:9.\n",
      "0 10 20 30 40 50 60 70\n",
      "Cycle length0102030405060Reward\n",
      "Figure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes\n",
      "indicate the number of occurrences for a given pair. For example, (2,15.43) is the most\n",
      "likely pair here, occurring 186 out of a 1000 times. It corresponds to the cycle path 1 !\n",
      "3!2!1.\n",
      "The long-run average reward is estimated as 2.50 for our data. But how accurate is this\n",
      "estimate? Figure 3.9 shows a density plot of the bootstrapped ratio estimates, where we\n",
      "independently resampled the data pairs 1000 times.\n",
      "2.2\n",
      " 2.4\n",
      " 2.6\n",
      " 2.8\n",
      "long-run average reward\n",
      "0\n",
      "2\n",
      "4density\n",
      "Figure 3.9: Density plot of the bootstrapped ratio estimates for the Markov chain renewal\n",
      "reward process.Chapter 3. Monte Carlo Methods 91\n",
      "Figure 3.9 indicates that the true long-run average reward lies between 2.2 and 2.8\n",
      "with high conﬁdence. More precisely, the 99% bootstrap conﬁdence interval (percentile\n",
      "method) is here (2.27, 2.77). The following Python script spells out the procedure.\n",
      "ratioest.py\n",
      "import numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
      "from numba import jit\n",
      "np.random.seed(123)\n",
      "n = 1000\n",
      "P = np.array([[0, 0.2, 0.5, 0.3],\n",
      "[0.5 ,0, 0.5, 0],\n",
      "[0.3, 0.7, 0, 0],\n",
      "[0.1, 0, 0, 0.9]])\n",
      "r = np.array([4,3,10,1])\n",
      "Corg = np.array(np.zeros((n,1)))\n",
      "Rorg = np.array(np.zeros((n,1)))\n",
      "rho=0.9\n",
      "@jit() #for speed -up; see Appendix\n",
      "def generate_cyclereward(n):\n",
      "for i in range(n):\n",
      "t=1\n",
      "xreg = 1 #regenerative state (out of 1,2,3,4)\n",
      "reward = r[0]\n",
      "x= np.amin(np.argwhere(np.cumsum(P[xreg -1,:]) > np.random.\n",
      "rand())) + 1\n",
      "while x != xreg:\n",
      "t += 1\n",
      "reward += rho**(t-1)*r[x-1]\n",
      "x = np.amin(np.where(np.cumsum(P[x-1,:]) > np.random.rand\n",
      "())) + 1\n",
      "Corg[i] = t\n",
      "Rorg[i] = reward\n",
      "return Corg , Rorg\n",
      "Corg , Rorg = generate_cyclereward(n)\n",
      "Aorg = np.mean(Rorg)/np.mean(Corg)\n",
      "K = 5000\n",
      "A = np.array(np.zeros((K,1)))\n",
      "C = np.array(np.zeros((n,1)))\n",
      "R = np.array(np.zeros((n,1)))\n",
      "for i in range(K):\n",
      "ind = np.ceil(n*np.random.rand(1,n)).astype(int)[0]-1\n",
      "C = Corg[ind]\n",
      "R = Rorg[ind]\n",
      "A[i] = np.mean(R)/np.mean(C)\n",
      "plt.xlabel( 'long -run average reward ')\n",
      "plt.ylabel( 'density ')\n",
      "sns.kdeplot(A.flatten(),shade=True)\n",
      "plt.show()92 3.3. Monte Carlo Estimation\n",
      "3.3.3 Variance Reduction\n",
      "The estimation of performance measures in Monte Carlo simulation can be made more\n",
      "e\u000ecient by utilizing known information about the simulation model. Variance reduction\n",
      "techniques include antithetic variables, control variables, importance sampling, conditional\n",
      "Monte Carlo, and stratiﬁed sampling; see, for example, [71, Chapter 9]. We shall only deal\n",
      "with control variables and importance sampling here.\n",
      "Suppose Yis the output of a simulation experiment. A random variable eY, obtained\n",
      "from the same simulation run, is called a control variable control\n",
      "variableforYifYandeYare correlated\n",
      "(negatively or positively) and the expectation of eYis known. The use of control variables\n",
      "for variance reduction is based on the following theorem. We leave its proof to Exercise 21.\n",
      "+118\n",
      "Theorem 3.3: Control Variable Estimation\n",
      "LetY1;:::; YNbe the output of Nindependent simulation runs and let eY1;:::;eYNbe\n",
      "the corresponding control variables, with EeYk=e\u0016known. Let %Y;eYbe the correlation\n",
      "coe\u000ecient between each YkandeYk. For each\u000b2Rthe estimator\n",
      "b\u0016(c)=1\n",
      "NNX\n",
      "k=1h\n",
      "Yk\u0000\u000b\u0010eYk\u0000e\u0016\u0011i\n",
      "(3.15)\n",
      "is an unbiased estimator for \u0016=EY. The minimal variance of b\u0016(c)is\n",
      "Varb\u0016(c)=1\n",
      "N(1\u0000%2\n",
      "Y;eY)VarY; (3.16)\n",
      "which is obtained for \u000b=%Y;eYq\n",
      "VarY=VareY.\n",
      "From (3.16) we see that, by using the optimal \u000bin (3.15), the variance of the control\n",
      "variate estimator is a factor 1 \u0000%2\n",
      "Y;eYsmaller than the variance of the crude Monte Carlo\n",
      "estimator. Thus, if eYis highly correlated with Y, a signiﬁcant variance reduction can be\n",
      "achieved. The optimal \u000bis usually unknown, but it can be easily estimated from the sample\n",
      "covariance matrix of f(Yk;eYk)g. +458\n",
      "In the next example, we estimate the multiple integral in Example 3.10 using control\n",
      "variables.\n",
      "Example 3.13 (Monte Carlo Integration (cont.)) +86 The random variable Y=jX1+X2+\n",
      "X3j1=2(2\u0019)3=2is positively correlated with the random variable eY=X2\n",
      "1+X2\n",
      "2+X2\n",
      "3, for the\n",
      "same choice of X1;X2;X3iid\u0018N(0;1). AsEeY=Var(X1+X2+X3)=3, we can use it as a\n",
      "control variable to estimate the expectation of Y. The following Python program is based\n",
      "on Theorem 3.3. It imports the crude Monte Carlo sampling code from Example 3.10.Chapter 3. Monte Carlo Methods 93\n",
      "mcintCV.py\n",
      "from mcint import *\n",
      "Yc = np.sum(x**2, axis=1) # control variable data\n",
      "yc = 3 # true expectation of control variable\n",
      "C = np.cov(y,Yc) # sample covariance matrix\n",
      "cor = C[0][1]/np.sqrt(C[0][0]*C[1][1])\n",
      "alpha = C[0][1]/C[1][1]\n",
      "est = np.mean(y-alpha*(Yc-yc))\n",
      "RECV = np.sqrt((1-cor**2)*C[0][0]/N)/est #relative error\n",
      "print( 'Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}), Corr = {:3.3f} '.\n",
      "format(est, est*(1-z*RECV), est*(1+z*RECV),cor))\n",
      "Estimate = 17.045, CI = (17.032,17.057), Corr = 0.480\n",
      "A typical estimate of the correlation coe \u000ecient%Y;eYis 0.48, which gives a reduction of\n",
      "the variance with a factor 1 \u00000:482\u00190:77 — a simulation speed-up of 23% compared with\n",
      "crude Monte Carlo. Although the gain is small in this case, due to the modest correlation\n",
      "between YandeY, little extra work was required to achieve this variance reduction.\n",
      "One of the most important variance reduction techniques is importance sampling importance\n",
      "sampling. This\n",
      "technique is especially useful for the estimation of very small probabilities. The standard\n",
      "setting is the estimation of a quantity\n",
      "\u0016=EfH(X)=Z\n",
      "H(x)f(x) dx; (3.17)\n",
      "where His a real-valued function and fthe probability density of a random vector X,\n",
      "called the nominal pdf . The subscript fis added to the expectation operator to indicate thatnominal pdfit is taken with respect to the density f.\n",
      "Letgbe another probability density such that g(x)=0 implies that H(x)f(x)=0.\n",
      "Using the density gwe can represent \u0016as\n",
      "\u0016=Z\n",
      "H(x)f(x)\n",
      "g(x)g(x) dx=Eg\"\n",
      "H(X)f(X)\n",
      "g(X)#\n",
      ": (3.18)\n",
      "Consequently, if X1;:::; XN\u0018iidg, then\n",
      "b\u0016=1\n",
      "NNX\n",
      "k=1H(Xk)f(Xk)\n",
      "g(Xk)(3.19)\n",
      "is an unbiased estimator of \u0016. This estimator is called the importance sampling estimatorimportance\n",
      "sampling\n",
      "estimatorandgis called the importance sampling density. The ratio of densities, f(x)=g(x), is called\n",
      "thelikelihood ratio . The importance sampling pseudo-code is given in Algorithm 3.3.2.likelihood ratio94 3.3. Monte Carlo Estimation\n",
      "Algorithm 3.3.2: Importance Sampling Estimation\n",
      "input: Function H, importance sampling density gsuch that g(x)=0 for all xfor\n",
      "which H(x)f(x)=0, sample size N, conﬁdence level 1 \u0000\u000b.\n",
      "output: Point estimate and approximate (1 \u0000\u000b) conﬁdence interval for\n",
      "\u0016=EH(X), where X\u0018f.\n",
      "1Simulate X1;:::; XNiid\u0018gand let Yi=H(Xi)f(Xi)=g(Xi);i=1;:::; N.\n",
      "2Estimate\u0016viab\u0016=Yand determine an approximate (1 \u0000\u000b) conﬁdence interval as\n",
      "I:= \n",
      "b\u0016\u0000z1\u0000\u000b=2Sp\n",
      "N;b\u0016+z1\u0000\u000b=2Sp\n",
      "N!\n",
      ";\n",
      "-quantile of the N(0;1) distribution and Sis the sample\n",
      "standard deviation of Y1;:::; YN.\n",
      "3return b\u0016and the intervalI.\n",
      "Example 3.14 (Importance Sampling) Let us examine the workings of importance\n",
      "sampling by estimating the area, \u0016say, under the graph of the function\n",
      "M(x1;x2)=e\u00001\n",
      "4p\n",
      "x2\n",
      "1+x2\n",
      "2\u0012\n",
      "sin\u0012\n",
      "2q\n",
      "x2\n",
      "1+x2\n",
      "2\u0013\n",
      "+1\u0013\n",
      ";(x1;x2)2R2: (3.20)\n",
      "We saw a similar function in Example 3.8 (but note the di \u000berent domain). A natural ap- +80\n",
      "proach to estimate the area is to truncate the domain to the square [ \u0000b;b]2, for large enough\n",
      "b, and to estimate the integral\n",
      "\u0016b=Zb\n",
      "\u0000bZb\n",
      "\u0000b(2b)2M(x)|      {z      }\n",
      "H(x)f(x) dx=EfH(X)\n",
      "via crude Monte Carlo, where f(x)=1=(2b)2;x2[\u0000b;b]2, is the pdf of the uniform distri-\n",
      "bution on [\u0000b;b]2. Here is the Python code which does just that.\n",
      "impsamp1.py\n",
      "import numpy as np\n",
      "from numpy import exp, sqrt , sin, pi, log, cos\n",
      "from numpy.random import rand\n",
      "b = 1000\n",
      "H = lambda x1, x2: (2*b)**2 * exp(-sqrt(x1**2+x2**2)/4)*(sin(2*sqrt(\n",
      "x1**2+x2**2))+1)*(x1**2 + x2**2 < b**2)\n",
      "f = 1/((2*b)**2)\n",
      "N = 10**6\n",
      "X1 = -b + 2*b*rand(N,1)\n",
      "X2 = -b + 2*b*rand(N,1)\n",
      "Z = H(X1,X2)\n",
      "estCMC = np.mean(Z).item() # to obtain scalar\n",
      "RECMC = np.std(Z)/estCMC/sqrt(N).item()\n",
      "print( 'CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format(estCMC*(1-1.96*\n",
      "RECMC), estCMC *(1+1.96*RECMC),RECMC))\n",
      "CI = (82.663,135.036), RE = 0.123Chapter 3. Monte Carlo Methods 95\n",
      "For a truncation level of b=1000 and a sample size of N=106, a typical estimate is\n",
      "108.8, with an estimated relative error of 0.123. We have two sources of error here. The\n",
      "ﬁrst is the error in approximating \u0016by\u0016b. However, as the function Hdecays exponentially\n",
      "fast, b=1000 is more than enough to ensure this error is negligible. The second type of\n",
      "error is the statistical error, due to the estimation process itself. This can be quantiﬁed by\n",
      "the estimated relative error, and can be reduced by increasing the sample size.\n",
      "Let us now consider an importance sampling approach in which the importance\n",
      "sampling pdf gis radially symmetric and decays exponentially in the radius, similar to the\n",
      "function H. In particular, we simulate ( X1;X2) in a way akin to Example 3.1, by ﬁrst gen- + 69\n",
      "erating a radius R\u0018Exp(\u0015) and an angle \u0002\u0018U(0;2\u0019), and then returning X1=Rcos(\u0002)\n",
      "andX2=Rsin(\u0002). By the Transformation Rule (Theorem C.4) we then have +435\n",
      "g(x)=fR;\u0002(r;\u0012)1\n",
      "r=\u0015e\u0000\u0015r1\n",
      "2\u00191\n",
      "r=\u0015e\u0000\u0015p\n",
      "x2\n",
      "1+x2\n",
      "2\n",
      "2\u0019q\n",
      "x2\n",
      "1+x2\n",
      "2;x2R2nf0g:\n",
      "The following code, which imports the one given above, implements the importance\n",
      "sampling steps, using the parameter \u0015=0:1.\n",
      "impsamp2.py\n",
      "from impsamp1 import *\n",
      "lam = 0.1;\n",
      "g = lambda x1, x2: lam*exp(-sqrt(x1**2 + x2**2)*lam)/sqrt(x1**2 + x2\n",
      "**2)/(2*pi);\n",
      "U = rand(N,1); V = rand(N,1)\n",
      "R = -log(U)/lam\n",
      "X1 = R*cos(2*pi*V)\n",
      "X2 = R*sin(2*pi*V)\n",
      "Z = H(X1,X2)*f/g(X1,X2)\n",
      "estIS = np.mean(Z).item() # obtain scalar\n",
      "REIS = np.std(Z)/estIS/sqrt(N).item()\n",
      "print( 'CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format(estIS*(1-1.96*\n",
      "REIS), estIS*(1+1.96*REIS),REIS))\n",
      "CI = (100.723,101.077), RE = 0.001\n",
      "A typical estimate is 100.90 with an estimated relative error of 1 \u000110\u00004, which gives\n",
      "a substantial variance reduction. In terms of approximate 95% conﬁdence intervals, we\n",
      "have (82.7,135.0) in the CMC case versus (100.7,101.1) in the importance sampling case.\n",
      "Of course, we could have reduced the truncation level bto improve the performance of\n",
      "CMC, but then the approximation error might become more signiﬁcant. For the importance\n",
      "sampling case, the relative error is hardly a \u000bected by the threshold level, but does depend\n",
      "on the choice of \u0015. We chose\u0015such that the decay rate is slower than the decay rate of the\n",
      "function H, which is 0.25.\n",
      "As illustrated in the above example, a main di \u000eculty in importance sampling is how to\n",
      "choose the importance sampling distribution. A poor choice of gmay seriously a \u000bect the\n",
      "accuracy of both the estimate and the conﬁdence interval. The theoretically optimal choice96 3.4. Monte Carlo for Optimization\n",
      "g\u0003for the importance sampling density minimizes the variance of b\u0016and is therefore the\n",
      "solution to the functional minimization program\n",
      "min\n",
      "gVarg \n",
      "H(X)f(X)\n",
      "g(X)!\n",
      ": (3.21)\n",
      "It is not di \u000ecult to show, see also Exercise 22, that if either H(x)>0 or H(x)60 for all +118\n",
      "x, then the optimal importance sampling pdf optimal\n",
      "importance\n",
      "sampling pdfis\n",
      "g\u0003(x)=H(x)f(x)\n",
      "\u0016: (3.22)\n",
      "Namely, in this case Varg\u0003b\u0016=Varg\u0003(H(X)f(X)=g(X))=Varg\u0003\u0016=0, so that the estimator b\u0016\n",
      "isconstant under g\u0003. An obvious di \u000eculty is that the evaluation of the optimal importance\n",
      "sampling density g\u0003is usually not possible, since g\u0003(x) in (3.22) depends on the unknown\n",
      "quantity\u0016. Nevertheless, one can typically choose a good importance sampling density g\n",
      "“close” to the minimum variance density g\u0003.\n",
      "One of the main considerations for choosing a good importance sampling pdf is that\n",
      "the estimator (3.19) should have ﬁnite variance. This is equivalent to the requirement\n",
      "that\n",
      "Eg\"\n",
      "H2(X)f2(X)\n",
      "g2(X)#\n",
      "=Ef\"\n",
      "H2(X)f(X)\n",
      "g(X)#\n",
      "<1: (3.23)\n",
      "This suggests that gshould not have lighter tails than fand that, preferably, the\n",
      "likelihood ratio, f=g, should be bounded.\n",
      "3.4 Monte Carlo for Optimization\n",
      "In this section we describe several Monte Carlo methods for optimization. Such random-\n",
      "ized algorithms can be useful for solving optimization problems with many local optima\n",
      "and complicated constraints, possibly involving a mix of continuous and discrete variables.\n",
      "Randomized algorithms are also used to solve noisy optimization problems, in which the\n",
      "objective function is unknown and has to be obtained via Monte Carlo simulation.\n",
      "3.4.1 Simulated Annealing\n",
      "Simulated annealing is a Monte Carlo technique for minimization that emulates the phys-Simulated\n",
      "annealing ical state of atoms in a metal when the metal is heated up and then slowly cooled down.\n",
      "When the cooling is performed very slowly, the atoms settle down to a minimum-energy\n",
      "state. Denoting the state as xand the energy of a state as S(x), the probability distribution\n",
      "of the (random) states is described by the Boltzmann pdf\n",
      "f(x)/e\u0000S(x)\n",
      "kT;x2X;\n",
      "where kis Boltzmann’s constant and Tis the temperature.Chapter 3. Monte Carlo Methods 97\n",
      "Going beyond the physical interpretation, suppose that S(x) is an arbitrary function to\n",
      "be minimized, with xtaking values in some discrete or continuous set X. The Gibbs pdfGibbs pdfcorresponding to S(x) is deﬁned as\n",
      "fT(x)=e\u0000S(x)\n",
      "T\n",
      "zT;x2X;\n",
      "provided that the normalization constant zT:=P\n",
      "xexp(\u0000S(x)=T) is ﬁnite. Note that this\n",
      "is simply the Boltzmann pdf with the Boltzmann constant kremoved. As T!0, the pdf\n",
      "becomes more and more peaked around the set of global minimizers of S.\n",
      "The idea of simulated annealing is to create a sequence of points X1;X2;:::that are ap-\n",
      "proximately distributed according to pdfs fT1(x);fT2(x);:::, where T1;T2;::: is a sequence\n",
      "of “temperatures” that decreases (is “cooled”) to 0 — known as the annealing schedule . Ifannealing\n",
      "schedule each Xtwere sampled exactly from fTt, then Xtwould converge to a global minimum of\n",
      "S(x) asTt!0. However, in practice sampling is approximate and convergence to a global\n",
      "minimum is not assured. A generic simulated annealing algorithm is as follows.\n",
      "Algorithm 3.4.1: Simulated Annealing\n",
      "input: Annealing schedule T0;T1;:::; , function S, initial value x0.\n",
      "output: Approximations to the global minimizer x\u0003and minimum value S(x\u0003).\n",
      "1SetX0 x0andt 1.\n",
      "2while not stopping do\n",
      "3 Approximately simulate Xtfrom fTt(x).\n",
      "4 t t+1\n",
      "5return Xt;S(Xt)\n",
      "A popular annealing schedule is geometric cooling , where Tt=\fTt\u00001,t=1;2;:::, forgeometric\n",
      "cooling a given initial temperature T0and a cooling factor \f2(0;1). Appropriate values for T0\n",
      "cooling factor and\fare problem-dependent and this has traditionally required tuning on the part of the\n",
      "user. A possible stopping criterion is to stop after a ﬁxed number of iterations, or when the\n",
      "temperature is “small enough”.\n",
      "Approximate sampling from a Gibbs distribution is most often carried out via Markov\n",
      "chain Monte Carlo. For each iteration t, the Markov chain should theoretically run for a\n",
      "large number of steps to accurately sample from the Gibbs pdf fTt. However, in practice,\n",
      "one often only runs a single step of the Markov chain, before updating the temperature, as\n",
      "in Algorithm 3.4.2 below.\n",
      "To sample from a Gibbs distribution fT, this algorithm uses a random walk Metropolis–\n",
      "Hastings sampler. From (3.7), the acceptance probability of a proposal yis thus + 80\n",
      "\u000b(x;y)=min8>><>>:e\u00001\n",
      "TS(y)\n",
      "e\u00001\n",
      "TS(x);19>>=>>;=minn\n",
      "e\u00001\n",
      "T(S(y)\u0000S(x));1o\n",
      ":\n",
      "Hence, if S(y)<S(x), then the proposal is aways accepted. Otherwise, the proposal is\n",
      "accepted with probability exp( \u00001\n",
      "T(S(y)\u0000S(x))).98 3.4. Monte Carlo for Optimization\n",
      "Algorithm 3.4.2: Simulated Annealing with a Random Walk Sampler\n",
      "input: Objective function S, starting state X0, initial temperature T0, number of\n",
      "iterations N, symmetric proposal density q(yjx), constant\f.\n",
      "output: Approximate minimizer and minimum value of S.\n",
      "1fort=0toN\u00001do\n",
      "2 Simulate a new state Yfrom the symmetric proposal q(yjXt).\n",
      "3 ifS(Y)<S(Xt)then\n",
      "4 Xt+1 Y\n",
      "5 else\n",
      "6 Draw U\u0018U(0;1).\n",
      "7 ifU6e\u0000(S(Y)\u0000S(Xt))=Ttthen\n",
      "8 Xt+1 Y\n",
      "9 else\n",
      "10 Xt+1 Xt\n",
      "11 Tt+1 \fTt\n",
      "12return XNandS(XN)\n",
      "Example 3.15 (Simulated Annealing for Minimization) Let us minimize the “wig-\n",
      "gly” function depicted in the bottom panel of Figure 3.10 and given by:\n",
      "S(x)=8>><>>:\u0000e\u0000x2=100sin(13 x\u0000x4)5sin(1\u00003x2)2;if\u000026x62;\n",
      "1; otherwise:\n",
      "012345f(x) = e!S(x)=T\n",
      "-2 -1.5 -1 -0.5 0 0.5 1 1.5 2\n",
      "x-101S(x)T= 1T= 0 :4T= 0 :2\n",
      "Figure 3.10: Lower panel: the “wiggly” function S(x). Upper panel: three (normalized)\n",
      "Gibbs pdfs for temperatures T=1;0:4;0:2. As the temperature decreases, the Gibbs pdf\n",
      "converges to the pdf that has all its mass concentrated at the minimizer of S.Chapter 3. Monte Carlo Methods 99\n",
      "The function has many local minima and maxima, with a global minimum around 1.4.\n",
      "The ﬁgure also illustrates the relationship between Sand the (unnormalized) Gibbs pdf fT.\n",
      "The following Python code implements a slight variant of Algorithm 3.4.2 where, in-\n",
      "stead of stopping after a ﬁxed number of iterations, the algorithm stops when the temper-\n",
      "ature is lower than some threshold (here 10\u00003).\n",
      "Instead of stopping after a ﬁxed number Nof iterations or when the temperature\n",
      "is low enough, it is useful to stop when consecutive function values are closer than\n",
      "some distance \"to each other, or when the best found function value has not changed\n",
      "over a ﬁxed number dof iterations.\n",
      "For a “current” state x, the proposal state Yis here drawn from the N(x;0:52) distri-\n",
      "bution. We use geometric cooling with decay parameter \f=0:999 and initial temperature\n",
      "T0=1. We set the initial state to x0=0. Figure 3.11 depicts a realization of the sequence\n",
      "of states xtfort=0;1;:::. After initially ﬂuctuating wildly, the sequence settles down\n",
      "to a value around 1.37, with S(1:37)=\u00000:92, corresponding to the global optimizer and\n",
      "minimum, respectively.\n",
      "simann.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "def wiggly(x):\n",
      "y = -np.exp(x**2/100)*np.sin(13*x-x**4)**5*np.sin(1-3*x**2)**2\n",
      "ind = np.vstack((np.argwhere(x<-2),np.argwhere(x>2)))\n",
      "y[ind]=float( 'inf')\n",
      "return y\n",
      "S = wiggly\n",
      "beta = 0.999\n",
      "sig = 0.5\n",
      "T=1\n",
      "x= np.array([0])\n",
      "xx=[]\n",
      "Sx=S(x)\n",
      "while T>10**(-3):\n",
      "T=beta*T\n",
      "y = x+sig*np.random.randn()\n",
      "Sy = S(y)\n",
      "alpha = np.amin((np.exp(-(Sy-Sx)/T),1))\n",
      "if np.random.uniform()<alpha:\n",
      "x=y\n",
      "Sx=Sy\n",
      "xx=np.hstack((xx,x))\n",
      "print( 'minimizer = {:3.3f}, minimum ={:3.3f} '.format(x[0],Sx[0]))\n",
      "plt.plot(xx)\n",
      "plt.show()\n",
      "minimizer = 1.365, minimum = -0.958100 3.4. Monte Carlo for Optimization\n",
      "0\n",
      " 1000\n",
      " 2000\n",
      " 3000\n",
      " 4000\n",
      " 5000\n",
      " 6000\n",
      " 7000\n",
      "number of iterations\n",
      "2.0\n",
      "1.5\n",
      "1.0\n",
      "0.5\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0state\n",
      "Figure 3.11: Typical states generated by the simulated annealing algorithm.\n",
      "3.4.2 Cross-Entropy Method\n",
      "Thecross-entropy (CE) method [103] is a simple Monte Carlo algorithm that can be usedcross -entropyfor both optimization and estimation.\n",
      "The basic idea of the CE method for minimizing a function Son a setXis to deﬁne\n",
      "a parametric family of probability densities ff(\u0001jv);v2Vg onXand to iteratively update\n",
      "the parameter vso that f(\u0001jv) places more mass on states xthat have smaller Svalues than\n",
      "on the previous iteration. In particular, the CE algorithm has two basic phases:\n",
      "Sampling : Samples X1;:::; XNare drawn independently according to f(\u0001jv). The\n",
      "objective function Sis evaluated at these points.\n",
      "Updating : A new parameter v0is selected on the basis of those Xifor which S(Xi)6\n",
      ". ThesefXigform the elite sample set,E.elite sample\n",
      "is chosen as the worst of the Nelite:=d%Ne\n",
      "best performing samples, where %2(0;1) is the rarity parameter rarity\n",
      "parameter— typically, %=0:1 or\n",
      "%=0:01. The parameter vis updated as a smoothed average \u000bv0+(1\u0000\u000b)v, where\u000b2(0;1)\n",
      "is the smoothing parameter smoothing\n",
      "parameterand\n",
      "v0:=argmax\n",
      "v2VX\n",
      "X2Elnf(Xjv): (3.24)\n",
      "The updating rule (3.24) is the result of minimizing the Kullback–Leibler divergence\n",
      ", and f(x;v); see [103].density of X\u0018f(xjv) given S(X)6\n",
      "Note that (3.24) yields the maximum likelihood estimator (MLE) of vbased on the elite +458\n",
      "samples. Hence, for many speciﬁc families of distributions, explicit solutions can be found.\n",
      "An important example is where X\u0018N(\u0016;diag(\u001b2)); that is, Xhas independent GaussianChapter 3. Monte Carlo Methods 101\n",
      "components. In this case, the mean vector \u0016and the vector of variances \u001b2are simply\n",
      "updated via the sample mean and sample variance of the elite samples. This is known as\n",
      "normal updating . A generic CE procedure for minimization is given in Algorithm 3.4.3.normal\n",
      "updating\n",
      "Algorithm 3.4.3: Cross-Entropy Method for Minimization\n",
      "input: Function S;initial sampling parameter v0, sample size N, rarity parameter\n",
      "%, smoothing parameter \u000b.\n",
      "output: Approximate minimum of Sand optimal sampling parameter v.\n",
      "1Initialize v0, set Nelite d%Neandt 0.\n",
      "2while a stopping criterion is not met do\n",
      "3 t t+1\n",
      "4 Simulate an iid sample X1;:::; XNfrom the density f(\u0001jvt\u00001).\n",
      "5 Evaluate the performances S(X1);:::; S(XN) and sort them from smallest to\n",
      "largest: S(1);:::; S(N).\n",
      "tbe the sample %-quantile of the performances:\n",
      "t S(Nelite): (3.25)\n",
      "tg.etermine the set of elite samples Et=fXi:S(Xi)6\n",
      "8 Letv0\n",
      "tbe the MLE of the elite samples:\n",
      "v0\n",
      "t argmax\n",
      "vX\n",
      "X2Etlnf(Xjv): (3.26)\n",
      "9 Update the sampling parameter as\n",
      "vt \u000bv0\n",
      "t+(1\u0000\u000b)vt\u00001: (3.27)\n",
      "t,vtturn\n",
      "tcon-;:::; such thatduces a sequence of pairs ( \n",
      "verges (approximately) to the minimal function value, and f(\u0001jvt) to a degenerate pdf that\n",
      "(approximately) concentrates all its mass at a minimizer of S, ast!1 . A possible stop-\n",
      "ping condition is to stop when the sampling distribution f(\u0001jvt) is su \u000eciently close to a\n",
      "degenerate distribution. For normal updating this means that the standard deviation is suf-\n",
      "ﬁciently small.\n",
      "The output of the CE algorithm could also include the overall best function value\n",
      "and corresponding solution.\n",
      "In the following example, we minimize the same function as in Example 3.15, but + 97\n",
      "instead use the CE algorithm.\n",
      "Example 3.16 (Cross-Entropy Method for Minimization) In this case we take the\n",
      "family of normal distributions fN(\u0016;\u001b2)gfor the sampling step (Step 4 of Algorithm 3.4.3),\n",
      "starting with \u0016=0 and\u001b=3. The choice of the initial parameter is quite arbitrary, as long\n",
      "as\u001bis large enough to sample a wide range of points. We take N=100 samples at each it-\n",
      "eration, set%=0:1, and keep the Nelite=10=dN%esmallest ones as the elite samples. The\n",
      "parameters \u0016and\u001bare then updated via the sample mean and sample standard deviation102 3.4. Monte Carlo for Optimization\n",
      "of the elite samples. In this case we do not use any smoothing ( \u000b=1). In the following\n",
      "Python code the 100 \u00022 matrix Sxstores the x-values in the ﬁrst column and the func-\n",
      "tion values in the second column. The rows of this matrix are sorted in ascending order\n",
      "according to the function values, giving the matrix sortSx . The ﬁrst Nelite=10 rows of\n",
      "this sorted matrix correspond to the elite samples and their function values. The updating\n",
      "of\u0016and\u001bis done in Lines 14 and 15. Figure 3.12 shows how the pdfs of the N(\u0016t;\u001b2\n",
      "t)\n",
      "sampling distributions degenerate to the point mass at the global minimizer 1 :366.\n",
      "CEmethod.py\n",
      "from simann import wiggly\n",
      "import numpy as np\n",
      "np.set_printoptions(precision=3)\n",
      "mu, sigma = 0, 3\n",
      "N, Nel = 100, 10\n",
      "eps = 10**-5\n",
      "S = wiggly\n",
      "while sigma > eps:\n",
      "X = np.random.randn(N,1)*sigma + np.array(np.ones((N,1)))*mu\n",
      "Sx = np.hstack((X, S(X)))\n",
      "sortSx = Sx[Sx[:,1].argsort(),]\n",
      "Elite = sortSx[0:Nel ,:-1]\n",
      "mu = np.mean(Elite , axis=0)\n",
      "sigma = np.std(Elite , axis=0)\n",
      "print( 'S(mu)= {}, mu: {}, sigma: {}\\n '.format(S(mu), mu, sigma))\n",
      "S(mu)= [0.071], mu: [0.414], sigma: [0.922]\n",
      "S(mu)= [0.063], mu: [0.81], sigma: [0.831]\n",
      "S(mu)= [-0.033], mu: [1.212], sigma: [0.69]\n",
      "S(mu)= [-0.588], mu: [1.447], sigma: [0.117]\n",
      "S(mu)= [-0.958], mu: [1.366], sigma: [0.007]\n",
      "S(mu)= [-0.958], mu: [1.366], sigma: [0.]\n",
      "S(mu)= [-0.958], mu: [1.366], sigma: [3.535e-05]\n",
      "S(mu)= [-0.958], mu: [1.366], sigma: [2.023e-06]\n",
      "-2 -1 0 1 2 3x00.511.52f(x;7;<)5 \n",
      "4 \n",
      "2 \n",
      " 1 iteration 03 \n",
      "Figure 3.12: The normal pdfs of the ﬁrst six sampling distributions, truncated to the interval\n",
      "[\u00002;3]. The initial sampling distribution is N(0;32).Chapter 3. Monte Carlo Methods 103\n",
      "3.4.3 Splitting for Optimization\n",
      "Minimizing a function S(x),x2X is closely related to drawing a random sample from a\n",
      "\u0003, these toel set contains the minimizer. Moreover, if \n",
      "volume of this level set will be small. So, a randomly selected point from this set is expected\n",
      ", the level sets willus, by gradually decreasing the level parameter \n",
      "gradually shrink towards the set fx\u0003g. Indeed, the CE method was developed with exactly\n",
      "this connection in mind; see, e.g., [102]. Note that the CE method employs a parametric\n",
      "sampling distribution to obtain samples from the level sets (the elite samples). In [34]\n",
      "anon-parametric sampling mechanism is introduced that uses an evolving collection of\n",
      "particles. The resulting optimization algorithm, called splitting for continuous optimization splitting for\n",
      "continuous\n",
      "optimization(SCO), provides a fast and accurate way to optimize complicated continuous functions. The\n",
      "details of SCO are given in Algorithm 3.4.4.\n",
      "Algorithm 3.4.4: Splitting for Continuous Optimization (SCO)\n",
      "input: Objective function S, sample size N, rarity parameter %, scale factor w,\n",
      "bounded regionB\u001aX that is known to contain a global minimizer, and\n",
      "maximum number of attempts MaxTry .\n",
      "output: Final iteration number tand sequence ( Xbest;1;b1);:::; (Xbest;t;bt) of best\n",
      "solutions and function values at each iteration.\n",
      "1SimulateY0=fY1;:::; YNguniformly onB. Set t 0 and Nelite dN%e.\n",
      "2while stopping condition is not satisﬁed do\n",
      "3 Determine the Nelitesmallest values, S(1)6\u0001\u0001\u00016S(Nelite), offS(X);X2Y tg,\n",
      "and store the corresponding vectors, X(1);:::; X(Nelite), inXt+1. Set bt+1 S(1)\n",
      "andXbest;t+1 X(1).\n",
      "4 Draw Bi\u0018Bernoulli (1\n",
      "2),i=1;:::; Nelite, withPNelite\n",
      "i=1Bi=Nmod Nelite.\n",
      "5 fori=1toNelitedo\n",
      "6 Ri j\n",
      "N\n",
      "Nelitek\n",
      "+Bi // random splitting factor\n",
      "7 Y X(i);Y0 Y\n",
      "8 forj=1toRido\n",
      "9 Draw I2f1;:::; Nelitegnfiguniformly and let \u001bi wjX(i)\u0000X(I)j.\n",
      "10 Simulate a uniform permutation \u0019=(\u00191;:::;\u0019 n) of (1;:::; n).\n",
      "11 fork=1tondo\n",
      "12 forTry=1toMaxTry do\n",
      "13 Y0(\u0019k) Y(\u0019k)+\u001bi(\u0019k)Z;Z\u0018N(0;1)\n",
      "14 ifS(Y0)<S(Y)then Y Y0andbreak .\n",
      "15 AddYtoYt+1\n",
      "16 t t+1\n",
      "17returnf(Xbest;k;bk);k=1;:::; tg\n",
      "At iteration t=0, the algorithm starts with a population of particles Y0=fY1;:::; YNg\n",
      "that are uniformly generated on some bounded region B, which is large enough to contain\n",
      "a global minimizer. The function values of all particles in Y0are sorted, and the best104 3.4. Monte Carlo for Optimization\n",
      "Nelite=dN%eform the elite particle set X1, exactly as in the CE method. Next, the elite\n",
      "particles are “split” into bN=Nelitecchildren particles, adding one extra child to some of\n",
      "the elite particles to ensure that the total number of children is again N. The purpose of\n",
      "Line 4 is to randomize which elite particles receive an extra child. Lines 8–15 describe\n",
      "how the children of the i-th elite particle are generated. First, in Line 9, we select one\n",
      "of the other elite particles uniformly at random. The same line deﬁnes an n-dimensional\n",
      "vector\u001biwhose components are the absolute di \u000berences between the vectors X(i)andX(I),\n",
      "multiplied by a constant w. That is,\n",
      "\u001bi=wjX(i)\u0000X(I)j:=w26666666666666664jX(i);1\u0000X(I);1j\n",
      "jX(i);2\u0000X(I);2j\n",
      ":::\n",
      "jX(i);n\u0000X(I);nj37777777777777775:\n",
      "Next, a uniform random permutation \u0019of (1;:::; n) is simulated (see Exercise 9). Lines +115\n",
      "11–14 describe how, starting from a candidate child point Y, each coordinate of Yis re-\n",
      "sampled, in the order determined by \u0019, by adding a standard normal random variable to\n",
      "that component, multiplied by the corresponding component of \u001bi(Line 13). If the result-\n",
      "ingY0has a function value that is less than that of Y, then the new candidate is accepted.\n",
      "Otherwise, the same coordinate is tried again. If no improvement is found in MaxTry at-\n",
      "tempts, the original component is retained. This process is performed for all elite samples,\n",
      "to produce the ﬁrst-generation population Y1. The procedure is then repeated for iterations\n",
      "t=1;2;:::, until some stopping criterion is met, e.g., when the best found function value\n",
      "does not change for a number of consecutive iterations, or when the total number of func-\n",
      "tion evaluations exceeds some threshold. The best found function value and corresponding\n",
      "argument (particle) are returned at the conclusion of the algorithm.\n",
      "The input variable MaxTry governs how much computational time is dedicated to up-\n",
      "dating a component. In most cases we have encountered, the choices w=0:5 and MaxTry\n",
      "=5 work well. Empirically, relatively high value for %work well, such as %=0:4;0:8, or\n",
      "even%=1. The latter case means that at each stage t all samples fromYt\u00001carry over to\n",
      "the elite setXt.\n",
      "Example 3.17 (Test Problem 112) Hock and Schittkowski [58] provide a rich source\n",
      "of test problems for multiextremal optimization. A challenging one is Problem 112, where\n",
      "the goal is to ﬁnd xso as to minimize the function\n",
      "S(x)=10X\n",
      "j=1xj \n",
      "cj+lnxj\n",
      "x1+\u0001\u0001\u0001+x10!\n",
      ";\n",
      "subject to the following set of constraints:\n",
      "x1+2x2+2x3+x6+x10\u00002=0;\n",
      "x4+2x5+x6+x7\u00001=0;\n",
      "x3+x7+x8+2x9+x10\u00001=0;\n",
      "xj>0:000001;j=1;:::; 10;\n",
      "where the constants fcigare given in Table 3.1.Chapter 3. Monte Carlo Methods 105\n",
      "Table 3.1: Constants for Test Problem 112.\n",
      "c1=\u00006:089 c2=\u000017:164 c3=\u000034:054 c4=\u00005:914 c5=\u000024:721\n",
      "c6=\u000014:986 c7=\u000024:100 c8=\u000010:708 c9=\u000026:662 c10=\u000022:179\n",
      "The best known minimal value in [58] was \u000047:707579. In [89] a better solution was\n",
      "found,\u000047:760765, using a genetic algorithm. The corresponding solution vector was\n",
      "completely di \u000berent from the one in [58]. A further improvement, \u000047:76109081, was\n",
      "found in [70], using the CE method, giving a similar solution vector to that in [89]:\n",
      "0.04067247 0.14765159 0.78323637 0.00141368 0.48526222\n",
      "0.00069291 0.02736897 0.01794290 0.03729653 0.09685870\n",
      "To obtain a solution with SCO, we ﬁrst converted this 10-dimensional problem into a\n",
      "7-dimensional one by deﬁning the objective function\n",
      "S7(y)=S(x);\n",
      "where x2=y1;x3=y2;x5=y3;x6=y4;x7=y5;x9=y6;x10=y7, and\n",
      "x1=2\u0000(2y1+2y2+y4+x7);\n",
      "x4=1\u0000(2y3+y4+y5);\n",
      "x8=1\u0000(y2+y5+2y6+y7);\n",
      "subject to x1;:::; x10>0:000001, where the fxigare taken as functions of the fyig. We then\n",
      "adopted a penalty approach (see Section B.4) by adding a penalty function to the original +417\n",
      "objective function:\n",
      "eS7(y)=S(x)+100010X\n",
      "i=1maxf\u0000(xi\u00000:000001);0g;\n",
      "where, again, thefxigare deﬁned in terms of the fyigas above.\n",
      "Optimizing this last function with SCO, we found, in less time than the other al-\n",
      "gorithms, a slightly smaller function value: \u000047:761090859365858, with solution vector\n",
      "0.040668102417464 0.147730393049955 0.783153291185250 0.001414221643059\n",
      "0.485246633088859 0.000693172682617 0.027399339496606 0.017947274343948\n",
      "0.037314369272343 0.096871356429511\n",
      "in line with the earlier solutions.\n",
      "3.4.4 Noisy Optimization\n",
      "Innoisy optimization noisy\n",
      "optimization, the objective function is unknown, but estimates of function val-\n",
      "ues are available, e.g., via simulation. For example, to ﬁnd an optimal prediction function\n",
      "gin supervised learning, the exact risk `(g)=ELoss( Y;g(x)) is usually unknown and\n",
      "only estimates of the risk are available. Optimizing the risk is thus typically a noisy op- + 20\n",
      "timization problem. Noisy optimization features prominently in simulation studies where106 3.4. Monte Carlo for Optimization\n",
      "the behavior of some system (e.g., vehicles on a road network) is simulated under certain\n",
      "parameters (e.g., the lengths of the tra \u000ec light intervals) and the aim is to choose those\n",
      "parameters optimally (e.g., to maximize the tra \u000ec throughput). For each parameter setting\n",
      "the exact value for the objective function is unknown but estimates can be obtained via the\n",
      "simulation.\n",
      "In general, suppose the goal is to minimize a function S, where Sis unknown, but\n",
      "an estimate of S(x) can be obtained for any choice of x2X. Because the gradient rSis\n",
      "unknown, one cannot directly apply classical optimization methods. The stochastic approx-\n",
      "imation method mimics the classical gradient descent method by replacing a deterministicstochastic\n",
      "approximation gradient with an estimate crS(x).\n",
      "A simple estimator for the i-th component ofrS(x) (that is,@S(u)=@xi), is the central\n",
      "di\u000berence estimator central\n",
      "difference\n",
      "estimatorbS(x+ei\u000e=2)\u0000bS(x\u0000ei\u000e=2)\n",
      "\u000e; (3.28)\n",
      "where eidenotes the i-th unit vector, and bS(x+ei\u000e=2) andbS(x\u0000ei\u000e=2) can be any estimators\n",
      "ofS(x+ei\u000e=2) and S(x\u0000ei\u000e=2), respectively. The di \u000berence parameter \u000e >0 should be\n",
      "small enough to reduce the bias of the estimator, but large enough to keep the variance of\n",
      "the estimator small.\n",
      "To reduce the variance in the estimator (3.28) it is important to have bS(x+ei\u000e=2)\n",
      "andbS(x\u0000ei\u000e=2) positively correlated. This can for example be achieved by using\n",
      "common random numbers common random\n",
      "numbersin the simulation.\n",
      "In direct analogy to gradient descent methods, the stochastic approximation method +414\n",
      "produces a sequence of iterates, starting with some x12X, via\n",
      "xt+1=xt\u0000\ftcrS(xt); (3.29)\n",
      "where\f1;\f2;:::is a sequence of strictly positive step sizes. A generic stochastic approx-\n",
      "imation algorithm for minimizing a function Sis thus as follows.\n",
      "Algorithm 3.4.5: Stochastic Approximation\n",
      "input: A mechanism to estimate any gradient rS(x) and step sizes \f1;\f2;:::.\n",
      "output: Approximate optimizer of S.\n",
      "1Initialize x12X. Set t 1.\n",
      "2while a stopping criterion is not met do\n",
      "3 Obtain an estimated gradient crS(xt) ofSatxt.\n",
      "4 Determine a step size \ft.\n",
      "5 Setxt+1 xt\u0000\ftcrS(xt).\n",
      "6 t t+1\n",
      "7return xt\n",
      "WhencrS(xt) is an unbiased estimator ofrS(xt) in (3.29) the stochastic approxima-\n",
      "tion Algorithm 3.4.5 is referred to as the Robbins–Monro algorithm. When ﬁnite di \u000ber-Robbins –Monroences are used to estimate crS(xt), as in (3.28), the resulting algorithm is known as theChapter 3. Monte Carlo Methods 107\n",
      "Kiefer–Wolfowitz algorithm. In Section 9.4.1 we will see how stochastic gradient descentKiefer –\n",
      "Wolfowitz is employed in deep learning to minimize the training loss, based on a “minibatch” of\n",
      "training data. +336\n",
      "It can be shown [72] that, under certain regularity conditions on S, the sequence\n",
      "x1;x2;:::converges to the true minimizer x\u0003when the step sizes decrease slowly enough\n",
      "to 0; in particular, when\n",
      "1X\n",
      "t=1\ft=1and1X\n",
      "t=1\f2\n",
      "t<1: (3.30)\n",
      "In practice, one rarely uses step sizes that satisfy (3.30), as the convergence of the\n",
      "sequence will be too slow to be of practical use.\n",
      "An alternative approach to stochastic approximation is the stochastic counterpart stochastic\n",
      "counterpartmethod, also called sample average approximation . It can be applied in situations where\n",
      "the noisy objective function is of the form\n",
      "S(x)=EeS(x;\u0018);x2X; (3.31)\n",
      "where\u0018is a random vector that can be simulated and eS(x;\u0018) can be evaluated exactly. The\n",
      "idea is to replace the optimization of (3.31) with that of the sample average\n",
      "bS(x)=1\n",
      "NNX\n",
      "i=1eS(x;\u0018i);x2X; (3.32)\n",
      "where\u00181;:::;\u0018Nare iid copies of \u0018. Note that bSis a deterministic function of xand so can\n",
      "be optimized using any optimization algorithm. A solution to this sample average version\n",
      "is taken to be an estimator of a solution x\u0003to the original problem (3.31).\n",
      "Example 3.18 (Determining Good Importance Sampling Parameters) The selection\n",
      "of good importance sampling parameters can be viewed as a stochastic optimization prob-\n",
      "lem. Consider, for instance, the importance sampling estimator in Example 3.14. Recall + 94\n",
      "that the nominal distribution is the uniform distribution on the square [ \u0000b;b]2, with pdf\n",
      "fb(x)=1\n",
      "(2b)2;x2[\u0000b;b]2;\n",
      "where bis large enough to ensure that \u0016bis close to\u0016; in that example, we chose b=1000.\n",
      "The importance sampling pdf is\n",
      "g\u0015(x)=fR;\u0002(r;\u0012)1\n",
      "r=\u0015e\u0000\u0015r1\n",
      "2\u00191\n",
      "r=\u0015e\u0000\u0015p\n",
      "x2\n",
      "1+x2\n",
      "2\n",
      "2\u0019q\n",
      "x2\n",
      "1+x2\n",
      "2;x=(x1;x2)2R2nf0g;\n",
      "which depends on a free parameter \u0015. In the example we chose \u0015=0:1. Is this the best\n",
      "choice? Maybe \u0015=0:05 or 0:2 would have resulted in a more accurate estimate. The im-\n",
      "portant thing to realize is that the “e \u000bectiveness” of \u0015can be measured in terms of the\n",
      "variance of the estimator b\u0016in (3.19), which is given by + 93108 3.4. Monte Carlo for Optimization\n",
      "1\n",
      "NVarg\u0015 \n",
      "H(X)f(X)\n",
      "g\u0015(X)!\n",
      "=1\n",
      "NEg\u0015\"\n",
      "H2(X)f2(X)\n",
      "g2\n",
      "\u0015(X)#\n",
      "\u0000\u00162\n",
      "N=1\n",
      "NEf\"\n",
      "H2(X)f(X)\n",
      "g\u0015(X)#\n",
      "\u0000\u00162\n",
      "N:\n",
      "Hence, the optimal parameter \u0015\u0003minimizes the function S(\u0015)=Ef[H2(X)f(X)=g\u0015(X)],\n",
      "which is unknown, but can be estimated from simulation. To solve this stochastic minim-\n",
      "ization problem, we ﬁrst use stochastic approximation. Thus, at each step of the algorithm,\n",
      "the gradient of S(\u0015) is estimated from realizations of bS(\u0015)=H2(X)f(X)=g\u0015(X), where\n",
      "X\u0018fb. As in the original problem (that is, the estimation of \u0016), the parameter bshould\n",
      "be large enough to avoid any bias in the estimator of \u0015\u0003, but also small enough to en-\n",
      "sure a small variance. The following Python code implements a particular instance of Al-\n",
      "gorithm 3.4.5. For sampling from fbhere, we used b=100 instead of b=1000, as this will\n",
      "improve the crude Monte Carlo estimation of \u0015\u0003, without noticeably a \u000becting the bias. The\n",
      "gradient of S(\u0015) is estimated in Lines 11–17, using the central di \u000berence estimator (3.28).\n",
      "Notice how for the S(\u0015\u0000\u000e=2) and S(\u0015+\u000e=2) the same random vector X=[X1;X2]>is used.\n",
      "This signiﬁcantly reduces the variance of the gradient estimator; see also Exercise 23. The +118\n",
      "step size\ftshould be such that \ftcrS(xt)\u0019\u0015t. Given the large gradient here, we choose\n",
      "\f0=10\u00007and decrease it each step by a factor of 0 :99. Figure 3.13 shows how the se-\n",
      "quence\u00150;\u00151;:::decreases towards approximately 0 :125, which we take as an estimator\n",
      "for the optimal importance sampling parameter \u0015\u0003.\n",
      "stochapprox.py\n",
      "import numpy as np\n",
      "from numpy import pi\n",
      "import matplotlib.pyplot as plt\n",
      "b=100 # choose b large enough , but not too large\n",
      "delta = 0.01\n",
      "H = lambda x1, x2: (2*b)**2*np.exp(-np.sqrt(x1**2 + x2**2)/4)*(np.\n",
      "sin(2*np.sqrt(x1**2+x2**2)+1))*(x1**2+x2**2<b**2)\n",
      "f = 1/(2*b)**2\n",
      "g = lambda x1, x2, lam: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.\n",
      "sqrt(x1**2+x2**2)/(2*pi)\n",
      "beta = 10**-7 #step size very small , as the gradient is large\n",
      "lam=0.25\n",
      "lams = np.array([lam])\n",
      "N=10**4\n",
      "for i in range(200):\n",
      "x1 = -b + 2*b*np.random.rand(N,1)\n",
      "x2 = -b + 2*b*np.random.rand(N,1)\n",
      "lamL = lam - delta/2\n",
      "lamR = lam + delta/2\n",
      "estL = np.mean(H(x1,x2)**2*f/g(x1, x2, lamL))\n",
      "estR = np.mean(H(x1,x2)**2*f/g(x1, x2, lamR)) #use SAME x1,x2\n",
      "gr = (estR -estL)/delta #gradient\n",
      "lam = lam - gr*beta #gradient descend\n",
      "lams = np.hstack((lams , lam))\n",
      "beta = beta*0.99\n",
      "lamsize=range(0, (lams.size))\n",
      "plt.plot(lamsize , lams)\n",
      "plt.show()Chapter 3. Monte Carlo Methods 109\n",
      "0\n",
      " 25\n",
      " 50\n",
      " 75\n",
      " 100\n",
      " 125\n",
      " 150\n",
      " 175\n",
      " 200\n",
      "steps\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.20\n",
      "0.22\n",
      "0.24\n",
      "Figure 3.13: The stochastic optimization algorithm produces a sequence \u0015t;t=0;1;2;:::\n",
      "that tends to an approximate estimate of the optimal importance sampling parameter \u0015\u0003\u0019\n",
      "0:125.\n",
      "Next, we estimate \u0015\u0003using a stochastic counterpart approach. As the objective function\n",
      "S(\u0015) is of the form (3.31) (with \u0015taking the role of xandXthe role of\u0018), we obtain the\n",
      "sample average\n",
      "bS(\u0015)=1\n",
      "NNX\n",
      "i=1H2(Xi)f(Xi)\n",
      "g\u0015(Xi); (3.33)\n",
      "where X1;:::; XN\u0018iidfb. Once the X1;:::; XN\u0018iidfbhave been simulated, bS(\u0015) is a de-\n",
      "terministic function of \u0015, which can be optimized by any means. We take the most basic\n",
      "approach and simply evaluate the function for \u0015=0:01;0:02;:::; 0:3 and select the min-\n",
      "imizing\u0015on this grid. The code is given below and Figure 3.14 shows bS(\u0015) as a function\n",
      "of\u0015. The minimum value found was 0 :60\u0001104for minimizer b\u0015\u0003=0:12, which is in accord-\n",
      "ance with the value obtained via stochastic approximation. The sensitivity of this estimate\n",
      "can be assessed from the graph: for a wide range of values (say from 0.04 to 0.15) bSstays\n",
      "rather ﬂat. So any of these values could be used in an importance sampling procedure to\n",
      "estimate\u0016. However, very small values (less than 0.02) and large values (greater than 0.25)\n",
      "should be avoided. Our original choice of \u0015=0:1 was therefore justiﬁed and we could not\n",
      "have done much better.\n",
      "stochcounterpart.py\n",
      "from stochapprox import *\n",
      "lams = np.linspace(0.01, 0.31, 1000)\n",
      "res=[]\n",
      "res = np.array(res)\n",
      "for i in range(lams.size):\n",
      "lam = lams[i]\n",
      "np.random.seed(1)\n",
      "g = lambda x1, x2: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.sqrt\n",
      "(x1**2+x2**2)/(2*pi)110 3.4. Monte Carlo for Optimization\n",
      "X=-b+2*b*np.random.rand(N,1)\n",
      "Y=-b+2*b*np.random.rand(N,1)\n",
      "Z=H(X,Y)**2*f/g(X,Y)\n",
      "estCMC = np.mean(Z)\n",
      "res = np.hstack((res, estCMC))\n",
      "plt.plot(lams , res)\n",
      "plt.xlabel(r '$\\lambda$ ')\n",
      "plt.ylabel(r '$\\hat{S}(\\lambda)$ ')\n",
      "plt.ticklabel_format(style= 'sci', axis= 'y', scilimits=(0,0))\n",
      "plt.show()\n",
      "0.00\n",
      " 0.05\n",
      " 0.10\n",
      " 0.15\n",
      " 0.20\n",
      " 0.25\n",
      " 0.30\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0S( )1e4\n",
      "Figure 3.14: The stochastic counterpart method replaces the unknown S(\u0015) (that is, the\n",
      "scaled variance of the importance sampling estimator) with its sample average, bS(\u0015). The\n",
      "minimum value of bSis attained around \u0015=0:12.\n",
      "A third method for stochastic optimization is the cross-entropy method. In particular,\n",
      "Algorithm 3.4.3 can easily be modiﬁed to minimize noisy functions S(x)=EeS(x;\u0018), as +101\n",
      "deﬁned in (3.31). The only change required in the algorithm is that every function value\n",
      "S(x) be replaced by its estimate bS(x). Depending on the level of noise in the function, the\n",
      "sample size Nmight have to be increased considerably.\n",
      "Example 3.19 (Cross-Entropy Method for Noisy Optimization) To explore the use\n",
      "of the CE method for noisy optimization, take the following noisy discrete optimization\n",
      "problem. Suppose there is a “black box” that contains an unknown binary sequence of n\n",
      "bits. If one feeds the black box any input vector, it will ﬁrst scramble the input by inde-\n",
      "pendently ﬂipping the bits (changing 0 to 1 and 1 to 0) with a probability \u0012and then return\n",
      "the number of bits that do not match the true (unknown) binary sequence. This is illustrated\n",
      "in Figure 3.15 for n=10.Chapter 3. Monte Carlo Methods 111\n",
      "Figure 3.15: A noisy optimization function as a black box. The input to the black box is a\n",
      "binary vector. Inside the black box the digits of the input vector are scrambled by ﬂipping\n",
      "bits with probability \u0012. The output is the number of bits of the scrambled vector that do not\n",
      "match the true (unknown) binary vector.\n",
      "Denoting by S(x) the true number of matching digits for a binary input vector x, the\n",
      "black box thus returns a noisy estimate bS(x). The objective is to estimate the binary se-\n",
      "quence inside the black box, by feeding it with many input vectors and observing their\n",
      "output. Or, to put it in a di \u000berent way, to minimize S(x) using bS(x) as a proxy. Since there\n",
      "are 2npossible input vectors, it is infeasible to try all possible vectors xeven for moderate\n",
      "n.\n",
      "The following Python program implements the noisy function bS(x) for n=100. Each\n",
      "input bit is ﬂipped with a rather high probability \u0012=0:4, so that the output is a poor indic-\n",
      "ator of how many bits actually match the true vector. This true vector has 1s at positions\n",
      "1;:::; 50 and 0s at 51 ;:::; 100.\n",
      "Snoisy.py\n",
      "import numpy as np\n",
      "def Snoisy(X): #takes a matrix\n",
      "n = X.shape[1]\n",
      "N = X.shape[0]\n",
      "# true binary vector\n",
      "xorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))\n",
      "theta = 0.4 # probability to flip the input\n",
      "# storing the number of bits unequal to the true vector\n",
      "s = np.zeros(N)\n",
      "for i in range(0,N):\n",
      "# determine which bits to flip\n",
      "flip = (np.random.uniform(size=(n)) < theta).astype(int)\n",
      "ind = flip >0\n",
      "X[i][ind] = 1-X[i][ind]\n",
      "s[i] = (X[i] != xorg).sum()\n",
      "return s\n",
      "The CE code below to optimize S(x) is quite similar to the continuous optimization\n",
      "code in Example 3.16. However, instead of sampling iid random variables X1;:::; XNfrom +101\n",
      "a normal distribution, we now sample iid binary vectors X1;:::; XNfrom a Ber(p) distribu-\n",
      "tion. More precisely, given a row vector of probabilities p=[p1;:::; pn], we independently\n",
      "simulate the components X1;:::; Xnof each binary vector Xaccording to Xi\u0018Ber(pi),\n",
      "i=1;:::; n. After each iteration, the vector pis updated as the (vector) mean of the elite112 3.4. Monte Carlo for Optimization\n",
      "samples. The sample size is N=1000 and the number of elite samples is 100. The compon-\n",
      "ents of the initial sampling vector pare all equal to 1 =2; that is, the Xare initially uniformly\n",
      "sampled from the set of all binary vectors of length n=100. At each subsequent iteration\n",
      "the parameter vector is updated via the mean of the elite samples and evolves towards a\n",
      "degenerate vector p\u0003with only 1s and 0s. Sampling from such a Ber(p\u0003) distribution gives\n",
      "an outcome x\u0003=p\u0003, which can be taken as an estimate for the minimizer of S; that is, the\n",
      "true binary vector hidden in the black box. The algorithm stops when phas degenerated\n",
      "su\u000eciently.\n",
      "Figure 3.16 shows the evolution of the vector of probabilities p. This ﬁgure may be\n",
      "seen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE\n",
      "method is able to ﬁnd the true state of the black box, and hence the minimum value of S.\n",
      "00.51\n",
      "00.51\n",
      "00.51\n",
      "00.51\n",
      "00.51\n",
      "0 10 20 30 40 50 60 70 80 90 10000.51\n",
      "Figure 3.16: Evolution of the vector of probabilities p=[p1;:::; pn] towards the degener-\n",
      "ate solution.Chapter 3. Monte Carlo Methods 113\n",
      "CEnoisy.py\n",
      "from Snoisy import Snoisy\n",
      "import numpy as np\n",
      "n = 100\n",
      "rho = 0.1\n",
      "N = 1000; Nel = int(N*rho); eps = 0.01\n",
      "p = 0.5*np.ones(n)\n",
      "i = 0\n",
      "pstart = p\n",
      "ps = np.zeros((1000,n))\n",
      "ps[0] = pstart\n",
      "pdist = np.zeros((1,1000))\n",
      "while np.max(np.minimum(p,1-p)) > eps:\n",
      "i += 1\n",
      "X = (np.random.uniform(size=(N,n)) < p).astype(int)\n",
      "X_tmp = np.array(X, copy=True)\n",
      "SX = Snoisy(X_tmp)\n",
      "ids = np.argsort(SX,axis=0)\n",
      "Elite = X[ids[0:Nel],:]\n",
      "p = np.mean(Elite ,axis=0)\n",
      "ps[i] = p\n",
      "print(p)\n",
      "Further Reading\n",
      "The article [68] explores why the Monte Carlo method is so important in today’s quantitat-\n",
      "ive investigations. The Handbook of Monte Carlo Methods [71] provides a comprehensive\n",
      "overview of Monte Carlo simulation that explores the latest topics, techniques, and real-\n",
      "world applications. Popular books on simulation and the Monte Carlo method include [42],\n",
      "[75], and [104]. A classic reference on random variable generation is [32]. Easy introduc-\n",
      "tions to stochastic simulation are given in [49], [98], and [100]. More advanced theory\n",
      "can be found in [5]. Markov chain Monte Carlo is detailed in [50] and [99]. The research\n",
      "monograph on the cross-entropy method is [103] and a tutorial is provided in [30]. A range\n",
      "of optimization applications of the CE method is given in [16]. Theoretical results on ad-\n",
      "aptive tuning schemes for simulated annealing may be found, for example, in [111]. There\n",
      "are several established ways for gradient estimation. These include the ﬁnite di \u000berence\n",
      "method, inﬁnitesimal perturbation analysis, the score function method, and the method of\n",
      "weak derivatives; see, for example, [51, Chapter 7].\n",
      "Exercises\n",
      "1. We can modify the Box–Muller method in Example 3.1 to draw XandYuniformly + 69\n",
      "on the unit disc,f(x;y)2R2:x2+y261g, in the following way: Independently draw\n",
      "a radius Rand an angle \u0002\u0018U(0;2\u0019), and return X=Rcos(\u0002);Y=Rsin(\u0002). The\n",
      "question is how to draw R.\n",
      "(a) Show that the cdf of Ris given by FR(r)=r2for 06r61 (with FR(r)=0 and114 Exercises\n",
      "FR(r)=1 for r<0 and r>1, respectively).\n",
      "(b) Explain how to simulate Rusing the inverse-transform method.\n",
      "(c) Simulate 100 independent draws of [ X;Y]>according to the method described\n",
      "above.\n",
      "2. A simple acceptance–rejection method to simulate a vector Xin the unit d-ballfx2\n",
      "Rd:kxk61gis to ﬁrst generate Xuniformly in the hyper cube [ \u00001;1]dand then to\n",
      "accept the point only if kXk61. Determine an analytic expression for the probability\n",
      "of acceptance as a function of dand plot this for d=1;:::; 50.\n",
      "3. Let the random variable Xhave pdf\n",
      "f(x)=8>><>>:1\n",
      "2x; 06x<1;\n",
      "1\n",
      "2; 16x65\n",
      "2:\n",
      "Simulate a random variable from f(x), using\n",
      "(a) the inverse-transform method;\n",
      "(b) the acceptance–rejection method, using the proposal density\n",
      "g(x)=8\n",
      "25x;06x65\n",
      "2:\n",
      "4. Construct simulation algorithms for the following distributions:\n",
      "(a) The Weib (\u000b;\u0015) distribution, with cdf F(x)=1\u0000e\u0000(\u0015x)\u000b;x>0, where\u0015>0 and\n",
      "\u000b>0.\n",
      "(b) The Pareto (\u000b;\u0015) distribution, with pdf f(x)=\u000b\u0015(1+\u0015x)\u0000(\u000b+1);x>0, where\n",
      "\u0015>0 and\u000b>0.\n",
      "5. We wish to sample from the pdf\n",
      "f(x)=xe\u0000x;x>0;\n",
      "using acceptance–rejection with the proposal pdf g(x)=e\u0000x=2=2,x>0.\n",
      "(a) Find the smallest Cfor which Cg(x)>f(x) for all x.\n",
      "(b) What is the e \u000eciency of this acceptance–rejection method?\n",
      "6. Let [ X;Y]>be uniformly distributed on the triangle with corners (0 ;0);(1;2), and\n",
      "(\u00001;1). Give the distribution of [ U;V]>deﬁned by the linear transformation\n",
      "\"U\n",
      "V#\n",
      "=\"1 2\n",
      "3 4#\"X\n",
      "Y#\n",
      ":\n",
      "7. Explain how to generate a random variable from the extreme value distribution ,\n",
      "which has cdf\n",
      "F(x)=1\u0000e\u0000exp(x\u0000\u0016\n",
      "\u001b);\u00001<x<1;(\u001b>0);\n",
      "via the inverse-transform method.Chapter 3. Monte Carlo Methods 115\n",
      "8. Write a program that generates and displays 100 random vectors that are uniformly\n",
      "distributed within the ellipse\n",
      "5x2+21x y+25y2=9:\n",
      "[Hint: Consider generating uniformly distributed samples within the circle of radius\n",
      "3 and use the fact that linear transformations preserve uniformity to transform the\n",
      "circle to the given ellipse.]\n",
      "9. Suppose that Xi\u0018Exp(\u0015i), independently, for all i=1;:::; n. Let\u0005=[\u00051;:::;\u0005n]>\n",
      "be the random permutation induced by the ordering X\u00051<X\u00052<\u0001\u0001\u0001<X\u0005n;and\n",
      "deﬁne Z1:=X\u00051andZj:=X\u0005j\u0000X\u0005j\u00001forj=2;:::; n.\n",
      "(a) Determine an n\u0002nmatrix Asuch that Z=AXand show that det( A)=1.\n",
      "(b) Denote the joint pdf of Xand\u0005as\n",
      "fX;\u0005(x;\u0019)=nY\n",
      "i=1\u0015\u0019iexp\u0000\u0000\u0015\u0019ix\u0019i\u0001\u00021fx\u00191<\u0001\u0001\u0001<x\u0019ng;x>0;\u00192P n;\n",
      "wherePnis the set of all n! permutations of f1;:::; ng. Use the multivariate\n",
      "transformation formula (C.22) to show that +434\n",
      "fZ;\u0005(z;\u0019)=exp0BBBBB@\u0000nX\n",
      "i=1ziX\n",
      "k>i\u0015\u0019k1CCCCCAnY\n",
      "i=1\u0015i; z>0;\u00192P n:\n",
      "Hence, conclude that the probability mass function of the random permutation\n",
      "\u0005is:\n",
      "P[\u0005=\u0019]=nY\n",
      "i=1\u0015\u0019iP\n",
      "k>i\u0015\u0019k;\u00192P n:\n",
      "(c) Write pseudo-code to simulate a uniform random permutation \u00052P n; that is,\n",
      "such thatP[\u0005=\u0019]=1\n",
      "n!, and explain how this uniform random permutation\n",
      "can be used to reshu \u000fe a training set \u001cn.\n",
      "10. Consider the Markov chain with transition graph given in Figure 3.17, starting in\n",
      "state 1.\n",
      "Start0.51\n",
      "0.80.9 0 .20.10.5\n",
      "0.2\n",
      "0.30.50.30.7\n",
      "43\n",
      "12 6\n",
      "5\n",
      "Figure 3.17: The transition graph for the Markov chain fXt;t=0;1;2;:::g.116 Exercises\n",
      "(a) Construct a computer program to simulate the Markov chain, and show a real-\n",
      "ization for N=100 steps.\n",
      "(b) Compute the limiting probabilities that the Markov chain is in state 1,2,. . . ,6,\n",
      "by solving the global balance equations (C.42). +454\n",
      "(c) Verify that the exact limiting probabilities correspond to the average fraction\n",
      "of times that the Markov process visits states 1,2,. . . ,6, for a large number of\n",
      "steps N.\n",
      "11. As a generalization of Example C.9, consider a random walk on an arbitrary undir- +455\n",
      "ected connected graph with a ﬁnite vertex set V. For any vertex v2V, letd(v) be\n",
      "the number of neighbors of v— called the degree ofv. The random walk can jump to\n",
      "each one of the neighbors with probability 1 =d(v) and can be described by a Markov\n",
      "chain. Show that, if the chain is aperiodic , the limiting probability that the chain is\n",
      "in state vis equal to d(v)=P\n",
      "v02Vd(v0).\n",
      "12. Let U;V\u0018iidU(0;1). The reason why in Example 3.7 the sample mean and sample +76\n",
      "median behave very di \u000berently is that E[U=V]=1, while the median of U=Vis\n",
      "ﬁnite. Show this, and compute the median. [Hint: start by determining the cdf of\n",
      "Z=U=Vby writing it as an expectation of an indicator function.]\n",
      "13. Consider the problem of generating samples from Y\u0018Gamma (2;10).\n",
      "(a) Direct simulation: Let U1;U2\u0018iidU(0;1). Show that\u0000ln(U1)=10\u0000ln(U2)=10\u0018\n",
      "Gamma (2;10). [Hint: derive the distribution of \u0000ln(U1)=10 and use Ex-\n",
      "ample C.1.] +429\n",
      "(b) Simulation via MCMC: Implement an independence sampler to simulate from\n",
      "theGamma (2;10) target pdf\n",
      "f(x)=100xe\u000010x;x>0;\n",
      "using proposal transition density q(yjx)=g(y), where g(y) is the pdf of an\n",
      "Exp(5) random variable. Generate N=500 samples, and compare the true cdf\n",
      "with the empirical cdf of the data.\n",
      "14. Let X=[X;Y]>be a random column vector with a bivariate normal distribution with\n",
      "expectation vector \u0016=[1;2]>and covariance matrix\n",
      "\u0006=\"1a\n",
      "a4#\n",
      ":\n",
      "(a) What are the conditional distributions of ( YjX=x) and ( XjY=y)? [Hint: use\n",
      "Theorem C.8.] +438\n",
      "(b) Implement a Gibbs sampler to draw 103samples from the bivariate distribution\n",
      "N(\u0016;\u0006) for a=0, 1, and 1:75, and plot the resulting samples.\n",
      "15. Here the objective is to sample from the 2-dimensional pdf\n",
      "f(x;y)=ce\u0000(xy+x+y);x>0;y>0;\n",
      "for some normalization constant c, using a Gibbs sampler. Let ( X;Y)\u0018f.Chapter 3. Monte Carlo Methods 117\n",
      "(a) Find the conditional pdf of Xgiven Y=y, and the conditional pdf of Ygiven\n",
      "X=x.\n",
      "(b) Write working Python code that implements the Gibbs sampler and outputs\n",
      "1000 points that are approximately distributed according to f.\n",
      "(c) Describe how the normalization constant ccould be estimated via Monte Carlo\n",
      "simulation, using random variables X1;:::; XN;Y1;:::; YNiid\u0018Exp(1).\n",
      "16. We wish to estimate \u0016=R2\n",
      "\u00002e\u0000x2=2dx=R\n",
      "H(x)f(x) dxvia Monte Carlo simulation\n",
      "using two di \u000berent approaches: (1) deﬁning H(x)=4 e\u0000x2=2and fthe pdf of the\n",
      "U[\u00002;2] distribution and (2) deﬁning H(x)=p\n",
      "2\u00191f\u000026x62gand fthe pdf of\n",
      "theN(0;1) distribution.\n",
      "(a) For both cases estimate \u0016via the estimator b\u0016\n",
      "b\u0016=N\u00001NX\n",
      "i=1H(Xi): (3.34)\n",
      "Use a sample size of N=1000.\n",
      "(b) For both cases estimate the relative error \u0014ofb\u0016using N=100.\n",
      "(c) Give a 95% conﬁdence interval for \u0016for both cases using N=100.\n",
      "(d) From part (b), assess how large Nshould be such that the relative width of the\n",
      "conﬁdence interval is less than 0 :01, and carry out the simulation with this N.\n",
      "Compare the result with the true value of \u0016.\n",
      "] of some random variable X,he tail probability \u0016=P[X>\n",
      "is large. The crude Monte Carlo estimator of \u0016is\n",
      "b\u0016=1\n",
      "NNX\n",
      "i=1Zi; (3.35)\n",
      "g,i=1;:::; N. XNare iid copies of XandZi=1fXi>\n",
      "(a) Show that b\u0016is unbiased; that is, Eb\u0016=\u0016.\n",
      "(b) Express the relative error of b\u0016, i.e.,\n",
      "RE=p\n",
      "Varb\u0016\n",
      "Eb\u0016;\n",
      "in terms of Nand\u0016.\n",
      "(c) Explain how to estimate the relative error of b\u0016from outcomes x1;:::; xNof\n",
      "X1;:::; XN, and how to construct a 95% conﬁdence interval for \u0016.\n",
      "(d) An unbiased estimator Zof\u0016is said to be logarithmically e \u000ecient if\n",
      "lim\n",
      "!1lnEZ2\n",
      "ln\u00162=1: (3.36)\n",
      "Show that the CMC estimator (3.35) with N=1 is not logarithmically e \u000ecient.118 Exercises\n",
      "18. One of the test cases in [70] involves the minimization of the Hougen function. Im-\n",
      "plement a cross-entropy and a simulated annealing algorithm to carry out this optim-\n",
      "ization task.\n",
      "19. In the binary knapsack problem , the goal is to solve the optimization problem:\n",
      "max\n",
      "x2f0;1gnp>x;\n",
      "subject to the constraints\n",
      "Ax6c;\n",
      "where pandwaren\u00021 vectors of non-negative numbers, A=(ai j) is an m\u0002n\n",
      "matrix, and cis an m\u00021 vector. The interpretation is that xj=1 or 0 depending\n",
      "on whether item jwith value pjis packed into the knapsack or not , j=1;:::; n;\n",
      "The variable ai jrepresents the i-th attribute (e.g., volume, weight) of the j-th item.\n",
      "Associated with each attribute is a maximal capacity, e.g., c1could be the maximum\n",
      "volume of the knapsack, c2the maximum weight, etc.\n",
      "Write a CE program to solve the Sento1.dat knapsack problem at http://peop\n",
      "le.brunel.ac.uk/~mastjjb/jeb/orlib/files/mknap2.txt , as described in\n",
      "[16].\n",
      "20. Let ( C1;R1);(C2;R2);::: be a renewal reward process, with ER1<1and\n",
      "EC1<1. Let At=PNt\n",
      "i=1Ri=tbe the average reward at time t=1;2;:::, where\n",
      "Nt=maxfn:Tn6tgand we have deﬁned Tn=Pn\n",
      "i=1Cias the time of the n-th re-\n",
      "newal.\n",
      "(a) Show that Tn=na:s:\u0000!EC1asn!1 .\n",
      "(b) Show that Nta:s:\u0000!1 ast!1 .\n",
      "(c) Show that Nt=ta:s:\u0000!1=EC1ast!1 . [Hint: Use the fact that TNt6t6TNt+1for\n",
      "allt=1;2;:::.]\n",
      "(d) Show that\n",
      "Ata:s:\u0000!ER1\n",
      "EC1ast!1:\n",
      "21. Prove Theorem 3.3. +92\n",
      "22. Prove that if H(x)>0 the importance sampling pdf g\u0003in (3.22) gives the zero- +96\n",
      "variance importance sampling estimator b\u0016=\u0016.\n",
      "23. Let XandYbe random variables (not necessarily independent) and suppose we wish\n",
      "to estimate the expected di \u000berence\u0016=E[X\u0000Y]=EX\u0000EY.\n",
      "(a) Show that if XandYarepositively correlated , the variance of X\u0000Yis smaller\n",
      "than if XandYareindependent .\n",
      "(b) Suppose now that Xand Yhave cdfs Fand G, respectively, and are\n",
      "simulated via the inverse-transform method: X=F\u00001(U),Y=G\u00001(V), with\n",
      "U;V\u0018U(0;1), not necessarily independent. Intuitively, one might expect thatChapter 3. Monte Carlo Methods 119\n",
      "ifUandVare positively correlated, the variance of X\u0000Ywould be smaller than\n",
      "ifUandVare independent. Show that this is not always the case by providing\n",
      "a counter-example.\n",
      "(c) Continuing (b), assume now that FandGare continuous. Show that the vari-\n",
      "ance of X\u0000Yby taking common random numbers U =Vis no larger than\n",
      "when UandVare independent. [Hint: Use the following lemma of Hoe \u000bding\n",
      "[41]: If ( X;Y) have joint cdf Hwith marginal cdfs of XandYbeing FandG,\n",
      "respectively, then\n",
      "Cov(X;Y)=Z1\n",
      "\u00001Z1\n",
      "\u00001(H(x;y)\u0000F(x)G(y))dxdy;\n",
      "providedCov(X;Y) exists.]120CHAPTER4\n",
      "UNSUPERVISED LEARNING\n",
      "When there is no distinction between response and explanatory variables, unsu-\n",
      "pervised methods are required to learn the structure of the data. In this chapter we\n",
      "look at various unsupervised learning techniques, such as density estimation, cluster-\n",
      "ing, and principal component analysis. Important tools in unsupervised learning in-\n",
      "clude the cross-entropy training loss, mixture models, the Expectation–Maximization\n",
      "algorithm, and the Singular Value Decomposition.\n",
      "4.1 Introduction\n",
      "In contrast to supervised learning, where an “output” (response) variable yis explained by\n",
      "an “input” (explanatory) vector x, in unsupervised learning there is no response variable\n",
      "and the overall goal is to extract useful information and patterns from the data, e.g., in\n",
      "the form\u001c=fx1;:::; xngor as a matrix X>=[x1;:::; xn]. In essence, the objective of\n",
      "unsupervised learning is to learn about the underlying probability distribution of the data.\n",
      "We start in Section 4.2 by setting up a framework for unsupervised learning that is\n",
      "similar to the framework used for supervised learning in Section 2.3. That is, we formulate + 23\n",
      "unsupervised learning in terms of risk and loss minimization; but now involving the cross-\n",
      "entropy risk, rather than the squared-error risk. In a natural way this leads to fundamental\n",
      "learning concepts such as likelihood, Fisher information, and the Akaike information cri-\n",
      "terion. Section 4.3 introduces the Expectation–Maximization (EM) algorithm as a useful\n",
      "method for maximizing likelihood functions when their solution cannot be found easily in\n",
      "closed form.\n",
      "If the data forms an iid sample from some unknown distribution, the “empirical dis-\n",
      "tribution” of the data provides valuable information about the unknown distribution. In\n",
      "Section 4.4 we formalize the concept of the empirical distribution (a generalization of the\n",
      "empirical cdf) and explain how we can produce an estimate of the underlying probability + 11\n",
      "density function of the data using kernel density estimators.\n",
      "Most unsupervised learning techniques focus on identifying certain traits of the under-\n",
      "lying distribution, such as its local maximizers. A related idea is to partition the data into\n",
      "clusters of points that are in some sense “similar” to each other. In Section 4.5 we formu-\n",
      "late the clustering problem in terms of a mixture model. In particular, the data are assumed +135\n",
      "121122 4.2. Risk and Loss in Unsupervised Learning\n",
      "to come from a mixture of (usually Gaussian) distributions, and the objective is to recover\n",
      "the parameters of the mixture distributions from the data. The principal tool for parameter\n",
      "estimation in mixture models is the EM algorithm.\n",
      "Section 4.6 discusses a more heuristic approach to clustering, where the data are\n",
      "grouped according to certain “cluster centers”, whose positions are found by solving an\n",
      "optimization problem. Section 4.7 describes how clusters can be constructed in a hierarch-\n",
      "ical manner.\n",
      "Finally, in Section 4.8 we discuss the unsupervised learning technique called Principal\n",
      "Component Analysis (PCA), which is an important tool for reducing the dimensionality of\n",
      "the data.\n",
      "We will revisit various unsupervised learning techniques in subsequent chapters on su-\n",
      "pervised learning. For example, cross-entropy training loss minimization will be important\n",
      "in logistic regression (Section 5.7) and classiﬁcation (Chapter 7), and PCA can be used +204\n",
      "+253 for variable selection and dimensionality reduction, to make models easier to train and\n",
      "increase their predictive power; see e.g., Sections 6.8 and 7.4.\n",
      "4.2 Risk and Loss in Unsupervised Learning\n",
      "In unsupervised learning, the training data T:=fX1;:::; Xngonly consists of (what are\n",
      "usually assumed to be) independent copies of a feature vector X; there is no response\n",
      "data. Suppose our objective is to learn the unknown pdf fofXbased on an outcome\n",
      "\u001c=fx1;:::; xngof the training data T. Conveniently, we can follow the same line of reas-\n",
      "oning as for supervised learning, discussed in Sections 2.3–2.5. Table 4.1 gives a summary +23\n",
      "of deﬁnitions for the case of unsupervised learning. Compare this with Table 2.1 for the\n",
      "supervised case. +25\n",
      "Similar to supervised learning, we wish to ﬁnd a function g, which is now a probability\n",
      "density (continuous or discrete), that best approximates the pdf fin terms of minimizing a\n",
      "risk\n",
      "`(g) :=ELoss( f(X);g(X)); (4.1)\n",
      "where Loss is a loss function. In (2.27), we already encountered the Kullback–Leibler risk\n",
      "`(g) :=Elnf(X)\n",
      "g(X)=Elnf(X)\u0000Elng(X): (4.2)\n",
      "IfGis a class of functions that contains f, then minimizing the Kullback–Leibler risk over\n",
      "Gwill yield the (correct) minimizer f. Of course, the problem is that minimization of (4.2)\n",
      "depends on f, which is generally not known. However, since the term Elnf(X) does not\n",
      "depend on g, it plays no role in the minimization of the Kullback–Leibler risk. By removing\n",
      "this term, we obtain the cross-entropy risk cross -entropy\n",
      "risk(for discrete Xreplace the integral with a sum):\n",
      "`(g) :=\u0000Elng(X)=\u0000Z\n",
      "f(x) lng(x) dx: (4.3)\n",
      "Thus, minimizing the cross-entropy risk (4.3) over all g2G, again gives the minimizer\n",
      "f, provided that f2G. Unfortunately, solving (4.3) is also infeasible in general, as it stillChapter 4. Unsupervised Learning 123\n",
      "Table 4.1: Summary of deﬁnitions for unsupervised learning.\n",
      "x Fixed feature vector.\n",
      "X Random feature vector.\n",
      "f(x) Pdf of Xevaluated at the point x.\n",
      "\u001cor\u001cn Fixed training data fxi;i=1;:::; ng.\n",
      "TorTn Random training data fXi;i=1;:::; ng.\n",
      "g Approximation of the pdf f.\n",
      "Loss( f(x);g(x)) Loss incurred when approximating f(x) with g(x).\n",
      "`(g) Risk for approximation function g; that is,ELoss( f(X);g(X)).\n",
      "gGOptimal approximation function in function class G; that is,\n",
      "argming2G`(g).\n",
      "`\u001c(g) Training loss for approximation function (guess) g; that is,\n",
      "the sample average estimate of `(g) based on a ﬁxed training\n",
      "sample\u001c.\n",
      "`T(g) The same as `\u001c(g), but now for a random training sample T.\n",
      "gG\n",
      "\u001corg\u001c Thelearner : argming2G`\u001c(g). That is, the optimal approxima-\n",
      "tion function based on a ﬁxed training set \u001cand function class\n",
      "G. We suppress the superscript Gif the function class is impli-\n",
      "cit.\n",
      "gG\n",
      "TorgT The learner for a random training set T.\n",
      "depends on f. Instead, we seek to minimize the cross-entropy training loss cross -entropy\n",
      "training loss:\n",
      "`\u001c(g) :=1\n",
      "nnX\n",
      "i=1Loss( f(xi);g(xi))=\u00001\n",
      "nnX\n",
      "i=1lng(xi) (4.4)\n",
      "over the class of functions G, where\u001c=fx1;:::; xngis an iid sample from f. This optimiz-\n",
      "ation is doable without knowing fand is equivalent to solving the maximization problem\n",
      "max\n",
      "g2GnX\n",
      "i=1lng(xi): (4.5)\n",
      "A key step in setting up the learning procedure is to select a suitable function class Gover\n",
      "which to optimize. The standard approach is to parameterize gwith a parameter \u0012and let\n",
      "Gbe the class of functions fg(\u0001j\u0012);\u00122\u0002gfor some p-dimensional parameter set \u0002. For the\n",
      "remainder of Section 4.2, we will be using this function class, as well as the cross-entropy\n",
      "risk.\n",
      "The function \u00127!g(xj\u0012) is called the likelihood function likelihood\n",
      "function. It gives the likelihood of\n",
      "the observed feature vector xunder g(\u0001j\u0012), as a function of the parameter \u0012. The natural\n",
      "logarithm of the likelihood function is called the log-likelihood function and its gradient\n",
      "with respect to \u0012is called the score function score function , denoted S(xj\u0012); that is,\n",
      "S(xj\u0012) :=@lng(xj\u0012)\n",
      "@\u0012=@g(xj\u0012)\n",
      "@\u0012\n",
      "g(xj\u0012): (4.6)124 4.2. Risk and Loss in Unsupervised Learning\n",
      "The random score S(Xj\u0012), with X\u0018g(\u0001j\u0012), is of particular interest. In many cases, its\n",
      "expectation is equal to the zero vector ; namely,\n",
      "E\u0012S(Xj\u0012)=Z@g(xj\u0012)\n",
      "@\u0012\n",
      "g(xj\u0012)g(xj\u0012) dx\n",
      "=Z@g(xj\u0012)\n",
      "@\u0012dx=@R\n",
      "g(xj\u0012) dx\n",
      "@\u0012=@1\n",
      "@\u0012=0;(4.7)\n",
      "provided that the interchange of di \u000berentiation and integration is justiﬁed. This is true for\n",
      "a large number of distributions, including the normal, exponential, and binomial distri-\n",
      "butions. Notable exceptions are distributions whose support depends on the distributional\n",
      "parameter; for example the U(0;\u0012) distribution.\n",
      "It is important to see whether expectations are taken with respect to X\u0018g(\u0001j\u0012) or\n",
      "X\u0018f. We use the expectation symbols E\u0012andEto distinguish the two cases.\n",
      "From now on we simply assume that the interchange of di \u000berentiation and integration\n",
      "is permitted; see, e.g., [76] for su \u000ecient conditions. The covariance matrix of the random\n",
      "score S(Xj\u0012) is called the Fisher information matrix Fisher\n",
      "information\n",
      "matrix, which we denote by ForF(\u0012) to\n",
      "show its dependence on \u0012. Since the expected score is 0, we have\n",
      "F(\u0012)=E\u0012[S(Xj\u0012)S(Xj\u0012)>]: (4.8)\n",
      "A related matrix is the expected Hessian matrix of \u0000lng(Xj\u0012): +400\n",
      "H(\u0012) :=E\"\n",
      "\u0000@S(Xj\u0012)\n",
      "@\u0012#\n",
      "=\u0000E26666666666666666666664@2lng(Xj\u0012)\n",
      "@2\u00121@2lng(Xj\u0012)\n",
      "@\u00121@\u00122\u0001\u0001\u0001@2lng(Xj\u0012)\n",
      "@\u00121@\u0012p\n",
      "@2lng(Xj\u0012)\n",
      "@\u00122@\u00121@2lng(Xj\u0012)\n",
      "@2\u00122\u0001\u0001\u0001@2lng(Xj\u0012)\n",
      "@\u00122@\u0012p::::::::::::\n",
      "@2lng(Xj\u0012)\n",
      "@\u0012p@\u00121@2lng(Xj\u0012)\n",
      "@\u0012p@\u00122\u0001\u0001\u0001@2lng(Xj\u0012)\n",
      "@2\u0012p37777777777777777777775: (4.9)\n",
      "Note that the expectation here is with respect to X\u0018f. It turns out that if f=g(\u0001j\u0012), the\n",
      "two matrices are the same ; that is,\n",
      "F(\u0012)=H(\u0012); (4.10)\n",
      "provided that we may swap the order of di \u000berentiation and integration (expectation). This\n",
      "result is called the information matrix equality information\n",
      "matrix equality. We leave the proof as Exercise 1.\n",
      "The matrices F(\u0012) and H(\u0012) play important roles in approximating the cross-entropy\n",
      "risk for large n. To set the scene, let gG=g(\u0001j\u0012\u0003) be the minimizer of the cross-entropy\n",
      "risk\n",
      "r(\u0012) :=\u0000Elng(Xj\u0012):\n",
      "We assume that r, as a function of \u0012, is well-behaved; in particular, that in the neighborhood\n",
      "of\u0012\u0003it is strictly convex and twice continuously di \u000berentiable (this holds true, for example,\n",
      "ifgis a Gaussian density). It follows that \u0012\u0003is a root ofES(Xj\u0012), because\n",
      "0=@r(\u0012\u0003)\n",
      "@\u0012=\u0000@Elng(Xj\u0012\u0003)\n",
      "@\u0012=\u0000E@lng(Xj\u0012\u0003)\n",
      "@\u0012=\u0000ES(Xj\u0012\u0003);Chapter 4. Unsupervised Learning 125\n",
      "again provided that the order of di \u000berentiation and integration (expectation) can be\n",
      "swapped. In the same way, H(\u0012) is then the Hessian matrix of r. Let g(\u0001jb\u0012n) be the minim-\n",
      "izer of the training loss\n",
      "rTn(\u0012) :=\u00001\n",
      "nnX\n",
      "i=1lng(Xij\u0012);\n",
      "whereTn=fX1;:::; Xngis a random training set. Let r\u0003be the smallest possible cross-\n",
      "entropy risk, taken over all functions; clearly, r\u0003=\u0000Elnf(X), where X\u0018f. Similar to\n",
      "the supervised learning case, we can decompose the generalization risk, `(g(\u0001jb\u0012n))=r(b\u0012n),\n",
      "into\n",
      "r(b\u0012n)=r\u0003+r(\u0012\u0003)\u0000r\u0003\n",
      "|     {z     }\n",
      "approx. error+r(b\u0012n)\u0000r(\u0012\u0003)|         {z         }\n",
      "statistical error=r(\u0012\u0003)\u0000Elng(Xj\u0012\u0003)\n",
      "g(Xjb\u0012n):\n",
      "The following theorem speciﬁes the asymptotic behavior of the components of the gener-\n",
      "alization risk. In the proof we assume that b\u0012nP\u0000!\u0012\u0003asn!1 . +441\n",
      "Theorem 4.1: Approximating the Cross-Entropy Risk\n",
      "It holds asymptotically ( n!1 ) that\n",
      "Er(b\u0012n)\u0000r(\u0012\u0003)'tr\u0010\n",
      "F(\u0012\u0003)H\u00001(\u0012\u0003)\u0011\n",
      "=(2n); (4.11)\n",
      "where\n",
      "r(\u0012\u0003)'ErTn(b\u0012n)+tr\u0010\n",
      "F(\u0012\u0003)H\u00001(\u0012\u0003)\u0011\n",
      "=(2n): (4.12)\n",
      "Proof: A Taylor expansion of r(b\u0012n) around\u0012\u0003gives the statistical error +402\n",
      "r(b\u0012n)\u0000r(\u0012\u0003)=(b\u0012n\u0000\u0012\u0003)>@r(\u0012\u0003)\n",
      "@\u0012| {z }\n",
      "=0+1\n",
      "2(b\u0012n\u0000\u0012\u0003)>H(\u0012n)(b\u0012n\u0000\u0012\u0003); (4.13)\n",
      "where\u0012nlies on the line segment between \u0012\u0003andb\u0012n. For large nwe may replace H(\u0012n) with\n",
      "H(\u0012\u0003) as, by assumption, b\u0012nconverges to \u0012\u0003. The matrix H(\u0012\u0003) is positive deﬁnite because\n",
      "r(\u0012) is strictly convex at \u0012\u0003by assumption, and therefore invertible. It is important to realize\n",
      "thatb\u0012nis in fact an M-estimator of \u0012\u0003. In particular, in the notation of Theorem C.19, we +451\n",
      "have =S,A=H(\u0012\u0003), and B=F(\u0012\u0003). Consequently, by that same theorem,\n",
      "pn(b\u0012n\u0000\u0012\u0003)d\u0000!N\u0010\n",
      "0;H\u00001(\u0012\u0003)F(\u0012\u0003)H\u0000>(\u0012\u0003)\u0011\n",
      ": (4.14)\n",
      "Combining (4.13) with (4.14), it follows from Theorem C.2 that asymptotically the +432\n",
      "expected estimation error is given by (4.11).\n",
      "Next, we consider a Taylor expansion of rTn(\u0012\u0003) around b\u0012n:\n",
      "rTn(\u0012\u0003)=rTn(b\u0012n)+(\u0012\u0003\u0000b\u0012n)>@rTn(b\u0012n)\n",
      "@\u0012|   {z   }\n",
      "=0+1\n",
      "2(\u0012\u0003\u0000b\u0012n)>HTn(\u0012n)(\u0012\u0003\u0000b\u0012n); (4.15)126 4.2. Risk and Loss in Unsupervised Learning\n",
      "where HTn(\u0012n) :=\u00001\n",
      "nPn\n",
      "i=1@S(Xij\u0012n)\n",
      "@\u0012is the Hessian of rTn(\u0012) at some\u0012nbetween b\u0012nand\u0012\u0003.\n",
      "Taking expectations on both sides of (4.15), we obtain\n",
      "r(\u0012\u0003)=ErTn(b\u0012n)+1\n",
      "2E(\u0012\u0003\u0000b\u0012n)>HTn(\u0012n)(\u0012\u0003\u0000b\u0012n):\n",
      "Replacing HTn(\u0012n) with H(\u0012\u0003) for large nand using (4.14), we have\n",
      "nE(\u0012\u0003\u0000b\u0012n)>HTn(\u0012n)(\u0012\u0003\u0000b\u0012n)\u0000!tr\u0010\n",
      "F(\u0012\u0003)H\u00001(\u0012\u0003)\u0011\n",
      ";n!1:\n",
      "Therefore, asymptotically as n!1 , we have (4.12). \u0003\n",
      "Theorem 4.1 has a number of interesting consequences:\n",
      "1. Similar to Section 2.5.1, the training loss `Tn(gTn)=rTn(b\u0012n) tends to underestimate the +35\n",
      "risk`(gG)=r(\u0012\u0003), because the training set Tnis used to both train g2G(that is, estimate\n",
      "\u0012\u0003) and to estimate the risk. The relation (4.12) tells us that on average the training loss\n",
      "underestimates the true risk by tr( F(\u0012\u0003)H\u00001(\u0012\u0003))=(2n).\n",
      "2. Adding equations (4.11) and (4.12), yields the following asymptotic approximation to\n",
      "the expected generalization risk:\n",
      "Er(b\u0012n)'ErTn(b\u0012n)+1\n",
      "ntr\u0010\n",
      "F(\u0012\u0003)H\u00001(\u0012\u0003)\u0011\n",
      "(4.16)\n",
      "The ﬁrst term on the right-hand side of (4.16) can be estimated (without bias) via the\n",
      "training loss rTn(b\u0012n). As for the second term, we have already mentioned that when the\n",
      "true model f2G, then F(\u0012\u0003)=H(\u0012\u0003). Therefore, when Gis deemed to be a su \u000eciently\n",
      "rich class of models parameterized by a p-dimensional vector \u0012, we may approximate the\n",
      "second term as tr( F(\u0012\u0003)H\u00001(\u0012\u0003))=n\u0019tr(Ip)=n=p=n. This suggests the following heuristic\n",
      "approximation to the (expected) generalization risk:\n",
      "Er(b\u0012n)\u0019rTn(b\u0012n)+p\n",
      "n: (4.17)\n",
      "3. Multiplying both sides of (4.16) by 2 nand substituting tr\u0010\n",
      "F(\u0012\u0003)H\u00001(\u0012\u0003)\u0011\n",
      "\u0019p, we obtain\n",
      "the approximation:\n",
      "2n r(b\u0012n)\u0019\u00002nX\n",
      "i=1lng(Xijb\u0012n)+2p: (4.18)\n",
      "The right-hand side of (4.18) is called the Akaike information criterion Akaike\n",
      "information\n",
      "criterion(AIC). Just like\n",
      "(4.17), the AIC approximation can be used to compare the di \u000berence in generalization risk\n",
      "of two or more learners. We prefer the learner with the smallest (estimated) generalization\n",
      "risk.\n",
      "Suppose that, for a training set T, the training loss rT(\u0012) has a unique minimum point\n",
      "b\u0012which lies in the interior of \u0002. IfrT(\u0012) is a di \u000berentiable function with respect to \u0012, then\n",
      "we can ﬁnd the optimal parameter b\u0012by solving\n",
      "@rT(\u0012)\n",
      "@\u0012=1\n",
      "nnX\n",
      "i=1S(Xij\u0012)\n",
      "|           {z           }\n",
      "ST(\u0012)=0:Chapter 4. Unsupervised Learning 127\n",
      "In other words, the maximum likelihood estimate b\u0012for\u0012is obtained by solving the root of\n",
      "the average score function, that is, by solving\n",
      "ST(\u0012)=0: (4.19)\n",
      "It is often not possible to ﬁnd b\u0012in an explicit form. In that case one needs to solve the\n",
      "equation (4.19) numerically. There exist many standard techniques for root-ﬁnding, e.g.,\n",
      "viaNewton’s method (see Section B.3.1), whereby, starting from an initial guess \u00120, sub-Newton ’s\n",
      "method\n",
      "+411sequent iterates are obtained via the iterative scheme\n",
      "\u0012t+1=\u0012t+H\u00001\n",
      "T(\u0012t)ST(\u0012t);\n",
      "where\n",
      "HT(\u0012) :=\u0000@ST(\u0012)\n",
      "@\u0012=1\n",
      "nnX\n",
      "i=1\u0000@S(Xij\u0012)\n",
      "@\u0012\n",
      "is the average Hessian matrix of f\u0000lng(Xij\u0012)gn\n",
      "i=1. Under f=g(\u0001j\u0012), the expectation of\n",
      "HT(\u0012) is equal to the information matrix F(\u0012), which does not depend on the data. This\n",
      "suggests an alternative iterative scheme, called Fisher’s scoring method Fisher ’s\n",
      "scoring method:\n",
      "\u0012t+1=\u0012t+F\u00001(\u0012t)ST(\u0012t); (4.20)\n",
      "which is not only easier to implement (if the information matrix can be readily evaluated),\n",
      "but also is more numerically stable.\n",
      "Example 4.1 (Maximum Likelihood for the Gamma Distribution) We wish to ap-\n",
      "proximate the density of the Gamma (\u000b\u0003;\u0015\u0003) distribution for some true but unknown para-\n",
      "meters\u000b\u0003and\u0015\u0003, on the basis of a training set \u001c=fx1;:::; xngof iid samples from this\n",
      "distribution. Choosing our approximating function g(\u0001j\u000b;\u0015) in the same class of gamma\n",
      "densities,\n",
      "g(xj\u000b;\u0015)=\u0015\u000bx\u000b\u00001e\u0000\u0015x\n",
      "\u0000(\u000b);x>0; (4.21)\n",
      "with\u000b>0 and\u0015>0, we seek to solve (4.19). Taking the logarithm in (4.21), the log-\n",
      "likelihood function is given by\n",
      "l(xj\u000b;\u0015) :=\u000bln\u0015\u0000ln\u0000(\u000b)+(\u000b\u00001) ln x\u0000\u0015x:\n",
      "It follows that\n",
      "S(\u000b;\u0015)=266664@\n",
      "@\u000bl(xj\u000b;\u0015)\n",
      "@\n",
      "@\u0015l(xj\u000b;\u0015)377775=\"ln\u0015\u0000 (\u000b)+lnx\n",
      "\u000b\n",
      "\u0015\u0000x#\n",
      ";\n",
      "where is the derivative of ln \u0000: the so-called digamma function digamma\n",
      "function. Hence,\n",
      "H(\u000b;\u0015)=\u0000E2666664@2\n",
      "@\u000b2l(Xj\u000b;\u0015)@2\n",
      "@\u000b@\u0015l(Xj\u000b;\u0015)\n",
      "@2\n",
      "@\u000b@\u0015l(Xj\u000b;\u0015)@2\n",
      "@\u00152l(Xj\u000b;\u0015)3777775=\u0000E\"\u0000 0(\u000b)1\n",
      "\u00151\n",
      "\u0015\u0000\u000b\n",
      "\u00152#\n",
      "=\" 0(\u000b)\u00001\n",
      "\u0015\n",
      "\u00001\n",
      "\u0015\u000b\n",
      "\u00152#\n",
      ":\n",
      "Fisher’s scoring method (4.20) can now be used to solve (4.19), with\n",
      "S\u001c(\u000b;\u0015)=\"ln\u0015\u0000 (\u000b)+n\u00001Pn\n",
      "i=1lnxi\n",
      "\u000b\n",
      "\u0015\u0000n\u00001Pn\n",
      "i=1xi#\n",
      "andF(\u000b;\u0015)=H(\u000b;\u0015).128 4.3. Expectation–Maximization (EM) Algorithm\n",
      "4.3 Expectation–Maximization (EM) Algorithm\n",
      "TheExpectation–Maximization algorithm (EM) is a general algorithm for maximization of\n",
      "complicated (log-)likelihood functions, through the introduction of auxiliary variables.\n",
      "To simplify the notation in this section, we use a Bayesian notation system, where\n",
      "the same symbol is used for di \u000berent (conditional) probability densities.\n",
      "As in the previous section, given independent observations \u001c=fx1;:::; xngfrom some\n",
      "unknown pdf f, the objective is to ﬁnd the best approximation to fin a function class\n",
      "G=fg(\u0001j\u0012);\u00122\u0002gby solving the maximum likelihood problem:\n",
      "\u0012\u0003=argmax\n",
      "\u00122\u0002g(\u001cj\u0012); (4.22)\n",
      "where g(\u001cj\u0012) :=g(x1j\u0012)\u0001\u0001\u0001g(xnj\u0012). The key element of the EM algorithm is the aug-\n",
      "mentation of the data \u001cwith a suitable vector of latent variables latent\n",
      "variables,z, such that\n",
      "g(\u001cj\u0012)=Z\n",
      "g(\u001c;zj\u0012) dz:\n",
      "The function \u00127!g(\u001c;zj\u0012) is usually referred to as the complete-data likelihood complete -data\n",
      "likelihoodfunction.\n",
      "The choice of the latent variables is guided by the desire to make the maximization of\n",
      "g(\u001c;zj\u0012) much easier than that of g(\u001cj\u0012).\n",
      "Suppose pdenotes an arbitrary density of the latent variables z. Then, we can write:\n",
      "lng(\u001cj\u0012)=Z\n",
      "p(z) lng(\u001cj\u0012) dz\n",
      "=Z\n",
      "p(z) ln g(\u001c;zj\u0012)=p(z)\n",
      "g(zj\u001c;\u0012)=p(z)!\n",
      "dz\n",
      "=Z\n",
      "p(z) ln g(\u001c;zj\u0012)\n",
      "p(z)!\n",
      "dz\u0000Z\n",
      "p(z) ln g(zj\u001c;\u0012)\n",
      "p(z)!\n",
      "dz\n",
      "=Z\n",
      "p(z) ln g(\u001c;zj\u0012)\n",
      "p(z)!\n",
      "dz+D(p;g(\u0001j\u001c;\u0012)); (4.23)\n",
      "whereD(p;g(\u0001j\u001c;\u0012)) is the Kullback–Leibler divergence from the density ptog(\u0001j\u001c;\u0012). +42\n",
      "SinceD>0, it follows that\n",
      "lng(\u001cj\u0012)>Z\n",
      "p(z) ln g(\u001c;zj\u0012)\n",
      "p(z)!\n",
      "dz=:L(p;\u0012)\n",
      "for all\u0012and any density pof the latent variables. In other words, L(p;\u0012) is a lower bound\n",
      "on the log-likelihood that involves the complete-data likelihood. The EM algorithm then\n",
      "aims to increase this lower bound as much as possible by starting with an initial guess \u0012(0)\n",
      "and then, for t=1;2;:::, solving the following two steps:\n",
      "1.p(t)=argmaxpL(p;\u0012(t\u00001)),\n",
      "2.\u0012(t)=argmax\u00122\u0002L(p(t);\u0012).Chapter 4. Unsupervised Learning 129\n",
      "The ﬁrst optimization problem can be solved explicitly. Namely, by (4.23), we have\n",
      "that\n",
      "p(t)=argmin\n",
      "pD(p;g(\u0001j\u001c;\u0012(t\u00001)))=g(\u0001j\u001c;\u0012(t\u00001)):\n",
      "That is, the optimal density is the conditional density of the latent variables given the data\n",
      "\u001cand the parameter \u0012(t\u00001). The second optimization problem can be simpliﬁed by writing\n",
      "L(p(t);\u0012)=Q(t)(\u0012)\u0000Ep(t)lnp(t)(Z), where\n",
      "Q(t)(\u0012) :=Ep(t)lng(\u001c;Zj\u0012)\n",
      "is the expected complete-data log-likelihood under Z\u0018p(t). Consequently, the maximiza-\n",
      "tion ofL(p(t);\u0012) with respect to \u0012is equivalent to ﬁnding\n",
      "\u0012(t)=argmax\n",
      "\u00122\u0002Q(t)(\u0012):\n",
      "This leads to the following generic EM algorithm.\n",
      "Algorithm 4.3.1: Generic EM Algorithm\n",
      "input: Data\u001c, initial guess \u0012(0).\n",
      "output: Approximation of the maximum likelihood estimate.\n",
      "1t 1\n",
      "2while a stopping criterion is not met do\n",
      "3 Expectation Step : Find p(t)(z) :=g(zj\u001c;\u0012(t\u00001)) and compute the expectation\n",
      "Q(t)(\u0012) :=Ep(t)lng(\u001c;Zj\u0012): (4.24)\n",
      "4 Maximization Step : Let\u0012(t) argmax\u00122\u0002Q(t)(\u0012).\n",
      "5 t t+1\n",
      "6return\u0012(t)\n",
      "A possible stopping criterion is to stop when\n",
      "\f\f\f\f\f\flng(\u001cj\u0012(t))\u0000lng(\u001cj\u0012(t\u00001))\n",
      "lng(\u001cj\u0012(t))\f\f\f\f\f\f6\"\n",
      "for some small tolerance \">0.\n",
      "Remark 4.1 (Properties of the EM Algorithm) The identity (4.23) can be used to\n",
      "show that the likelihood g(\u001cj\u0012(t)) does not decrease with every iteration of the algorithm.\n",
      "This property is one of the strengths of the algorithm. For example, it can be used to debug\n",
      "computer implementations of the EM algorithm: if the likelihood is observed to decrease\n",
      "at any iteration, then one has detected a bug in the program.\n",
      "The convergence of the sequence f\u0012(t)gto a global maximum (if it exists) is highly\n",
      "dependent on the initial value \u0012(0)and, in many cases, an appropriate choice of \u0012(0)may not\n",
      "be clear. Typically, practitioners run the algorithm from di \u000berent random starting points\n",
      "over\u0002, to ascertain empirically that a suitable optimum is achieved.130 4.3. Expectation–Maximization (EM) Algorithm\n",
      "Example 4.2 (Censored Data) Suppose the lifetime (in years) of a certain type of\n",
      "machine is modeled via a N(\u0016;\u001b2) distribution. To estimate \u0016and\u001b2, the lifetimes of\n",
      "n(independent) machines are recorded up to cyears. Denote these censored lifetimes\n",
      "byx1;:::; xn. Thefxigare thus realizations of iid random variables fXig, distributed as\n",
      "minfY;cg, where Y\u0018N(\u0016;\u001b2).\n",
      "By the law of total probability (see (C.9)), the marginal pdf of each Xcan be written +430\n",
      "as:\n",
      "g(xj\u0016;\u001b2)=((c\u0000\u0016)=\u001b)|          {z          }\n",
      "P[Y<c]'\u001b2(x\u0000\u0016)\n",
      "\b((c\u0000\u0016)=\u001b)1fx<cg((c\u0000\u0016)=\u001b)|          {z          }\n",
      "P[Y>c]1fx=cg;\n",
      "where'\u001b2(\u0001) is the pdf of the N(0;\u001b2) distribution,is the cdf of the standard normal\n",
      "distribution, and:=1. It follows that the likelihood of the data \u001c=fx1;:::; xngas a\n",
      "function of the parameter \u0012:=[\u0016;\u001b2]>is:\n",
      "g(\u001cj\u0012)=Y\n",
      "i:xi<cexp\u0010\n",
      "\u0000(xi\u0000\u0016)2\n",
      "2\u001b2\u0011\n",
      "p\n",
      "2\u0019\u001b2\u0002Y\n",
      "i:xi=((c\u0000\u0016)=\u001b):\n",
      "Letncbe the total number of xisuch that xi=c. Using nclatent variables z=[z1;:::; znc]>,\n",
      "we can write the joint pdf:\n",
      "g(\u001c;zj\u0012)=1\n",
      "(2\u0019\u001b2)n=2exp \n",
      "\u0000P\n",
      "i:xi<c(xi\u0000\u0016)2\n",
      "2\u001b2\u0000Pnc\n",
      "i=1(zi\u0000\u0016)2\n",
      "2\u001b2!\n",
      "1\u001a\n",
      "min\n",
      "izi>c\u001b\n",
      ";\n",
      "so thatR\n",
      "g(\u001c;zj\u0012) dz=g(\u001cj\u0012). We can thus apply the EM algorithm to maximize the like-\n",
      "lihood, as follows.\n",
      "For the E(xpectation)-step, we have for a ﬁxed \u0012:\n",
      "g(zj\u001c;\u0012)=ncY\n",
      "i=1g(zij\u001c;\u0012);\n",
      "where g(zj\u001c;\u0012)=1fz>cg'\u001b2(z\u0000\u0016)((c\u0000\u0016)=\u001b) is simply the pdf of the N(\u0016;\u001b2)\n",
      "distribution, truncated to [ c;1).\n",
      "For the M(aximization)-step, we compute the expectation of the complete log-\n",
      "likelihood with respect to a ﬁxed g(zj\u001c;\u0012) and use the fact that Z1;:::; Zncare iid:\n",
      "Elng(\u001c;Zj\u0012)=\u0000P\n",
      "i:xi<c(xi\u0000\u0016)2\n",
      "2\u001b2\u0000ncE(Z\u0000\u0016)2\n",
      "2\u001b2\u0000n\n",
      "2ln\u001b2\u0000n\n",
      "2ln(2\u0019);\n",
      "where Zhas aN(\u0016;\u001b2) distribution, truncated to [ c;1). To maximize the last expression\n",
      "with respect to \u0016we set the derivative with respect to \u0016to zero, and obtain:\n",
      "\u0016=ncEZ+P\n",
      "i:xi<cxi\n",
      "n:\n",
      "Similarly, setting the derivative with respect to \u001b2to zero gives:\n",
      "\u001b2=ncE(Z\u0000\u0016)2+P\n",
      "i:xi<c(xi\u0000\u0016)2\n",
      "n:\n",
      "In summary, the EM iterates for t=1;2;:::are as follows.Chapter 4. Unsupervised Learning 131\n",
      "E-step. Given the current estimate \u0012t:=[\u0016t;\u001b2\n",
      "t]>, compute the expectations \u0017t:=EZand\n",
      "\u00102\n",
      "t:=E(Z\u0000\u0016t)2, where Z\u0018N(\u0016t;\u001b2\n",
      "t), conditional on Z>c; that is,\n",
      "\u0017t:=\u0016t+\u001b2\n",
      "t'\u001b2\n",
      "t(c\u0000\u0016t)\n",
      "\b((c\u0000\u0016t)=\u001bt)\n",
      "\u00102\n",
      "t:=\u001b2\n",
      "t0BBBB@1+(c\u0000\u0016t)'\u001b2\n",
      "t(c\u0000\u0016t)\n",
      "\b((c\u0000\u0016t)=\u001bt)1CCCCA:\n",
      "M-step. Update the estimate to \u0012t+1:=[\u0016t+1;\u001b2\n",
      "t+1]>via the formulas:\n",
      "\u0016t+1=nc\u0017t+P\n",
      "i:xi<cxi\n",
      "n\n",
      "\u001b2\n",
      "t+1=nc\u00102\n",
      "t+P\n",
      "i:xi<c(xi\u0000\u0016t+1)2\n",
      "n:\n",
      "4.4 Empirical Distribution and Density Estimation\n",
      "In Section 1.5.2.3 we saw how the empirical cdf bFn, obtained from an iid training set + 11\n",
      "\u001c=fx1;:::; xngfrom an unknown distribution on R, gives an estimate of the unknown cdf\n",
      "Fof this sampling distribution. The function bFnis a genuine cdf, as it is right-continuous,\n",
      "increasing, and lies between 0 and 1. The corresponding discrete probability distribution\n",
      "is called the empirical distribution empirical\n",
      "distributionof the data. A random variable Xdistributed according\n",
      "to this empirical distribution takes the values x1;:::; xnwith equal probability 1 =n. The\n",
      "concept of empirical distribution naturally generalizes to higher dimensions: a random\n",
      "vector Xthat is distributed according to the empirical distribution of x1;:::; xnhas discrete\n",
      "pdfP[X=xi]=1=n;i=1;:::; n. Sampling from such a distribution — in other words\n",
      "resampling the original data — was discussed in Section 3.2.4. The preeminent usage of + 76\n",
      "such sampling is the bootstrap method, discussed in Section 3.3.2. + 88\n",
      "In a way, the empirical distribution is the natural answer to the unsupervised learning\n",
      "question: what is the underlying probability distribution of the data? However, the empir-\n",
      "ical distribution is, by deﬁnition, a discrete distribution, whereas the true sampling distri-\n",
      "bution might be continuous. For continuous data it makes sense to also consider estimation\n",
      "of the pdf of the data. A common approach is to estimate the density via a kernel density\n",
      "estimate (KDE), the most prevalent learner to carry this out is given next.\n",
      "Deﬁnition 4.1: Gaussian KDE\n",
      "Letx1;:::; xn2Rdbe the outcomes of an iid sample from a continuous pdf f. A\n",
      "Gaussian kernel density estimate Gaussian\n",
      "kernel density\n",
      "estimateoffis a mixture of normal pdfs, of the form\n",
      "g\u001cn(xj\u001b)=1\n",
      "nnX\n",
      "i=11\n",
      "(2\u0019)d=2\u001bde\u0000kx\u0000xik2\n",
      "2\u001b2;x2Rd; (4.25)\n",
      "where\u001b>0 is called the bandwidth .132 4.4. Empirical Distribution and Density Estimation\n",
      "We see that g\u001cnin (4.25) is the average of a collection of nnormal pdfs, where each\n",
      "normal distribution is centered at the data point xiand has covariance matrix \u001b2Id. A major\n",
      "question is how to choose the bandwidth \u001bso as to best approximate the unknown pdf f.\n",
      "Choosing very small \u001bwill result in a “spiky” estimate, whereas a large \u001bwill produce\n",
      "an over-smoothed estimate that may not identify important peaks that are present in the\n",
      "unknown pdf. Figure 4.1 illustrates this phenomenon. In this case the data are comprised\n",
      "of 20 points uniformly drawn from the unit square. The true pdf is thus 1 on [0 ;1]2and 0\n",
      "elsewhere.\n",
      "Figure 4.1: Two two-dimensional Gaussian KDEs, with \u001b=0:01 (left) and \u001b=0:1 (right).\n",
      "Let us write the Gaussian KDE in (4.25) as\n",
      "g\u001cn(xj\u001b)=1\n",
      "nnX\n",
      "i=11\n",
      "\u001bd\u001e\u0012x\u0000xi\n",
      "\u001b\u0013\n",
      "; (4.26)\n",
      "where\n",
      "\u001e(z)=1\n",
      "(2\u0019)d=2e\u0000kzk2\n",
      "2;z2Rd(4.27)\n",
      "is the pdf of the d-dimensional standard normal distribution. By choosing a di \u000berent prob-\n",
      "ability density \u001ein (4.26), satisfying \u001e(x)=\u001e(\u0000x) for all x, we can obtain a wide variety\n",
      "of kernel density estimates. A simple pdf \u001eis, for example, the uniform pdf on [ \u00001;1]d:\n",
      "\u001e(z)=8>><>>:2\u0000d;ifz2[\u00001;1]d;\n",
      "0; otherwise:\n",
      "Figure 4.2 shows the graph of the corresponding KDE, using the same data as in Figure 4.1\n",
      "and with bandwidth \u001b=0:1. We observe qualitatively similar behavior for the Gaussian\n",
      "and uniform KDEs. As a rule, the choice of the function \u001eis less important than the choice\n",
      "of the bandwidth in determining the quality of the estimate.\n",
      "The important issue of bandwidth selection has been extensively studied for one-\n",
      "dimensional data. To explain the ideas, we use our usual setup and let \u001c=fx1;:::; xng\n",
      "be the observed (one-dimensional) data from the unknown pdf f. First, we deﬁne the loss\n",
      "function as\n",
      "Loss( f(x);g(x))=(f(x)\u0000g(x))2\n",
      "f(x): (4.28)Chapter 4. Unsupervised Learning 133\n",
      "Figure 4.2: A two-dimensional uniform KDE, with bandwidth \u001b=0:1.\n",
      "The risk to minimize is thus `(g) :=EfLoss( f(X);g(X))=R\n",
      "(f(x)\u0000g(x))2dx:We bypass\n",
      "the selection of a class of approximation functions by choosing the learner to be speciﬁed\n",
      "by (4.25) for a ﬁxed \u001b. The objective is now to ﬁnd a \u001bthat minimizes the generalization\n",
      "risk`(g\u001c(\u0001j\u001b)) or the expected generalization risk E`(gT(\u0001j\u001b)). The generalization risk is\n",
      "in this caseZ\n",
      "(f(x)\u0000g\u001c(xj\u001b))2dx=Z\n",
      "f2(x) dx\u00002Z\n",
      "f(x)g\u001c(xj\u001b) dx+Z\n",
      "g2\n",
      "\u001c(xj\u001b) dx:\n",
      "Minimizing this expression with respect to \u001bis equivalent to minimizing the last two terms,\n",
      "which can be written as\n",
      "\u00002Efg\u001c(Xj\u001b)+Z0BBBBB@1\n",
      "nnX\n",
      "i=11\n",
      "\u001b\u001e\u0012x\u0000xi\n",
      "\u001b\u00131CCCCCA2\n",
      "dx:\n",
      "This expression in turn can be estimated by using a test sample fx0\n",
      "1:::;x0\n",
      "n0gfrom f, yielding\n",
      "the following minimization problem:\n",
      "min\n",
      "\u001b\u00002\n",
      "n0n0X\n",
      "i=1g\u001c(x0\n",
      "ij\u001b)+1\n",
      "n2nX\n",
      "i=1nX\n",
      "j=1Z1\n",
      "\u001b2\u001e\u0012x\u0000xi\n",
      "\u001b\u0013\n",
      "\u001e\u0012x\u0000xj\n",
      "\u001b\u0013\n",
      "dx;\n",
      "whereR1\n",
      "\u001b2\u001e\u0010x\u0000xi\n",
      "\u001b\u0011\n",
      "\u001e\u0010x\u0000xj\n",
      "\u001b\u0011\n",
      "dx=1p\n",
      "2\u001b\u001e\u0010xi\u0000xjp\n",
      "2\u001b\u0011\n",
      "in the case of the Gaussian kernel (4.27) with\n",
      "d=1. To estimate \u001bin this way clearly requires a test sample, or at least an application of + 37\n",
      "cross-validation . Another approach is to minimize the expected generalization risk, (that\n",
      "is, averaged over all training sets):\n",
      "EZ\n",
      "(f(x)\u0000gT(xj\u001b))2dx:\n",
      "This is called the mean integrated squared error mean integrated\n",
      "squared error(MISE). It can be decomposed into an\n",
      "integrated squared bias and integrated variance component:\n",
      "Z\n",
      "(f(x)\u0000EgT(xj\u001b))2dx+Z\n",
      "Var(gT(xj\u001b)) dx:134 4.4. Empirical Distribution and Density Estimation\n",
      "A typical analysis now proceeds by investigating how the MISE behaves for large n, under\n",
      "various assumptions on f. For example, it is shown in [114] that, for \u001b!0 and n\u001b!1 ,\n",
      "the asymptotic approximation to the MISE of the Gaussian kernel density estimator (4.25)\n",
      "(ford=1) is given by\n",
      "1\n",
      "4\u001b4kf00k2+1\n",
      "2np\n",
      "\u0019\u001b2; (4.29)\n",
      "wherekf00k2:=R\n",
      "(f00(x))2dx. The asymptotically optimal value of \u001bis the minimizer\n",
      "\u001b\u0003:= 1\n",
      "2np\u0019kf00k2!1=5\n",
      ": (4.30)\n",
      "To compute the optimal \u001b\u0003in (4.30), one needs to estimate the functional kf00k2. The\n",
      "Gaussian rule of thumb Gaussian rule\n",
      "of thumbis to assume that fis the density of the N(x;s2) distribution, where\n",
      "xands2are the sample mean and variance of the data, respectively [113]. In this case\n",
      "kf00k2=s\u00005\u0019\u00001=23=8 and the Gaussian rule of thumb becomes:\n",
      "\u001brot= 4s5\n",
      "3n!1=5\n",
      "\u00191:06s n\u00001=5:\n",
      "We recommend, however, the fast and reliable theta KDE theta KDE of [14], which chooses the\n",
      "bandwidth in an optimal way via a ﬁxed-point procedure. Figures 4.1 and 4.2 illustrate a\n",
      "common problem with traditional KDEs: for distributions on a bounded domain, such as\n",
      "the uniform distribution on [0 ;1]2, the KDE assigns positive probability mass outside this\n",
      "domain. An additional advantage of the theta KDE is that it largely avoids this boundary\n",
      "e\u000bect. We illustrate the theta KDE with the following example.\n",
      "Example 4.3 (Comparison of Gaussian and theta KDEs) The following Python pro-\n",
      "gram draws an iid sample from the Exp(1) distribution and constructs a Gaussian kernel\n",
      "density estimate. We see in Figure 4.3 that with an appropriate choice of the bandwidth\n",
      "a good ﬁt to the true pdf can be achieved, except at the boundary x=0. The theta KDE\n",
      "does not exhibit this boundary e \u000bect. Moreover, it chooses the bandwidth automatically,\n",
      "to achieve a superior ﬁt. The theta KDE source code is available as kde.py on the book’s\n",
      "GitHub site.\n",
      "0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Gaussian KDE\n",
      "Theta KDE\n",
      "True density\n",
      "Figure 4.3: Kernel density estimates for Exp(1)-distributed data.Chapter 4. Unsupervised Learning 135\n",
      "gausthetakde.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from kde import *\n",
      "sig = 0.1; sig2 = sig**2; c = 1/np.sqrt(2*np.pi)/sig #Constants\n",
      "phi = lambda x,x0: np.exp(-(x-x0)**2/(2*sig2)) #Unscaled Kernel\n",
      "f = lambda x: np.exp(-x)*(x >= 0) # True PDF\n",
      "n = 10**4 # Sample Size\n",
      "x = -np.log(np.random.uniform(size=n))# Generate Data via IT method\n",
      "xx = np.arange(-0.5,6,0.01, dtype = \"d\")# Plot Range\n",
      "phis = np.zeros(len(xx))\n",
      "for i in range(0,n):\n",
      "phis = phis + phi(xx,x[i])\n",
      "phis = c*phis/n\n",
      "plt.plot(xx,phis , 'r')# Plot Gaussian KDE\n",
      "[bandwidth ,density ,xmesh ,cdf] = kde(x,2**12,0,max(x))\n",
      "idx = (xmesh <= 6)\n",
      "plt.plot(xmesh[idx],density[idx])# Plot Theta KDE\n",
      "plt.plot(xx,f(xx))# Plot True PDF\n",
      "4.5 Clustering via Mixture Models\n",
      "Clustering is concerned with the grouping of unlabeled feature vectors into clusters, such\n",
      "that samples within a cluster are more similar to each other than samples belonging to\n",
      "di\u000berent clusters. Usually, it is assumed that the number of clusters is known in advance,\n",
      "but otherwise no prior information is given about the data. Applications of clustering can\n",
      "be found in the areas of communication, data compression and storage, database searching,\n",
      "pattern matching, and object recognition.\n",
      "A common approach to clustering analysis is to assume that the data comes from a mix-\n",
      "ture of (usually Gaussian) distributions, and thus the objective is to estimate the parameters\n",
      "of the mixture model by maximizing the likelihood function for the data. Direct optimiza-\n",
      "tion of the likelihood function in this case is not a simple task, due to necessary constraints\n",
      "on the parameters (more about this later) and the complicated nature of the likelihood func-\n",
      "tion, which in general has a great number of local maxima and saddle-points. A popular\n",
      "method to estimate the parameters of the mixture model is the EM algorithm, which was\n",
      "discussed in a more general setting in Section 4.3. In this section we explain the basics of +128\n",
      "mixture modeling and explain the workings of the EM method in this context. In addition,\n",
      "we show how direct optimization methods can be used to maximize the likelihood.\n",
      "4.5.1 Mixture Models\n",
      "LetT:=fX1;:::; Xngbe iid random vectors taking values in some set X\u0012Rd, each Xi\n",
      "being distributed according to the mixture density mixture density\n",
      "g(xj\u0012)=w1\u001e1(x)+\u0001\u0001\u0001+wK\u001eK(x);x2X; (4.31)136 4.5. Clustering via Mixture Models\n",
      "where\u001e1;:::;\u001e Kare probability densities (discrete or continuous) on X, and the positive\n",
      "weights w 1;:::; wKsum up to 1. This mixture pdf can be interpreted in the following way.weightsLetZbe a discrete random variable taking values 1 ;2;:::; Kwith probabilities w1;:::; wK,\n",
      "and let Xbe a random vector whose conditional pdf, given Z=z, is\u001ez. By the product rule\n",
      "(C.17), the joint pdf of ZandXis given by +433\n",
      "\u001eZ;X(z;x)=\u001eZ(z)\u001eXjZ(xjz)=wz\u001ez(x)\n",
      "and the marginal pdf of Xis found by summing the joint pdf over the values of z, which\n",
      "gives (4.31). A random vector X\u0018gcan thus be simulated in two steps:\n",
      "1. First, draw Zaccording to the probabilities P[Z=z]=wz,z=1;:::; K.\n",
      "2. Then draw Xaccording to the pdf \u001eZ.\n",
      "AsTonly contain thefXigvariables, thefZigare viewed as latent variables. We can inter-\n",
      "pretZias the hidden label of the cluster to which Xibelongs.\n",
      "Typically, each \u001ekin (4.31) is assumed to be known up to some parameter vector \u0011k. It\n",
      "is customary1in clustering analysis to work with Gaussian mixtures; that is, each density\n",
      "\u001ekis Gaussian with some unknown expectation vector \u0016kand covariance matrix \u0006k. We\n",
      "gather all unknown parameters, including the weights fwkg, into a parameter vector \u0012. As\n",
      "usual,\u001c=fx1;:::; xngdenotes the outcome of T. As the components of Tare iid, their\n",
      "(joint) pdf is given by\n",
      "g(\u001cj\u0012) :=nY\n",
      "i=1g(xij\u0012)=nY\n",
      "i=1KX\n",
      "k=1wk\u001ek(xij\u0016k;\u0006k): (4.32)\n",
      "Following the same reasoning as for (4.5), we can estimate \u0012from an outcome \u001cby max-\n",
      "imizing the log-likelihood function\n",
      "l(\u0012j\u001c) :=nX\n",
      "i=1lng(xij\u0012)=nX\n",
      "i=1ln0BBBBB@KX\n",
      "k=1wk\u001ek(xij\u0016k;\u0006k)1CCCCCA: (4.33)\n",
      "However, ﬁnding the maximizer of l(\u0012j\u001c) is not easy in general, since the function is typ-\n",
      "ically multiextremal.\n",
      "Example 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-\n",
      "sists of 300 data points that were independently generated from three bivariate normal\n",
      "distributions, whose parameters are given in that same ﬁgure. For each of these three dis-\n",
      "tributions, exactly 100 points were generated. Ideally, we would like to cluster the data into\n",
      "three clusters that correspond to the three cases.\n",
      "To cluster the data into three groups, a possible model for the data is to assume that\n",
      "the points are iid draws from an (unknown) mixture of three 2-dimensional Gaussian dis-\n",
      "tributions. This is a sensible approach, although in reality the data were not simulated\n",
      "in this way. It is instructive to understand the di \u000berence between the two models. In the\n",
      "mixture model, each cluster label Ztakes the valuef1;2;3gwith equal probability, and\n",
      "hence, drawing the labels independently, the total number of points in each cluster would\n",
      "1Other common mixture distributions include Student tandBeta distributions.Chapter 4. Unsupervised Learning 137\n",
      "-8 -6 -4 -2 0 2 4-6-4-2024\n",
      "cluster mean vector covariance matrix\n",
      "1\"\u00004\n",
      "0# \"2 1:4\n",
      "1:4 1:5#\n",
      "2\"0:5\n",
      "\u00001# \"2\u00000:95\n",
      "\u00000:95 1#\n",
      "3\"\u00001:5\n",
      "\u00003# \"2 0:1\n",
      "0:1 0:1#\n",
      "Figure 4.4: Cluster the 300 data points (left) into three clusters, without making any as-\n",
      "sumptions about the probability distribution of the data. In fact, the data were generated\n",
      "from three bivariate normal distributions, whose parameters are listed on the right.\n",
      "beBin(300;1=3) distributed. However, in the actual simulation, the number of points in\n",
      "each cluster is exactly 100. Nevertheless, the mixture model would be an accurate (al-\n",
      "though not exact) model for these data. Figure 4.5 displays the “target” Gaussian mixture\n",
      "density for the data in Figure 4.4; that is, the mixture with equal weights and with the exact\n",
      "parameters as speciﬁed in Figure 4.4.\n",
      "Figure 4.5: The target mixture density for the data in Figure 4.4.\n",
      "In the next section we will carry out the clustering by using the EM algorithm.\n",
      "4.5.2 EM Algorithm for Mixture Models\n",
      "As we saw in Section 4.3, instead of maximizing the log-likelihood function (4.33) directly\n",
      "from the data \u001c=fx1;:::; xng, the EM algorithm ﬁrst augments the data data\n",
      "augmentationwith the vector of\n",
      "latent variables — in this case the hidden cluster labels z=fz1;:::; zng. The idea is that \u001cis138 4.5. Clustering via Mixture Models\n",
      "only the observed part of the complete random data ( T;Z), which were generated via the\n",
      "two-step procedure described above. That is, for each data point X, ﬁrst draw the cluster\n",
      "label Z2f1;:::; Kgaccording to probabilities fw1;:::; wKgand then, given Z=z, draw X\n",
      "from\u001ez. The joint pdf ofTandZis\n",
      "g(\u001c;zj\u0012)=nY\n",
      "i=1wzi\u001ezi(xi);\n",
      "which is of a much simpler form than (4.32). It follows that the complete-data log-\n",
      "likelihood complete -data\n",
      "log-likelihoodfunction\n",
      "el(\u0012j\u001c;z)=nX\n",
      "i=1ln[wzi\u001ezi(xi)] (4.34)\n",
      "is often easier to maximize than the original log-likelihood (4.33), for any given ( \u001c;z). But,\n",
      "of course the latent variables zare not observed and so el(\u0012j\u001c;z) cannot be evaluated. In the\n",
      "E-step of the EM algorithm, the complete-data log-likelihood is replaced with the expect-\n",
      "ationEpel(\u0012j\u001c;Z), where the subscript pin the expectation indicates that Zis distributed\n",
      "according to the conditional pdf of ZgivenT=\u001c; that is, with pdf\n",
      "p(z)=g(zj\u001c;\u0012)/g(\u001c;zj\u0012): (4.35)\n",
      "Note that p(z) is of the form p1(z1)\u0001\u0001\u0001pn(zn) so that, givenT=\u001c, the components of Zare\n",
      "independent of each other. The EM algorithm for mixture models can now be formulated\n",
      "as follows.\n",
      "Algorithm 4.5.1: EM Algorithm for Mixture Models\n",
      "input: Data\u001c, initial guess \u0012(0).\n",
      "output: Approximation of the maximum likelihood estimate.\n",
      "1t 1\n",
      "2while a stopping criterion is not met do\n",
      "3 Expectation Step : Find p(t)(z) :=g(zj\u001c;\u0012(t\u00001)) and Q(t)(\u0012) :=Ep(t)el(\u0012j\u001c;Z).\n",
      "4 Maximization Step : Let\u0012(t) argmax\u0012Q(t)(\u0012).\n",
      "5 t t+1\n",
      "6return\u0012(t)\n",
      "A possible termination condition is to stop when\f\f\fl(\u0012(t)j\u001c)\u0000l(\u0012(t\u00001)j\u001c)\f\f\f=\f\f\fl(\u0012(t)j\u001c)\f\f\f< \"\n",
      "for some small tolerance \" > 0. As was mentioned in Section 4.3, the sequence of log-\n",
      "likelihood values does not decrease with each iteration. Under certain continuity con-\n",
      "ditions, the sequence f\u0012(t)gis guaranteed to converge to a local maximizer of the log-\n",
      "likelihood l. Convergence to a global maximizer (if it exists) depends on the appropriate\n",
      "choice for the starting value. Typically, the algorithm is run from di \u000berent random starting\n",
      "points.\n",
      "For the case of Gaussian mixtures, each \u001ek=\u001e(\u0001j\u0016k;\u0006k);k=1;:::; Kis the density\n",
      "of a d-dimensional Gaussian distribution. Let \u0012(t\u00001)be the current guess for the optimal\n",
      "parameter vector, consisting of the weights fw(t\u00001)\n",
      "kg, mean vectorsf\u0016(t\u00001)\n",
      "kg, and covariance\n",
      "matricesf\u0006(t\u00001)\n",
      "kg. We ﬁrst determine p(t)— the pdf of Zconditional onT=\u001c— for the\n",
      "given guess\u0012(t\u00001). As mentioned before, the components of ZgivenT=\u001care independent,Chapter 4. Unsupervised Learning 139\n",
      "so it su \u000eces to specify the discrete pdf, p(t)\n",
      "isay, of each Zigiven the observed point Xi=xi.\n",
      "The latter can be found from Bayes’ formula:\n",
      "p(t)\n",
      "i(k)/w(t\u00001)\n",
      "k\u001ek(xij\u0016(t\u00001)\n",
      "k;\u0006(t\u00001)\n",
      "k);k=1;:::; K: (4.36)\n",
      "Next, in view of (4.34), the function Q(t)(\u0012) can be written as\n",
      "Q(t)(\u0012)=Ep(t)nX\n",
      "i=1\u0010\n",
      "lnwZi+ln\u001eZi(xij\u0016Zi;\u0006Zi)\u0011\n",
      "=nX\n",
      "i=1Ep(t)\n",
      "ih\n",
      "lnwZi+ln\u001eZi(xij\u0016Zi;\u0006Zi)i\n",
      ";\n",
      "where thefZigare independent and Ziis distributed according to p(t)\n",
      "iin (4.36). This com-\n",
      "pletes the E-step . In the M-step we maximize Q(t)with respect to the parameter \u0012; that is,\n",
      "with respect to the fwkg,f\u0016kg, andf\u0006kg. In particular, we maximize\n",
      "nX\n",
      "i=1KX\n",
      "k=1p(t)\n",
      "i(k)\u0002lnwk+ln\u001ek(xij\u0016k;\u0006k)\u0003;\n",
      "under the conditionP\n",
      "kwk=1. Using Lagrange multipliers and the fact thatPK\n",
      "k=1p(t)\n",
      "i(k)=1\n",
      "gives the solution for the fwkg:\n",
      "wk=1\n",
      "nnX\n",
      "i=1p(t)\n",
      "i(k);k=1;:::; K: (4.37)\n",
      "The solutions for \u0016kand\u0006know follow from maximizingPn\n",
      "i=1p(t)\n",
      "i(k) ln\u001ek(xij\u0016k;\u0006k), lead-\n",
      "ing to\n",
      "\u0016k=Pn\n",
      "i=1p(t)\n",
      "i(k)xiPn\n",
      "i=1p(t)\n",
      "i(k);k=1;:::; K (4.38)\n",
      "and\n",
      "\u0006k=Pn\n",
      "i=1p(t)\n",
      "i(k) (xi\u0000\u0016k)(xi\u0000\u0016k)>\n",
      "Pn\n",
      "i=1p(t)\n",
      "i(k);k=1;:::; K; (4.39)\n",
      "which are very similar to the well-known formulas for the MLEs of the parameters of a\n",
      "Gaussian distribution. After assigning the solution parameters to \u0012(t)and increasing the\n",
      "iteration counter tby 1, the steps (4.36), (4.37), (4.38), and (4.39) are repeated until con-\n",
      "vergence is reached. Convergence of the EM algorithm is very sensitive to the choice of\n",
      "initial parameters. It is therefore recommended to try various di \u000berent starting conditions.\n",
      "For a further discussion of the theoretical and practical aspects of the EM algorithm we\n",
      "refer to [85].\n",
      "Example 4.5 (Clustering via EM) We return to the data in Example 4.4, depicted in\n",
      "Figure 4.4, and adopt the model that the data is coming from a mixture of three bivariate\n",
      "Gaussian distributions.\n",
      "The Python code below implements the EM procedure described in Algorithm 4.5.1.\n",
      "The initial mean vectors f\u0016kgof the bivariate Gaussian distributions are chosen (from visual\n",
      "inspection) to lie roughly in the middle of each cluster, in this case [ \u00002;\u00003]>;[\u00004;1]>, and\n",
      "[0;\u00001]>. The corresponding covariance matrices are initially chosen as identity matrices,\n",
      "which is appropriate given the observed spread of the data in Figure 4.4. Finally, the initial\n",
      "weights are 1 =3;1=3;1=3. For simplicity, the algorithm stops after 100 iterations, which in\n",
      "this case is more than enough to guarantee convergence. The code and data are available\n",
      "from the book’s website in the GitHub folder Chapter4.140 4.5. Clustering via Mixture Models\n",
      "EMclust.py\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal\n",
      "Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\n",
      "K = 3\n",
      "n, D = Xmat.shape\n",
      "W = np.array([[1/3,1/3,1/3]])\n",
      "M = np.array([[-2.0,-4,0],[-3,1,-1]], dtype=np.float32)\n",
      "# Note that if above *all* entries were written as integers , M would\n",
      "# be defined to be of integer type , which will give the wrong answer\n",
      "C = np.zeros((3,2,2))\n",
      "C[:,0,0] = 1\n",
      "C[:,1,1] = 1\n",
      "p = np.zeros((3,300))\n",
      "for i in range(0,100):\n",
      "#E-step\n",
      "for k in range(0,K):\n",
      "mvn = multivariate_normal( M[:,k].T, C[k,:,:] )\n",
      "p[k,:] = W[0,k]*mvn.pdf(Xmat)\n",
      "# M-Step\n",
      "p = (p/sum(p,0)) #normalize\n",
      "W = np.mean(p,1).reshape(1,3)\n",
      "for k in range(0,K):\n",
      "M[:,k] = (Xmat.T @ p[k,:].T)/sum(p[k,:])\n",
      "xm = Xmat.T - M[:,k].reshape(2,1)\n",
      "C[k,:,:] = xm @ (xm*p[k,:]).T/sum(p[k,:])\n",
      "The estimated parameters of the mixture distribution are given on the right-hand side\n",
      "of Figure 4.6. After relabeling of the clusters, we can observe a close match with the\n",
      "parameters in Figure 4.4.\n",
      "The ellipses on the left-hand side of Figure 4.6 show a close match between the 95%\n",
      "probability ellipses2of the original Gaussian distributions (in gray) and the estimated ones.\n",
      "A natural way to cluster each point xiis to assign it to the cluster kfor which the conditional\n",
      "probability pi(k) is maximal (with ties resolved arbitrarily). This gives the clustering of the\n",
      "points into red, green, and blue clusters in the ﬁgure.\n",
      "2For each mixture component, the contour of the corresponding bivariate normal pdf is shown that en-\n",
      "closes 95% of the probability mass.Chapter 4. Unsupervised Learning 141\n",
      "-6 -4 -2 0 2 4-4-3-2-10123\n",
      "weight mean vector covariance matrix\n",
      "0:33\"\u00001:51\n",
      "\u00003:01# \"1:75 0:03\n",
      "0:03 0:095#\n",
      "0:32\"\u00004:08\n",
      "\u00000:033# \"1:37 0:92\n",
      "0:92 1:03#\n",
      "0:35\"0:36\n",
      "\u00000:88# \"1:93\u00001:20\n",
      "\u00001:20 1:44#\n",
      "Figure 4.6: The results of the EM clustering algorithm applied to the data depicted in\n",
      "Figure 4.4.\n",
      "As an alternative to the EM algorithm, one can of course use continuous multiextremal\n",
      "optimization algorithms to directly optimize the log-likelihood function l(\u0012j\u001c)=lng(\u001cj\u0012)\n",
      "in (4.33) over the set \u0002of all possible \u0012. This is done for example in [15], demonstrating\n",
      "superior results to EM when there are few data points. Closer investigation of the likelihood\n",
      "function reveals that there is a hidden problem with any maximum likelihood approach for\n",
      "clustering if \u0002is chosen as large as possible — i.e., any mixture distribution is possible. To\n",
      "demonstrate this problem, consider Figure 4.7, depicting the probability density function,\n",
      "g(\u0001j\u0012) of a mixture of two Gaussian distributions, where \u0012=[w;\u00161;\u001b2\n",
      "1;\u00162;\u001b2\n",
      "2]>is the\n",
      "vector of parameters for the mixture distribution. The log-likelihood function is given by\n",
      "l(\u0012j\u001c)=P4\n",
      "i=1lng(xij\u0012), where x1;:::; x4are the data (indicated by dots in the ﬁgure).\n",
      "-4 -2 2 4 6 80.10.20.30.40.5\n",
      "Figure 4.7: Mixture of two Gaussian distributions.\n",
      "It is clear that by ﬁxing the mixing constant wat 0.25 (say) and centering the ﬁrst\n",
      "cluster at x1, one can obtain an arbitrarily large likelihood value by taking the variance of\n",
      "the ﬁrst cluster to be arbitrarily small. Similarly, for higher dimensional data, by choosing\n",
      "“point” or “line” clusters, or in general “degenerate” clusters, one can make the value of\n",
      "the likelihood inﬁnite. This is a manifestation of the familiar overﬁtting problem for the142 4.6. Clustering via Vector Quantization\n",
      "training loss that we already encountered in Chapter 2. Thus, the unconstrained maximiza-\n",
      "tion of the log-likelihood function is an ill-posed problem, irrespective of the choice of the\n",
      "optimization algorithm!\n",
      "Two possible solutions to this “overﬁtting” problem are:\n",
      "1. Restrict the parameter set \u0002in such a way that degenerate clusters (sometimes called\n",
      "spurious clusters) are not allowed.\n",
      "2. Run the given algorithm and if the solution is degenerate, discard it and run the\n",
      "algorithm afresh. Keep restarting the algorithm until a non-degenerate solution is\n",
      "obtained.\n",
      "The ﬁrst approach is usually applied to multiextremal optimization algorithms and the\n",
      "second is used for the EM algorithm.\n",
      "4.6 Clustering via Vector Quantization\n",
      "In the previous section we introduced clustering via mixture models, as a form of paramet-\n",
      "ric density estimation (as opposed to the nonparametric density estimation in Section 4.4).\n",
      "The clusters were modeled in a natural way via the latent variables and the EM algorithm\n",
      "provided a convenient way to assign the cluster members. In this section we consider a\n",
      "more heuristic approach to clustering by ignoring the distributional properties of the data.\n",
      "The resulting algorithms tend to scale better with the number of samples nand the dimen-\n",
      "sionality d.\n",
      "In mathematical terms, we consider the following clustering (also called data segment-\n",
      "ation) problem. Given a collection \u001c=fx1;:::; xngof data points in some d-dimensional\n",
      "spaceX, divide this data set into Kclusters (groups) such that some loss function is min-\n",
      "imized. A convenient way to determine these clusters is to ﬁrst divide up the entire space\n",
      "X, using some distance function dist( \u0001;\u0001) on this space. A standard choice is the Euclidean\n",
      "(orL2) distance:\n",
      "dist(x;x0)=kx\u0000x0k=vtdX\n",
      "i=1(xi\u0000x0\n",
      "i)2:\n",
      "Other commonly used distance measures on Rdinclude the Manhattan distance Manhattan\n",
      "distance:\n",
      "dX\n",
      "i=1jxi\u0000x0\n",
      "ij\n",
      "and the maximum distance maximum\n",
      "distance:\n",
      "max\n",
      "i=1;:::;djxi\u0000x0\n",
      "ij:\n",
      "On the set of strings of length d, an often-used distance measure is the Hamming distance Hamming\n",
      "distance:\n",
      "dX\n",
      "i=11fxi,x0\n",
      "ig;\n",
      "that is, the number of mismatched characters. For example, the Hamming distance between\n",
      "010101 and 011010 is 4.Chapter 4. Unsupervised Learning 143\n",
      "We can partition the space Xinto regions as follows: First, we choose Kpoints\n",
      "c1;:::; cKcalled cluster centers orsource vectors source vectors . For each k=1;:::; K, let\n",
      "Rk=fx2X: dist( x;ck)6dist(x;ci) for all i,kg\n",
      "be the set of points in Xthat lie closer to ckthan any other center. The regions or cells\n",
      "fRkgdivide the spaceXinto what is called a Voronoi diagram or a Voronoi tessellation Voronoi\n",
      "tessellation.\n",
      "Figure 4.8 shows a V oronoi tessellation of the plane into ten regions, using the Euclidean\n",
      "distance. Note that here the boundaries between the V oronoi cells are straight line seg-\n",
      "ments. In particular, if cell RiandRjshare a border, then a point on this border must satisfy\n",
      "kx\u0000cik=kx\u0000cjk; that is, it must lie on the line that passes through the point ( cj+ci)=2\n",
      "(that is, the midway point of the line segment between ciandcj) and be perpendicular to\n",
      "cj\u0000ci.\n",
      "-2 0 2 4-202\n",
      "Figure 4.8: A V oronoi tessellation of the plane into ten cells, determined by the (red) cen-\n",
      "ters.\n",
      "Once the centers (and thus the cells fRkg) are chosen, the points in \u001ccan be clustered\n",
      "according to their nearest center. Points on the boundary have to be treated separately. This\n",
      "is a moot point for continuous data, as generally no data points will lie exactly on the\n",
      "boundary.\n",
      "The main remaining issue is how to choose the centers so as to cluster the data in some\n",
      "optimal way. In terms of our (unsupervised) learning framework, we wish to approximate\n",
      "a vector xvia one of c1;:::; cK, using a piecewise constant vector-valued function\n",
      "g(xjC) :=KX\n",
      "k=1ck1fx2R kg;\n",
      "where Cis the d\u0002Kmatrix [ c1;:::; cK]. Thus, g(xjC)=ckwhen xfalls in regionRk(we\n",
      "ignore ties). Within this class Gof functions, parameterized by C, our aim is to minimize\n",
      "the training loss. In particular, for the squared-error loss, Loss( x;x0)=kx\u0000x0k2, the training\n",
      "loss is\n",
      "`\u001cn(g(\u0001jC))=1\n",
      "nnX\n",
      "i=1kxi\u0000g(xijC)k2=1\n",
      "nKX\n",
      "k=1X\n",
      "x2Rk\\\u001cnjjx\u0000ckjj2: (4.40)\n",
      "Thus, the training loss minimizes the average squared distance between the centers. This\n",
      "framework also combines both the encoding and decoding steps in vector quantization vector\n",
      "quantization144 4.6. Clustering via Vector Quantization\n",
      "[125]. Namely, we wish to “quantize” or “encode” the vectors in \u001cin such a way that each\n",
      "vector is represented by one of Ksource vectors c1;:::; cK, such that the loss (4.40) of this\n",
      "representation is minimized.\n",
      "Most well-known clustering and vector quantization methods update the vector of cen-\n",
      "ters, starting from some initial choice and using iterative (typically gradient-based) proced-\n",
      "ures. It is important to realize that in this case (4.40) is seen as a function of the centers,\n",
      "where each point xis assigned to the nearest center, thus determining the clusters. It is well\n",
      "known that this type of problem — optimization with respect to the centers — is highly\n",
      "multiextremal and, depending on the initial clusters, gradient-based procedures tend to\n",
      "converge to a local minimum rather than a global minimum.\n",
      "4.6.1 K-Means\n",
      "One of the simplest methods for clustering is the K-means method. It is an iterative method\n",
      "where, starting from an initial guess for the centers, new centers are formed by taking\n",
      "sample means of the current points in each cluster. The new centers are thus the centroids centroids\n",
      "of the points in each cell. Although there exist many di \u000berent varieties of the K-means\n",
      "algorithm, they are all essentially of the following form:\n",
      "Algorithm 4.6.1: K-Means\n",
      "input: Collection of points \u001c=fx1;:::; xng, number of clusters K, initial centers\n",
      "c1;:::; cK.\n",
      "output: Cluster centers and cells (regions).\n",
      "1while a stopping criterion is not met do\n",
      "2R1;:::;RK ; (empty sets).\n",
      "3 fori=1tondo\n",
      "4 d [dist( xi;c1);:::; dist(xi;cK)] // distances to centers\n",
      "5 k argminjdj\n",
      "6Rk R k[fxig // assign xito cluster k\n",
      "7 fork=1toKdo\n",
      "8 ck P\n",
      "x2Rkx\n",
      "jRkj// compute the new center as a centroid of points\n",
      "9returnfckg,fRkg\n",
      "Thus, at each iteration, for a given choice of centers, each point in \u001cis assigned to\n",
      "its nearest center. After all points have been assigned, the centers are recomputed as the\n",
      "centroids of all the points in the current cluster (Line 8). A typical stopping criterion is to\n",
      "stop when the centers no longer change very much. As the algorithm is quite sensitive to\n",
      "the choice of the initial centers, it is prudent to try multiple starting values, e.g., chosen\n",
      "randomly from the bounding box of the data points.\n",
      "We can see the K-means method as a deterministic (or “hard”) version of the probab-\n",
      "ilistic (or “soft”) EM algorithm as follows. Suppose in the EM algorithm we have Gaus-\n",
      "sian mixtures with a ﬁxed covariance matrix \u0006k=\u001b2Id,k=1;:::; K, where\u001b2should be\n",
      "thought of as being inﬁnitesimally small. Consider iteration tof the EM algorithm. Having\n",
      "obtained the expectation vectors \u0016(t\u00001)\n",
      "kand weights w(t\u00001)\n",
      "k;k=1;:::; K, each point xiis as-\n",
      "signed a cluster label Ziaccording to the probabilities p(t)\n",
      "i(k);k=1;:::; Kgiven in (4.36).Chapter 4. Unsupervised Learning 145\n",
      "But for\u001b2!0 the probability distribution fp(t)\n",
      "i(k)gbecomes degenerate, putting all its\n",
      "probability mass on argminkkxi\u0000\u0016kk2. This corresponds to the K-means rule of assigning\n",
      "xito its nearest cluster center. Moreover, in the M-step (4.38) each cluster center \u0016(t)\n",
      "kis now\n",
      "updated according to the average of the fxigthat have been assigned to cluster k. We thus\n",
      "obtain the same deterministic updating rule as in K-means.\n",
      "Example 4.6 ( K-means Clustering) We cluster the data from Figure 4.4 via K-means,\n",
      "using the Python implementation below. Note that the data points are stored as a 300 \u00022\n",
      "matrix Xmat . We take the same starting centers as in the EM example: c1=[\u00002;\u00003]>;c2=\n",
      "[\u00004;1]>, and c3=[0;\u00001]>. Note also that squared Euclidean distances are used in the\n",
      "computations, as these are slightly faster to compute than Euclidean distances (as no square\n",
      "root computations are required) while yielding exactly the same cluster center evaluations.\n",
      "Kmeans.py\n",
      "import numpy as np\n",
      "Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\n",
      "K = 3\n",
      "n, D = Xmat.shape\n",
      "c = np.array([[-2.0,-4,0],[-3,1,-1]]) #initialize centers\n",
      "cold = np.zeros(c.shape)\n",
      "dist2 = np.zeros((K,n))\n",
      "while np.abs(c - cold).sum() > 0.001:\n",
      "cold = c.copy()\n",
      "for i in range(0,K): #compute the squared distances\n",
      "dist2[i,:] = np.sum((Xmat - c[:,i].T)**2, 1)\n",
      "label = np.argmin(dist2 ,0) #assign the points to nearest centroid\n",
      "minvals = np.amin(dist2 ,0)\n",
      "for i in range(0,K): # recompute the centroids\n",
      "c[:,i] = np.mean(Xmat[np.where(label == i) ,:],1).reshape(1,2)\n",
      "print( 'Loss = {:3.3f} '.format(minvals.mean()))\n",
      "Loss = 2.288\n",
      "-6 -4 -2 0 2 4-5-4-3-2-10123\n",
      "Figure 4.9: Results of the K-means algorithm applied to the data in Figure 4.4. The thick\n",
      "black circles are the centroids and the dotted lines deﬁne the cell boundaries.146 4.6. Clustering via Vector Quantization\n",
      "We found the cluster centers c1=[\u00001:9286;\u00003:0416]>;c2=[\u00003:9237;0:0131]>, and\n",
      "c3=[0:5611;\u00001:2980]>, giving the clustering depicted in Figure 4.9. The corresponding\n",
      "loss (4.40) was found to be 2 :288.\n",
      "4.6.2 Clustering via Continuous Multiextremal Optimization\n",
      "As already mentioned, the exact minimization of the loss function (4.40) is di \u000ecult to\n",
      "accomplish via standard local search methods such as gradient descent, as the function\n",
      "is highly multimodal. However, nothing is preventing us from using global optimization\n",
      "methods such as the CE or SCO methods discussed in Sections 3.4.2 and 3.4.3. +100\n",
      "Example 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and\n",
      "cluster the points via minimization of the loss (4.40) using the CE method. The Python\n",
      "code below is very similar to the code in Example 3.16, except that now we are dealing +101\n",
      "with a six-dimensional optimization problem. The loss function is implemented in the func-\n",
      "tionScluster , which essentially reuses the squared distance computation of the K-means\n",
      "code in Example 4.6. The CE program typically converges to a loss of 2 :287, correspond-\n",
      "ing to the (global) minimizers c1=[\u00001:9286;\u00003:0416]>;c2=[\u00003:8681;0:0456]>, and\n",
      "c3=[0:5880;\u00001:3526]>, which slightly di \u000bers from the local minimizers for the K-means\n",
      "algorithm.\n",
      "clustCE.py\n",
      "import numpy as np\n",
      "np.set_printoptions(precision=4)\n",
      "Xmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\n",
      "K = 3\n",
      "n, D = Xmat.shape\n",
      "def Scluster(c):\n",
      "n, D = Xmat.shape\n",
      "dist2 = np.zeros((K,n))\n",
      "cc = c.reshape(D,K)\n",
      "for i in range(0,K):\n",
      "dist2[i,:] = np.sum((Xmat - cc[:,i].T)**2, 1)\n",
      "minvals = np.amin(dist2 ,0)\n",
      "return minvals.mean()\n",
      "numvar = K*D\n",
      "mu = np.zeros(numvar) #initialize centers\n",
      "sigma = np.ones(numvar)*2\n",
      "rho = 0.1\n",
      "N = 500; Nel = int(N*rho); eps = 0.001\n",
      "func = Scluster\n",
      "best_trj = np.array(numvar)\n",
      "best_perf = np.Inf\n",
      "trj = np.zeros(shape=(N,numvar))\n",
      "while(np.max(sigma)>eps):\n",
      "for i in range(0,numvar):Chapter 4. Unsupervised Learning 147\n",
      "trj[:,i] = (np.random.randn(N,1)*sigma[i]+ mu[i]).reshape(N,)\n",
      "S = np.zeros(N)\n",
      "for i in range(0,N):\n",
      "S[i] = func(trj[i])\n",
      "sortedids = np.argsort(S) # from smallest to largest\n",
      "S_sorted = S[sortedids]\n",
      "best_trj = np.array(n)\n",
      "best_perf = np.Inf\n",
      "eliteids = sortedids[range(0,Nel)]\n",
      "eliteTrj = trj[eliteids ,:]\n",
      "mu = np.mean(eliteTrj ,axis=0)\n",
      "sigma = np.std(eliteTrj ,axis=0)\n",
      "if(best_perf >S_sorted[0]):\n",
      "best_perf = S_sorted[0]\n",
      "best_trj = trj[sortedids[0]]\n",
      "print(best_perf)\n",
      "print(best_trj.reshape(2,3))\n",
      "2.2874901831572947\n",
      "[[-3.9238 -1.8477 0.5895]\n",
      "[ 0.0134 -3.0292 -1.2442]]\n",
      "4.7 Hierarchical Clustering\n",
      "It is sometimes useful to determine data clusters in a hierarchical manner; an example\n",
      "is the construction of evolutionary relationships between animal species. Establishing a\n",
      "hierarchy of clusters can be done in a bottom-up or a top-down manner. In the bottom-up\n",
      "approach, also called agglomerative clustering agglomerative\n",
      "clustering, the data points are merged in larger and\n",
      "larger clusters until all the points have been merged into a single cluster. In the top-down\n",
      "ordivisive clustering divisive\n",
      "clusteringapproach, the data set is divided up into smaller and smaller clusters.\n",
      "The left panel of Figure 4.10 depicts a hierarchy of clusters.\n",
      "In Figure 4.10, each cluster is given a cluster identiﬁer. At the lowest level are clusters\n",
      "comprised of the original data points (identiﬁers 1 ;:::; 8). The union of clusters 1 and 2\n",
      "form a cluster with identiﬁer 9, and the union of 3 and 4 form a cluster with identiﬁer 10.\n",
      "In turn the union of clusters 9 and 10 constitutes cluster 12, and so on.\n",
      "The right panel of Figure 4.10 shows a convenient way to visualize cluster hierarchies\n",
      "using a dendrogram dendrogram (from the Greek dendro for tree). A dendrogram not only summarizes\n",
      "how clusters are merged or split, but also shows the distance between clusters, here on the\n",
      "vertical axis. The horizontal axis shows which cluster each data point (label) belongs to.\n",
      "Many di \u000berent types of hierarchical clustering can be performed, depending on how\n",
      "the distance is deﬁned between two data points and between two clusters. Denote the data\n",
      "set byX=fxi;i=1;:::; ng. As in Section 4.6, let dist( xi;xj) be the distance between data\n",
      "points xiandxj. The default choice is the Euclidean distance dist( xi;xj)=kxi\u0000xjk.\n",
      "LetIandJbe two disjoint subsets of f1;:::; ng. These sets correspond to two disjoint\n",
      "subsets (that is, clusters) of X:fxi;i=Igandfxj;j=Jg. We denote the distance between148 4.7. Hierarchical Clustering\n",
      "7 8 6 1 2 3 4 5\n",
      "Labels10203040Distance\n",
      "Figure 4.10: Left: a cluster hierarchy of 15 clusters. Right: the corresponding dendrogram.\n",
      "these two clusters by d(I;J). By specifying the function d, we indicate how the clusters\n",
      "are linked. For this reason it is also referred to as the linkage linkage criterion. We give a number\n",
      "of examples:\n",
      "Single linkage. The closest distance between the clusters.\n",
      "dmin(I;J) :=min\n",
      "i2I;j2Jdist(xi;xj):\n",
      "Complete linkage. The furthest distance between the clusters.\n",
      "dmax(I;J) :=max\n",
      "i2I;j2Jdist(xi;xj):\n",
      "Group average. The mean distance between the clusters. Note that this depends on\n",
      "the cluster sizes.\n",
      "davg(I;J) :=1\n",
      "jIjjJjX\n",
      "i2IX\n",
      "j2Jdist(xi;xj):\n",
      "For these linkage criteria, Xis usually assumed to be Rdwith the Euclidean distance.\n",
      "Another notable measure for the distance between clusters is Ward’s minimum vari-\n",
      "ance linkage criterion. Ward’s linkage Here, the distance between clusters is expressed as the additional\n",
      "amount of “variance” (expressed in terms of the sum of squares) that would be intro-\n",
      "duced if the two clusters were merged. More precisely, for any set Kof indices (labels) let\n",
      "xK=P\n",
      "k2Kxk=jKjdenote its corresponding cluster mean. Then\n",
      "dWard(I;J) :=X\n",
      "k2I[Jkxk\u0000xI[Jk2\u00000BBBBBB@X\n",
      "i2Ikxi\u0000xIk2+X\n",
      "j2Jkxj\u0000xJk21CCCCCCA: (4.41)\n",
      "It can be shown (see Exercise 8) that the Ward linkage depends only on the cluster means\n",
      "and the cluster sizes for IandJ:\n",
      "dWard(I;J)=jIjjJj\n",
      "jIj+jJjkxI\u0000xJk2:Chapter 4. Unsupervised Learning 149\n",
      "In software implementations, the Ward linkage function is often rescaled by mul-\n",
      "tiplying it by a factor of 2. In this way, the distance between one-point clusters fxig\n",
      "andfxjgis the squared Euclidean distance kxi\u0000xjk2.\n",
      "Having chosen a distance on Xand a linkage criterion, a general agglomerative clus-\n",
      "tering algorithm proceeds in the following “greedy” manner.\n",
      "Algorithm 4.7.1: Greedy Agglomerative Clustering\n",
      "input: Distance function dist, linkage function d, number of clusters K.\n",
      "output: The label sets for the tree.\n",
      "1Initialize the set of cluster identiﬁers: I=f1;:::; ng.\n",
      "2Initialize the corresponding label sets: Li=fig,i2I.\n",
      "3Initialize a distance matrix D=[di j] with di j=d(fig;fjg).\n",
      "4fork=n+1to2n\u0000Kdo\n",
      "5 Find iandj>iinIsuch that di jis minimal.\n",
      "6 Create a new label set Lk:=Li[L j.\n",
      "7 Add the new identiﬁer ktoIand remove the old identiﬁers iandjfromI.\n",
      "8 Update the distance matrix Dwith respect to the identiﬁers i,j, and k.\n",
      "9returnLi;i=1;:::; 2n\u0000K\n",
      "Initially, the distance matrix Dcontains the (linkage) distances between the one-point\n",
      "clusters containing one of the data points x1;:::; xn, and hence with identiﬁers 1 ;:::; n.\n",
      "Finding the shortest distance amounts to a table lookup in D. When the closest clusters\n",
      "are found, they are merged into a new cluster, and a new identiﬁer k(the smallest positive\n",
      "integer that has not yet been used as an identiﬁer) is assigned to this cluster. The old iden-\n",
      "tiﬁers iand jare removed from the cluster identiﬁer set I. The matrix Dis then updated\n",
      "by adding a k-th column and row that contain the distances between kand any m2I. This\n",
      "updating step could be computationally quite costly if the cluster sizes are large and the\n",
      "linkage distance between the clusters depends on all points within the clusters. Fortunately,\n",
      "for many linkage functions, the matrix Dcan be updated in an e \u000ecient manner.\n",
      "Suppose that at some stage in the algorithm, clusters IandJ, with identiﬁers iandj,\n",
      "respectively, are merged into a cluster K=I[J with identiﬁer k. LetM, with identiﬁer\n",
      "m, be a previously assigned cluster. An update rule of the linkage distance dkmbetweenK\n",
      "andMis called a Lance–Williams Lance–\n",
      "Williamsupdate if it can be written in the form\n",
      "di j+\u000ejdim\u0000djmj;\n",
      "where\u000b;:::;\u000e depend only on simple characteristics of the clusters involved, such as the\n",
      "number of elements within the clusters. Table 4.2 shows the update constants for a number\n",
      "of common linkage functions. For example, for single linkage, dimis the minimal distance\n",
      "betweenIandM, and djmis the minimal distance between JandM. The smallest of\n",
      "these is the minimal distance between KandM. That is, dkm=minfdim;djmg=dim=2+\n",
      "djm=2\u0000jdim\u0000djmj=2.150 4.7. Hierarchical Clustering\n",
      "Table 4.2: Constants for the Lance–Williams update rule for various linkage functions,\n",
      "with ni;nj;nmdenoting the number of elements in the corresponding clusters.\n",
      " \u000enkage \u000b \f \n",
      "Single 1 =2 1 =2 0 \u00001=2\n",
      "Complete 1 =2 1 =2 0 1 =2\n",
      "Group avg.ni\n",
      "ni+njnj\n",
      "ni+nj0 0\n",
      "Wardni+nm\n",
      "ni+nj+nmnj+nm\n",
      "ni+nj+nm\u0000nm\n",
      "ni+nj+nm0\n",
      "In practice, Algorithm 4.7.1 is run until a single cluster is obtained. Instead of returning\n",
      "the label sets of all 2 n\u00001 clusters, a linkage matrix linkage matrix is returned that contains the same\n",
      "information. At the end of each iteration (Line 8) the linkage matrix stores the merged\n",
      "labels iandj, as well as the (minimal) distance di j. Other information such as the number\n",
      "of elements in the merged cluster can also be stored. Dendrograms and cluster labels can be\n",
      "directly constructed from the linkage matrix. In the following example, the linkage matrix\n",
      "is returned by the method agg_cluster .\n",
      "Example 4.8 (Agglomerative Hierarchical Clustering) The Python code below gives\n",
      "a basic implementation of Algorithm 4.7.1 using the Ward linkage function. The methods\n",
      "fcluster anddendrogram from the scipy module can be used to identify the labels in\n",
      "a cluster and to draw the corresponding dendrogram.\n",
      "AggCluster.py\n",
      "import numpy as np\n",
      "from scipy.spatial.distance import cdist\n",
      "def update_distances(D,i,j, sizes): # distances for merged cluster\n",
      "n = D.shape[0]\n",
      "d = np.inf * np.ones(n+1)\n",
      "for k in range(n): # Update distances\n",
      "d[k] = ((sizes[i]+sizes[k])*D[i,k] +\n",
      "(sizes[j]+sizes[k])*D[j,k] -\n",
      "sizes[k]*D[i,j])/(sizes[i] + sizes[j] + sizes[k])\n",
      "infs = np.inf * np.ones(n) # array of infinity\n",
      "D[i,:],D[:,i],D[j,:],D[:,j] = infs ,infs ,infs ,infs # deactivate\n",
      "new_D = np.inf * np.ones((n+1,n+1))\n",
      "new_D[0:n,0:n] = D # copy old matrix into new_D\n",
      "new_D[-1,:], new_D[:,-1] = d,d # add new row and column\n",
      "return new_D\n",
      "def agg_cluster(X):\n",
      "n = X.shape[0]\n",
      "sizes = np.ones(n)\n",
      "D = cdist(X, X,metric = 'sqeuclidean ') # initialize dist. matrix\n",
      ".\n",
      "np.fill_diagonal(D, np.inf * np.ones(D.shape[0]))\n",
      "Z = np.zeros((n-1,4)) #linkage matrix encodes hierarchy tree\n",
      "for t in range(n-1):Chapter 4. Unsupervised Learning 151\n",
      "i,j = np.unravel_index(D.argmin(), D.shape) # minimizer pair\n",
      "sizes = np.append(sizes , sizes[i] + sizes[j])\n",
      "Z[t,:]=np.array([i, j, np.sqrt(D[i,j]), sizes[-1]])\n",
      "D = update_distances(D, i,j, sizes) # update distance matr.\n",
      "return Z\n",
      "import scipy.cluster.hierarchy as h\n",
      "X = np.genfromtxt( 'clusterdata.csv ',delimiter= ',') # read the data\n",
      "Z = agg_cluster(X) # form the linkage matrix\n",
      "h.dendrogram(Z) # SciPy can produce a dendrogram from Z\n",
      "# fcluster function assigns cluster ids to all points based on Z\n",
      "cl = h.fcluster(Z, criterion = 'maxclust ', t=3)\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(2), plt.clf()\n",
      "cols = [ 'red','green ','blue ']\n",
      "colors = [cols[i-1] for i in cl]\n",
      "plt.scatter(X[:,0], X[:,1],c=colors)\n",
      "plt.show()\n",
      "Note that the distance matrix is initialized with the squared Euclidean distance, so that\n",
      "the Ward linkage is rescaled by a factor of 2. Also, note that the linkage matrix stores\n",
      "the square root of the minimal cluster distances rather than the distances themselves. We\n",
      "leave it as an exercise to check that by using these modiﬁcations the results agree with the\n",
      "linkage method from scipy ; see Exercise 9.\n",
      "In contrast to the bottom-up (agglomerative) approach to hierarchical clustering, the\n",
      "divisive approach starts with one cluster, which is divided into two clusters that are as\n",
      "“dissimilar” as possible, which can then be further divided, and so on. We can use the same\n",
      "linkage criteria as for agglomerative clustering to divide a parent cluster into two child\n",
      "clusters by maximizing the distance between the child clusters. Although it is a natural to try\n",
      "to group together data by separating dissimilar ones as far as possible, the implementation\n",
      "of this idea tends to scale poorly with n. The problem is related to the well-known max-cut\n",
      "problem max-cut\n",
      "problem: given an n\u0002nmatrix of positive costs ci j;i;j2f1;:::; ng, partition the index set\n",
      "I=f1;:::; nginto two subsetsJandKsuch that the total cost across the sets, that is,\n",
      "X\n",
      "j2JX\n",
      "k2Kdjk;\n",
      "is maximal. If instead we maximize according to the average distance, we obtain the group\n",
      "average linkage criterion.\n",
      "Example 4.9 (Divisive Clustering via CE) The following Python code is used to di-\n",
      "vide a small data set (of size 300) into two parts according to maximal group average link-\n",
      "age. It uses a short cross-entropy algorithm similar to the one presented in Example 3.19.\n",
      "Given a vector of probabilities fpi;i=1;:::; ng, the algorithm generates an n\u0002nmatrix +110\n",
      "of Bernoulli random variables with success probability pifor column i. For each row, the\n",
      "0s and 1s divide the index set into two clusters, and the corresponding average linkage152 4.7. Hierarchical Clustering\n",
      "distance is computed. The matrix is then sorted row-wise according to these distances. Fi-\n",
      "nally, the probabilities fpigare updated according to the mean values of the best 10% rows.\n",
      "The process is repeated until the fpigdegenerate to a binary vector. This then presents the\n",
      "(approximate) solution.\n",
      "clustCE2.py\n",
      "import numpy as np\n",
      "from numpy import genfromtxt\n",
      "from scipy.spatial.distance import squareform\n",
      "from scipy.spatial.distance import pdist\n",
      "import matplotlib.pyplot as plt\n",
      "def S(x,D):\n",
      "V1 = np.where(x==0)[0] # {V1,V2} is the partition\n",
      "V2 = np.where(x==1)[0]\n",
      "tmp = D[V1]\n",
      "tmp = tmp[:,V2]\n",
      "return np.mean(tmp) # the size of the cut\n",
      "def maxcut(D,N,eps,rho,alpha):\n",
      "n = D.shape[1]\n",
      "Ne = int(rho*N)\n",
      "p = 1/2*np.ones(n)\n",
      "p[0] = 1.0\n",
      "while (np.max(np.minimum(p,np.subtract(1,p))) > eps):\n",
      "x = np.array(np.random.uniform(0,1,(N,n))<=p, dtype=np.int64)\n",
      "sx = np.zeros(N)\n",
      "for i in range(N):\n",
      "sx[i] = S(x[i],D)\n",
      "sortSX = np.flip(np.argsort(sx))\n",
      "#print(\"gamma = \",sx[sortSX[Ne-1]], \" best=\",sx[sortSX[0]])\n",
      "elIds = sortSX[0:Ne]\n",
      "elites = x[elIds]\n",
      "pnew = np.mean(elites , axis=0)\n",
      "p = alpha*pnew + (1.0-alpha)*p\n",
      "return np.round(p)\n",
      "Xmat = genfromtxt( 'clusterdata.csv ', delimiter= ',')\n",
      "n = Xmat.shape[0]\n",
      "D = squareform(pdist(Xmat))\n",
      "N = 1000\n",
      "eps = 10**-2\n",
      "rho = 0.1\n",
      "alpha = 0.9\n",
      "# CE\n",
      "pout = maxcut(D,N,eps,rho, alpha);\n",
      "cutval = S(pout ,D)Chapter 4. Unsupervised Learning 153\n",
      "print(\"cutvalue \",cutval)\n",
      "#plot\n",
      "V1 = np.where(pout==0)[0]\n",
      "xblue = Xmat[V1]\n",
      "V2 = np.where(pout==1)[0]\n",
      "xred = Xmat[V2]\n",
      "plt.scatter(xblue[:,0],xblue[:,1], c=\"blue\")\n",
      "plt.scatter(xred[:,0],xred[:,1], c=\"red\")\n",
      "cutvalue 4.625207676517948\n",
      "6\n",
      " 4\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 4\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Figure 4.11: Division of the data in Figure 4.4 into two clusters, via the cross-entropy\n",
      "method.\n",
      "4.8 Principal Component Analysis (PCA)\n",
      "The main idea of principal component analysis principal\n",
      "component\n",
      "analysis(PCA) is to reduce the dimensionality of\n",
      "a data set consisting of many variables. PCA is a feature reduction (orfeature extraction )\n",
      "mechanism, that helps us to handle high-dimensional data with more features than is con-\n",
      "venient to interpret.\n",
      "4.8.1 Motivation: Principal Axes of an Ellipsoid\n",
      "Consider a d-dimensional normal distribution with mean vector 0and covariance matrix\n",
      "\u0006. The corresponding pdf (see (2.33)) is + 45\n",
      "f(x)=1p(2\u0019)nj\u0006je\u00001\n",
      "2x>\u0006\u00001x;x2Rd:\n",
      "If we were to draw many iid samples from this pdf, the points would roughly have an\n",
      "ellipsoid pattern, as illustrated in Figure 3.1, and correspond to the contours of f: sets of + 71154 4.8. Principal Component Analysis (PCA)\n",
      "points xsuch that x>\u0006\u00001x=c, for some c>0. In particular, consider the ellipsoid\n",
      "x>\u0006\u00001x=1;x2Rd: (4.42)\n",
      "Let\u0006=BB>, where Bis for example the (lower) Cholesky matrix. Then, as explained +375\n",
      "in Example A.5, the ellipsoid (4.42) can also be viewed as the linear transformation of +368\n",
      "d-dimensional unit sphere via matrix B. Moreover, the principal axes of the ellipsoid canprincipal axesbe found via a singular value decomposition (SVD) of B(or\u0006); see Section A.6.5 andsingular value\n",
      "decomposition Example A.8. In particular, suppose that an SVD of Bis\n",
      "+380\n",
      "B=UDV>(note that an SVD of \u0006is then UD2U>):\n",
      "The columns of the matrix UDcorrespond to the principal axes of the ellipsoid, and the\n",
      "relative magnitudes of the axes are given by the elements of the diagonal matrix D. If some\n",
      "of these magnitudes are small compared to the others, a reduction in the dimension of the\n",
      "space may be achieved by projecting each point x2Rdonto the subspace spanned by the\n",
      "main (say k\u001cd) columns of U— the so-called principal components principal\n",
      "components. Suppose without\n",
      "loss of generality that the ﬁrst kprincipal components are given by the ﬁrst kcolumns of\n",
      "U, and let Ukbe the corresponding d\u0002kmatrix.\n",
      "With respect to the standard basis feig, the vector x=x1e1+\u0001\u0001\u0001+xdedis represented by\n",
      "thed-dimensional vector [ x1;:::; xd]>. With respect to the orthonormal basis fuigformed\n",
      "by the columns of matrix U, the representation of xisU>x. Similarly, the projection of\n",
      "any point xonto the subspace spanned by the ﬁrst kprincipal vectors is represented by the\n",
      "k-dimensional vector U>\n",
      "kx, with respect to the orthonormal basis formed by the columns of\n",
      "Uk. So, the idea is that if a point xlies close to its projection UkU>\n",
      "kx, we may represent it via\n",
      "knumbers instead of d, using the combined features given by the kprincipal components.\n",
      "See Section A.4 for a review of projections and orthonormal bases. +364\n",
      "Example 4.10 (Principal Components) Consider the matrix\n",
      "\u0006=266666666414 8 3\n",
      "8 5 2\n",
      "3 2 13777777775;\n",
      "which can be written as \u0006=BB>, with\n",
      "B=26666666641 2 3\n",
      "0 1 2\n",
      "0 0 13777777775:\n",
      "Figure 4.12 depicts the ellipsoid x>\u0006\u00001x=1, which can be obtained by linearly transform-\n",
      "ing the points on the unit sphere by means of the matrix B. The principal axes and sizes of\n",
      "the ellipsoid are found through a singular value decomposition B=UDV>, where Uand\n",
      "Dare\n",
      "U=26666666640:8460 0:4828 0:2261\n",
      "0:4973\u00000:5618\u00000:6611\n",
      "0:1922\u00000:6718 0:71543777777775and D=26666666644:4027 0 0\n",
      "0 0:7187 0\n",
      "0 0 0 :31603777777775:Chapter 4. Unsupervised Learning 155\n",
      "The columns of Ushow the directions of the principal axes of the ellipsoid, and the di-\n",
      "agonal elements of Dindicate the relative magnitudes of the principal axes. We see that\n",
      "the ﬁrst principal component is given by the ﬁrst column of U, and the second principal\n",
      "component by the second column of U.\n",
      "The projection of the point x=[1:052;0:6648;0:2271]>onto the 1-dimensional space\n",
      "spanned by the ﬁrst principal component u1=[0:8460;0:4972;0:1922]>isz=u1u>\n",
      "1x=\n",
      "[1:0696;0:6287;0:2429]>. With respect to the basis vector u1,zis represented by the num-\n",
      "beru>\n",
      "1z=1:2643. That is, z=1:2643u1.\n",
      "Figure 4.12: A “surfboard” ellipsoid where one principal axis is signiﬁcantly larger than\n",
      "the other two.\n",
      "4.8.2 PCA and Singular Value Decomposition (SVD)\n",
      "In the setting above, we did not consider any data set drawn from a multivariate pdf f. The\n",
      "whole analysis rested on linear algebra. In principal component analysis principal\n",
      "component\n",
      "analysis(PCA) we start\n",
      "with data x1;:::; xn, where each xisd-dimensional. PCA does not require assumptions\n",
      "how the data were obtained, but to make the link with the previous section, we can think\n",
      "of the data as iid draws from a multivariate normal pdf.\n",
      "Let us collect the data in a matrix Xin the usual way; that is, + 43\n",
      "X=26666666666666664x11x12::: x1d\n",
      "x21x22::: x2d\n",
      "::::::::::::\n",
      "xn1xn2::: xnd37777777777777775=26666666666666664x>\n",
      "1\n",
      "x>\n",
      "2:::\n",
      "x>\n",
      "n37777777777777775:\n",
      "The matrix Xwill be the PCA’s input. Under this setting, the data consists of points in d-\n",
      "dimensional space, and our goal is to present the data using nfeature vectors of dimension\n",
      "k<d.\n",
      "In accordance with the previous section, we assume that underlying distribution of the\n",
      "data has expectation vector 0. In practice, this means that before PCA is applied, the data\n",
      "needs to be centered by subtracting the column mean in every column:\n",
      "x0\n",
      "i j=xi j\u0000xj;156 4.8. Principal Component Analysis (PCA)\n",
      "where xj=1\n",
      "nPn\n",
      "i=1xi j.\n",
      "We assume from now on that the data comes from a general d-dimensional distribution\n",
      "with mean vector 0and some covariance matrix \u0006. The covariance matrix \u0006is by deﬁnition\n",
      "equal to the expectation of the random matrix XX>, and can be estimated from the data\n",
      "x1;:::; xnvia the sample average\n",
      "b\u0006=1\n",
      "nnX\n",
      "i=1xix>\n",
      "i=1\n",
      "nX>X:\n",
      "Asb\u0006is a covariance matrix, we may conduct the same analysis for b\u0006as we did for \u0006in the\n",
      "previous section. Speciﬁcally, suppose b\u0006=UD2U>is an SVD of b\u0006and let Ukbe the matrix\n",
      "whose columns are the kprincipal components; that is, the kcolumns of Ucorresponding to\n",
      "the largest diagonal elements in D2. Note that we have used D2instead of Dto be compat-\n",
      "ible with the previous section. The transformation zi=UkU>\n",
      "kximaps each vector xi2Rd\n",
      "(thus, with dfeatures) to a vector zi2Rdlying in the subspace spanned by the columns of\n",
      "Uk. With respect to this basis, the point zihas representation zi=U>\n",
      "k(UkU>\n",
      "kxi)=U>\n",
      "kxi2Rk\n",
      "(thus with kfeatures). The corresponding covariance matrix of the zi;i=1;:::; nis diag-\n",
      "onal. The diagonal elements fd``gofDcan be interpreted as standard deviations of the data\n",
      "in the directions of the principal components. The quantity v=P\n",
      "`=1d2\n",
      "``(that is, the trace of\n",
      "D2) is thus a measure for the amount of variance in the data. The proportion d2\n",
      "``=vindicates\n",
      "how much of the variance in the data is explained by the `-th principal component.\n",
      "Another way to look at PCA is by considering the question: How can we best project the\n",
      "data onto a k-dimensional subspace in such a way that the total squared distance between\n",
      "the projected points and the original points is minimal? From Section A.4, we know that +364\n",
      "any orthogonal projection to a k-dimensional subspace Vkcan be represented by a matrix\n",
      "UkU>\n",
      "k, where Uk=[u1;:::; uk] and thefu`;`=1;:::; kgare orthogonal vectors of length 1\n",
      "that spanVk. The above question can thus be formulated as the minimization program:\n",
      "min\n",
      "u1;:::;uknX\n",
      "i=1kxi\u0000UkU>\n",
      "kxik2: (4.43)\n",
      "Now observe that\n",
      "1\n",
      "nnX\n",
      "i=1kxi\u0000UkU>\n",
      "kxik2=1\n",
      "nnX\n",
      "i=1(x>\n",
      "i\u0000x>\n",
      "iUkU>\n",
      "k)(xi\u0000UkU>\n",
      "kxi)\n",
      "=1\n",
      "nnX\n",
      "i=1kxik2\n",
      "|       {z       }\n",
      "c\u00001\n",
      "nnX\n",
      "i=1x>\n",
      "iUkU>\n",
      "kxi=c\u00001\n",
      "nnX\n",
      "i=1kX\n",
      "`=1tr(x>\n",
      "iu`u>\n",
      "`xi)\n",
      "=c\u00001\n",
      "nkX\n",
      "`=1nX\n",
      "i=1u>\n",
      "`xix>\n",
      "iu`=c\u0000kX\n",
      "`=1u>\n",
      "`b\u0006u`;\n",
      "where we have used the cyclic property of a trace (Theorem A.1) and the fact that UkU>\n",
      "k +359\n",
      "can be written asPk\n",
      "`=1u`u>\n",
      "`. It follows that the minimization problem(4.43) is equivalent\n",
      "to the maximization problem\n",
      "max\n",
      "u1;:::;ukkX\n",
      "`=1u>\n",
      "`b\u0006u`: (4.44)Chapter 4. Unsupervised Learning 157\n",
      "This maximum can be at mostPk\n",
      "`=1d2\n",
      "``and is attained precisely when u1;:::; ukare the\n",
      "ﬁrstkprincipal components of b\u0006.\n",
      "Example 4.11 (Singular Value Decomposition) The following data set consists of in-\n",
      "dependent samples from the three-dimensional Gaussian distribution with mean vector 0\n",
      "and covariance matrix \u0006given in Example 4.10:\n",
      "X=26666666666666666666666666666666666666666666643:1209 1:7438 0:5479\n",
      "\u00002:6628\u00001:5310\u00000:2763\n",
      "3:7284 3:0648 1:8451\n",
      "0:4203 0:3553 0:4268\n",
      "\u00000:7155\u00000:6871\u00000:1414\n",
      "5:8728 4:0180 1:4541\n",
      "4:8163 2:4799 0:5637\n",
      "2:6948 1:2384 0:1533\n",
      "\u00001:1376\u00000:4677\u00000:2219\n",
      "\u00001:2452\u00000:9942\u00000:44493777777777777777777777777777777777777777777775:\n",
      "After replacing Xwith its centered version, an SVD UD2U>ofb\u0006=X>X=nyields the\n",
      "principal component matrix Uand diagonal matrix D:\n",
      "U=2666666664\u00000:8277 0:4613 0:3195\n",
      "\u00000:5300\u00000:4556\u00000:7152\n",
      "\u00000:1843\u00000:7613 0:62163777777775and D=26666666643:3424 0 0\n",
      "0 0:4778 0\n",
      "0 0 0 :10383777777775:\n",
      "We also observe that, apart from the sign of the ﬁrst column, the principal component\n",
      "matrix Uis similar to that in Example 4.10. Likewise for the matrix D. We see that 97.90%\n",
      "of the total variance is explained by the ﬁrst principal component. Figure 4.13 shows the\n",
      "projection of the centered data onto the subspace spanned by this principal component.\n",
      "x42024y\n",
      "210123z\n",
      "1.00.50.00.51.01.5\n",
      "Figure 4.13: Data from the “surfboard” pdf is projected onto the subspace spanned by the\n",
      "largest principal component.\n",
      "The following Python code was used.158 4.8. Principal Component Analysis (PCA)\n",
      "PCAdat.py\n",
      "import numpy as np\n",
      "X = np.genfromtxt( 'pcadat.csv ', delimiter= ',')\n",
      "n = X.shape[0]\n",
      "X = X - X.mean(axis=0)\n",
      "G = X.T @ X\n",
      "U, _ , _ = np.linalg.svd(G/n)\n",
      "# projected points\n",
      "Y = X @ np.outer(U[:,0],U[:,0])\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111, projection= '3d')\n",
      "ax.w_xaxis.set_pane_color((0, 0, 0, 0))\n",
      "ax.plot(Y[:,0], Y[:,1], Y[:,2], c= 'k', linewidth=1)\n",
      "ax.scatter(X[:,0], X[:,1], X[:,2], c= 'b')\n",
      "ax.scatter(Y[:,0], Y[:,1], Y[:,2], c= 'r')\n",
      "for i in range(n):\n",
      "ax.plot([X[i,0], Y[i,0]], [X[i,1],Y[i,1]], [X[i,2],Y[i,2]], 'b')\n",
      "ax.set_xlabel( 'x')\n",
      "ax.set_ylabel( 'y')\n",
      "ax.set_zlabel( 'z')\n",
      "plt.show()\n",
      "Next is an application of PCA to Fisher’s famous iris data set, already mentioned in\n",
      "Section 1.1, and Exercise 1.5. +2\n",
      "Example 4.12 (PCA for the Iris Data Set) Theiris data set contains measurements\n",
      "on four features of the iris plant: sepal length and width, and petal length and width, for a\n",
      "total of 150 specimens. The full data set also contains the species name, but for the purpose\n",
      "of this example we ignore it.\n",
      "Figure 1.9 shows that there is a signiﬁcant correlation between the di \u000berent features. +17\n",
      "Can we perhaps describe the data using fewer features by taking certain linear combin-\n",
      "ations of the original features? To investigate this, let us perform a PCA, ﬁrst centering\n",
      "the data. The following Python code implements the PCA. It is assumed that a CSV ﬁle\n",
      "irisX.csv has been made that contains the iris data set (without the species information).\n",
      "PCAiris.py\n",
      "import seaborn as sns, numpy as np\n",
      "np.set_printoptions(precision=4)\n",
      "X = np.genfromtxt( 'IrisX.csv ',delimiter= ',')\n",
      "n = X.shape[0]Chapter 4. Unsupervised Learning 159\n",
      "X = X - np.mean(X, axis=0)\n",
      "[U,D2,UT]= np.linalg.svd((X.T @ X)/n)\n",
      "print( 'U = \\n ', U); print( '\\n diag(D^2) = ', D2)\n",
      "z = U[:,0].T @ X.T\n",
      "sns.kdeplot(z, bw=0.15)\n",
      "U =\n",
      "[[-0.3614 -0.6566 0.582 0.3155]\n",
      "[ 0.0845 -0.7302 -0.5979 -0.3197]\n",
      "[-0.8567 0.1734 -0.0762 -0.4798]\n",
      "[-0.3583 0.0755 -0.5458 0.7537]]\n",
      "diag(D^2) = [4.2001 0.2411 0.0777 0.0237]\n",
      "The output above shows the principal component matrix (which we called U) as well as\n",
      "the diagonal of matrix D2. We see that a large proportion of the variance, 4 :2001=(4:2001 +\n",
      "0:2411+0:0777+0:0237) =92:46%, is explained by the ﬁrst principal component. Thus, it\n",
      "makes sense to transform each data point x2R4tou>\n",
      "1x2R. Figure 4.14 shows the kernel\n",
      "density estimate of the transformed data. Interestingly, we see two modes, indicating at\n",
      "least two clusters in the data.\n",
      "-4 -3 -2 -1 0 1 2 3 4\n",
      "PCA-combined data00.20.40.6kernel density estimate\n",
      "Figure 4.14: Kernel density estimate of the PCA-combined iris data.\n",
      "Further Reading\n",
      "Various information-theoretic measures to quantify uncertainty, including the Shannon en-\n",
      "tropy and Kullback–Leibler divergence, may be found in [28]. The Fisher information, the\n",
      "prominent information measure in statistics, is discussed in detail in [78]. Akaike’s inform-\n",
      "ation criterion appeared in [2]. The EM algorithm was introduced in [31] and [85] gives an\n",
      "in-depth treatment. Convergence proofs for the EM algorithm may be found in [19, 128].\n",
      "A classical reference on kernel density estimation is [113], and [14] is the main reference\n",
      "for the theta kernel density estimator. Theory and applications on ﬁnite mixture models\n",
      "may be found in [86]. For more details on clustering applications and algorithms as well\n",
      "as references on data compression, vector quantization, and pattern recognition, we refer160 Exercises\n",
      "to [1, 35, 107, 125]. A useful modiﬁcation of the K-means algorithm is the fuzzy K-means\n",
      "algorithm; see, e.g., [9]. A popular way to choose the starting positions in K-means is given\n",
      "by the K-means ++heuristic, introduced in [4].\n",
      "Exercises\n",
      "1. This exercise is to show that the Fisher information matrix F(\u0012) in (4.8) is equal to the\n",
      "matrix H(\u0012) in (4.9), in the special case where f=g(\u0001j\u0012), and under the assumption that\n",
      "integration and di \u000berentiation orders can be interchanged.\n",
      "(a) Let hbe a vector-valued function and ka real-valued function. Prove the following\n",
      "quotient rule for di \u000berentiation quotient rule\n",
      "for\n",
      "differentiation:\n",
      "@[h(\u0012)=k(\u0012)]\n",
      "@\u0012=1\n",
      "k(\u0012)@h(\u0012)\n",
      "@\u0012\u00001\n",
      "k2(\u0012)@k(\u0012)\n",
      "@\u0012h(\u0012)>: (4.45)\n",
      "(b) Now take h(\u0012)=@g(Xj\u0012)\n",
      "@\u0012andk(\u0012)=g(Xj\u0012) in (4.45) and take expectations with\n",
      "respect toE\u0012on both sides to show that\n",
      "\u0000H(\u0012)=E\u001226666641\n",
      "g(Xj\u0012)@@g(Xj\u0012)\n",
      "@\u0012\n",
      "@\u00123777775\n",
      "|                    {z                    }\n",
      "A\u0000F(\u0012):\n",
      "(c) Finally show that Ais the zero matrix.\n",
      "2. Plot the mixture of N(0;1),U(0;1), and Exp(1) distributions, with weights w1=w2=\n",
      "w3=1=3.\n",
      "3. Denote the pdfs in Exercise 2 by f1;f2;f3, respectively. Suppose that Xis simulated via\n",
      "the two-step procedure: First, draw Zfromf1;2;3g, then draw Xfrom fZ. How likely is it\n",
      "that the outcome x=0:5 ofXhas come from the uniform pdf f2?\n",
      "4. Simulate an iid training set of size 100 from the Gamma (2:3;0:5) distribution, and\n",
      "implement the Fisher scoring method in Example 4.1 to ﬁnd the maximum likelihood es-\n",
      "timate. Plot the true and approximate pdfs.\n",
      "5. LetT=fX1;:::; Xngbe iid data from a pdf g(xj\u0012) with Fisher matrix F(\u0012). Explain\n",
      "why, under the conditions where (4.7) holds,\n",
      "ST(\u0012) :=1\n",
      "nnX\n",
      "i=1S(Xij\u0012)\n",
      "for large nhas approximately a multivariate normal distribution with expectation vector 0\n",
      "and covariance matrix F(\u0012)=n.\n",
      "6. Figure 4.15 shows a Gaussian KDE with bandwidth \u001b=0:2 on the points\u00000:5;0,\n",
      "0:2;0:9, and 1:5. Reproduce the plot in Python. Using the same bandwidth, plot also the\n",
      "KDE for the same data, but now with \u001e(z)=1=2;z2[\u00001;1].Chapter 4. Unsupervised Learning 161\n",
      "-1 0 1 200.20.40.60.8\n",
      "Figure 4.15: The Gaussian KDE (solid line) is the equally weighted mixture of normal pdfs\n",
      "centered around the data and with standard deviation \u001b=0:2 (dashed).\n",
      "7. For ﬁxed x0, the Gaussian kernel function\n",
      "f(xjt) :=1p\n",
      "2\u0019te\u00001\n",
      "2(x\u0000x0)2\n",
      "t\n",
      "is the solution to Fourier’s heat equation\n",
      "@\n",
      "@tf(xjt)=1\n",
      "2@2\n",
      "@x2f(xjt);x2R;t>0;\n",
      "with initial condition f(xj0)=\u000e(x\u0000x0) (the Dirac function at x0). Show this. As a con-\n",
      "sequence, the Gaussian KDE is the solution to the same heat equation, but now with initial\n",
      "condition f(xj0)=n\u00001Pn\n",
      "i=1\u000e(x\u0000xi). This was the motivation for the theta KDE [14],\n",
      "which is a solution to the same heat equation but now on a bounded interval.\n",
      "8. Show that the Ward linkage given in (4.41) is equal to\n",
      "dWard(I;J)=jIjjJj\n",
      "jIj+jJjkxI\u0000xJk2:\n",
      "9. Carry out the agglomerative hierarchical clustering of Example 4.8 via the linkage\n",
      "method from scipy.cluster.hierarchy . Show that the linkage matrices are the same.\n",
      "Give a scatterplot of the data, color coded into K=3 clusters.\n",
      "10. Suppose that we have the data \u001cn=fx1;:::; xnginRand decide to train the two-\n",
      "component Gaussian mixture model\n",
      "g(xj\u0012)=w11q\n",
      "2\u0019\u001b2\n",
      "1exp \n",
      "\u0000(x\u0000\u00161)2\n",
      "2\u001b2\n",
      "1!\n",
      "+w21q\n",
      "2\u0019\u001b2\n",
      "2exp \n",
      "\u0000(x\u0000\u00162)2\n",
      "2\u001b2\n",
      "2!\n",
      ";\n",
      "where the parameter vector \u0012=[\u00161;\u00162;\u001b1;\u001b2;w1;w2]>belongs to the set\n",
      "\u0002 =f\u0012:w1+w2=1;w12[0;1];\u0016i2R;\u001bi>0;8ig:\n",
      "Suppose that the training is via the maximum likelihood in (2.28). Show that\n",
      "sup\n",
      "\u00122\u00021\n",
      "nnX\n",
      "i=1lng(xij\u0012)=1:\n",
      "In other words, ﬁnd a sequence of values for \u00122\u0002such that the likelihood grows without\n",
      "bound. How can we restrict the set \u0002to ensure that the likelihood remains bounded?162 Exercises\n",
      "11. A d-dimensional normal random vector X\u0018N(\u0016;\u0006) can be deﬁned via an a \u000ene\n",
      "transformation, X=\u0016+\u00061=2Z, of a standard normal random vector Z\u0018N(0;Id), where\n",
      "\u00061=2(\u00061=2)>= \u0006. In a similar way, we can deﬁne a d-dimensional Student random vector\n",
      "X\u0018t\u000b(\u0016;\u0006) via a transformation\n",
      "X=\u0016+1p\n",
      "S\u00061=2Z; (4.46)\n",
      "where, Z\u0018N(0;Id) and S\u0018Gamma (\u000b\n",
      "2;\u000b\n",
      "2) are independent, \u000b > 0, and \u00061=2(\u00061=2)>= \u0006.\n",
      "Note that we obtain the multivariate normal distribution as a limiting case for \u000b!1 .\n",
      "(a) Show that the density of the t\u000b(0;Id) distribution is given by\n",
      "t\u000b(x) :=\u0000((\u000b+d)=2)\n",
      "(\u0019\u000b)d=2\u0000(\u000b=2) \n",
      "1+1\n",
      "\u000bkxk2!\u0000\u000b+d\n",
      "2\n",
      ":\n",
      "By the transformation rule (C.23), it follows that the density of X\u0018t\u000b(\u0016;\u0006) is given +435\n",
      "byt\u000b;\u0006(x\u0000\u0016), where\n",
      "t\u000b;\u0006(x) :=1\n",
      "j\u00061=2jt\u000b(\u0006\u00001=2x):\n",
      "[Hint: conditional on S=s,Xhas aN(0;Id=s) distribution.]\n",
      "(b) We wish to ﬁt a t\u0017(\u0016;\u0006) distribution to given data \u001c=fx1;:::; xnginRdvia the EM\n",
      "method. We use the representation (4.46) and augment the data with the vector S=\n",
      "[S1;:::; Sn]>of hidden variables. Show that the complete-data likelihood is given by\n",
      "g(\u001c;sj\u0012)=Y\n",
      "i(\u000b=2)\u000b=2s(\u000b+d)=2\u00001\n",
      "iexp(\u0000si\n",
      "2\u000b\u0000si\n",
      "2k\u0006\u00001=2(xi\u0000\u0016)k2)\n",
      "\u0000(\u000b=2)(2\u0019)d=2j\u00061=2j: (4.47)\n",
      "(c) Show that, as a consequence, conditional on the data \u001cand parameter \u0012, the hidden\n",
      "data are mutually independent, and\n",
      "(Sij\u001c;\u0012)\u0018Gamma \u000b+d\n",
      "2;\u000b+k\u0006\u00001=2(xi\u0000\u0016)k2\n",
      "2!\n",
      ";i=1;:::; n:\n",
      "(d) At iteration tof the EM algorithm, let g(t)(s)=g(sj\u001c;\u0012(t\u00001)) be the density of the\n",
      "missing data, given the observed data \u001cand the current parameter guess \u0012(t\u00001). Verify\n",
      "that the expected complete-data log-likelihood is given by:\n",
      "Eg(t)lng(\u001c;Sj\u0012)=n\u000b\n",
      "2ln\u000b\n",
      "2\u0000nd\n",
      "2ln(2\u0019)\u0000nln\u0000\u0012\u000b\n",
      "2\u0013\n",
      "\u0000n\n",
      "2lnj\u0006j\n",
      "+\u000b+d\u00002\n",
      "2nX\n",
      "i=1Eg(t)lnSi\u0000nX\n",
      "i=1\u000b+k\u0006\u00001=2(xi\u0000\u0016)k2\n",
      "2Eg(t)Si:\n",
      "Show that\n",
      "Eg(t)Si=\u000b(t\u00001)+d\n",
      "\u000b(t\u00001)+k(\u0006(t\u00001))\u00001=2(xi\u0000\u0016(t\u00001))k2=:w(t\u00001)\n",
      "i\n",
      "Eg(t)lnSi=  \u000b(t\u00001)+d\n",
      "2!\n",
      "\u0000ln \u000b(t\u00001)+d\n",
      "2!\n",
      "+lnw(t\u00001)\n",
      "i;\n",
      "where :=(ln\u0000)0isdigamma function.Chapter 4. Unsupervised Learning 163\n",
      "(e) Finally, show that in the M-step of the EM algorithm \u0012(t)is updated from \u0012(t\u00001)as\n",
      "follows:\n",
      "\u0016(t)=Pn\n",
      "i=1w(t\u00001)\n",
      "ixiPn\n",
      "i=1w(t\u00001)\n",
      "i\n",
      "\u0006(t)=1\n",
      "nnX\n",
      "i=1w(t\u00001)\n",
      "i(xi\u0000\u0016(t))(xi\u0000\u0016(t))>;\n",
      "and\u000b(t)is deﬁned implicitly through the solution of the nonlinear equation:\n",
      "ln\u0012\u000b\n",
      "2\u0013\n",
      "\u0000 \u0012\u000b\n",
      "2\u0013\n",
      "+  \u000b(t)+d\n",
      "2!\n",
      "\u0000ln \u000b(t)+d\n",
      "2!\n",
      "+1+Pn\n",
      "i=1\u0010\n",
      "ln(w(t\u00001)\n",
      "i)\u0000w(t\u00001)\n",
      "i\u0011\n",
      "n=0:\n",
      "12. A generalization of both the gamma and inverse-gamma distribution is the generalized\n",
      "inverse-gamma distribution generalized\n",
      "inverse -gamma\n",
      "distribution, which has density\n",
      "f(s)=(a=b)p=2\n",
      "2Kp(p\n",
      "ab)sp\u00001e\u00001\n",
      "2(as+b=s);a;b;s>0;p2R; (4.48)\n",
      "where Kpis the modiﬁed Bessel function of the second kind modified Bessel\n",
      "function of the\n",
      "second kind, which can be deﬁned as the\n",
      "integral\n",
      "Kp(x)=Z1\n",
      "0e\u0000xcosh( t)cosh( pt) dt;x>0;p2R: (4.49)\n",
      "We write S\u0018GIG(a;b;p) to denote that Shas a pdf of the form (4.48). The function Kp\n",
      "has many interesting properties. Special cases include\n",
      "K1=2(x)=r\n",
      "x\u0019\n",
      "2e\u0000x1\n",
      "x\n",
      "K3=2(x)=r\n",
      "x\u0019\n",
      "2e\u0000x 1\n",
      "x+1\n",
      "x2!\n",
      "K5=2(x)=r\n",
      "x\u0019\n",
      "2e\u0000x 1\n",
      "x+3\n",
      "x2+3\n",
      "x3!\n",
      ":\n",
      "More generally, Kpsatisﬁes the recursion\n",
      "Kp+1(x)=Kp\u00001(x)+2p\n",
      "xKp(x): (4.50)\n",
      "(a) Using the change of variables ez=spa=b, show that\n",
      "Z1\n",
      "0sp\u00001e\u00001\n",
      "2(as+b=s)ds=2Kp(p\n",
      "ab)(b=a)p=2:\n",
      "(b) Let S\u0018GIG(a;b;p). Show that\n",
      "ES=p\n",
      "b K p+1(p\n",
      "ab)\n",
      "pa K p(p\n",
      "ab)(4.51)\n",
      "and\n",
      "ES\u00001=pa K p+1(p\n",
      "ab)p\n",
      "b K p(p\n",
      "ab)\u00002p\n",
      "b: (4.52)164 Exercises\n",
      "13. In Exercise 11 we viewed the multivariate Student t\u000bdistribution as a scale-mixture scale -mixture\n",
      "of the N(0;Id) distribution. In this exercise, we consider a similar transformation, but now\n",
      "\u00061=2Z\u0018N(0;\u0006) is not divided but is multiplied byp\n",
      "S, with S\u0018Gamma (\u000b=2;\u000b=2):\n",
      "X=\u0016+p\n",
      "S\u00061=2Z; (4.53)\n",
      "where SandZare independent and \u000b>0.\n",
      "(a) Show, using Exercise 12, that for \u00061=2=Idand\u0016=0, the random vector Xhas a\n",
      "d-dimensional Bessel distribution Bessel\n",
      "distribution, with density:\n",
      "\u0014\u000b(x) :=21\u0000(\u000b+d)=2\u000b(\u000b+d)=4kxk(\u000b\u0000d)=2\n",
      "\u0019d=2\u0000(\u000b=2)K(\u000b\u0000d)=2\u0010\n",
      "kxkp\u000b\u0011\n",
      ";x2Rd;\n",
      "where Kpis the modiﬁed Bessel function of the second kind given in (4.49). We write\n",
      "X\u0018Bessel\u000b(0;Id). A random vector Xis said to have a Bessel\u000b(\u0016;\u0006) distribution if\n",
      "it can be written in the form (4.53). By the transformation rule (C.23), its density is\n",
      "given by1pj\u0006j\u0014\u000b(\u0006\u00001=2(x\u0000\u0016)). Special instances of the Bessel pdf include:\n",
      "\u00142(x)=exp(\u0000p\n",
      "2jxj)p\n",
      "2\n",
      "\u00144(x)=1+2jxj\n",
      "2exp(\u00002jxj)\n",
      "\u00144(x1;x2;x3)=1\n",
      "\u0019exp\u0012\n",
      "\u00002q\n",
      "x2\n",
      "1+x2\n",
      "2+x2\n",
      "3\u0013\n",
      "\u0014d+1(x)=((d+1)=2)d=2p\u0019\n",
      "(2\u0019)d=2\u0000((d+1)=2)exp\u0010\n",
      "\u0000p\n",
      "d+1kxk\u0011\n",
      ";x2Rd:\n",
      "Note that k2is the (scaled) pdf of the double-exponential or Laplace distribution.\n",
      "(b) Given the data \u001c=fx1;:::; xnginRd, we wish to ﬁt a Bessel pdf to the data by\n",
      "employing the EM algorithm, augmenting the data with the vector S=[S1;:::; Sn]>\n",
      "of missing data. We assume that \u000bis known and \u000b > d. Show that conditional on\n",
      "\u001c(and given\u0012), the missing data vector Shas independent components, with Si\u0018\n",
      "GIG(\u000b;bi;(\u000b\u0000d)=2), with bi:=k\u0006\u00001=2(xi\u0000\u0016)k2,i=1;:::; n.\n",
      "(c) At iteration tof the EM algorithm, let g(t)(s)=g(sj\u001c;\u0012(t\u00001)) be the density of the\n",
      "missing data, given the observed data \u001cand the current parameter guess \u0012(t\u00001). Show\n",
      "that the expected complete-data log-likelihood is given by:\n",
      "Q(t)(\u0012) :=Eg(t)lng(\u001c;Sj\u0012)=\u00001\n",
      "2nX\n",
      "i=1bi(\u0012)w(t\u00001)\n",
      "i+constant; (4.54)\n",
      "where bi(\u0012)=k\u0006\u00001=2(xi\u0000\u0016)k2and\n",
      "w(t\u00001)\n",
      "i:=p\u000bK(\u000b\u0000d+2)=2\u0010p\n",
      "\u000bbi(\u0012(t\u00001))\u0011\n",
      "p\n",
      "bi(\u0012(t\u00001))K(\u000b\u0000d)=2\u0010p\n",
      "\u000bbi(\u0012(t\u00001))\u0011\u0000\u000b\u0000d\n",
      "bi(\u0012(t\u00001));i=1;:::; n:\n",
      "(d) From (4.54) derive the M-step of the EM algorithm. That is, show how \u0012(t)is updated\n",
      "from\u0012(t\u00001).Chapter 4. Unsupervised Learning 165\n",
      "14. Consider the ellipsoid E=fx2Rd:x\u0006\u00001x=1gin (4.42). Let UD2U>be an SVD of\n",
      "\u0006. Show that the linear transformation x7!U>D\u00001xmaps the points on Eonto the unit\n",
      "spherefz2Rd:kzk=1g.\n",
      "15. Figure 4.13 shows how the centered “surfboard” data are projected onto the ﬁrst\n",
      "column of the principal component matrix U. Suppose we project the data instead onto\n",
      "the plane spanned by the ﬁrst twocolumns of U. What are aandbin the representation\n",
      "ax1+bx2=x3of this plane?\n",
      "16. Figure 4.14 suggests that we can assign each feature vector xin the iris data set to\n",
      "one of two clusters, based on the value of u>\n",
      "1x, where u1is the ﬁrst principal component.\n",
      "Plot the sepal lengths against petal lengths and color the points for which u>\n",
      "1x<1:5 di\u000ber-\n",
      "ently to points for which u>\n",
      "1x>1:5. To which species of iris do these clusters correspond?166CHAPTER5\n",
      "REGRESSION\n",
      "Many supervised learning techniques can be gathered under the name “regression”.\n",
      "The purpose of this chapter is to explain the mathematical ideas behind regression\n",
      "models and their practical aspects. We analyze the fundamental linear model in detail,\n",
      "and also discuss nonlinear and generalized linear models.\n",
      "5.1 Introduction\n",
      "Francis Galton observed in an article in 1889 that the heights of adult o \u000bspring are, on the\n",
      "whole, more “average” than the heights of their parents. Galton interpreted this as a degen-\n",
      "erative phenomenon, using the term “regression” to indicate this “return to mediocrity”.\n",
      "Nowadays, regression regression refers to a broad class of supervised learning techniques where the\n",
      "aim is to predict a quantitative response (output) variable yvia a function g(x) of an ex-\n",
      "planatory (input) vector x=[x1;:::; xp]>, consisting of pfeatures, each of which can be\n",
      "continuous or discrete. For instance, regression could be used to predict the birth weight of\n",
      "a baby (the response variable) from the weight of the mother, her socio-economic status,\n",
      "and her smoking habits (the explanatory variables).\n",
      "Let us recapitulate the framework of supervised learning established in Chapter 2. The + 19\n",
      "aim is to ﬁnd a prediction function gthat best guesses1what the random output Ywill be\n",
      "for a random input vector X. The joint pdf f(x;y) ofXandYis unknown, but a training\n",
      "set\u001c=f(x1;y1);:::; (xn;yn)gis available, which is thought of as the outcome of a random\n",
      "training setT=f(X1;Y1);:::; (Xn;Yn)gof iid copies of ( X;Y). Once we have selected a\n",
      "loss function Loss( y;by), such as the squared-error loss squared -error\n",
      "loss\n",
      "Loss( y;by)=(y\u0000by)2; (5.1)\n",
      "then the “best” prediction function gis deﬁned as the one that minimizes the risk risk `(g)=\n",
      "ELoss( Y;g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-\n",
      "tion function is the conditional expectation\n",
      "g\u0003(x)=E[YjX=x]:\n",
      "1Recall the mnemonic use of “g” for “guess”\n",
      "167168 5.1. Introduction\n",
      "As the squared-error loss is the most widely-used loss function for regression, we will\n",
      "adopt this loss function in most of this chapter.\n",
      "The optimal prediction function g\u0003has to be learned from the training set \u001cby minim-\n",
      "izing the training loss\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1(yi\u0000g(xi))2(5.2)\n",
      "over a suitable class of functions G. Note that in the above deﬁnition, the training set \u001cis\n",
      "assumed to be ﬁxed. For a random training set T, we will write the training loss as `T(g).\n",
      "The function gG\n",
      "\u001cthat minimizes the training loss is the function we use for prediction —\n",
      "the so-called learner learner . When the function class Gis clear from the context, we drop the\n",
      "superscript in the notation.\n",
      "As we already saw in (2.2), conditional on X=x, the response Ycan be written as +21\n",
      "Y=g\u0003(x)+\"(x);\n",
      "whereE\"(x)=0. This motivates a standard modeling assumption in supervised learn-\n",
      "ing, in which the responses Y1;:::; Yn, conditional on the explanatory variables X1=\n",
      "x1;:::; Xn=xn, are assumed to be of the form\n",
      "Yi=g(xi)+\"i;i=1;:::; n;\n",
      "where thef\"igare independent with E\"i=0 andVar\"i=\u001b2for some function g2Gand\n",
      "variance\u001b2. The above model is usually further speciﬁed by assuming that gis completely\n",
      "known up to an unknown parameter vector; that is,\n",
      "Yi=g(xij\f)+\"i;i=1;:::; n: (5.3)\n",
      "While the model (5.3) is described conditional on the explanatory variables, it will be\n",
      "convenient to make one further model simpliﬁcation, and view (5.3) as if the fxigwere\n",
      "ﬁxed , while thefYigare random.\n",
      "For the remainder of this chapter, we assume that the training feature vectors fxigare\n",
      "ﬁxed and only the responses are random; that is, T=f(x1;Y1);:::; (xn;Yn)g.\n",
      "The advantage of the model (5.3) is that the problem of estimating the function g from\n",
      "the training data is reduced to the (much simpler) problem of estimating the parameter\n",
      "vector\f. An obvious disadvantage is that functions of the form g(\u0001j\f) may not accurately\n",
      "approximate the true unknown g\u0003. The remainder of this chapter deals with the analysis\n",
      "of models of the form (5.3). In the important case where the function g(\u0001j\f) islinear , the\n",
      "analysis proceeds through the class of linear models. If, in addition, the error terms f\"igare\n",
      "assumed to be Gaussian , this analysis can be carried out using the rich theory of normal\n",
      "linear models.Chapter 5. Regression 169\n",
      "5.2 Linear Regression\n",
      "The most basic regression model involves a linear relationship between the response and a\n",
      "single explanatory variable. In particular, we have measurements ( x1;y1);:::; (xn;yn) that\n",
      "lie approximately on a straight line, as in Figure 5.1.\n",
      "-3 -2 -1 0 1 2 3-5051015\n",
      "Figure 5.1: Data from a simple linear regression model.\n",
      "Following the general scheme captured in (5.3), a simple model for these data is that\n",
      "thefxigare ﬁxed and variables fYigare random such that\n",
      "Yi=\f0+\f1xi+\"i;i=1;:::; n; (5.4)\n",
      "for certain unknown parameters \f0and\f1. Thef\"igare assumed to be independent with\n",
      "expectation 0 and unknown variance \u001b2. The unknown line\n",
      "y=\f0+\f1x|    {z    }\n",
      "g(xj\f)(5.5)\n",
      "is called the regression line regression line . Thus, we view the responses as random variables that would\n",
      "lie exactly on the regression line, were it not for some “disturbance” or “error” term repres-\n",
      "ented by thef\"ig. The extent of the disturbance is modeled by the parameter \u001b2. The model\n",
      "in (5.4) is called simple linear regression simple linear\n",
      "regression\n",
      "model. This model can easily be extended to incorporate\n",
      "more than one explanatory variable, as follows.\n",
      "Deﬁnition 5.1: Multiple Linear Regression Model\n",
      "In a multiple linear regression model multiple linear\n",
      "regression\n",
      "modelthe response Ydepends on a d-dimensional\n",
      "explanatory vector x=[x1;:::; xd]>, via the linear relationship\n",
      "Y=\f0+\f1x1+\u0001\u0001\u0001+\fdxd+\"; (5.6)\n",
      "whereE\"=0 andVar\"=\u001b2.170 5.2. Linear Regression\n",
      "Thus, the data lie approximately on a d-dimensional a \u000ene hyperplane\n",
      "y=\f0+\f1x1+\u0001\u0001\u0001+\fdxd|                      {z                      }\n",
      "g(xj\f);\n",
      "where we deﬁne \f=[\f0;\f1;:::;\f d]>. The function g(xj\f) is linear in \f, but not linear in\n",
      "the feature vector x, due to the constant \f0. However, augmenting the feature space with\n",
      "the constant 1, the mapping [1 ;x>]>7!g(xj\f) :=[1;x>]\fbecomes linear in the feature\n",
      "space and so (5.6) becomes a linear model (see Section 2.1). Most software packages for +43\n",
      "regression include 1 as a feature by default.\n",
      "Note that in (5.6) we only speciﬁed the model for a single pair ( x;Y). The model for the\n",
      "training setT=f(x1;Y1);:::; (xn;Yn)gis simply that each Yisatisﬁes (5.6) (with x=xi)\n",
      "and that thefYigare independent. Setting Y=[Y1;:::; Yn]>, we can write the multiple\n",
      "linear regression model for the training data compactly as\n",
      "Y=X\f+\"; (5.7)\n",
      "where\"=[\"1;:::;\" n]>is a vector of iid copies of \"andXis the model matrix model matrix given by\n",
      "X=266666666666666641x11x12\u0001\u0001\u0001 x1d\n",
      "1x21x22\u0001\u0001\u0001 x2d\n",
      ":::::::::::::::\n",
      "1xn1xn2\u0001\u0001\u0001 xnd37777777777777775=266666666666666641x>\n",
      "1\n",
      "1x>\n",
      "2::::::\n",
      "1x>\n",
      "n37777777777777775:\n",
      "Example 5.1 (Multiple Linear Regression Model) Figure 5.2 depicts a realization of\n",
      "the multiple linear regression model\n",
      "Yi=xi1+xi2+\"i;i=1;:::; 100;\n",
      "where\"1;:::;\" 100\u0018iidN(0;1=16). The ﬁxed feature vectors (vectors of explanatory vari-\n",
      "ables) xi=[xi1;xi2]>;i=1;:::; 100 lie in the unit square.\n",
      "10\n",
      "012\n",
      "10\n",
      "Figure 5.2: Data from a multiple linear regression model.Chapter 5. Regression 171\n",
      "5.3 Analysis via Linear Models\n",
      "Analysis of data from a linear regression model is greatly simpliﬁed through the linear\n",
      "model representation (5.7). In this section we present the main ideas for parameter estima-\n",
      "tion and model selection for a general linear model of the form\n",
      "Y=X\f+\"; (5.8)\n",
      "where Xis an n\u0002pmatrix,\f=[\f1;:::;\f p]>a vector of pparameters, and \"=[\"1;:::;\" n]>\n",
      "ann-dimensional vector of independent error terms, with E\"i=0 andVar\"i=\u001b2,i=\n",
      "1;:::; n. Note that the model matrix Xis assumed to be ﬁxed, and Yand\"are random. A\n",
      "speciﬁc outcome of Yis denoted by y(in accordance with the notation in Section 2.8). + 46\n",
      "Note that the multiple linear regression model in (5.7) was deﬁned using a di \u000berent\n",
      "parameterization; in particular, there we used \f=[\f0;\f1;:::;\f d]>. So, when apply-\n",
      "ing the results in the present section to such models, be aware that p=d+1. Also,\n",
      "in this section a feature vector xincludes the constant 1, so that X>=[x1;:::; xn].\n",
      "5.3.1 Parameter Estimation\n",
      "The linear model Y=X\f+\"contains two unknown parameters, \fand\u001b2, which have\n",
      "to be estimated from the training data \u001c. To estimate \f, we can repeat exactly the same\n",
      "reasoning used in our recurring polynomial regression Example 2.1 as follows. For a linear + 26\n",
      "prediction function g(x)=x>\f, the (squared-error) training loss can be written as\n",
      "`\u001c(g)=1\n",
      "nky\u0000X\fk2;\n",
      "and the optimal learner g\u001cminimizes this quantity, leading to the least-squares estimate b\f,\n",
      "which satisﬁes the normal equations\n",
      "X>X\f=X>y: (5.9)\n",
      "The corresponding training loss can be taken as an estimate of \u001b2; that is,\n",
      "c\u001b2=1\n",
      "nky\u0000Xb\fk2: (5.10)\n",
      "To justify the latter, note that \u001b2is the second moment of the model errors \"i;i=1;:::; n,\n",
      "in (5.8) and could be estimated via the method of moments (see Section C.12.1) using the +457\n",
      "sample average n\u00001P\n",
      "i\"2\n",
      "i=k\"k2=n=kY\u0000X\fk2=n, if\fwere known. By replacing \fwith\n",
      "its estimator, we arrive at (5.10). Note that no distributional properties of the f\"igwere used\n",
      "other thanE\"i=0 andVar\"i=\u001b2;i=1;:::; n. The vector e:=y\u0000Xb\fis called the\n",
      "vector of residuals residuals and approximates the (unknown) vector of model errors \". The quantity\n",
      "kek2=Pn\n",
      "i=1e2\n",
      "iis called the residual sum of squares (RSS). Dividing the RSS by n\u0000pgivesresidual sum of\n",
      "squares an unbiased estimate of \u001b2, which we call the estimated residual squared error (RSE); see\n",
      "residual\n",
      "squared errorExercise 12.172 5.3. Analysis via Linear Models\n",
      "In terms of the notation given in the summary Table 2.1 for supervised learning, we\n",
      "thus have: +25\n",
      "1. The (observed) training data is \u001c=fX;yg.\n",
      "2. The function class Gis the class of linear functions of x; that isG=fg(\u0001j\f) :x7!\n",
      "x>\f;\f2Rpg.\n",
      "3. The (squared-error) training loss is `\u001c(g(\u0001j\f))=ky\u0000X\fk2=n:\n",
      "4. The learner g\u001cis given by g\u001c(x)=x>b\f, where b\f=argmin\f2Rpky\u0000X\fk2.\n",
      "5. The minimal training loss is `\u001c(g\u001c)=ky\u0000Xb\fk2=n=c\u001b2.\n",
      "5.3.2 Model Selection and Prediction\n",
      "Even if we restrict the learner to be a linear function, there is still the issue of which explan-\n",
      "atory variables (features) to include. While including too few features may result in large\n",
      "approximation error (underﬁtting), including too many may result in large statistical error\n",
      "(overﬁtting). As discussed in Section 2.4, we need to select the features which provide the +31\n",
      "best tradeo \u000bbetween the approximation and statistical errors, so that the (expected) gener-\n",
      "alization risk of the learner is minimized. Depending on how the (expected) generalization\n",
      "risk is estimated, there are a number of strategies for feature selection:\n",
      "1. Use test data\u001c0=(X0;y0) that are obtained independently from the training data \u001c,\n",
      "to estimate the generalization risk EkY\u0000g\u001c(X)k2via the test loss (2.7). Then choose +24\n",
      "the collection of features that minimizes the test loss. When there is an abundance of\n",
      "data, part of the data can be reserved as test data, while the remaining data is used as\n",
      "training data.\n",
      "2. When there is a limited amount of data, we can use cross-validation to estimate the\n",
      "expected generalization risk EkY\u0000gT(X)k2(whereTis a random training set), as\n",
      "explained in Section 2.5.2. This is then minimized over the set of possible choices +37\n",
      "for the explanatory variables.\n",
      "3. When one has to choose between many potential explanatory variables, techniques\n",
      "such as regularized least-squares and lasso regression become important. Such\n",
      "methods o \u000ber another approach to model selection, via the regularization (or ho-\n",
      "motopy) paths. This will be the topic of Section 6.2 in the next chapter. +216\n",
      "4. Rather than using computer-intensive techniques, such as the ones above, one can\n",
      "usetheoretical estimates of the expected generalization risk, such as the in-sample\n",
      "risk, AIC, and BIC, as in Section 2.5, and minimize this to determine a good set of +35\n",
      "explanatory variables.\n",
      "5. All of the above approaches do not assume any distributional properties of the error\n",
      "termsf\"igin the linear model, other than that they are independent with expectation\n",
      "0 and variance \u001b2. If, however, they are assumed to have a normal (Gaussian) distri-\n",
      "bution, (that is,f\"ig\u0018iidN(0;\u001b2)), then the inclusion and exclusion of variables canChapter 5. Regression 173\n",
      "be decided by means of hypotheses tests . This is the classical approach to model\n",
      "selection, and will be discussed in Section 5.4. As a consequence of the central limit\n",
      "theorem, one can use the same approach when the error terms are not necessarily\n",
      "normal, provided their variance is ﬁnite and the sample size nis large.\n",
      "6. Finally, when using a Bayesian approach, comparison of two models can be achieved\n",
      "by computing their so-called Bayes factor (see Section 2.9).\n",
      "All of the above strategies can be thought of as speciﬁcations of a simple rule formu-\n",
      "lated by William of Occam, which can be interpreted as:\n",
      "When presented with competing models, choose the simplest one that explains\n",
      "the data.\n",
      "This age-old principle, known as Occam’s razor Occam ’s razor , is mirrored in a famous quote of Einstein:\n",
      "Everything should be made as simple as possible, but not simpler.\n",
      "In linear regression, the number of parameters or predictors is usually a reasonable measure\n",
      "of the simplicity of the model.\n",
      "5.3.3 Cross-Validation and Predictive Residual Sum of Squares\n",
      "We start by considering the n-fold cross-validation, also called leave-one-out cross-\n",
      "validation leave -one-out\n",
      "cross -validation, for the linear model (5.8). We partition the data into ndata sets, leaving out\n",
      "precisely one observation per data set, which we then predict based on the n\u00001 remaining\n",
      "observations; see Section 2.5.2 for the general case. Let by\u0000idenote the prediction for the + 37\n",
      "i-th observation using all the data except yi. The error in the prediction, yi\u0000by\u0000i, is called a\n",
      "predicted residual predicted\n",
      "residual— in contrast to an ordinary residual, ei=yi\u0000byi, which is the di \u000berence\n",
      "between an observation and its ﬁtted value byi=g\u001c(xi) obtained using the whole sample. In\n",
      "this way, we obtain the collection of predicted residuals fyi\u0000by\u0000ign\n",
      "i=1and summarize them\n",
      "through the predicted residual sum of squares (PRESS PRESS ):\n",
      "PRESS =nX\n",
      "i=1(yi\u0000by\u0000i)2:\n",
      "Dividing the PRESS by ngives an estimate of the expected generalization risk.\n",
      "In general, computing the PRESS is computationally intensive as it involves training\n",
      "and predicting nseparate times. For linear models, however, the predicted residuals can be +171\n",
      "calculated quickly using only the ordinary residuals and the projection matrix P=XX+\n",
      "onto the linear space spanned by the columns of the model matrix X(see (2.13)). The i-th + 28\n",
      "diagonal element Piiof the projection matrix is called the i-thleverage leverage , and it can be shown\n",
      "that 06Pii61 (see Exercise 10).174 5.3. Analysis via Linear Models\n",
      "Theorem 5.1: PRESS for Linear Models\n",
      "Consider the linear model (5.8), where the n\u0002pmodel matrix Xis of full rank. Given\n",
      "an outcome y=[y1;:::; yn]>ofY, the ﬁtted values can be obtained as by=Py;where\n",
      "P=XX+=X(X>X)\u00001X>is the projection matrix. If the leverage value pi:=Pii,1\n",
      "for all i=1;:::; n, then the predicted residual sum of squares can be written as\n",
      "PRESS =nX\n",
      "i=1 ei\n",
      "1\u0000pi!2\n",
      ";\n",
      "where ei=yi\u0000byi=yi\u0000(Xb\f)iis the i-th residual.\n",
      "Proof: It su\u000eces to show that the i-th predicted residual can be written as yi\u0000by\u0000i=\n",
      "ei=(1\u0000pi). Let X\u0000idenote the model matrix Xwith the i-th row, x>\n",
      "i, removed, and deﬁne\n",
      "y\u0000isimilarly. Then, the least-squares estimate for \fusing all but the i-th observation is\n",
      "b\f\u0000i=(X>\n",
      "\u0000iX\u0000i)\u00001X>\n",
      "\u0000iy\u0000i. Writing X>X=X>\n",
      "\u0000iX\u0000i+xix>\n",
      "i, we have by the Sherman–Morrison\n",
      "formula +373\n",
      "(X>\n",
      "\u0000iX\u0000i)\u00001=(X>X)\u00001+(X>X)\u00001xix>\n",
      "i(X>X)\u00001\n",
      "1\u0000x>\n",
      "i(X>X)\u00001xi;\n",
      "where x>\n",
      "i(X>X)\u00001xi=pi<1. Also, X>\n",
      "\u0000iy\u0000i=X>y\u0000xiyi. Combining all these identities,\n",
      "we have\n",
      "b\f\u0000i=(X>\n",
      "\u0000iX\u0000i)\u00001X>\n",
      "\u0000iy\u0000i\n",
      "= \n",
      "(X>X)\u00001+(X>X)\u00001xix>\n",
      "i(X>X)\u00001\n",
      "1\u0000pi!\n",
      "(X>y\u0000xiyi)\n",
      "=b\f+(X>X)\u00001xix>\n",
      "ib\f\n",
      "1\u0000pi\u0000(X>X)\u00001xiyi\u0000(X>X)\u00001xipiyi\n",
      "1\u0000pi\n",
      "=b\f+(X>X)\u00001xix>\n",
      "ib\f\n",
      "1\u0000pi\u0000(X>X)\u00001xiyi\n",
      "1\u0000pi\n",
      "=b\f\u0000(X>X)\u00001xi(yi\u0000x>\n",
      "ib\f)\n",
      "1\u0000pi=b\f\u0000(X>X)\u00001xiei\n",
      "1\u0000pi:\n",
      "It follows that the predicted value for the i-th observation is given by\n",
      "by\u0000i=x>\n",
      "ib\f\u0000i=x>\n",
      "ib\f\u0000x>\n",
      "i(X>X)\u00001xiei\n",
      "1\u0000pi=byi\u0000piei\n",
      "1\u0000pi:\n",
      "Hence, yi\u0000by\u0000i=ei+piei=(1\u0000pi)=ei=(1\u0000pi). \u0003\n",
      "Example 5.2 (Polynomial Regression (cont.)) We return to Example 2.1, where we +26\n",
      "estimated the generalization risk for various polynomial prediction functions using inde-\n",
      "pendent validation data. Instead, let us estimate the expected generalization risk via cross-\n",
      "validation (thus using only the training set) and apply Theorem 5.1 to compute the PRESS.\n",
      "+174Chapter 5. Regression 175\n",
      "polyregpress.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "def generate_data(beta , sig, n):\n",
      "u = np.random.rand(n, 1)\n",
      "y = u ** np.arange(0, 4) @ beta.reshape(4,1) + (\n",
      "sig * np.random.randn(n, 1))\n",
      "return u, y\n",
      "np.random.seed(12)\n",
      "beta = np.array([[10.0, -140, 400, -250]]).T;\n",
      "sig=5; n = 10**2;\n",
      "u,y = generate_data(beta ,sig,n)\n",
      "X = np.ones((n, 1))\n",
      "K = 12 #maximum number of parameters\n",
      "press = np.zeros(K+1)\n",
      "for k in range(1,K):\n",
      "if k > 1:\n",
      "X = np.hstack((X, u**(k-1))) # add column to matrix\n",
      "P = X @ np.linalg.pinv(X) # projection matrix\n",
      "e = y - P @ y\n",
      "press[k] = np.sum((e/(1-np.diag(P).reshape(n,1)))**2)\n",
      "plt.plot(press[1:K]/n)\n",
      "The PRESS values divided by n=100 for the constant, linear, quadratic, cubic, and\n",
      "quartic order polynomial regression models are, respectively, 152 :487;56:249;51:606,\n",
      "30:999, and 31 :634. Hence, the cubic polynomial regression model has the lowest PRESS,\n",
      "indicating that it has the best predictive performance.\n",
      "5.3.4 In-Sample Risk and Akaike Information Criterion\n",
      "In Section 2.5.1 we introduced the in-sample risk as a measure for the accuracy of the + 35\n",
      "prediction function. To recapitulate, given a ﬁxed data set \u001cwith associated response vector\n",
      "yandn\u0002pmatrix of explanatory variables X, the in-sample risk of a prediction function\n",
      "gis deﬁned as\n",
      "`in(g) :=EXLoss( Y;g(X)); (5.11)\n",
      "whereEXsigniﬁes that the expectation is taken under a di \u000berent probability model, in\n",
      "which Xtakes the values x1;:::; xnwith equal probability, and given X=xithe random\n",
      "variable Yis drawn from the conditional pdf f(yjxi). The di \u000berence between the in-sample\n",
      "risk and the training loss is called the optimism . For the squared-error loss, Theorem 2.2 ex- + 36\n",
      "presses the expected optimism of a learner gTas two times the average covariance between\n",
      "the predicted values and the responses.\n",
      "If the conditional variance of the error Y\u0000g\u0003(X) given X=xdoes not depend on x,\n",
      "then the expected in-sample risk of a learner g\u001c, averaged over all training sets, has a simple\n",
      "expression:176 5.3. Analysis via Linear Models\n",
      "Theorem 5.2: Expected In-Sample Risk for Linear Models\n",
      "LetXbe the model matrix for a linear model, of dimension n\u0002p. IfVar[Y\u0000\n",
      "g\u0003(X)jX=x]=:v2does not depend on x, then the expected in-sample risk (with\n",
      "respect to the squared-error loss) for a random learner gTis given by\n",
      "EX`in(gT)=EX`T(gT)+2`\u0003p\n",
      "n; (5.12)\n",
      "where`\u0003is the irreducible risk.\n",
      "Proof: The expected optimism is, by deﬁnition, EX[`in(gT)\u0000`T(gT)] which, for the\n",
      "squared-error loss, is equal to 2 `\u0003p=n, using exactly the same reasoning as in Example 2.3.\n",
      "Note that here `\u0003=v2. \u0003\n",
      "Equation (5.12) is the basis of the following model comparison heuristic: Estimate the\n",
      "irreducible risk `\u0003=v2viabv2, using a model with relatively high complexity. Then choose\n",
      "the linear model with the lowest value of\n",
      "ky\u0000Xb\fk2+2bv2p: (5.13)\n",
      "We can also use the Akaike information criterion (AIC) as a heuristic for model com-\n",
      "parison. We discussed the AIC in the unsupervised learning setting in Section 4.2, but the +122\n",
      "arguments used there can also be applied to the supervised case, under the in-sample model\n",
      "for the data. In particular, let Z=(X;Y). We wish to predict the joint density\n",
      "f(z)=f(x;y) :=1\n",
      "nnX\n",
      "i=11fx=xigf(yjxi);\n",
      "using a prediction function g(zj\u0012) from a familyG:=fg(zj\u0012);\u00122Rqg, where\n",
      "g(zj\u0012)=g(x;yj\u0012) :=1\n",
      "nnX\n",
      "i=11fx=xiggi(yj\u0012):\n",
      "Note that qis the number of parameters (typically larger than pfor a linear model with a\n",
      "n\u0002pdesign matrix).\n",
      "Following Section 4.2, the in-sample cross-entropy risk in this case is\n",
      "r(\u0012) :=\u0000EXlng(Zj\u0012);\n",
      "and to approximate the optimal parameter \u0012\u0003we minimize the corresponding training loss\n",
      "r\u001cn(\u0012) :=\u00001\n",
      "nnX\n",
      "j=1lng(zjj\u0012):\n",
      "The optimal parameter b\u0012nfor the training loss is thus found by minimizing\n",
      "\u00001\n",
      "nnX\n",
      "j=1\u0010\n",
      "\u0000lnn+lngj(yjj\u0012)\u0011\n",
      ":Chapter 5. Regression 177\n",
      "That is, it is the maximum likelihood estimate of \u0012:\n",
      "b\u0012n=argmax\n",
      "\u0012nX\n",
      "i=1lngi(yij\u0012):\n",
      "Under the assumption that f=g(\u0001j\u0012\u0003) for some parameter \u0012\u0003, we have from Theorem 4.1\n",
      "that the estimated in-sample generalization risk can be approximated as +125\n",
      "EXr(b\u0012n)\u0019rTn(b\u0012n)+q\n",
      "n=lnn\u00001\n",
      "nnX\n",
      "j=1lngj(yjjb\u0012n)+q\n",
      "n:\n",
      "This leads to the heuristic of selecting the learner g(\u0001jb\u0012n) with the smallest value of the\n",
      "AIC:\n",
      "\u00002nX\n",
      "i=1lngi(yijb\u0012n)+2q: (5.14)\n",
      "Example 5.3 (Normal Linear Model) For the normal linear model Y\u0018N(x>\f;\u001b2)\n",
      "(see (2.34)), with a p-dimensional vector \f, we have + 46\n",
      "gi(yij\f;\u001b2\n",
      "|{z}\n",
      "=\u0012)=1p\n",
      "2\u0019\u001b2exp \n",
      "\u00001\n",
      "2(yi\u0000x>\n",
      "i\f)2\n",
      "\u001b2!\n",
      ";i=1;:::; n;\n",
      "so that the AIC is\n",
      "nln(2\u0019)+nlnb\u001b2+ky\u0000Xb\fk2\n",
      "b\u001b2+2q; (5.15)\n",
      "where ( b\f;b\u001b2) is the maximum likelihood estimate and q=p+1 is the number of parameters\n",
      "(including\u001b2). For model comparison we may remove the nln(2\u0019) term if all the models\n",
      "are normal linear models.\n",
      "Certain software packages report the AIC without the nlnb\u001b2term in (5.15). This\n",
      "may lead to sub-optimal model selection if normal models are compared with non-\n",
      "normal ones.\n",
      "5.3.5 Categorical Features\n",
      "Suppose that, as described in Chapter 1, the data is given in the form of a spreadsheet or\n",
      "data frame with nrows and p+1 columns, where the ﬁrst element of row iis the response\n",
      "variable yi, and the remaining pelements form the vector of explanatory variables x>\n",
      "i.\n",
      "When all the explanatory variables (features, predictors) are quantitative , then the model\n",
      "matrix Xcan be directly read o \u000bfrom the data frame as the n\u0002pmatrix with rows x>\n",
      "i;i=\n",
      "1;:::; n.\n",
      "However, when some explanatory variables are qualitative (categorical), such a one-to-\n",
      "one correspondence between data frame and model matrix no longer holds. The solution is\n",
      "to include indicator ordummy variables.178 5.3. Analysis via Linear Models\n",
      "Linear models with continuous responses and categorical explanatory variables often\n",
      "arise in factorial experiments . These are controlled statistical experiments in which thefactorial\n",
      "experiments aim is to assess how a response variable is a \u000bected by one or more factors tested at several\n",
      "factors levels . A typical example is an agricultural experiment where one wishes to investigate\n",
      "levelshow the yield of a food crop depends on factors such as location, pesticide, and fertilizer.\n",
      "Example 5.4 (Crop Yield) The data in Table 5.1 lists the yield of a food crop for four\n",
      "di\u000berent crop treatments (e.g., strengths of fertilizer) on four di \u000berent blocks (plots).\n",
      "Table 5.1: Crop yield for di \u000berent treatments and blocks.\n",
      "Treatment\n",
      "Block 1 2 3 4\n",
      "1 9.2988 9.4978 9.7604 10.1025\n",
      "2 8.2111 8.3387 8.5018 8.1942\n",
      "3 9.0688 9.1284 9.3484 9.5086\n",
      "4 8.2552 7.8999 8.4859 8.9485\n",
      "The corresponding data frame, given in Table 5.2, has 16 rows and 3 columns: one\n",
      "column for the crop yield (the response variable), one column for the Treatment, with\n",
      "levels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,\n",
      "2, 3, and 4 have no quantitative meaning (it does not make sense to take their average, for\n",
      "example) — they merely identify the category of the treatment or block.\n",
      "Table 5.2: Crop yield data organized as a data frame in standard format.\n",
      "Yield Treatment Block\n",
      "9.2988 1 1\n",
      "8.2111 1 2\n",
      "9.0688 1 3\n",
      "8.2552 1 4\n",
      "9.4978 2 1\n",
      "8.3387 2 2\n",
      ":::::::::\n",
      "9.5086 4 3\n",
      "8.9485 4 4\n",
      "In general, suppose there are rfactor (categorical) variables u1;:::; ur, where the j-\n",
      "th factor has pjmutually exclusive levels, denoted by 1 ;:::; pj. In order to include these\n",
      "categorical variables in a linear model, a common approach is to introduce an indicator\n",
      "feature indicator\n",
      "featurexjk=1fuj=kgfor each factor jat level k. Thus, xjk=1 if the value of factor j\n",
      "iskand 0 otherwise. SinceP\n",
      "k1fuj=kg=1, it su \u000eces to consider only pj\u00001 of these\n",
      "indicator features for each factor j(this prevents the model matrix from being rank deﬁ-\n",
      "cient). For a single response Y, the feature vector x>is thus a row vector of binary variablesChapter 5. Regression 179\n",
      "that indicates which levels were observed for each factor. The model assumption is that Y\n",
      "depends in a linear way on the indicator features, apart from an error term. That is,\n",
      "Y=\f0+rX\n",
      "j=1pjX\n",
      "k=2\fjk1fuj=kg|     {z     }\n",
      "xjk+\";\n",
      "where we have omitted one indicator feature (corresponding to level 1) for each factor\n",
      "j. For independent responses Y1;:::; Yn, where each Yicorresponds to the factor values\n",
      "ui1;:::; uir, letxi jk=1fui j=kg. Then, the linear model for the data becomes\n",
      "Yi=\f0+rX\n",
      "j=1pjX\n",
      "k=2\fjkxi jk+\"i; (5.16)\n",
      "where thef\"igare independent with expectation 0 and some variance \u001b2. By gathering the\n",
      "\f0andf\fjkginto a vector \f, and thefxi jkginto a matrix X, we have again a linear model of\n",
      "the form (5.8). The model matrix Xhasnrows and 1 +Pr\n",
      "j=1(pj\u00001) columns. Using the\n",
      "above convention that the \fj1parameters are subsumed in the parameter \f0(correspond-\n",
      "ing to the “constant” feature), we can interpret \f0as a baseline response when using the\n",
      "explanatory vector x>for which xj1=1 for all factors j=1;:::; r. The other parameters\n",
      "f\fjkgcan be viewed as incremental e \u000bects incremental\n",
      "effectsrelative to this baseline e \u000bect. For example, \f12\n",
      "describes by how much the response is expected to change if level 2 is used instead of level\n",
      "1 for factor 1.\n",
      "Example 5.5 (Crop Yield (cont.)) In Example 5.4, the linear model (5.16) has eight\n",
      "parameters: \f0;\f12;\f13;\f14;\f22;\f23;\f24, and\u001b2. The model matrix Xdepends on how\n",
      "the crop yields are organized in a vector yand on the ordering of the factors. Let\n",
      "us order ycolumn-wise from Table 5.1, as in y=[9:2988;8:2111;9:0688;8:2552;\n",
      "9:4978;:::; 8:9485]>, and let Treatment be Factor 1 and Block be Factor 2. Then we can\n",
      "write (5.16) as\n",
      "Y=2666666666666641 0 0 0 C\n",
      "1 1 0 0 C\n",
      "1 0 1 0 C\n",
      "1 0 0 1 C377777777777775\n",
      "|               {z               }\n",
      "X2666666666666666666666666666664\f0\n",
      "\f12\n",
      "\f13\n",
      "\f14\n",
      "\f22\n",
      "\f23\n",
      "\f243777777777777777777777777777775\n",
      "|{z}\n",
      "\f+\";where C=2666666666666640 0 0\n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1377777777777775;\n",
      "and with 1=[1;1;1;1]>and0=[0;0;0;0]>. Estimation of \fand\u001b2, model selection,\n",
      "and prediction can now be carried out in the usual manner for linear models.\n",
      "In the context of factorial experiments, the model matrix is often called the design\n",
      "matrix design matrix , as it speciﬁes the design of the experiment; e.g., how many replications are taken\n",
      "for each combination of factor levels. The model (5.16) can be extended by adding products\n",
      "of indicator variables as new features. Such features are called interaction interaction terms.180 5.3. Analysis via Linear Models\n",
      "5.3.6 Nested Models\n",
      "LetXbe a n\u0002pmodel matrix of the form X=[X1;X2], where X1andX2are model\n",
      "matrices of dimension n\u0002kandn\u0002(p\u0000k), respectively. The linear models Y=X1\f1+\"\n",
      "andY=X2\f2+\"are said to be nested models nested within the linear model Y=X\f+\". This simply\n",
      "means that certain features in Xare ignored in each of the ﬁrst two models. Note that \f,\f1,\n",
      "and\f2are parameter vectors of dimension p,k, and p\u0000k, respectively. In what follows,\n",
      "we assume that n>pand that all model matrices are full-rank.\n",
      "Suppose we wish to assess whether to use the full model matrix Xor the reduced model\n",
      "matrix X1. Letb\fbe the estimate of \funder the full model (that is, obtained via (5.9)), and\n",
      "letb\f1denote the estimate of \f1for the reduced model. Let Y(2)=Xb\fbe the projection of Y\n",
      "onto the space Span( X) spanned by the columns of X; and let Y(1)=X1b\f1be the projection\n",
      "ofYonto the space Span( X1) spanned by the columns of X1only; see Figure 5.3. In order\n",
      "to decide whether the features in X2are needed, we may compare the estimated error terms\n",
      "of the two models, as calculated by (5.10); that is, by the residual sum of squares divided\n",
      "by the number of observations n. If the outcome of this comparison is that there is little\n",
      "di\u000berence between the model error for the full and reduced model, then it is appropriate to\n",
      "adopt the reduced model, as it has fewer parameters than the full model, while explaining\n",
      "the data just as well. The comparison is thus between the squared norms kY\u0000Y(2)k2and\n",
      "kY\u0000Y(1)k2. Because of the nested nature of the linear models, Span( X1) is a subspace of\n",
      "Span( X) and, consequently, the orthogonal projection of Y(2)onto Span( X1) is the same\n",
      "as the orthogonal projection of Yonto Span( X1); that is, Y(1). By Pythagoras’ theorem, we\n",
      "thus have the decomposition kY(2)\u0000Y(1)k2+kY\u0000Y(2)k2=kY\u0000Y(1)k2. This is also illustrated\n",
      "in Figure 5.3.\n",
      "Y\n",
      "Y−Y(1)Y−Y(2)\n",
      "Y(2)O\n",
      "Span( X)\n",
      "Span( X1)Y(2)−Y(1)\n",
      "Y(1)\n",
      "Figure 5.3: The residual sum of squares for the full model corresponds to kY\u0000Y(2)k2and for\n",
      "the reduced model it is kY\u0000Y(1)k2. By Pythagoras’s theorem, the di \u000berence iskY(2)\u0000Y(1)k2.\n",
      "The above decomposition can be generalized to more than two model matrices. Sup-\n",
      "pose that the model matrix can be decomposed into dsubmatrices: X=[X1;X2;:::; Xd],\n",
      "where the matrix Xihaspicolumns and nrows, i=1;:::; d. Thus, the number of columns2\n",
      "2As always, we assume the columns are linearly independent.Chapter 5. Regression 181\n",
      "in the full model matrix is p=p1+\u0001\u0001\u0001+pd. This creates an increasing sequence of “nested”\n",
      "model matrices: X1;[X1;X2];:::; [X1;X2;:::; Xd], from (say) the baseline normal model\n",
      "matrix X1=1to the full model matrix X. Think of each model matrix corresponding to\n",
      "speciﬁc variables in the model.\n",
      "We follow a similar projection procedure as in Figure 5.3: First project Yonto Span( X)\n",
      "to yield the vector Y(d), then project Y(d)onto Span([ X1;:::; Xd\u00001]) to obtain Y(d\u00001), and so\n",
      "on, until Y(2)is projected onto Span( X1) to yield Y(1)=Y1(in the case that X1=1).\n",
      "By applying Pythagoras’ theorem, the total sum of squares can be decomposed as\n",
      "kY\u0000Y(1)k2\n",
      "|       {z       }\n",
      "df=n\u0000p1=kY\u0000Y(d)k2\n",
      "|       {z       }\n",
      "df=n\u0000p+kY(d)\u0000Y(d\u00001)k2\n",
      "|            {z            }\n",
      "df=pd+\u0001\u0001\u0001+kY(2)\u0000Y(1)k2\n",
      "|          {z          }\n",
      "df=p2: (5.17)\n",
      "Software packages typically report the sums of squares as well as the corresponding de-\n",
      "grees of freedom (df): n\u0000p;pd;:::; p2.degrees of\n",
      "freedom\n",
      "5.3.7 Coefﬁcient of Determination\n",
      "To assess how a linear model Y=X\f+\"compares to the default model Y=\f01+\", we\n",
      "can compare the variance of the original data, estimated viaP\n",
      "i(Yi\u0000Y)2=n=kY\u0000Y1k2=n,\n",
      "with the variance of the ﬁtted data; estimated viaP\n",
      "i(bYi\u0000Y)2=n=kbY\u0000Y1k2=n, where\n",
      "bY=Xb\f. The sumP\n",
      "i(Yi\u0000Y)2=n=kY\u0000Y1k2is sometimes called the total sum of squares total sum of\n",
      "squares(TSS), and the quantity\n",
      "R2=kbY\u0000Y1k2\n",
      "kY\u0000Y1k2(5.18)\n",
      "is called the coe\u000ecient of determination coefficient of\n",
      "determinationof the linear model. In the notation of Figure 5.3,\n",
      "bY=Y(2)andY1=Y(1), so that\n",
      "R2=kY(2)\u0000Y(1)k2\n",
      "kY\u0000Y(1)k2=kY\u0000Y(1)k2\u0000kY\u0000Y(2)k2\n",
      "kY\u0000Y(1)k2=TSS\u0000RSS\n",
      "TSS:\n",
      "Note that R2lies between 0 and 1. An R2value close to 1 indicates that a large propor-\n",
      "tion of the variance in the data has been explained by the model.\n",
      "Many software packages also give the adjusted coe \u000ecient of determination adjusted\n",
      "coefficient of\n",
      "determination, or simply\n",
      "the adjusted R2, deﬁned by\n",
      "R2\n",
      "adjusted =1\u0000(1\u0000R2)n\u00001\n",
      "n\u0000p:\n",
      "The regular R2is always non-decreasing in the number of parameters (see Exercise 15),\n",
      "but this may not indicate better predictive power. The adjusted R2compensates for this\n",
      "increase by decreasing the regular R2as the number of variables increases. This heuristic\n",
      "adjustment can make it easier to compare the quality of two competing models.182 5.4. Inference for Normal Linear Models\n",
      "5.4 Inference for Normal Linear Models\n",
      "So far we have not assumed any distribution for the random vector of errors \"=\n",
      "[\"1;:::;\" n]>in a linear model Y=X\f+\". When the error terms f\"igare assumed to be\n",
      "normally distributed (that is, f\"ig\u0018iidN(0;\u001b2)), whole new avenues open up for inference\n",
      "on linear models. In Section 2.8 we already saw that for such normal linear models , estim- +46\n",
      "ation of\fand\u001b2can be carried out via maximum likelihood methods, yielding the same\n",
      "estimators from (5.9) and (5.10).\n",
      "The following theorem lists the properties of these estimators. In particular, it shows\n",
      "thatb\fandc\u001b2n=(n\u0000p) are independent and unbiased estimators of \fand\u001b2, respectively.\n",
      "Theorem 5.3: Properties of the Estimators for a Normal Linear Model\n",
      "Consider the linear model Y=X\f+\", with\"\u0018N(0;\u001b2In), where\fis a p-\n",
      "dimensional vector of parameters and \u001b2a dispersion parameter. The following res-\n",
      "ults hold.\n",
      "1. The maximum likelihood estimators b\fandc\u001b2are independent.\n",
      "2.b\f\u0018N(\f; \u001b2(X>X)+).\n",
      "3.nc\u001b2=\u001b2\u0018\u001f2\n",
      "n\u0000p, where p=rank( X).\n",
      "Proof: Using the pseudo-inverse (Deﬁnition A.2), we can write the random vector b\fas +362\n",
      "X+Y, which is a linear transformation of a normal random vector. Consequently, b\fhas a\n",
      "multivariate normal distribution; see Theorem C.6. The mean vector and covariance matrix +437\n",
      "follow from the same theorem:\n",
      "Eb\f=X+EY=X+X\f=\f\n",
      "and\n",
      "Cov(b\f)=X+\u001b2In(X+)>=\u001b2(X>X)+:\n",
      "To show that b\fandc\u001b2are independent, deﬁne Y(2)=Xb\f. Note that Y=\u001bhas aN(\u0016;In)\n",
      "distribution, with expectation vector \u0016=X\f=\u001b. A direct application of Theorem C.10\n",
      "now shows that ( Y\u0000Y(2))=\u001bis independent of Y(2)=\u001b. Since b\f=X+Xb\f=X+Y(2)and +440\n",
      "c\u001b2=kY\u0000Y(2)k2=n, it follows that c\u001b2is independent of b\f. Finally, by the same theorem,\n",
      "the random variable kY\u0000Y(2)k2=\u001b2has a\u001f2\n",
      "n\u0000pdistribution, as Y(2)has the same expectation\n",
      "vector as Y. \u0003\n",
      "As a corollary, we see that each estimator b\fiof\fihas a normal distribution with expect-\n",
      "ation\fiand variance \u001b2u>\n",
      "iX+(X+)>ui=\u001b2ku>\n",
      "iX+k2, where ui=[0;:::; 0;1;0;:::; 0]>is\n",
      "thei-th unit vector; in other words, the variance is \u001b2[(X>X)+]ii.\n",
      "It is of interest to test whether certain regression parameters \fiare 0 or not, since if\n",
      "\fi=0, the i-th explanatory variable has no direct e \u000bect on the expected response and so\n",
      "could be removed from the model. A standard procedure is to conduct a hypothesis test\n",
      "(see Section C.14 for a review of hypothesis testing) to test the null hypothesis H0:\fi=0 +460Chapter 5. Regression 183\n",
      "against the alternative H1:\fi,0, using the test statistic\n",
      "T=b\fi=ku>\n",
      "iX+k\n",
      "p\n",
      "RSE; (5.19)\n",
      "where RSE is the residual squared error; that is RSE =RSS=(n\u0000p). This test statistic has\n",
      "atn\u0000pdistribution under H0. To see this, write T=Z=p\n",
      "V=(n\u0000p), with\n",
      "Z=b\fi\n",
      "\u001bku>\n",
      "iX+kand V=nc\u001b2=\u001b2:\n",
      "Then, by Theorem 5.3, Z\u0018N(0;1) under H0,V\u0018\u001f2\n",
      "n\u0000p, and ZandVare independent. The\n",
      "result now follows directly from Corollary C.1. +441\n",
      "5.4.1 Comparing Two Normal Linear Models\n",
      "Suppose we have the following normal linear model for data Y=[Y1;:::; Yn]>:\n",
      "Y=X1\f1+X2\f2|          {z          }\n",
      "X\f+\";\"\u0018N(0;\u001b2In); (5.20)\n",
      "where\f1and\f2are unknown vectors of dimension kand p\u0000k, respectively; and X1\n",
      "andX2are full-rank model matrices of dimensions n\u0002kandn\u0002(p\u0000k), respectively.\n",
      "Above we implicitly deﬁned X=[X1;X2] and\f>=[\f>\n",
      "1;\f>\n",
      "2]. Suppose we wish to test the\n",
      "hypothesis H0:\f2=0against H1:\f2,0. Following Section 5.3.6, the idea is to compare\n",
      "the residual sum of squares for both models, expressed as kY\u0000Y(2)k2andkY\u0000Y(1)k2. Using\n",
      "Pythagoras’ theorem we saw that kY\u0000Y(2)k2\u0000kY\u0000Y(1)k2=kY(2)\u0000Y(1)k2, and so it makes\n",
      "sense to base the decision whether to retain or reject H0on the basis of the quotient of\n",
      "kY(2)\u0000Y(1)k2andkY\u0000Y(2)k2. This leads to the following test statistics.\n",
      "Theorem 5.4: Test Statistic for Comparing Two Normal Linear Models\n",
      "For the model (5.20), let Y(2)andY(1)be the projections of Yonto the space spanned\n",
      "by the pcolumns of Xand the kcolumns of X1, respectively. Then under H0:\f2=0\n",
      "the test statistic\n",
      "T=kY(2)\u0000Y(1)k2=(p\u0000k)\n",
      "kY\u0000Y(2)k2=(n\u0000p)(5.21)\n",
      "has an F(p\u0000k;n\u0000p) distribution.\n",
      "Proof: Deﬁne X:=Y=\u001bwith expectation \u0016:=X\f=\u001b, and Xj:=Y(j)=\u001bwith expectation\n",
      "\u0016j,j=k;p. Note that\u0016p=\u0016and, under H0,\u0016k=\u0016p. We can directly apply Theorem C.10\n",
      "to ﬁnd thatkY\u0000Y(2)k2=\u001b2=kX\u0000Xpk2\u0018\u001f2\n",
      "n\u0000pand, under H0,kY(2)\u0000Y(1)k2=\u001b2=kXp\u0000 +440\n",
      "Xkk2\u0018\u001f2\n",
      "p\u0000k. Moreover, these random variables are independent of each other. The proof\n",
      "is completed by applying Theorem C.11. \u0003184 5.4. Inference for Normal Linear Models\n",
      "Note that H0is rejected for large values of T. The testing procedure thus proceeds as\n",
      "follows:\n",
      "1. Compute the outcome, tsay, of the test statistic Tin (5.21).\n",
      "2. Evaluate the P-value P(T>t), with T\u0018F(p\u0000k;n\u0000p).\n",
      "3. Reject H0if this P-value is too small, say less than 0.05.\n",
      "For nested models [ X1;X2;:::; Xi],i=1;2;:::; d, as in Section 5.3.6, the Ftest statistic\n",
      "in Theorem 5.4 can now be used to test whether certain Xiare needed or not. In particular, +183\n",
      "software packages will report the outcomes of\n",
      "Fi=kY(i)\u0000Y(i\u00001)k2=pi\n",
      "kY\u0000Y(d)k2=(n\u0000p); (5.22)\n",
      "in the order i=2;3;:::; d. Under the null hypothesis that Y(i)andY(i\u00001)have the same ex-\n",
      "pectation (that is, adding XitoXi\u00001has no additional e \u000bect on reducing the approximation\n",
      "error), the test statistic Fihas an F(pi;n\u0000p) distribution, and the corresponding P-values\n",
      "quantify the strength of the decision to include an additional variable in the model or not.\n",
      "This procedure is called analysis of variance (ANOV A).analysis of\n",
      "variance\n",
      "Note that the output of an ANOV A table depends on the order in which the variables\n",
      "are considered.\n",
      "Example 5.6 (Crop Yield (cont.)) We continue Examples 5.4 and 5.5. Decompose the\n",
      "linear model as\n",
      "Y=2666666666666641\n",
      "1\n",
      "1\n",
      "1377777777777775\n",
      "|{z}\n",
      "X1\f0|{z}\n",
      "\f1+2666666666666640 0 0\n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1377777777777775\n",
      "|     {z     }\n",
      "X22666666664\f12\n",
      "\f13\n",
      "\f143777777775\n",
      "|{z}\n",
      "\f2+266666666666664C\n",
      "C\n",
      "C\n",
      "C377777777777775\n",
      "|{z}\n",
      "X32666666664\f22\n",
      "\f23\n",
      "\f243777777775\n",
      "|{z}\n",
      "\f3+\":\n",
      "Is the crop yield dependent on treatment levels as well as blocks? We ﬁrst test whether we\n",
      "can remove Block as a factor in the model against it playing a signiﬁcant role in explain-\n",
      "ing the crop yields. Speciﬁcally, we test \f3=0versus\f3,0using Theorem 5.4. Now\n",
      "the vector Y(2)is the projection of Yonto the ( p=7)-dimensional space spanned by the\n",
      "columns of X=[X1;X2;X3]; and Y(1)is the projection of Yonto the ( k=4)-dimensional\n",
      "space spanned by the columns of X12:=[X1;X2]. The test statistic, T12say, under H0has\n",
      "anF(3;9) distribution.\n",
      "The Python code below calculates the outcome of the test statistic T12and the corres-\n",
      "ponding P-value. We ﬁnd t12=34:9998, which gives a P-value 2 :73\u000210\u00005. This shows\n",
      "that the block e \u000bects are extremely important for explaining the data.\n",
      "Using the extended model (including the block e \u000bects), we can test whether \f2=0or\n",
      "not; that is, whether the treatments have a signiﬁcant e \u000bect on the crop yield in the presence\n",
      "of the Block factor. This is done in the last six lines of the code below. The outcome ofChapter 5. Regression 185\n",
      "the test statistic is 4 :4878, with a P-value of 0 :0346. By including the block e \u000bects, we\n",
      "e\u000bectively reduce the uncertainty in the model and are able to more accurately assess the\n",
      "e\u000bects of the treatments, to conclude that the treatment seems to have an e \u000bect on the crop\n",
      "yield. A closer look at the data shows that within each block (row) the crop yield roughly\n",
      "increases with the treatment level.\n",
      "crop.py\n",
      "import numpy as np\n",
      "from scipy.stats import f\n",
      "from numpy.linalg import lstsq , norm\n",
      "yy = np.array([9.2988, 9.4978, 9.7604, 10.1025,\n",
      "8.2111, 8.3387, 8.5018, 8.1942,\n",
      "9.0688, 9.1284, 9.3484, 9.5086,\n",
      "8.2552, 7.8999, 8.4859, 8.9485]).reshape(4,4).T\n",
      "nrow , ncol = yy.shape[0], yy.shape[1]\n",
      "n = nrow * ncol\n",
      "y = yy.reshape(16,)\n",
      "X_1 = np.ones((n,1))\n",
      "KM = np.kron(np.eye(ncol),np.ones((nrow ,1)))\n",
      "KM[:,0]\n",
      "X_2 = KM[:,1:ncol]\n",
      "IM = np.eye(nrow)\n",
      "C = IM[:,1:nrow]\n",
      "X_3 = np.vstack((C, C))\n",
      "X_3 = np.vstack((X_3, C))\n",
      "X_3 = np.vstack((X_3, C))\n",
      "X = np.hstack((X_1,X_2))\n",
      "X = np.hstack((X,X_3))\n",
      "p = X.shape[1] #number of parameters in full model\n",
      "betahat = lstsq(X, y,rcond=None)[0] #estimate under the full model\n",
      "ym = X @ betahat\n",
      "X_12 = np.hstack((X_1, X_2)) #omitting the block effect\n",
      "k = X_12.shape[1] #number of parameters in reduced model\n",
      "betahat_12 = lstsq(X_12 , y,rcond=None)[0]\n",
      "y_12 = X_12 @ betahat_12\n",
      "T_12=(n-p)/(p-k)*(norm(y-y_12)**2 - norm(y-ym)**2)/norm(y-ym)**2\n",
      "pval_12 = 1 - f.cdf(T_12 ,p-k,n-p)\n",
      "X_13 = np.hstack((X_1, X_3)) #omitting the treatment effect\n",
      "k = X_13.shape[1] #number of parameters in reduced model\n",
      "betahat_13 = lstsq(X_13 , y,rcond=None)[0]\n",
      "y_13 = X_13 @ betahat_13\n",
      "T_13=(n-p)/(p-k)*(norm(y-y_13)**2 - norm(y-ym)**2)/norm(y-ym)**2\n",
      "pval_13 = 1 - f.cdf(T_13 ,p-k,n-p)186 5.4. Inference for Normal Linear Models\n",
      "5.4.2 Conﬁdence and Prediction Intervals\n",
      "As in all supervised learning settings, linear regression is most useful when we wish to\n",
      "predict how a new response variable will behave on the basis of a new explanatory vector\n",
      "x. For example, it may be di \u000ecult to measure the response variable, but by knowing the\n",
      "estimated regression line and the value for x, we will have a reasonably good idea what Y\n",
      "or the expected value of Yis going to be.\n",
      "Thus, consider a new xand let Y\u0018N(x>\f;\u001b2), with\fand\u001b2unknown. First we\n",
      "are going to look at the expected value of Y, that isEY=x>\f. Since\fis unknown, we\n",
      "do not know EYeither. However, we can estimate it via the estimator bY=x>b\f, where\n",
      "b\f\u0018N(\f; \u001b2(X>X)+), by Theorem 5.3. Being linear in the components of \f,bYtherefore\n",
      "has a normal distribution with expectation x>\fand variance \u001b2kx>X+k2. Let Z\u0018N(0;1)\n",
      "be the standardized version of bYandV=kY\u0000Xb\fk2=\u001b2\u0018\u001f2\n",
      "n\u0000p. Then the random variable\n",
      "T:=(x>b\f\u0000x>\f)=kx>X+k\n",
      "kY\u0000Xb\fk=p\n",
      "(n\u0000p)=Zp\n",
      "V=(n\u0000p)(5.23)\n",
      "has, by Corollary C.1, a tn\u0000pdistribution. After rearranging the identity P(jTj6tn\u0000p;1\u0000\u000b=2)= +441\n",
      "1\u0000\u000b, where tn\u0000p;1\u0000\u000b=2is the (1\u0000\u000b=2) quantile of the tn\u0000pdistribution, we arrive at the\n",
      "stochastic conﬁdence interval confidence\n",
      "interval\n",
      "x>b\f\u0006tn\u0000p;1\u0000\u000b=2p\n",
      "RSEkx>X+k; (5.24)\n",
      "where we have identiﬁed kY\u0000Xb\fk2=(n\u0000p) with RSE. This conﬁdence interval quantiﬁes\n",
      "the uncertainty in the learner (regression surface).\n",
      "Aprediction interval prediction\n",
      "intervalfor a new response Yis di\u000berent from a conﬁdence interval for\n",
      "EY. Here the idea is to construct an interval such that Ylies in this interval with a certain\n",
      "guaranteed probability. Note that now we have twosources of variation:\n",
      "1.Y\u0018N(x>\f;\u001b2) itself is a random variable.\n",
      "2. Estimating x>\fviabYbrings another source of variation.\n",
      "We can construct a (1 \u0000\u000b) prediction interval, by ﬁnding two random bounds such that\n",
      "the random variable Ylies between these bounds with probability 1 \u0000\u000b. We can reason as\n",
      "follows. Firstly, note that Y\u0018N(x>\f;\u001b2) andbY\u0018N(x>\f;\u001b2kx>X+k2) are independent. It\n",
      "follows that Y\u0000bYhas a normal distribution with expectation 0 and variance\n",
      "\u001b2(1+kx>X+k2): (5.25)\n",
      "Secondly, letting Z\u0018N(0;1) be the standardized version of Y\u0000bY, and repeating the\n",
      "steps used for the construction of the conﬁdence interval (5.24), we arrive at the prediction\n",
      "interval\n",
      "x>b\f\u0006tn\u0000p;1\u0000\u000b=2p\n",
      "RSEp\n",
      "1+kx>X+k2: (5.26)\n",
      "This prediction interval captures the uncertainty from an as-yet-unobserved response as\n",
      "well as the uncertainty in the parameters of the regression model itself.Chapter 5. Regression 187\n",
      "Example 5.7 (Conﬁdence Limits in Simple Linear Regression) The following pro-\n",
      "gram draws n=100 samples from a simple linear regression model with parameters\n",
      "\f=[6;13]>and\u001b=2, where the x-coordinates are evenly spaced on the interval [0 ;1].\n",
      "The parameters are estimated in the third block of the code. Estimates for \fand\u001bare\n",
      "[6:03;13:09]>andb\u001b=1:60, respectively. The program then proceeds by calculating the\n",
      "95% numeric conﬁdence and prediction intervals for various values of the explanatory\n",
      "variable. Figure 5.4 shows the results.\n",
      "confpred.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import t\n",
      "from numpy.linalg import inv, lstsq , norm\n",
      "np.random.seed(123)\n",
      "n = 100\n",
      "x = np.linspace(0.01,1,100).reshape(n,1)\n",
      "# parameters\n",
      "beta = np.array([6,13])\n",
      "sigma = 2\n",
      "Xmat = np.hstack((np.ones((n,1)), x)) #design matrix\n",
      "y = Xmat @ beta + sigma*np.random.randn(n)\n",
      "# solve the normal equations\n",
      "betahat = lstsq(Xmat , y,rcond=None)[0]\n",
      "# estimate for sigma\n",
      "sqMSE = norm(y - Xmat @ betahat)/np.sqrt(n-2)\n",
      "tquant = t.ppf(0.975,n-2) # 0.975 quantile\n",
      "ucl = np.zeros(n) #upper conf. limits\n",
      "lcl = np.zeros(n) #lower conf. limits\n",
      "upl = np.zeros(n)\n",
      "lpl = np.zeros(n)\n",
      "rl = np.zeros(n) # (true) regression line\n",
      "u = 0\n",
      "for i in range(n):\n",
      "u = u + 1/n;\n",
      "xvec = np.array([1,u])\n",
      "sqc = np.sqrt(xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\n",
      "sqp = np.sqrt(1 + xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\n",
      "rl[i] = xvec.T @ beta;\n",
      "ucl[i] = xvec.T @ betahat + tquant*sqMSE*sqc;\n",
      "lcl[i] = xvec.T @ betahat - tquant*sqMSE*sqc;\n",
      "upl[i] = xvec.T @ betahat + tquant*sqMSE*sqp;\n",
      "lpl[i] = xvec.T @ betahat - tquant*sqMSE*sqp;\n",
      "plt.plot(x,y, '.')\n",
      "plt.plot(x,rl, 'b')\n",
      "plt.plot(x,ucl, 'k:')\n",
      "plt.plot(x,lcl, 'k:')\n",
      "plt.plot(x,upl, 'r--')\n",
      "plt.plot(x,lpl, 'r--')188 5.5. Nonlinear Regression Models\n",
      "0.0\n",
      " 0.2\n",
      " 0.4\n",
      " 0.6\n",
      " 0.8\n",
      " 1.0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "Figure 5.4: The true regression line (blue, solid) and the upper and lower 95% prediction\n",
      "curves (red, dashed) and conﬁdence curves (dotted).\n",
      "5.5 Nonlinear Regression Models\n",
      "So far we have been mostly dealing with linear regression models, in which the predic-\n",
      "tion function is of the form g(xj\f)=x>\f. In this section we discuss some strategies for\n",
      "handling general prediction functions g(xj\f), where the functional form is known up to an\n",
      "unknown parameter vector \f. So the regression model becomes\n",
      "Yi=g(xij\f)+\"i;i=1;:::; n; (5.27)\n",
      "where\"1;:::;\" nare independent with expectation 0 and unknown variance \u001b2. The model\n",
      "can be further speciﬁed by assuming that the error terms have a normal distribution.\n",
      "Table 5.3 gives some common examples of nonlinear prediction functions for data tak-\n",
      "ing values in R.\n",
      "Table 5.3: Common nonlinear prediction functions for one-dimensional data.\n",
      "Name g(xj\f)\f\n",
      "Exponential aebxa;b\n",
      "Power law a xba;b\n",
      "Logistic (1 +ea+bx)\u00001a;b\n",
      "Weibull 1\u0000exp(\u0000xb=a)a;b\n",
      "PolynomialPp\u00001\n",
      "k=0\fkxkp,f\fkgp\u00001\n",
      "k=0\n",
      "The logistic and polynomial prediction functions in Table 5.3 can be readily gener-\n",
      "alized to higher dimensions. For example, for x2R2a general second-order polynomial\n",
      "prediction function is of the form\n",
      "g(xj\f)=\f0+\f1x1+\f2x2+\f11x2\n",
      "1+\f22x2\n",
      "2+\f12x1x2: (5.28)Chapter 5. Regression 189\n",
      "This function can be viewed as a second-order approximation to a general smooth predic-\n",
      "tion function g(x1;x2); see also Exercise 4. Polynomial regression models are also called\n",
      "response surface models. response\n",
      "surface modelThe generalization of the above logistic prediction to Rdis\n",
      "g(xj\f)=(1+e\u0000x>\f)\u00001: (5.29)\n",
      "This function will make its appearance in Section 5.7 and later on in Chapters 7 and 9.\n",
      "The ﬁrst strategy for performing regression with nonlinear prediction functions is to\n",
      "extend the feature space to obtain a simpler (ideally linear) prediction function in the ex-\n",
      "tended feature space. We already saw an application of this strategy in Example 2.1 for + 26\n",
      "the polynomial regression model, where the original feature uwas extended to the feature\n",
      "vector x=[1;u;u2;:::; up\u00001]>, yielding a linear prediction function. In a similar way, the\n",
      "right-hand side of the polynomial prediction function in (5.28) can be viewed as a linear\n",
      "function of the extended feature vector \u001e(x)=[1;x1;x2;x2\n",
      "1;x2\n",
      "2;x1x2]>. The function \u001eis\n",
      "called a feature map feature map .\n",
      "The second strategy is to transform the response variable yand possibly also the ex-\n",
      "planatory variable xsuch that the transformed variables ey,exare related in a simpler (ideally\n",
      "linear) way. For example, for the exponential prediction function y=ae\u0000bx, we have\n",
      "lny=lna\u0000bx, which is a linear relation between ln yand [1;x]>.\n",
      "Example 5.8 (Chlorine) Table 5.4 lists the free chlorine concentration (in mg per liter)\n",
      "in a swimming pool, recorded every 8 hours for 4 days. A simple chemistry-based model\n",
      "for the chlorine concentration yas a function of time tisy=ae\u0000b t, where ais the initial\n",
      "concentration and b>0 is the reaction rate.\n",
      "Table 5.4: Chlorine concentration (in mg /L) as a function of time (hours).\n",
      "Hours Concentration\n",
      "0 1.0056\n",
      "8 0.8497\n",
      "16 0.6682\n",
      "24 0.6056\n",
      "32 0.4735\n",
      "40 0.4745\n",
      "48 0.3563Hours Concentration\n",
      "56 0.3293\n",
      "64 0.2617\n",
      "72 0.2460\n",
      "80 0.1839\n",
      "88 0.1867\n",
      "96 0.1688\n",
      "The exponential relationship y=ae\u0000btsuggests that a log transformation of ywill result\n",
      "in alinear relationship between ln yand the feature vector [1 ;t]>. Thus, if for some given\n",
      "data ( t1;y1);:::; (tn;yn), we plot ( t1;lny1);:::; (tn;lnyn), these points should approximately\n",
      "lie on a straight line, and hence the simple linear regression model applies. The left panel of\n",
      "Figure 5.5 illustrates that the transformed data indeed lie approximately on a straight line.\n",
      "The estimated regression line is also drawn here. The intercept and slope are \f0=\u00000:0555\n",
      "and\f1=\u00000:0190 here. The original (non-transformed) data is shown in the right panel\n",
      "of Figure 5.5, along with the ﬁtted curve y=bae\u0000bbt, where ba=exp(b\f0)=0:9461 and\n",
      "bb=\u0000b\f1=0:0190.190 5.5. Nonlinear Regression Models\n",
      "0 50 100\n",
      "t-2-1.5-1-0.500.5log y\n",
      "0 50 100\n",
      "t00.511.5y\n",
      "Figure 5.5: The chlorine concentration seems to have an exponential decay.\n",
      "Recall that for a general regression problem the learner g\u001c(x) for a given training set \u001c\n",
      "is obtained by minimizing the training (squared-error) loss\n",
      "`\u001c(g(\u0001j\f))=1\n",
      "nnX\n",
      "i=1(yi\u0000g(xij\f))2: (5.30)\n",
      "The third strategy for regression with nonlinear prediction functions is to directly minimize\n",
      "(5.30) by any means possible, as illustrated in the next example.\n",
      "Example 5.9 (Hougen Function) In [7] the reaction rate yof a certain chemical reac-\n",
      "tion is posited to depend on three input variables: quantities of hydrogen x1, n-pentane x2,\n",
      "and isopentane x3. The functional relationship is given by the Hougen function:\n",
      "y=\f1x2\u0000x3=\f5\n",
      "1+\f2x1+\f3x2+\f4x3;\n",
      "where\f1;:::;\f 5are the unknown parameters. The objective is to estimate the model para-\n",
      "metersf\figfrom the data, as given in Table 5.5.\n",
      "Table 5.5: Data for the Hougen function.\n",
      "x1 x2 x3 y\n",
      "470 300 10 8.55\n",
      "285 80 10 3.79\n",
      "470 300 120 4.82\n",
      "470 80 120 0.02\n",
      "470 80 10 2.75\n",
      "100 190 10 14.39\n",
      "100 80 65 2.54x1 x2 x3 y\n",
      "470 190 65 4.35\n",
      "100 300 54 13.00\n",
      "100 300 120 8.50\n",
      "100 80 120 0.05\n",
      "285 300 10 11.32\n",
      "285 190 120 3.13\n",
      "The estimation is carried out via the least-squares method. The objective function to\n",
      "minimize is thus\n",
      "`\u001c(g(\u0001j\f))=1\n",
      "1313X\n",
      "i=1 \n",
      "yi\u0000\f1xi2\u0000xi3=\f5\n",
      "1+\f2xi1+\f3xi2+\f4xi3!2\n",
      "; (5.31)Chapter 5. Regression 191\n",
      "where thefyigandfxi jgare given in Table 5.5.\n",
      "This is a highly nonlinear optimization problem, for which standard nonlinear least- +416\n",
      "squares methods do not work well. Instead, one can use global optimization methods such\n",
      "as CE and SCO (see Sections 3.4.2 and 3.4.3). Using the CE method, we found the minimal +100\n",
      "value 0.02299 for the objective function, which is attained at\n",
      "b\f=[1:2526;0:0628;0:0400;0:1124;1:1914]>:\n",
      "5.6 Linear Models in Python\n",
      "In this section we describe how to deﬁne and analyze linear models using Python and the\n",
      "data science module statsmodels . We encourage the reader to regularly refer back to\n",
      "the theory in the preceding sections of this chapter, so as to avoid using Python merely\n",
      "as a black box without understanding the underlying principles. To run the code start by\n",
      "importing the following code snippet:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.formula.api import ols\n",
      "5.6.1 Modeling\n",
      "Although specifying a normal3linear model in Python is relatively easy, it requires some\n",
      "subtlety. The main thing to realize is that Python treats quantitative and qualitative (that\n",
      "is, categorical) explanatory variables di \u000berently. In statsmodels , ordinary least-squares\n",
      "linear models are speciﬁed via the function ols (short for ordinary least-squares). The\n",
      "main argument of this function is a formula of the form\n",
      "y\u0018x1+x2+\u0001\u0001\u0001+xd; (5.32)\n",
      "where yis the name of the response variable and x1, . . . , xdare the names of the explan-\n",
      "atory variables. If all variables are quantitative , this describes the linear model\n",
      "Yi=\f0+\f1xi1+\f2xi2+\u0001\u0001\u0001+\fdxid+\"i;i=1;:::; n; (5.33)\n",
      "where xi jis the j-th explanatory variable for the i-th observation and the errors \"iare\n",
      "independent normal random variables such that E\"i=0 andVar\"i=\u001b2. Or, in matrix\n",
      "form: Y=X\f+\", with\n",
      "Y=26666666664Y1\n",
      ":::\n",
      "Yn37777777775;X=266666666666666641x11\u0001\u0001\u0001 x1d\n",
      "1x21\u0001\u0001\u0001 x2d\n",
      "::::::::::::\n",
      "1xn1\u0001\u0001\u0001 xnd37777777777777775;\f=26666666664\f0\n",
      ":::\n",
      "\fd37777777775;and\"=26666666664\"1:::\n",
      "\"n37777777775:\n",
      "3For the rest of this section, we assume all linear models to be normal.192 5.6. Linear Models in Python\n",
      "Thus, the ﬁrst column is always taken as an “intercept” parameter, unless otherwise spe-\n",
      "ciﬁed. To remove the intercept term, add -1to the olsformula, as in ols('y\u0018x-1') .\n",
      "For any linear model, the model matrix can be retrieved via the construction:\n",
      "model_matrix = pd.DataFrame(model.exog,columns=model.exog_names)\n",
      "Let us look at some examples of linear models. In the ﬁrst model the variables x1andx2\n",
      "are both considered (by Python) to be quantitative.\n",
      "myData = pd.DataFrame({ 'y': [10,9,4,2,4,9],\n",
      "'x1': [7.4,1.2,3.1,4.8,2.8,6.5],\n",
      "'x2': [1,1,2,2,3,3]})\n",
      "mod = ols(\"y~x1+x2\", data=myData)\n",
      "mod_matrix = pd.DataFrame(mod.exog ,columns=mod.exog_names)\n",
      "print(mod_matrix)\n",
      "Intercept x1 x2\n",
      "0 1.0 7.4 1.0\n",
      "1 1.0 1.2 1.0\n",
      "2 1.0 3.1 2.0\n",
      "3 1.0 4.8 2.0\n",
      "4 1.0 2.8 3.0\n",
      "5 1.0 6.5 3.0\n",
      "Suppose the second variable is actually qualitative; e.g., it represents a color, and the\n",
      "levels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical\n",
      "variable by using the astype method to redeﬁne the data type (see Section 1.2). +3\n",
      "myData[ 'x2'] = myData[ 'x2'].astype( 'category ')\n",
      "Alternatively, a categorical variable can be speciﬁed in the model formula by wrapping\n",
      "it with C(). Observe how this changes the model matrix.\n",
      "mod2 = ols(\"y~x1+C(x2)\", data=myData)\n",
      "mod2_matrix = pd.DataFrame(mod2.exog ,columns=mod2.exog_names)\n",
      "print(mod2_matrix)\n",
      "Intercept C(x2)[T.2] C(x2)[T.3] x1\n",
      "0 1.0 0.0 0.0 7.4\n",
      "1 1.0 0.0 0.0 1.2\n",
      "2 1.0 1.0 0.0 3.1\n",
      "3 1.0 1.0 0.0 4.8\n",
      "4 1.0 0.0 1.0 2.8\n",
      "5 1.0 0.0 1.0 6.5\n",
      "Thus, if a statsmodels formula of the form (5.32) contains factor (qualitative) variables,\n",
      "the model is no longer of the form (5.33), but contains indicator variables for each level of\n",
      "the factor variable, except the ﬁrst level.\n",
      "For the case above, the corresponding linear model is\n",
      "Yi=\f0+\f1xi1+\u000b21fxi2=2g+\u000b31fxi2=3g+\"i;i=1;:::; 6; (5.34)\n",
      "where we have used parameters \u000b2and\u000b3to correspond to the indicator features of the\n",
      "qualitative variable. The parameter \u000b2describes how much the response is expected toChapter 5. Regression 193\n",
      "change if the factor x2switches from level 1 to 2. A similar interpretation holds for \u000b3.\n",
      "Such parameters can thus be viewed as incremental e \u000bects.\n",
      "It is also possible to model interaction interaction between two variables. For two continuous\n",
      "variables, this simply adds the products of the original features to the model matrix. Adding\n",
      "interaction terms in Python is achieved by replacing “ +” in the formula with “ *”, as the\n",
      "following example illustrates.\n",
      "mod3 = ols(\"y~x1*C(x2)\", data=myData)\n",
      "mod3_matrix = pd.DataFrame(mod3.exog ,columns=mod3.exog_names)\n",
      "print(mod3_matrix)\n",
      "Intercept C(x2)[T.2] C(x2)[T.3] x1 x1:C(x2)[T.2] x1:C(x2)[T.3]\n",
      "0 1.0 0.0 0.0 7.4 0.0 0.0\n",
      "1 1.0 0.0 0.0 1.2 0.0 0.0\n",
      "2 1.0 1.0 0.0 3.1 3.1 0.0\n",
      "3 1.0 1.0 0.0 4.8 4.8 0.0\n",
      "4 1.0 0.0 1.0 2.8 0.0 2.8\n",
      "5 1.0 0.0 1.0 6.5 0.0 6.5\n",
      "5.6.2 Analysis\n",
      "Let us consider some easy linear regression models by using the student survey data set\n",
      "survey.csv from the book’s GitHub site, which contains measurements such as height,\n",
      "weight, sex, etc., from a survey conducted among n=100 university students. Suppose we\n",
      "wish to investigate the relation between the shoe size (explanatory variable) and the height\n",
      "(response variable) of a person. First, we load the data and draw a scatterplot of the points\n",
      "(height versus shoe size); see Figure 5.6 (without the ﬁtted line).\n",
      "survey = pd.read_csv( 'survey.csv ')\n",
      "plt.scatter(survey.shoe , survey.height)\n",
      "plt.xlabel(\"Shoe size\")\n",
      "plt.ylabel(\"Height\")\n",
      "We observe a slight increase in the height as the shoe size increases, although this\n",
      "relationship is not very distinct. We analyze the data through the simple linear regression\n",
      "model Yi=\f0+\f1xi+\"i;i=1;:::; n. In statsmodels this is performed via the ols +169\n",
      "method as follows:\n",
      "model = ols(\"height~shoe\", data=survey) # define the model\n",
      "fit = model.fit() #fit the model defined above\n",
      "b0, b1 = fit.params\n",
      "print(fit.params)\n",
      "Intercept 145.777570\n",
      "shoe 1.004803\n",
      "dtype: float64\n",
      "The above output gives the least-squares estimates of \f0and\f1. For this example, we\n",
      "haveb\f0=145:778 and b\f1=1:005. Figure 5.6, which includes the regression line, was\n",
      "obtained as follows:194 5.6. Linear Models in Python\n",
      "15\n",
      " 20\n",
      " 25\n",
      " 30\n",
      " 35\n",
      "Shoe size\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200Height\n",
      "Figure 5.6: Scatterplot of height (cm) against shoe size (cm), with the ﬁtted line.\n",
      "plt.plot(survey.shoe , b0 + b1*survey.shoe)\n",
      "plt.scatter(survey.shoe , survey.height)\n",
      "plt.xlabel(\"Shoe size\")\n",
      "plt.ylabel(\"Height\")\n",
      "Although olsperforms a complete analysis of the linear model, not all its calculations\n",
      "need to be presented. A summary of the results can be obtained with the method summary .\n",
      "print(fit.summary())\n",
      "Dep. Variable: height R-squared: 0.178\n",
      "Model: OLS Adj. R-squared: 0.170\n",
      "Method: Least Squares F-statistic: 21.28\n",
      "No. Observations: 100 Prob (F-statistic): 1.20e-05\n",
      "Df Residuals: 98 Log-Likelihood: -363.88\n",
      "Df Model: 1 AIC: 731.8\n",
      "Covariance Type: nonrobust BIC: 737.0\n",
      "=====================================================================\n",
      "coef std err t P>|t| [0.025 0.975]\n",
      "--------------------------------------------------------------------\n",
      "Intercept 145.7776 5.763 25.296 0.000 134.341 157.214\n",
      "shoe 1.0048 0.218 4.613 0.000 0.573 1.437\n",
      "=====================================================================\n",
      "Omnibus: 1.958 Durbin -Watson: 1.772\n",
      "Prob(Omnibus): 0.376 Jarque -Bera (JB): 1.459\n",
      "Skew: -0.072 Prob(JB): 0.482\n",
      "Kurtosis: 2.426 Cond. No. 164.\n",
      "The main output items are the following:\n",
      "coef: Estimates of the parameters of the regression line.\n",
      "std error: Standard deviations of the estimators of the regression line. These are\n",
      "the square roots of the variances of the fb\figobtained in (5.25). +186Chapter 5. Regression 195\n",
      "t:Realization of Student’s test statistics associated with the hypotheses H0:\fi=0\n",
      "andH1:\fi,0,i=0;1. In particular, the outcome of Tin (5.19). +183\n",
      "P>|t| : P-value of Student’s test (two-sided test).\n",
      "[0.025 0.975] : 95% conﬁdence intervals for the parameters.\n",
      "R-Squared: Coe\u000ecient of determination R2(percentage of variation explained by\n",
      "the regression), as deﬁned in (5.18). +181\n",
      "Adj. R-Squared: adjusted R2(explained in Section 5.3.7).\n",
      "F-statistic: Realization of the Ftest statistic (5.21) associated with testing the +183\n",
      "full model against the default model. The associated degrees of freedom ( Df Model\n",
      "=1 and Df Residuals =n\u00002) are given, as is the P-value: Prob (F-statistic) .\n",
      "AIC: The AIC number in (5.15); that is, minus two times the log-likelihood plus two +177\n",
      "times the number of model parameters (which is 3 here).\n",
      "You can access all the numerical values as they are attributes of the fit object. First\n",
      "check which names are available, as in:\n",
      "dir(fit)\n",
      "Then access the values via the dot construction. For example, the following extracts the\n",
      "P-value for the slope.\n",
      "fit.pvalues[1]\n",
      "1.1994e-05\n",
      "The results show strong evidence for a linear relationship between shoe size and height\n",
      "(or, more accurately, strong evidence that the slope of the regression line is not zero), as\n",
      "the P-value for the corresponding test is very small (1 :2\u000110\u00005). The estimate of the slope\n",
      "indicates that the di \u000berence between the average height of students whose shoe size is\n",
      "di\u000berent by one cm is 1.0048 cm.\n",
      "Only 17:84% of the variability of student height is explained by the shoe size. We\n",
      "therefore need to add other explanatory variables to the model (multiple linear regression)\n",
      "to increase the model’s predictive power.\n",
      "5.6.3 Analysis of Variance (ANOVA)\n",
      "We continue the student survey example of the previous section, but now add an extra\n",
      "variable, and also consider an analysis of variance of the model. Instead of “explaining”\n",
      "the student height via their shoe size, we include weight as an explanatory variable. The\n",
      "corresponding olsformula for this model is\n",
      "height\u0018shoe +weight;196 5.6. Linear Models in Python\n",
      "meaning that each random height, denoted by Height , satisﬁes\n",
      "Height =\f0+\f1shoe +\f2weight +\";\n",
      "where\"is a normally distributed error term with mean 0 and variance \u001b2. Thus, the model\n",
      "has 4 parameters. Before analyzing the model we present a scatterplot of all pairs of vari-\n",
      "ables, using scatter_matrix .\n",
      "model = ols(\"height~shoe+weight\", data=survey)\n",
      "fit = model.fit()\n",
      "axes = pd.plotting.scatter_matrix(\n",
      "survey[[ 'height ','shoe ','weight ']])\n",
      "plt.show()\n",
      "150\n",
      "175height\n",
      "20\n",
      "30shoe\n",
      "150\n",
      "175\n",
      "height\n",
      "50\n",
      "100weight\n",
      "20\n",
      "30\n",
      "shoe\n",
      "50\n",
      "100\n",
      "weight\n",
      "Figure 5.7: Scatterplot of all pairs of variables: height (cm), shoe (cm), and weight (kg).\n",
      "As for the simple linear regression model in the previous section, we can analyze the\n",
      "model using the summary method (below we have omitted some output):\n",
      "fit.summary()\n",
      "Dep. Variable: height R-squared: 0.430\n",
      "Model: OLS Adj. R-squared: 0.418\n",
      "Method: Least Squares F-statistic: 36.61\n",
      "No. Observations: 100 Prob (F-statistic): 1.43e-12\n",
      "Df Residuals: 97 Log-Likelihood: -345.58\n",
      "Df Model: 2 AIC: 697.2\n",
      "BIC: 705.0\n",
      "======================================================================\n",
      "coef std err t P>|t| [0.025 0.975]\n",
      "----------------------------------------------------------------------\n",
      "Intercept 132.2677 5.247 25.207 0.000 121.853 142.682Chapter 5. Regression 197\n",
      "shoe 0.5304 0.196 2.703 0.008 0.141 0.920\n",
      "weight 0.3744 0.057 6.546 0.000 0.261 0.488\n",
      "TheF-statistic is used to test whether the full model (here with two explanatory\n",
      "variables) is better at “explaining” the height than the default model. The corresponding\n",
      "null hypothesis is H0:\f1=\f2=0. The assertion of interest is H1: at least one of the coe \u000e-\n",
      "cients\fj(j=1;2) is signiﬁcantly di \u000berent from zero. Given the result of this test (P-value\n",
      "=1:429\u000110\u000012), we can conclude that at least one of the explanatory variables is associated\n",
      "with height. The individual Student tests indicate that:\n",
      "shoe size is linearly associated with student height, after adjusting for weight, with\n",
      "P-value 0.0081. At the same weight, an increase of one cm in shoe size corresponds\n",
      "to an increase of 0.53 cm in average student height;\n",
      "weight is linearly associated with student height, after adjusting for shoe size (the\n",
      "P-value is actually 2 :82\u000110\u000009; the reported value of 0.000 should be read as “less\n",
      "than 0.001”). At the same shoe size, an increase of one kg in weight corresponds to\n",
      "an increase of 0.3744 cm in average student height.\n",
      "Further understanding is extracted from the model by conducting an analysis of vari-\n",
      "ance. The standard statsmodels function is anova_lm . The input to this function is the\n",
      "ﬁt object (obtained from model.fit() ) and the output is a DataFrame object.\n",
      "table = sm.stats.anova_lm(fit)\n",
      "print(table)\n",
      "df sum_sq mean_sq F PR(>F)\n",
      "shoe 1.0 1840.467359 1840.467359 30.371310 2.938651e-07\n",
      "weight 1.0 2596.275747 2596.275747 42.843626 2.816065e-09\n",
      "Residual 97.0 5878.091294 60.598879 NaN NaN\n",
      "The meaning of the columns is as follows.\n",
      "df: The degrees of freedom of the variables, according to the sum of squares decom-\n",
      "position (5.17). As both shoe andweight are quantitative variables, their degrees +181\n",
      "of freedom are both 1 (each corresponding to a single column in the overall model\n",
      "matrix). The degrees of freedom for the residuals is n\u0000p=100\u00003=97.\n",
      "sum sq: The sum of squares according to (5.17). The total sum of squares is the\n",
      "sum of all the entries in this column. The residual error in the model that cannot be\n",
      "explained by the variables is RSS \u00195878.\n",
      "mean sq: The sum of squares divided by their degrees of freedom. Note that the\n",
      "residual square error RSE =RSS=(n\u0000p)=60:6 is an unbiased estimate of the\n",
      "model variance \u001b2; see Section 5.4. +182\n",
      "F: These are the outcomes of the test statistic (5.22). +184\n",
      "PR(>F) : These are the P-values corresponding to the test statistic in the preceding\n",
      "column and are computed using an Fdistribution whose degrees of freedom are\n",
      "given in the dfcolumn.198 5.6. Linear Models in Python\n",
      "The ANOV A table indicates that the shoe variable explains a reasonable amount of the\n",
      "variation in the model, as evidenced by a sum of squares contribution of 1840 out of 1840 +\n",
      "2596+5878 =10314 and a very small P-value. After shoe is included in the model, it turns\n",
      "out that the weight variable explains even more of the remaining variability, with an even\n",
      "smaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,\n",
      "yielding a 43% reduction, in accordance with the R2value reported in the summary for the\n",
      "olsmethod. As mentioned in Section 5.4.1, the order in which the ANOV A is conducted\n",
      "is important. To illustrate this, consider the output of the following commands.\n",
      "model = ols(\"height~weight+shoe\", data=survey)\n",
      "fit = model.fit()\n",
      "table = sm.stats.anova_lm(fit)\n",
      "print(table)\n",
      "df sum_sq mean_sq F PR(>F)\n",
      "weight 1.0 3993.860167 3993.860167 65.906502 1.503553e-12\n",
      "shoe 1.0 442.882938 442.882938 7.308434 8.104688e-03\n",
      "Residual 97.0 5878.091294 60.598879 NaN NaN\n",
      "We see that weight as a single model variable explains much more of the variability\n",
      "than shoe did. If we now also include shoe , we only obtain a small (but according to the\n",
      "P-value still signiﬁcant) reduction in the model variability.\n",
      "5.6.4 Conﬁdence and Prediction Intervals\n",
      "Instatsmodels a method for computing conﬁdence or prediction intervals from a dic-\n",
      "tionary of explanatory variables is get_prediction . It simply executes formula (5.24) or\n",
      "(5.26). A simpler version is predict , which only returns the predicted value. +186\n",
      "Continuing the student survey example, suppose we wish to predict the height of a\n",
      "person with shoe size 30 cm and weight 75 kg. Conﬁdence and prediction intervals can\n",
      "be obtained as given in the code below. The new explanatory variable is entered as a dic-\n",
      "tionary. Notice that the 95% prediction interval (for the corresponding random response) is\n",
      "much wider than the 95% conﬁdence interval (for the expectation of the random response).\n",
      "x = { 'shoe ': [30.0], 'weight ': [75.0]} # new input (dictionary)\n",
      "pred = fit.get_prediction(x)\n",
      "pred.summary_frame(alpha=0.05).unstack()\n",
      "mean 0 176.261722 # predicted value\n",
      "mean_se 0 1.054015\n",
      "mean_ci_lower 0 174.169795 # lower bound for CI\n",
      "mean_ci_upper 0 178.353650 # upper bound for CI\n",
      "obs_ci_lower 0 160.670610 # lower bound for PI\n",
      "obs_ci_upper 0 191.852835 # upper bound for PI\n",
      "dtype: float64\n",
      "5.6.5 Model Validation\n",
      "We can perform an analysis of residuals to examine whether the underlying assumptions\n",
      "of the (normal) linear regression model are veriﬁed. Various plots of the residuals can beChapter 5. Regression 199\n",
      "used to inspect whether the assumptions on the errors f\"igare satisﬁed. Figure 5.8 gives two\n",
      "such plots. The ﬁrst is a scatterplot of the residuals feigagainst the ﬁtted values byi. When the\n",
      "model assumptions are valid, the residuals, as approximations of the model error, should\n",
      "behave approximately as iid normal random variables for each of the ﬁtted values, with a\n",
      "constant variance. In this case we see no strong aberrant structure in this plot. The residuals\n",
      "are fairly evenly spread and symmetrical about the y=0 line (not shown). The second plot\n",
      "is a quantile–quantile (or qq) plot. This is a useful way to check for normality of the error\n",
      "terms, by plotting the sample quantiles of the residuals against the theoretical quantiles\n",
      "of the standard normal distribution. Under the model assumptions, the points should lie\n",
      "approximately on a straight line. For the current case there does not seem to be an extreme\n",
      "departure from normality. Drawing a histogram or density plot of the residuals will also\n",
      "help to verify the normality assumption. The following code was used.\n",
      "plt.plot(fit.fittedvalues ,fit.resid , '.')\n",
      "plt.xlabel(\"fitted values\")\n",
      "plt.ylabel(\"residuals\")\n",
      "sm.qqplot(fit.resid)\n",
      "155\n",
      " 160\n",
      " 165\n",
      " 170\n",
      " 175\n",
      " 180\n",
      " 185\n",
      " 190\n",
      " 195\n",
      "fitted values\n",
      "25\n",
      "20\n",
      "15\n",
      "10\n",
      "5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20residuals\n",
      "3\n",
      " 2\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "Theoretical Quantiles\n",
      "25\n",
      "20\n",
      "15\n",
      "10\n",
      "5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20Sample Quantiles\n",
      "Figure 5.8: Left: residuals against ﬁtted values. Right: a qq plot of the residuals. Neither\n",
      "shows clear evidence against the model assumptions of constant variance and normality.\n",
      "5.6.6 Variable Selection\n",
      "Among the large number of possible explanatory variables, we wish to select those which\n",
      "best explain the observed responses. By eliminating redundant explanatory variables, we\n",
      "reduce the statistical error without increasing the approximation error, and thus reduce the\n",
      "(expected) generalization risk of the learner.\n",
      "In this section, we brieﬂy present two methods for variable selection. They are illus-\n",
      "trated on a few variables from the data set birthwt discussed in Section 1.5.3.2. The data + 13\n",
      "set contains information on the birth weights (masses) of babies, as well as various char-\n",
      "acteristics of the mother, such as whether she smokes, her age, etc. We wish to explain\n",
      "the child’s weight at birth using various characteristics of the mother, her family history,\n",
      "and her behavior during pregnancy. The response variable is weight at birth (quantitative\n",
      "variable bwt, expressed in grams); the explanatory variables are given below.200 5.6. Linear Models in Python\n",
      "The data can be obtained as explained in Section 1.5.3.2, or from statsmodels in the\n",
      "following way:\n",
      "bwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\n",
      "Here is some information about the explanatory variables that we will investigate.\n",
      "age: mother 's age in years\n",
      "lwt: mother 's weight in lbs\n",
      "race: mother 's race (1 = white, 2 = black, 3 = other)\n",
      "smoke: smoking status during pregnancy (0 = no, 1 = yes)\n",
      "ptl: no. of previous premature labors\n",
      "ht: history of hypertension (0 = no, 1 = yes)\n",
      "ui: presence of uterine irritability (0 = no, 1 = yes)\n",
      "ftv: no. of physician visits during first trimester\n",
      "bwt: birth weight in grams\n",
      "We can see the structure of the variables via bwt.info() . Check yourself that all\n",
      "variables are deﬁned as quantitative (int64 ). However, the variables race ,smoke ,ht,\n",
      "anduishould really be interpreted as qualitative (factors). To ﬁx this, we could redeﬁne\n",
      "them with the method astype , similar to what we did in Chapter 1. Alternatively, we could\n",
      "use the C()construction in a statsmodels formula to let the program know that certain\n",
      "variables are factors. We will use the latter approach.\n",
      "Forbinary features it does not matter whether the variables are interpreted as\n",
      "factorial or numerical as the numerical and summary results are identical.\n",
      "We consider the explanatory variables lwt,age,ui,smoke ,ht, and two recoded binary\n",
      "variables ftv1 andptl1 . We deﬁne ftv1 =1 if there was at least one visit to a physician,\n",
      "andftv1 =0 otherwise. Similarly, we deﬁne ptl1 =1 if there is at least one preterm birth\n",
      "in the family history, and ptl1 =0 otherwise.\n",
      "ftv1 = (bwt[ 'ftv']>=1).astype(int)\n",
      "ptl1 = (bwt[ 'ptl']>=1).astype(int)\n",
      "5.6.6.1 Forward Selection and Backward Elimination\n",
      "The forward selection forward\n",
      "selectionmethod is an iterative method for variable selection. In the ﬁrst\n",
      "iteration we consider which feature f1is the most signiﬁcant in terms of its P-value in the\n",
      "models bwt\u0018f1, with f12flwt;age;:::g. This feature is then selected into the model. In\n",
      "the second iteration, the feature f2that has the smallest P-value in the models bwt\u0018f1+f2\n",
      "is selected, where f2,f1, and so on. Usually only features are selected that have a P-\n",
      "value of at most 0.05. The following Python program automates this procedure. Instead of\n",
      "selecting on the P-value one could select on the AIC or BIC value.Chapter 5. Regression 201\n",
      "forwardselection.py\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.formula.api import ols\n",
      "bwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\n",
      "ftv1 = (bwt[ 'ftv']>=1).astype(int)\n",
      "ptl1 = (bwt[ 'ptl']>=1).astype(int)\n",
      "remaining_features = { 'lwt','age','C(ui) ','smoke ',\n",
      "'C(ht) ','ftv1 ','ptl1 '}\n",
      "selected_features = []\n",
      "while remaining_features:\n",
      "PF = [] #list of (P value , feature)\n",
      "for f in remaining_features:\n",
      "temp = selected_features + [f] #temporary list of features\n",
      "formula = 'bwt~ '+'+'.join(temp)\n",
      "fit = ols(formula ,data=bwt).fit()\n",
      "pval= fit.pvalues[-1]\n",
      "if pval < 0.05:\n",
      "PF.append((pval ,f))\n",
      "if PF: #if not empty\n",
      "PF.sort(reverse=True)\n",
      "(best_pval , best_f) = PF.pop()\n",
      "remaining_features.remove(best_f)\n",
      "print( 'feature {} with P-value = {:.2E} '.\n",
      "format(best_f , best_pval))\n",
      "selected_features.append(best_f)\n",
      "else:\n",
      "break\n",
      "feature C(ui) with P-value = 7.52E-05\n",
      "feature C(ht) with P-value = 1.08E-02\n",
      "feature lwt with P-value = 6.01E-03\n",
      "feature smoke with P-value = 7.27E-03\n",
      "Inbackward elimination backward\n",
      "eliminationwe start with the complete model (all features included) and\n",
      "at each step, we remove the variable with the highest P-value, as long as it is not signiﬁcant\n",
      "(greater than 0.05). We leave it as an exercise to verify that the order in which the fea-\n",
      "tures are removed is: age,ftv1 , and ptl1 . In this case, forward selection and backward\n",
      "elimination result in the same model, but this need not be the case in general.\n",
      "This way of model selection has the advantage of being easy to use and of treating the\n",
      "question of variable selection in a systematic manner. The main drawback is that variables\n",
      "are included or deleted based on purely statistical criteria, without taking into account the\n",
      "aim of the study. This usually leads to a model which may be satisfactory from a statistical\n",
      "point of view, but in which the variables are not necessarily the most relevant when it comes\n",
      "to understanding and interpreting the data in the study.\n",
      "Of course, we can choose to investigate any combination of features, not just the ones\n",
      "suggested by the above variable selection methods. For example, let us see if the mother’s\n",
      "weight, her age, her race, and whether she smokes explain the baby’s birthweight.202 5.6. Linear Models in Python\n",
      "formula = 'bwt~lwt+age+C(race)+ smoke '\n",
      "bwt_model = ols(formula , data=bwt).fit()\n",
      "print(bwt_model.summary())\n",
      "OLS Regression Results\n",
      "======================================================================\n",
      "Dep. Variable: bwt R-squared: 0.148\n",
      "Model: OLS Adj. R-squared: 0.125\n",
      "Method: Least Squares F-statistic: 6.373\n",
      "No. Observations: 189 Prob (F-statistic): 1.76e-05\n",
      "Df Residuals: 183 Log-Likelihood: -1498.4\n",
      "Df Model: 5 AIC: 3009.\n",
      "BIC: 3028.\n",
      "=====================================================================\n",
      "coef std err t P>|t| [0.025 0.975]\n",
      "----------------------------------------------------------------------\n",
      "Intercept 2839.4334 321.435 8.834 0.000 2205.239 3473.628\n",
      "C(race)[T.2] -510.5015 157.077 -3.250 0.001 -820.416 -200.587\n",
      "C(race)[T.3] -398.6439 119.579 -3.334 0.001 -634.575 -162.713\n",
      "smoke -401.7205 109.241 -3.677 0.000 -617.254 -186.187\n",
      "lwt 3.9999 1.738 2.301 0.022 0.571 7.429\n",
      "age -1.9478 9.820 -0.198 0.843 -21.323 17.427\n",
      "======================================================================\n",
      "Omnibus: 3.916 Durbin -Watson: 0.458\n",
      "Prob(Omnibus): 0.141 Jarque -Bera (JB): 3.718\n",
      "Skew: -0.343 Prob(JB): 0.156\n",
      "Kurtosis: 3.038 Cond. No. 899.\n",
      "Given the result of Fisher’s global test given by Prob (F-Statistic) in the summary\n",
      "(P-value =1:76\u000210\u00005), we can conclude that at least one of the explanatory variables is\n",
      "associated with child weight at birth, after adjusting for the other variables. The individual\n",
      "Student tests indicate that:\n",
      "the mother’s weight is linearly associated with child weight, after adjusting for age,\n",
      "race, and smoking status (P-value =0.022). At the same age, race, and smoking\n",
      "status, an increase of one pound in the mother’s weight corresponds to an increase\n",
      "of 4 g in the average child weight at birth;\n",
      "the age of the mother is not signiﬁcantly linearly associated with child weight at\n",
      "birth, when mother weight, race, and smoking status are already taken into account\n",
      "(P-value =0.843);\n",
      "weight at birth is signiﬁcantly lower for a child born to a mother who smokes, com-\n",
      "pared to children born to non-smoking mothers of the same age, race, and weight,\n",
      "with a P-value of 0.00031 (to see this, inspect bwt_model.pvalues ). At the same\n",
      "age, race, and mother weight, the child’s weight at birth is 401.720 g less for a\n",
      "smoking mother than for a non-smoking mother;\n",
      "regarding the interpretation of the variable race , we note that the ﬁrst level of this\n",
      "categorical variable corresponds to white mothers. The estimate of \u0000510:501 g for\n",
      "C(race)[T.2] represents the di \u000berence in the child’s birth weight between black\n",
      "mothers and white mothers (reference group), and this result is signiﬁcantly di \u000berentChapter 5. Regression 203\n",
      "from zero (P-value =0.001) in a model adjusted for the mother’s weight, age, and\n",
      "smoking status.\n",
      "5.6.6.2 Interaction\n",
      "We can also include interaction terms in the model. Let us see whether there is any inter-\n",
      "action e \u000bect between smoke andagevia the model\n",
      "Bwt=\f0+\f1age+\f2smoke +\f3age\u0002smoke +\":\n",
      "InPython this can be done as follows (below we have removed some output):\n",
      "formula = 'bwt~age*smoke '\n",
      "bwt_model = ols(formula , data=bwt).fit()\n",
      "print(bwt_model.summary())\n",
      "OLS Regression Results\n",
      "======================================================================\n",
      "Dep. Variable: bwt R-squared: 0.069\n",
      "Model: OLS Adj. R-squared: 0.054\n",
      "Method: Least Squares F-statistic: 4.577\n",
      "No. Observations: 189 Prob (F-statistic): 0.00407\n",
      "Df Residuals: 183 Log-Likelihood: -1506.8\n",
      "Df Model: 5 AIC: 3009.\n",
      "BIC: 3028.\n",
      "======================================================================\n",
      "coef std err t P>|t| [0.025 0.975]\n",
      "----------------------------------------------------------------------\n",
      "Intercept 2406.1 292.190 8.235 0.000 1829.6 2982.5\n",
      "smoke 798.2 484.342 1.648 0.101 -157.4 1753.7\n",
      "age 27.7 12.149 2.283 0.024 3.8 51.7\n",
      "age:smoke -46.6 20.447 -2.278 0.024 -86.9 -6.2\n",
      "We observe that the estimate for \f3(\u000046:6) is signiﬁcantly di \u000berent from zero (P-value\n",
      "=0.024). We therefore conclude that the e \u000bect of the mother’s age on the child’s weight\n",
      "depends on the smoking status of the mother. The results on association between mother\n",
      "age and child weight must therefore be presented separately for the smoking and the non-\n",
      "smoking group. For non-smoking mothers ( smoke = 0 ), the mean child weight at birth\n",
      "increases on average by 27 :7 grams for each year of the mother’s age. This is statistically\n",
      "signiﬁcant, as can be seen from the 95% conﬁdence intervals for the parameters (which\n",
      "does not contain zero):\n",
      "bwt_model.conf_int()\n",
      "0 1\n",
      "Intercept 1829.605754 2982.510194\n",
      "age 3.762780 51.699977\n",
      "smoke -157.368023 1753.717779\n",
      "age:smoke -86.911405 -6.232425\n",
      "Similarly, for smoking mothers, there seems to be a decrease in birthweight, b\f1+b\f3=\n",
      "27:7\u000046:6=\u000018:9, but this is not statistically signiﬁcant; see Exercise 6.204 5.7. Generalized Linear Models\n",
      "5.7 Generalized Linear Models\n",
      "The normal linear model in Section 2.8 deals with continuous response variables — such\n",
      "as height and crop yield — and continuous or discrete explanatory variables. Given the\n",
      "feature vectorsfxig, the responsesfYigare independent of each other, and each has a normal\n",
      "distribution with mean x>\n",
      "i\f, where x>\n",
      "iis the i-th row of the model matrix X. Generalized\n",
      "linear models allow for arbitrary response distributions, including discrete ones.\n",
      "Deﬁnition 5.2: Generalized Linear Model\n",
      "In ageneralized linear model generalized\n",
      "linear model(GLM) the expected response for a given feature vec-\n",
      "torx=[x1;:::; xp]>is of the form\n",
      "E[YjX=x]=h(x>\f) (5.35)\n",
      "for some function h, which is called the activation function activation\n",
      "function. The distribution of\n",
      "Y(for a given x) may depend on additional dispersion parameters that model the\n",
      "randomness in the data that is not explained by x.\n",
      "Theinverse of function his called the link function link function . As for the linear model, (5.35) is\n",
      "a model for a single pair ( x;Y). Using the model simpliﬁcation introduced at the end of\n",
      "Section 5.1, the corresponding model for a whole training set T=f(xi;Yi)gis that thefxig\n",
      "are ﬁxed and that the fYigare independent; each Yisatisfying (5.35) with x=xi. Writing\n",
      "Y=[Y1;:::; Yn]>and deﬁning has the multivalued function with components h, we have\n",
      "EXY=h(X\f);\n",
      "where Xis the (model) matrix with rows x>\n",
      "1;:::; x>\n",
      "n. A common assumption is that\n",
      "Y1;:::; Yncome from the same family of distributions, e.g., normal, Bernoulli, or Pois-\n",
      "son. The central focus is the parameter vector \f, which summarizes how the matrix of\n",
      "explanatory variables Xa\u000bects the response vector Y. The class of generalized linear mod-\n",
      "els can encompass a wide variety of models. Obviously the normal linear model (2.34) is\n",
      "a generalized linear model, with E[YjX=x]=x>\f, so that his the identity function. In\n",
      "this case, Y\u0018N(x>\f;\u001b2);i=1;:::; n, where\u001b2is a dispersion parameter.\n",
      "Example 5.10 (Logistic Regression) In a logistic regression logistic\n",
      "regressionorlogit model , we as-\n",
      "sume that the response variables Y1;:::; Ynare independent and distributed according to\n",
      "Yi\u0018Ber(h(x>\n",
      "i\f));where hhere is deﬁned as the cdf of the logistic distribution logistic\n",
      "distribution:\n",
      "h(x)=1\n",
      "1+e\u0000x:\n",
      "Large values of x>\n",
      "i\fthus lead to a high probability that Yi=1, and small (negative) values\n",
      "ofx>\n",
      "i\fcause Yito be 0 with high probability. Estimation of the parameter vector \ffrom\n",
      "the observed data is not as straightforward as for the ordinary linear model, but can be\n",
      "accomplished via the minimization of a suitable training loss, as explained below.\n",
      "As thefYigare independent, the pdf of Y=[Y1;:::; Yn]>is\n",
      "g(yj\f;X)=nY\n",
      "i=1[h(x>\n",
      "i\f)]yi[1\u0000h(x>\n",
      "i\f)]1\u0000yi:Chapter 5. Regression 205\n",
      "Maximizing the log-likelihood ln g(yj\f;X) with respect to \fgives the maximum likeli-\n",
      "hood estimator of \f. In a supervised learning framework, this is equivalent to minimizing:\n",
      "\u00001\n",
      "nlng(yj\f;X)=\u00001\n",
      "nnX\n",
      "i=1lng(yij\f;xi)\n",
      "=\u00001\n",
      "nnX\n",
      "i=1\u0002yilnh(x>\n",
      "i\f)+(1\u0000yi) ln(1\u0000h(x>\n",
      "i\f))\u0003:(5.36)\n",
      "By comparing (5.36) with (4.4), we see that we can interpret (5.36) as the cross-entropy +123\n",
      "training loss associated with comparing a true conditional pdf f(yjx) with an approxima-\n",
      "tion pdf g(yj\f;x) via the loss function\n",
      "Loss( f(yjx);g(yj\f;x)) :=\u0000lng(yj\f;x)=\u0000ylnh(x>\f)\u0000(1\u0000y) ln(1\u0000h(x>\f)):\n",
      "Minimizing (5.36) in terms of \factually constitutes a convex optimization problem. Since\n",
      "lnh(x>\f)=\u0000ln(1+e\u0000x>\f) and ln(1\u0000h(x>\f))=\u0000x>\f\u0000ln(1+e\u0000x>\f), the cross-entropy\n",
      "training loss (5.36) can be rewritten as\n",
      "r\u001c(\f) :=1\n",
      "nnX\n",
      "i=1h\n",
      "(1\u0000yi)x>\n",
      "i\f+ln\u0010\n",
      "1+e\u0000x>\n",
      "i\f\u0011i\n",
      ":\n",
      "We leave it as Exercise 7 to show that the gradient rr\u001c(\f) and Hessian H(\f) ofr\u001c(\f) are\n",
      "given by\n",
      "rr\u001c(\f)=1\n",
      "nnX\n",
      "i=1(\u0016i\u0000yi)xi (5.37)\n",
      "and\n",
      "H(\f)=1\n",
      "nnX\n",
      "i=1\u0016i(1\u0000\u0016i)xix>\n",
      "i; (5.38)\n",
      "respectively, where \u0016i:=h(x>\n",
      "i\f).\n",
      "Notice that H(\f) is a positive semideﬁnite matrix for all values of \f, implying the +405\n",
      "convexity of r\u001c(\f). Consequently, we can ﬁnd an optimal \fe\u000eciently; e.g., via Newton’s\n",
      "method. Speciﬁcally, given an initial value \f0, for t=1;2;:::; iteratively compute +411\n",
      "\ft=\ft\u00001\u0000H\u00001(\ft\u00001)rr\u001c(\ft\u00001); (5.39)\n",
      "until the sequence \f0;\f1;\f2;:::is deemed to have converged, using some pre-ﬁxed con-\n",
      "vergence criterion.\n",
      "Figure 5.9 shows the outcomes of 100 independent Bernoulli random variables, where\n",
      "each success probability, (1 +exp(\u0000(\f0+\f1x)))\u00001, depends on xand\f0=\u00003; \f1=10. The\n",
      "true logistic curve is also shown (dashed line). The minimum training loss curve (red line)\n",
      "is obtained via the Newton scheme (5.39), giving estimates b\f0=\u00002:66 and b\f1=10:08.\n",
      "The Python code is given below.206 5.7. Generalized Linear Models\n",
      "-1 -0.5 0 0.5 100.20.40.60.81\n",
      "Figure 5.9: Logistic regression data (blue dots), ﬁtted curve (red), and true curve (black\n",
      "dashed).\n",
      "logreg1d.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy.linalg import lstsq\n",
      "n = 100 # sample size\n",
      "x = (2*np.random.rand(n)-1).reshape(n,1) # explanatory variables\n",
      "beta = np.array([-3, 10])\n",
      "Xmat = np.hstack((np.ones((n,1)), x))\n",
      "p = 1/(1 + np.exp(-Xmat @ beta))\n",
      "y = np.random.binomial(1,p,n) # response variables\n",
      "# initial guess\n",
      "betat = lstsq((Xmat.T @ Xmat),Xmat.T @ y, rcond=None)[0]\n",
      "grad = np.array([2,1]) # gradient\n",
      "while (np.sum(np.abs(grad)) > 1e-5) : # stopping criteria\n",
      "mu = 1/(1+np.exp(-Xmat @ betat))\n",
      "# gradient\n",
      "delta = (mu - y).reshape(n,1)\n",
      "grad = np.sum(np.multiply( np.hstack((delta ,delta)),Xmat), axis\n",
      "=0).T\n",
      "# Hessian\n",
      "H = Xmat.T @ np.diag(np.multiply(mu,(1-mu))) @ Xmat\n",
      "betat = betat - lstsq(H,grad ,rcond=None)[0]\n",
      "print(betat)\n",
      "plt.plot(x,y, '.') # plot data\n",
      "xx = np.linspace(-1,1,40).reshape(40,1)\n",
      "XXmat = np.hstack( (np.ones((len(xx),1)), xx))\n",
      "yy = 1/(1 + np.exp(-XXmat @ beta))\n",
      "plt.plot(xx,yy, 'r-') #true logistic curve\n",
      "yy = 1/(1 + np.exp(-XXmat @ betat));\n",
      "plt.plot(xx,yy, 'k--')Chapter 5. Regression 207\n",
      "Further Reading\n",
      "An excellent overview of regression is provided in [33] and an accessible mathematical\n",
      "treatment of linear regression models can be found in [108]. For extensions to nonlinear\n",
      "regression we refer the reader to [7]. A practical introduction to multilevel /hierarchical\n",
      "models is given in [47]. For further discussion on regression with discrete responses (clas-\n",
      "siﬁcation) we refer to Chapter 7 and the further reading therein. On the important question +253\n",
      "of how to handle missing data, the classic reference is [80] (see also [85]) and a modern\n",
      "applied reference is [120].\n",
      "Exercises\n",
      "1. Following his mentor Francis Galton, the mathematician /statistician Karl Pearson con-\n",
      "ducted comprehensive studies comparing hereditary traits between members of the same\n",
      "family. Figure 5.10 depicts the measurements of the heights of 1078 fathers and their\n",
      "adult sons (one son per father). The data is available from the book’s GitHub site as\n",
      "pearson.csv .\n",
      "58 60 62 64 66 68 70 72 74 76\n",
      "Height Father (in)60657075Height Son (in)\n",
      "Figure 5.10: A scatterplot of heights from Pearson’s data.\n",
      "(a) Show that sons are on average 1 inch taller than the fathers.\n",
      "(b) We could try to “explain” the height of the son by taking the height of his father and\n",
      "adding 1 inch. The prediction line y=x+1 (red dashed) is given Figure 5.10. The\n",
      "black solid line is the ﬁtted regression line. This line has a slope less than 1, and\n",
      "demonstrates Galton’s “regression” to the average. Find the intercept and slope of the\n",
      "ﬁtted regression line.\n",
      "2. For the simple linear regression model, show that the values for b\f1andb\f0that solve the208 Exercises\n",
      "equations (5.9) are:\n",
      "b\f1=Pn\n",
      "i=1(xi\u0000x)(yi\u0000y)Pn\n",
      "i=1(xi\u0000x)2(5.40)\n",
      "b\f0=y\u0000b\f1x; (5.41)\n",
      "provided that not all xiare the same.\n",
      "3. Edwin Hubble discovered that the universe is expanding. If vis a galaxy’s recession ve-\n",
      "locity (relative to any other galaxy) and dis its distance (from that same galaxy), Hubble’s\n",
      "law states that\n",
      "v=Hd;\n",
      "where His known as Hubble’s constant. The following are distance (in millions of light-\n",
      "years) and velocity (thousands of miles per second) measurements made on ﬁve galactic\n",
      "clusters.\n",
      "distance 68 137 315 405 700\n",
      "velocity 2.4 4.7 12.0 14.4 26.0\n",
      "State the regression model and estimate H.\n",
      "4. The multiple linear regression model (5.6) can be viewed as a ﬁrst-order approximation\n",
      "of the general model\n",
      "Y=g(x)+\"; (5.42)\n",
      "whereE\"=0,Var\"=\u001b2, and g(x) is some known or unknown function of a d-\n",
      "dimensional vector xof explanatory variables. To see this, replace g(x) with its ﬁrst-order\n",
      "Taylor approximation around some point x0and write this as \f0+x>\f. Express\f0and\f\n",
      "in terms of gandx0.\n",
      "5. Table 5.6 shows data from an agricultural experiment where crop yield was measured\n",
      "for two levels of pesticide and three levels of fertilizer. There are three responses for each\n",
      "combination.\n",
      "Table 5.6: Crop yields for pesticide and fertilizer combinations.\n",
      "Fertilizer\n",
      "Pesticide Low Medium High\n",
      "No 3.23, 3.20, 3.16 2.99, 2.85, 2.77 5.72, 5.77, 5.62\n",
      "Yes 6.78, 6.73, 6.79 9.07, 9.09, 8.86 8.12, 8.04, 8.31\n",
      "(a) Organize the data in standard form, where each row corresponds to a single meas-\n",
      "urement and the columns correspond to the response variable and the two factor vari-\n",
      "ables.Chapter 5. Regression 209\n",
      "(b) Let Yi jkbe the response for the k-th replication at level ifor factor 1 and level j\n",
      "for factor 2. To assess which factors best explain the response variable, we use the\n",
      "ANOV A model\n",
      "i j+\"i jk; (5.43)\n",
      "whereP\n",
      "i\u000bi=P\n",
      "j\fj=P\n",
      "i j=P\n",
      "12;=0. Deﬁne\f=[\u0016;\u000b 1;\u000b2;\f1;\f2; \f3;\n",
      "23]>. Give the corresponding 18 \u000212 model matrix.\n",
      "(c) Note that the parameters are linearly dependent in this case. For example, \u000b2=\u0000\u000b1\n",
      "12). To retain only 6 linearly independent variables consider the\n",
      "12]>. Find the matrix Msuchtor e\f=[\u0016;\u000b 1;\f1;\f2;\n",
      "thatMe\f=\f.\n",
      "(d) Give the model matrix corresponding to e\f.\n",
      "6. Show that for the birthweight data in Section 5.6.6.2 there is no signiﬁcant decrease\n",
      "in birthweight for smoking mothers. [Hint: create a new variable nonsmoke =1\u0000smoke ,\n",
      "which reverses the encoding for the smoking and non-smoking mothers. Then, the para-\n",
      "meter\f1+\f3in the original model is the same as the parameter \f1in the model\n",
      "Bwt=\f0+\f1age+\f2nonsmoke +\f3age\u0002nonsmoke +\":\n",
      "Now ﬁnd a 95% for \f3and see if it contains zero.]\n",
      "7. Prove (5.37) and (5.38).\n",
      "8. In the Tobit regression Tobit\n",
      "regressionmodel with normally distributed errors, the response is modeled\n",
      "as:\n",
      "Yi=8>><>>:Zi;ifui<Zi\n",
      "ui;ifZi6ui; Z\u0018N(X\f;\u001b2In);\n",
      "where the model matrix Xand the thresholds u1;:::; unare given. Typically, ui=0;i=\n",
      "1;:::; n. Suppose we wish to estimate \u0012:=(\f;\u001b2) via the Expectation–Maximization\n",
      "method, similar to the censored data Example 4.2. Let y=[y1;:::; yn]>be the vector +130\n",
      "of observed data.\n",
      "(a) Show that the likelihood of yis:\n",
      "g(yj\u0012)=Y\n",
      "i:yi>ui'\u001b2(yi\u0000x>\n",
      "i\f)\u0002Y\n",
      "i:yi=u((ui\u0000x>\n",
      "i\f)=\u001b);\n",
      "whereis the cdf of the N(0;1) distribution and '\u001b2the pdf of the N(0;\u001b2) distribu-\n",
      "tion.\n",
      "(b) Let yandybe vectors that collect all yi>uiandyi=ui, respectively. Denote the\n",
      "corresponding matrix of predictors by XandX, respectively. For each observation\n",
      "yi=uiintroduce a latent variable ziand collect these into a vector z. For the same\n",
      "indices icollect the corresponding uiinto a vector c. Show that the complete-data\n",
      "likelihood is given by\n",
      "g(y;zj\u0012)=1\n",
      "(2\u0019\u001b2)n=2exp0BBBB@\u0000ky\u0000X\fk2\n",
      "2\u001b2\u0000kz\u0000X\fk2\n",
      "2\u001b21CCCCA1fz6cg:210 Exercises\n",
      "(c) For the E-step, show that, for a ﬁxed \u0012,\n",
      "g(zjy;\u0012)=Y\n",
      "ig(zijy;\u0012);\n",
      "where each g(zijy;\u0012) is the pdf of the N((X\f)i;\u001b2) distribution, truncated to the in-\n",
      "terval (\u00001;ci].\n",
      "(d) For the M-step, compute the expectation of the complete log-likelihood\n",
      "\u0000n\n",
      "2ln\u001b2\u0000n\n",
      "2ln(2\u0019)\u0000ky\u0000X\fk2\n",
      "2\u001b2\u0000EkZ\u0000X\fk2\n",
      "2\u001b2:\n",
      "Then, derive the formulas for \fand\u001b2that maximize the expectation of the complete\n",
      "log-likelihood.\n",
      "9. Dowload data set WomenWage.csv from the book’s website. This data set is a tidied-up\n",
      "version of the women’s wages data set from [91]. The ﬁrst column of the data ( hours ) is\n",
      "the response variable Y. It shows the hours spent in the labor force by married women in\n",
      "the 1970s. We want to understand what factors determine the participation rate of women\n",
      "in the labor force. The predictor variables are:\n",
      "Table 5.7: Features for the women’s wage data set.\n",
      "Feature Description\n",
      "kidslt6 Number of children younger than 6 years.\n",
      "kidsge6 Number of children older than 6 years.\n",
      "age Age of the married woman.\n",
      "educ Number of years of formal education.\n",
      "exper Number of years of “work experience”.\n",
      "nwifeinc Non-wife income, that is, the income of the husband.\n",
      "expersq The square of exper , to capture any nonlinear relationships.\n",
      "We observe that some of the responses are Y=0, that is, some women did not particip-\n",
      "ate in the labor force. For this reason, we model the data using the Tobit regression model,\n",
      "in which the response Yis given as:\n",
      "Yi=8>><>>:Zi;ifZi>0\n",
      "0;ifZi60; Z\u0018N(X\f;\u001b2In):\n",
      "With\u0012=(\f;\u001b2), the likelihood of the data y=[y1;:::; yn]>is:\n",
      "g(yj\u0012)=Q\n",
      "i:yi>0'\u001b2(yi\u0000x>\n",
      "i\f)\u0002Q\n",
      "i:yi=((ui\u0000x>\n",
      "i\f)=\u001b);\n",
      "whereis the standard normal cdf. In Exercise 8, we derived the EM algorithm for max-\n",
      "imizing the log-likelihood.\n",
      "(a) Write down the EM algorithm in pseudo code as it applies to this Tobit regression.Chapter 5. Regression 211\n",
      "(b) Implement the EM algorithm pseudo code in Python. Comment on which factor you\n",
      "think is important in determining the labor participation rate of women living in the\n",
      "USA in the 1970s.\n",
      "10. Let Pbe a projection matrix. Show that the diagonal elements of Pall lie in the interval\n",
      "[0;1]. In particular, for P=XX+in Theorem 5.1, the leverage value pi:=Piisatisﬁes\n",
      "06pi61 for all i.\n",
      "11. Consider the linear model Y=X\f+\"in (5.8), with Xbeing the n\u0002pmodel matrix\n",
      "and\"having expectation vector 0and covariance matrix \u001b2In. Suppose that b\f\u0000iis the\n",
      "least-squares estimate obtained by omitting the i-th observation, Yi; that is,\n",
      "b\f\u0000i=argmin\n",
      "\fX\n",
      "j,i(Yj\u0000x>\n",
      "j\f)2;\n",
      "where x>\n",
      "jis the j-th row of X. LetbY\u0000i=x>\n",
      "ib\f\u0000ibe the corresponding ﬁtted value at xi. Also,\n",
      "deﬁne Bias the least-squares estimator of \fbased on the response data\n",
      "Y(i):=[Y1;:::; Yi\u00001;bY\u0000i;Yi+1;:::; Yn]>:\n",
      "(a) Prove that b\f\u0000i=Bi; that is, the linear model obtained from ﬁtting all responses except\n",
      "thei-th is the same as the one obtained from ﬁtting the data Y(i).\n",
      "(b) Use the previous result to verify that\n",
      "Yi\u0000bY\u0000i=(Yi\u0000bYi)=(1\u0000Pii);\n",
      "where P=XX+is the projection matrix onto the columns of X. Hence, deduce the\n",
      "PRESS formula in Theorem 5.1. +174\n",
      "12. Take the linear model Y=X\f+\",where Xis an n\u0002pmodel matrix, \"=0, and\n",
      "Cov(\")=\u001b2In. Let P=XX+be the projection matrix onto the columns of X.\n",
      "(a) Using the properties of the pseudo-inverse (see Deﬁnition A.2), show that PP>=P. +362\n",
      "(b) Let E=Y\u0000bYbe the (random) vector of residuals, where bY=PY. Show that the i-th\n",
      "residual has a normal distribution with expectation 0 and variance \u001b2(1\u0000Pii) (that is,\n",
      "\u001b2times 1 minus the i-th leverage).\n",
      "(c) Show that \u001b2can be unbiasedly estimated via\n",
      "S2:=1\n",
      "n\u0000pkY\u0000bYk2=1\n",
      "n\u0000pkY\u0000Xb\fk2: (5.44)\n",
      "[Hint: use the cyclic property of the trace as in Example 2.3.]\n",
      "13. Consider a normal linear model Y=X\f+\", where Xis an n\u0002pmodel matrix and\n",
      "\"\u0018N(0;\u001b2In). Exercise 12 shows that for any such model the i-th standardized residual\n",
      "Ei=(\u001bp1\u0000Pii) has a standard normal distribution. This motivates the use of the leverage\n",
      "Piito assess whether the i-th observation is an outlier depending on the size of the i-th\n",
      "residual relative top1\u0000Pii. A more robust approach is to include an estimate for \u001busing212 Exercises\n",
      "all data except the i-th observation. This gives rise to the studentized residual studentized\n",
      "residualTi, deﬁned\n",
      "as\n",
      "Ti:=Ei\n",
      "S\u0000ip1\u0000Pii;\n",
      "where S\u0000iis an estimate of \u001bobtained by ﬁtting all the observations except the i-th and\n",
      "Ei=Yi\u0000bYiis the i-th (random) residual. Exercise 12 shows that we can take, for example,\n",
      "S2\n",
      "\u0000i=1\n",
      "n\u00001\u0000pkY\u0000i\u0000X\u0000ib\f\u0000ik2; (5.45)\n",
      "where X\u0000iis the model matrix Xwith the i-th row removed, is an unbiased estimator of\n",
      "\u001b2. We wish to compute S2\n",
      "\u0000ie\u000eciently, using S2in (5.44), as the latter will typically be\n",
      "available once we have ﬁtted the linear model. To this end, deﬁne uias the i-th unit vector\n",
      "[0;:::; 0;1;0;:::; 0]>, and let\n",
      "Y(i):=Y\u0000(Yi\u0000bY\u0000i)ui=Y\u0000Ei\n",
      "1\u0000Piiui;\n",
      "where we have used the fact that Yi\u0000bY\u0000i=Ei=(1\u0000Pii), as derived in the proof of The-\n",
      "orem 5.1. Now apply Exercise 11 to prove that\n",
      "S2\n",
      "\u0000i=(n\u0000p)S2\u0000E2\n",
      "i=(1\u0000Pii)\n",
      "n\u0000p\u00001:\n",
      "14. Using the notation from Exercises 11–13, Cook’s distance Cook’s distance for observation iis deﬁned\n",
      "as\n",
      "Di:=kbY\u0000bY(i)k2\n",
      "p S2:\n",
      "It measures the change in the ﬁtted values when the i-th observation is removed, relative to\n",
      "the residual variance of the model (estimated via S2).\n",
      "By using similar arguments as those in Exercise 13, show that\n",
      "Di=PiiE2\n",
      "i\n",
      "(1\u0000Pii)2p S2:\n",
      "It follows that there is no need to “omit and reﬁt” the linear model in order to compute\n",
      "Cook’s distance for the i-th response.\n",
      "15. Prove that if we add an additional feature to the general linear model, then R2, the\n",
      "coe\u000ecient of determination, is necessarily non-decreasing in value and hence cannot be\n",
      "used to compare models with di \u000berent numbers of predictors.\n",
      "16. Let X:=[X1;:::; Xn]>and\u0016:=[\u00161;:::;\u0016 n]>. In the fundamental Theorem C.9, we\n",
      "use the fact that if Xi\u0018N(\u0016i;1),i=1;:::; nare independent, then kXk2has (per deﬁnition)\n",
      "a noncentral \u001f2\n",
      "ndistribution. Show that kXk2has moment generating function\n",
      "etk\u0016k2=(1\u00002t)\n",
      "(1\u00002t)n=2;t<1=2;\n",
      "and so the distribution of kXk2depends on\u0016only through the norm k\u0016k.Chapter 5. Regression 213\n",
      "17. Carry out a logistic regression analysis on a (partial) wine data set classiﬁcation prob-\n",
      "lem. The data can be loaded using the following code.\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "data = datasets.load_wine()\n",
      "X = data.data[:, [9,10]]\n",
      "y = np.array(data.target==1,dtype=np.uint)\n",
      "X = np.append(np.ones(len(X)).reshape(-1,1),X,axis=1)\n",
      "The model matrix has three features, including the constant feature. Instead of using\n",
      "Newton’s method (5.39) to estimate \f, implement a simple gradient descent procedure\n",
      "\ft=\ft\u00001\u0000\u000brr\u001c(\ft\u00001);\n",
      "with learning rate \u000b=0:0001, and run it for 106steps. Your procedure should deliver three\n",
      "coe\u000ecients; one for the intercept and the rest for the explanatory variables. Solve the same\n",
      "problem using the Logit method of statsmodels.api and compare the results.\n",
      "18. Consider again Example 5.10, where we train the learner via the Newton iteration\n",
      "(5.39). If X>:=[x1;:::; xn] deﬁnes the matrix of predictors and \u0016t:=h(X\ft), then the +205\n",
      "gradient (5.37) and Hessian (5.38) for Newton’s method can be written as:\n",
      "rr\u001c(\ft)=1\n",
      "nX>(\u0016t\u0000y) and H(\ft)=1\n",
      "nX>DtX;\n",
      "where Dt:=diag(\u0016t\f(1\u0000\u0016t)) is a diagonal matrix. Show that the Newton iteration (5.39)\n",
      "can be written as the iterative reweighted least-squares iterative\n",
      "reweighted\n",
      "least squaresmethod:\n",
      "\ft=argmin\n",
      "\f(eyt\u00001\u0000X\f)>Dt\u00001(eyt\u00001\u0000X\f);\n",
      "whereeyt\u00001:=X\ft\u00001+D\u00001\n",
      "t\u00001(y\u0000\u0016t\u00001) is the so-called adjusted response . [Hint: use the fact\n",
      "that ( M>M)\u00001M>zis the minimizer of kM\f\u0000zk2.]\n",
      "19. In multi-output linear regression multi -output\n",
      "linear\n",
      "regression, the response variable is a real-valued vector of di-\n",
      "mension, say, m. Similar to (5.8), the model can be written in matrix notation:\n",
      "Y=XB+26666666664\">\n",
      "1:::\n",
      "\">\n",
      "n37777777775;\n",
      "where:\n",
      "Yis an n\u0002mmatrix of nindependent responses (stored as row vectors of length m);\n",
      "Xis the usual n\u0002pmodel matrix;\n",
      "Bis an p\u0002mmatrix of model parameters;\n",
      "\"1;:::;\"n2Rmare independent error terms with E\"=0andE\"\">=\u0006.214 Exercises\n",
      "We wish to learn the matrix parameters Band\u0006from the training set fY;Xg. To this end,\n",
      "consider minimizing the training loss:\n",
      "1\n",
      "ntr\u0010\n",
      "(Y\u0000XB)\u0006\u00001(Y\u0000XB)>\u0011\n",
      ";\n",
      "where tr(\u0001) is the trace of a matrix. +359\n",
      "(a) Show that the minimizer of the training loss, denoted bB, satisﬁes the normal equa-\n",
      "tions:\n",
      "X>XbB=X>Y:\n",
      "(b) Noting that\n",
      "(Y\u0000XB)>(Y\u0000XB)=nX\n",
      "i=1\"i\">\n",
      "i;\n",
      "explain why\n",
      "b\u0006:=(Y\u0000XbB)>(Y\u0000XbB)\n",
      "n\n",
      "is a method-of-moments estimator of \u0006, just like the one given in (5.10).CHAPTER6\n",
      "REGULARIZATION AND KERNEL\n",
      "METHODS\n",
      "The purpose of this chapter is to familiarize the reader with two central concepts\n",
      "in modern data science and machine learning: regularization and kernel methods. Reg-\n",
      "ularization provides a natural way to guard against overﬁtting and kernel methods of-\n",
      "fer a broad generalization of linear models. Here, we discuss regularized regression\n",
      "(ridge, lasso) as a bridge to the fundamentals of kernel methods. We introduce repro-\n",
      "ducing kernel Hilbert spaces and show that selecting the best prediction function in\n",
      "such spaces is in fact a ﬁnite-dimensional optimization problem. Applications to spline\n",
      "ﬁtting, Gaussian process regression, and kernel PCA are given.\n",
      "6.1 Introduction\n",
      "In this chapter we return to the supervised learning setting of Chapter 5 (regression) and ex-\n",
      "pand its scope. Given training data \u001c=f(x1;y1);:::; (xn;yn)g, we wish to ﬁnd a prediction\n",
      "function (the learner) g\u001cthat minimizes the (squared-error) training loss\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1(yi\u0000g(xi))2\n",
      "within a class of functions G. As noted in Chapter 2, if Gis the set of all possible functions\n",
      "then choosing anyfunction gwith the property that g(xi)=yifor all iwill give zero training\n",
      "loss, but will likely have poor generalization performance (that is, su \u000ber from overﬁtting).\n",
      "Recall from Theorem 2.1 that the best possible prediction function (over all g) for + 21\n",
      "the squared-error riskE(Y\u0000g(X))2is given by g\u0003(x)=E[YjX=x]. The classGshould\n",
      "be simple enough to permit theoretical understanding and analysis but, at the same time,\n",
      "rich enough to contain the optimal function g\u0003(or a function close to g\u0003). This ideal can\n",
      "be realized by taking Gto be a Hilbert space Hilbert space (i.e., a complete inner product space) of\n",
      "functions; see Appendix A.7. +386\n",
      "Many of the classes of functions that we have encountered so far are in fact Hilbert\n",
      "spaces. In particular, the set Goflinear functions on Rpis a Hilbert space. To see this,\n",
      "215216 6.2. Regularization\n",
      "identify with each element \f2Rpthe linear function g\f:x7!x>\fand deﬁne the inner\n",
      ". In this way,Gbehaves in exactly the same way as (is\n",
      "isomorphic to) the space Rpequipped with the Euclidean inner product (dot product). The +362\n",
      "latter is a Hilbert space, because it is complete complete\n",
      "vector spacewith respect to the Euclidean norm. See\n",
      "Exercise 12 for a further discussion.\n",
      "Let us now turn to our “running” polynomial regression Example 2.1, where the feature +26\n",
      "vector x=[1;u;u2;:::; up\u00001]>=:\u001e(u) is itself a vector-valued function of another feature\n",
      "u. Then, the space of functions h\f:u7!\u001e(u)>\fis a Hilbert space, through the identiﬁca-\n",
      "tionh\f\u0011\f. In fact, this is true for anyfeature mapping \u001e:u7![\u001e1(u);:::;\u001e p(u)]>.\n",
      "This can be further generalized by considering feature maps u7!\u0014u, where each \u0014ufeature mapsis a real-valued function v7!\u0014u(v) on the feature space. As we shall soon see (in Sec-\n",
      "tion 6.3), functions of the form u7!P1\n",
      "i=1\fi\u0014vi(u) live in a Hilbert space of functions called\n",
      "areproducing kernel Hilbert space (RKHS). RKHS In Section 6.3 we introduce the notion of a\n",
      "RKHS formally, give speciﬁc examples, including the linear and Gaussian kernels, and de-\n",
      "rive various useful properties, the most important of which is the representer Theorem 6.6.\n",
      "Applications of such spaces include the smoothing splines (Section 6.6), Gaussian pro- +235\n",
      "cess regression (Section 6.7), kernel PCA (Section 6.8), and support vector machines for\n",
      "classiﬁcation (Section 7.7). +271\n",
      "The RKHS formalism also makes it easier to treat the important topic of regularization .regularizationThe aim of regularization is to improve the predictive performance of the best learner in\n",
      "some class of functions Gby adding a penalty term to the training loss that penalizes\n",
      "learners that tend to overﬁt the data. In the next section we introduce the main ideas behind\n",
      "regularization, which then segues into a discussion of kernel methods in the subsequent\n",
      "sections.\n",
      "6.2 Regularization\n",
      "LetGbe the Hilbert space of functions over which we search for the minimizer, g\u001c, of the\n",
      "training loss `\u001c(g). Often, the Hilbert space Gis rich enough so that we can ﬁnd a learner\n",
      "g\u001cwithinGsuch that the training loss is zero or close to zero. Consequently, if the space of\n",
      "functionsGis su\u000eciently rich, we run the risk of overﬁtting. One way to avoid overﬁtting\n",
      "is to restrict attention to a subset of the space Gby introducing a non-negative functional\n",
      "J:G!R+which penalizes complex models (functions). In particular, we want to ﬁnd\n",
      "functions g2Gsuch that J(g)<cfor some “regularization” constant c>0. Thus we can\n",
      "formulate the quintessential supervised learning problem as:\n",
      "minf`\u001c(g) :g2G;J(g)<cg; (6.1)\n",
      "the solution (argmin) of which is our learner. When this optimization problem is convex, it\n",
      "can be solved by ﬁrst obtaining the Lagrangian dual function\n",
      "L\u0003(\u0015) :=min\n",
      "g2Gf`\u001c(g)+\u0015(J(g)\u0000c)g;\n",
      "and then maximizing L\u0003(\u0015) with respect to \u0015>0; see Section B.2.3. +409\n",
      "In order to introduce the overall ideas of kernel methods and regularization, we will\n",
      "proceed by exploring (6.1) in the special case of ridge regression ridge\n",
      "regression, with the following run-\n",
      "ning example.Chapter 6. Regularization and Kernel Methods 217\n",
      "Example 6.1 (Ridge Regression) Ridge regression is simply linear regression with a\n",
      "squared-norm penalty functional (also called a regularization function, or regularizer regularizer ).\n",
      "Suppose we have a training set \u001c=f(xi;yi);i=1;:::; ng, with each xi2Rpand we use a\n",
      "squared-norm penalty with regularization parameter regularization\n",
      ">0. Then, the problem is to solve\n",
      "min\n",
      "g2G1\n",
      "nnX\n",
      "kgk2; (6.2)))2+\n",
      "whereGis the Hilbert space of linear functions on Rp. As explained in Section 6.1, we\n",
      "can identify each g2Gwith a vector \f2Rpand, consequently, kgk2=h\f;\fi=k\fk2. The\n",
      "above functional optimization problem is thus equivalent to the parametric optimization\n",
      "problem\n",
      "min\n",
      "\f2Rp1\n",
      "nnX\n",
      "i=1\u0000yi\u0000x>\n",
      "k\fk2; (6.3)\n",
      "which, in the notation of Chapter 5, further simpliﬁes to\n",
      "min\n",
      "\f2Rp1\n",
      "k\fk2: (6.4)\n",
      "In other words, the solution to (6.2) is of the form x7!x>\f\u0003, where\f\u0003solves (6.3) (or\n",
      "!1 , the regularization term becomes dominant and\n",
      "consequently the optimal gbecomes identically zero.\n",
      "The optimization problem in (6.4) is convex, and by multiplying by the constant n=2\n",
      "and setting the gradient equal to zero, we obtain\n",
      "\f=0: (6.5)\n",
      "=0 these are simply the normal equations , albeit written in a slightly di \u000berent form. + 28\n",
      " >0; see Exercise 13), is the case for any \n",
      "then the solution to these modiﬁed normal equations is\n",
      "Ip)\u00001X>y:\n",
      "When using regularization with respect to some Hilbert space G, it is sometimes useful\n",
      "to decomposeGinto two orthogonal subspaces, HandCsay, such that every g2G can\n",
      "be uniquely written as g=h+c, with h2H,c2C, andhh;ci=0. Such aGis said to be\n",
      "thedirect sum direct sum ofCandH, and we writeG=C . Decompositions of this form become\n",
      "useful when functions in Hare penalized but functions in Care not. We illustrate this\n",
      "decomposition with the ridge regression example where one of the features is a constant\n",
      "term, which we do not wish to penalize.\n",
      "Example 6.2 (Ridge Regression (cont.)) Suppose one of the features in Example 6.1\n",
      "is the constant 1, which we do not wish to penalize. The reason for this is to ensure that\n",
      "!1 , the optimal gbecomes the “constant” model, g(x)=\f0, rather than the\n",
      "“zero” model, g(x)=0. Let us alter the notation slightly by considering the feature vectors\n",
      "to be of the form ex=[1;x>]>, where x=[x1;:::; xp]>. We thus have p+1 features, rather218 6.2. Regularization\n",
      "than p. LetGbe the space of linear functions of ex. Each linear function gofexcan be\n",
      "written as g:ex7!\f0+x>\f, which is the sum of the constant function c:ex7!\f0and\n",
      "h:ex7!x>\f. Moreover, the two functions are orthogonal with respect to the inner product\n",
      "onG:hc;hi=[\f0;0>][0;\f>]>=0, where 0is a column vector of zeros.\n",
      "As subspaces ofG, bothCandHare again Hilbert spaces, and their inner products and\n",
      "norms follow directly from the inner product on G. For example, each function h:ex7!\n",
      "x>\finHhas normkhkH=k\fk, and the constant function c:ex7!\f0inChas normj\f0j.\n",
      "The modiﬁcation of the regularized optimization problem (6.2) where the constant term\n",
      "is not penalized can now be written as\n",
      "min\n",
      "g2C1\n",
      "nnX\n",
      "kgk2yi\u0000g(exi))2+\n",
      "H; (6.6)\n",
      "which further simpliﬁes to\n",
      "min\n",
      "\f0;\f1\n",
      "k\fk2; (6.7)2+\n",
      "!1 the optimal gtends to of 1s. Observe that, in this case, as \n",
      "the sample mean yof thefyig; that is, we obtain the “default” regression model, without ex-\n",
      "planatory variables. Again, this is a convex optimization problem, and the solution follows\n",
      "from\n",
      "\f=0; (6.8)y)+n\n",
      "with\n",
      "n\f0=1>(y\u0000X\f): (6.9)\n",
      "This results in solving for \ffrom\n",
      "Ip)\f=(X>\u0000n\u00001X>11>)y; (6.10)\n",
      "and determining \f0from (6.9).\n",
      "As a precursor to the kernel methods in the following sections, let us assume that n>p\n",
      "and that Xhas full (column) rank p. Then any vector \f2Rpcan be written as a linear\n",
      "combination of the feature vectors fxig; that is, as linear combinations of the columns of\n",
      "the matrix X>. In particular, let \f=X>\u000b, where\u000b=[\u000b1;:::;\u000b n]>2Rn. In this case (6.10)\n",
      "reduces to\n",
      "In)\u000b=(In\u0000n\u0000111>)y:\n",
      "In), we have the solution ( XX>\u0000n\u0000111>XX>+n\n",
      "In)\u00001(In\u0000n\u0000111>)y;n\n",
      "which depends on the training feature vectors fxigonly through the n\u0002nmatrix of inner\n",
      "products: XX>=[hxi;xji]. This matrix is called the Gram matrix Gram matrix of thefxig. From (6.9),\n",
      "the solution for the constant term is b\f0=n\u000011>(y\u0000XX>b\u000b). It follows that the learner is a\n",
      "linear combination of inner products fhxi;xigplus a constant:\n",
      "g\u001c(ex)=b\f0+x>X>b\u000b=b\f0+nX\n",
      "i=1b\u000bihxi;xi;Chapter 6. Regularization and Kernel Methods 219\n",
      "where the coe \u000ecientsb\f0andb\u000bionly depend on the inner products fhxi;xjig. We will see\n",
      "shortly that the representer Theorem 6.6 generalizes this result to a broad class of regular- +232\n",
      "ized optimization problems.\n",
      "We illustrate in Figure 6.1 how the solutions of the ridge regression problems appearing\n",
      "for aamples 6.1 and 6.2 are qualitatively a \u000bected by the regularization parameter \n",
      "simple linear regression model. The data was generated from the model yi=\u00001:5+0:5xi+\n",
      "\"i,i=1;:::; 100, where each xiis drawn independently and uniformly from the interval\n",
      "[0;10] and each \"iis drawn independently from the standard normal distribution.\n",
      ".= 0 :1\n",
      "-2-1012-1.= 1 .= 10\n",
      "-2 0 2\n",
      "-0-2-1012-1\n",
      "-2 0 2\n",
      "-0-2 0 2\n",
      "-0\n",
      "Figure 6.1: Ridge regression solutions for a simple linear regression problem. Each panel\n",
      "shows contours of the loss function (log scale) and the e \u000bect of the regularization parameter\n",
      "2f0:1;1;10g, appearing in (6.4) and (6.7). Top row: both terms are penalized. Bottom\n",
      "row: only the non-constant term is penalized. Penalized (plus) and unpenalized (diamond)\n",
      "solutions are shown in each case.\n",
      "The contours are those of the squared-error loss (actually the logarithm thereof), which\n",
      "is minimized with respect to the model parameters \f0and\f1. The diamonds all repres-\n",
      "ent the same minimizer of this loss. The plusses show each minimizer [ \f\u0003\n",
      "0;\f\u0003\n",
      "1]>of the\n",
      "regularized minimization problems (6.4) and (6.7) for three choices of the regularization\n",
      ". For the top three panels the regularization involves both \f0and\f1, through\n",
      "the squared norm \f2\n",
      "0+\f2\n",
      "1. The circles show the points that have the same squared norm as220 6.2. Regularization\n",
      "the optimal solution. For the bottom three panels only \f1is regularized; there, horizontal\n",
      "lines indicate vectors [ \f0;\f1]>for whichj\f1j=j\f\u0003\n",
      "1j.\n",
      "The problem of ridge regression discussed in Example 6.2 boils down to solving a\n",
      "problem of the form in (6.7), involving a squared 2-norm penalty k\fk2. A natural ques-\n",
      "tion to ask is whether we can replace the squared 2-norm penalty by a di \u000berent penalty\n",
      "term. Replacing it with a 1-norm gives the lasso (least absolute shrinkage and selection +410\n",
      "lasso operator). The lasso equivalent of the ridge regression problem (6.7) is thus:\n",
      "min\n",
      "\f0;\f1\n",
      "k\fk1; (6.11)+\n",
      "wherek\fk1=Pp\n",
      "i=1j\fij.\n",
      "This is again a convex optimization problem. Unlike ridge regression, the lasso gener-\n",
      "ally does not have an explicit solution, and so numerical methods must be used to solve it.\n",
      "Note that the problem (6.11) is of the form\n",
      "min\n",
      "x;zf(x)+g(z)\n",
      "subject to Ax+Bz=c;(6.12)\n",
      "with x:=[\f0;\f>]>,z:=\f,A:=[0p;Ip],B:=\u0000Ip, and c:=0p(vector of zeros), and\n",
      "convex functions f(x) :=1\n",
      "kzk1. There exist e \u000ecient al-\n",
      "gorithms for solving such problems, including the alternating direction method of mul-\n",
      "tipliers (ADMM) [17]. We refer to Example ??for details on this algorithm. +418\n",
      "We repeat the examples from Figure 6.1, but now using lasso regression and taking\n",
      "the square roots of the previous regularization parameters. The results are displayed in\n",
      "Figure 6.2.Chapter 6. Regularization and Kernel Methods 221\n",
      ".=p\n",
      "0:1\n",
      "-2-1012-1.= 1 .=p\n",
      "10\n",
      "-2 0 2\n",
      "-0-2-1012-1\n",
      "-2 0 2\n",
      "-0-2 0 2\n",
      "-0\n",
      "Figure 6.2: Lasso regression solutions. Compare with Figure 6.1.\n",
      "One advantage of using the lasso regularization is that the resulting optimal parameter\n",
      "vector often has several components that are exactly 0. For example, in the top middle\n",
      "and right panels of Figure 6.2, the optimal solution lies exactly at a corner point of the\n",
      "squaref[\f0;\f1]>:j\f0j+j\f1j=j\f\u0003\n",
      "0j+j\f\u0003\n",
      "1jg; in this case \f\u0003\n",
      "0=0. For statistical models with\n",
      "many parameters, the lasso can provide a methodology for model selection. Namely, as the\n",
      "regularization parameter increases (or, equivalently, as the L1norm of the optimal solution\n",
      "decreases), the solution vector will have fewer and fewer non-zero parameters. By plotting\n",
      "orL1one obtains the so-called regularization paths regularization\n",
      "paths(also called homotopy paths orcoe\u000ecient proﬁles ) for the variables. Inspection of such\n",
      "paths may help assess which of the model parameters are relevant to explain the variability\n",
      "in the observed responses fyig.\n",
      "Example 6.3 (Regularization Paths) Figure 6.3 shows the regularization paths for p=\n",
      "60 coe \u000ecients from a multiple linear regression model +169\n",
      "Yi=60X\n",
      "j=1\fjxi j+\"i;i=1;:::; 150;\n",
      "where\fj=1 for j=1;:::; 10 and\fj=0 for j=11;:::; 60. The error terms f\"igare inde-\n",
      "pendent and standard normal. The explanatory variables fxi jgwere independently generated\n",
      "from a standard normal distribution. As it is clear from the ﬁgure, the estimates of the 10222 6.3. Reproducing Kernel Hilbert Spaces\n",
      "non-zero coe \u000ecients are ﬁrst selected, as the L1norm of the solutions increases. By the\n",
      "time the L1norm reaches around 4, all 10 variables for which \fj=1 have been correctly\n",
      "identiﬁed and the remaining 50 parameters are estimated as exactly 0. Only after the L1\n",
      "norm reaches around 8, will these “spurious” parameters be estimated to be non-zero. For\n",
      "varied from 10\u00004to 10.larization parameter \n",
      "0 5 10 15\n",
      "L1norm-0.500.511.5b-\n",
      "Figure 6.3: Regularization paths for lasso regression solutions as a function of the L1norm\n",
      "of the solutions.\n",
      "6.3 Reproducing Kernel Hilbert Spaces\n",
      "In this section, we formalize the idea outlined at the end of Section 6.1 of extending ﬁnite\n",
      "dimensional feature maps to those that are functions by introducing a special type of Hil-\n",
      "bert space of functions known as a reproducing kernel Hilbert space (RKHS). Although\n",
      "the theory extends naturally to Hilbert spaces of complex-valued functions, we restrict\n",
      "attention to Hilbert spaces of real-valued functions here.\n",
      "To evaluate the loss of a learner gin some class of functions G, we do not need to expli-\n",
      "citly construct g— rather, it is only required that we can evaluate gat all the feature vectors\n",
      "x1;:::; xnof the training set. A deﬁning property of an RKHS is that function evaluation\n",
      "at a point xcan be performed by simply taking the inner product of gwith some feature\n",
      "function\u0014xassociated with x. We will see that this property becomes particularly useful\n",
      "in light of the representer theorem (see Section 6.5), which states that the learner gitself +231\n",
      "can be represented as a linear combination of the set of feature functions f\u0014xi;i=1;:::; ng.\n",
      "Consequently, we can evaluate a learner gat the feature vectors fxigby taking linear com-\n",
      "binations of terms of the form \u0014(xi;xj)=h\u0014xi;\u0014xjiG. Collecting these inner products into\n",
      "a matrix K=[\u0014(xi;xj);i;j=1;:::; n] (the Gram matrix of the f\u0014xig), we will see that the\n",
      "feature vectorsfxigonly enter the loss minimization problem through K.Chapter 6. Regularization and Kernel Methods 223\n",
      "Deﬁnition 6.1: Reproducing Kernel Hilbert Space\n",
      "For a non-empty set X, a Hilbert spaceGof functions g:X!Rwith inner product\n",
      "h\u0001;\u0001iGis called a reproducing kernel Hilbert space reproducing\n",
      "kernel Hilbert\n",
      "space(RKHS) with reproducing kernel\n",
      "\u0014:X\u0002X!Rif:\n",
      "1. for every x2X,\u0014x:=\u0014(x;\u0001) is inG,\n",
      "2.\u0014(x;x)<1for all x2X,\n",
      "3. for every x2Xandg2G,g(x)=hg;\u0014xiG.\n",
      "The reproducing kernel of a Hilbert space of functions, if it exists, is unique; see Exer-\n",
      "cise 2. The main (third) condition in Deﬁnition 6.1 is known as the reproducing property reproducing\n",
      "property.\n",
      "This property allows us to evaluate any function g2Gat a point x2Xby taking the inner\n",
      "product of gand\u0014x; as such,\u0014xis called the representer of evaluation . Further, by taking\n",
      "g=\u0014x0and applying the reproducing property, we have h\u0014x0;\u0014xiG=\u0014(x0;x), and so by sym-\n",
      "metry of the inner product it follows that \u0014(x;x0)=\u0014(x0;x). As a consequence, reproducing\n",
      "kernels are necessarily symmetric functions. Moreover, a reproducing kernel \u0014is apositive\n",
      "semideﬁnite positive\n",
      "semidefinitefunction, meaning that for every n>1 and every choice of \u000b1;:::;\u000b n2Rand\n",
      "x1;:::; xn2X, it holds that\n",
      "nX\n",
      "i=1nX\n",
      "j=1\u000bi\u0014(xi;xj)\u000bj>0: (6.13)\n",
      "In other words, every Gram matrix Kassociated with \u0014is a positive semideﬁnite matrix;\n",
      "that is\u000b>K\u000b>0 for all\u000b. The proof is addressed in Exercise 1.\n",
      "The following theorem gives an alternative characterization of an RKHS. The proof\n",
      "uses the Riesz representation Theorem A.17. Also note that in the theorem below we could +392\n",
      "have replaced the word “bounded” with “continuous”, as the two are equivalent for linear\n",
      "functionals; see Theorem A.16.\n",
      "Theorem 6.1: Continuous Evaluation Functionals Characterize a RKHS\n",
      "An RKHSGon a setXis a Hilbert space in which every evaluation functional evaluation\n",
      "functional\u000ex:g7!g(x) is bounded. Conversely, a Hilbert space Gof functionsX!Rfor\n",
      "which every evaluation functional is bounded is an RKHS.\n",
      "Proof: Note that, since evaluation functionals \u000exare linear operators, showing bounded-\n",
      "ness is equivalent to showing continuity. Given an RKHS with reproducing kernel \u0014, sup-\n",
      "pose that we have a sequence gn2Gconverging to g2G, that iskgn\u0000gkG!0. We apply\n",
      "the Cauchy–Schwarz inequality (Theorem A.15) and the reproducing property of \u0014to ﬁnd +391\n",
      "that for every x2Xand any n:\n",
      "j\u000exgn\u0000\u000exgj=jgn(x)\u0000g(x)j=jhgn\u0000g;\u0014xiGj6kgn\u0000gkGk\u0014xkG=kgn\u0000gkGp\n",
      "h\u0014x;\u0014xiG\n",
      "=kgn\u0000gkGp\n",
      "\u0014(x;x):\n",
      "Noting thatp\u0014(x;x)<1by deﬁnition for every x2X, and thatkgn\u0000gkG!0 asn!1 ,\n",
      "we have shown continuity of \u000ex, that isj\u000exgn\u0000\u000exgj!0 asn!1 for every x2X.224 6.3. Reproducing Kernel Hilbert Spaces\n",
      "Conversely, suppose that evaluation functionals are bounded. Then from the Riesz\n",
      "representation Theorem A.17, there exists some g\u000ex2Gsuch that\u000exg=hg;g\u000exiGfor all\n",
      "g2G— the representer of evaluation. If we deﬁne \u0014(x;x0)=g\u000ex(x0) for all x;x02X, then\n",
      "\u0014x:=\u0014(x;\u0001)=g\u000exis an element ofGfor every x2Xandhg;\u0014xiG=\u000exg=g(x), so that the\n",
      "reproducing property in Deﬁnition 6.1 is veriﬁed. \u0003\n",
      "The fact that an RKHS has continuous evaluation functionals means that if two func-\n",
      "tions g;h2Gare “close” with respect to k\u0001kG, then their evaluations g(x);h(x) are close\n",
      "for every x2X. Formally, convergence in k\u0001kGnorm implies pointwise convergence for\n",
      "allx2X.\n",
      "The following theorem shows that any ﬁnite function \u0014:X\u0002X!Rcan serve as a\n",
      "reproducing kernel as long as it is ﬁnite, symmetric, and positive semideﬁnite. The cor-\n",
      "responding (unique!) RKHS Gis the completion of the set of all functions of the formPn\n",
      "i=1\u000bi\u0014xiwhere\u000bi2Rfor all i=1;:::; n.\n",
      "Theorem 6.2: Moore–Aronszajn\n",
      "Given a non-empty set Xand any ﬁnite symmetric positive semideﬁnite function\n",
      "\u0014:X\u0002X!R, there exists an RKHS Gof functions g:X!Rwith reproducing\n",
      "kernel\u0014. Moreover,Gis unique.\n",
      "Proof: (Sketch) As the proof of uniqueness is treated in Exercise 2, the objective is to\n",
      "prove existence. The idea is to construct a pre-RKHS G0from the given function \u0014that has\n",
      "the essential structure and then to extend G0to an RKHSG.\n",
      "In particular, deﬁne G0as the set of ﬁnite linear combinations of functions \u0014x,x2X:\n",
      "G0:=\u001a\n",
      "g=nX\n",
      "i=1\u000bi\u0014xi\f\f\f\f\fx1;:::; xn2X; \u000bi2R;n2N\u001b\n",
      ":\n",
      "Deﬁne onG0the following inner product:\n",
      "hf;giG0:=*nX\n",
      "i=1\u000bi\u0014xi;mX\n",
      "j=1\fj\u0014x0\n",
      "j+\n",
      "G0:=nX\n",
      "i=1mX\n",
      "j=1\u000bi\fj\u0014(xi;x0\n",
      "j):\n",
      "ThenG0is an inner product space. In fact, G0has the essential structure we require, namely\n",
      "that (i) evaluation functionals are bounded /continuous (Exercise 4) and (ii) Cauchy se-\n",
      "quences inG0that converge pointwise also converge in norm (see Exercise 5).\n",
      "We then enlargeG0to the setGof all functions g:X!Rfor which there exists a\n",
      "Cauchy sequence in G0converging pointwise to gand deﬁne an inner product on Gas the\n",
      "limit\n",
      "hf;giG:=lim\n",
      "n!1hfn;gniG0; (6.14)\n",
      "where fn!fandgn!g. To show thatGis an RKHS it remains to be shown that (1) this\n",
      "inner product is well deﬁned; (2) evaluation functionals remain bounded; and (3) the space\n",
      "Gis complete. A detailed proof is established in Exercises 6 and 7. \u0003Chapter 6. Regularization and Kernel Methods 225\n",
      "6.4 Construction of Reproducing Kernels\n",
      "In this section we describe various ways to construct a reproducing kernel \u0014:X\u0002X!\n",
      "Rfor some feature space X. Recall that \u0014needs to be a ﬁnite, symmetric, and positive\n",
      "semideﬁnite function (that is, it satisﬁes (6.13)). In view of Theorem 6.2, specifying the\n",
      "spaceXand a reproducing kernel \u0014:X\u0002X!Rcorresponds to uniquely specifying an\n",
      "RKHS.\n",
      "6.4.1 Reproducing Kernels via Feature Mapping\n",
      "Perhaps the most fundamental way to construct a reproducing kernel \u0014is via a feature\n",
      "map\u001e:X!Rp. We deﬁne \u0014(x;x0) :=h\u001e(x);\u001e(x0)i, whereh;idenotes the Euclidean\n",
      "inner product. The function is clearly ﬁnite and symmetric. To verify that \u0014is positive\n",
      "semideﬁnite, letbe the matrix with rows \u001e(x1)>;:::;\u001e(xn)>and let\u000b=[\u000b1;:::;\u000b n]>2\n",
      "Rn. Then,\n",
      "nX\n",
      "i=1nX\n",
      "j=1\u000bi\u0014(xi;xj)\u000bj=nX\n",
      "i=1nX\n",
      "j=1\u000bi\u001e>(xi)\u001e(xj)\u000bj=>\u000b=>\u000bk2>0:\n",
      "Example 6.4 (Linear Kernel) Taking the identity feature map \u001e(x)=xonX=Rp,\n",
      "gives the linear kernel linear kernel\n",
      "\u0014(x;x0)=hx;x0i=x>x0:\n",
      "As can be seen from the proof of Theorem 6.2, the RKHS of functions corresponding to\n",
      "the linear kernel is the space of linear functions on Rp. This space is isomorphic to Rp\n",
      "itself, as discussed in the introduction (see also Exercise 12).\n",
      "It is natural to wonder whether a given kernel function corresponds uniquely to a feature\n",
      "map. The answer is no, as we shall see by way of example.\n",
      "Example 6.5 (Feature Maps and Kernel Functions) LetX=Rand consider feature\n",
      "maps\u001e1:X!Rand\u001e2:X!R2, with\u001e1(x) :=xand\u001e2(x) :=[x;x]>=p\n",
      "2. Then\n",
      "\u0014\u001e1(x;x0)=h\u001e1(x);\u001e1(x0)i=xx0;\n",
      "but also\n",
      "\u0014\u001e2(x;x0)=h\u001e2(x);\u001e2(x0)i=xx0:\n",
      "Thus, we arrive at the same kernel function deﬁned for the same underlying set Xvia two\n",
      "di\u000berent feature maps.\n",
      "6.4.2 Kernels from Characteristic Functions\n",
      "Another way to construct reproducing kernels on X=Rpmakes use of the properties of\n",
      "characteristic functions . In particular, we have the following result. We leave its proof as +443\n",
      "Exercise 10.226 6.4. Construction of Reproducing Kernels\n",
      "Theorem 6.3: Reproducing Kernel from a Characteristic Function\n",
      "LetX\u0018\u0016be anRp-valued random vector that is symmetric about the origin (that\n",
      "is,Xand\u0000Xare identically distributed), and let  be its characteristic function:\n",
      " (t)=Eeit>X=R\n",
      "eit>x\u0016(dx) for t2Rp. Then\u0014(x;x0) := (x\u0000x0) is a valid repro-\n",
      "ducing kernel on Rp.\n",
      "Example 6.6 (Gaussian Kernel) The multivariate normal distribution with mean vec-\n",
      "tor0and covariance matrix b2Ipis clearly symmetric around the origin. Its characteristic\n",
      "function is\n",
      " (t)=exp \n",
      "\u00001\n",
      "2b2ktk2!\n",
      ";t2Rp:\n",
      "Taking b2=1=\u001b2, this gives the popular Gaussian kernel Gaussian\n",
      "kernelonRp:\n",
      "\u0014(x;x0)=exp \n",
      "\u00001\n",
      "2kx\u0000x0k2\n",
      "\u001b2!\n",
      ": (6.15)\n",
      "The parameter \u001bis sometimes called the bandwidth bandwidth . Note that in the machine learning\n",
      "literature, the Gaussian kernel is sometimes referred to as “the” radial basis function (rbf)\n",
      "kernel radial basis\n",
      "function (rbf)\n",
      "kernel.1\n",
      "From the proof of Theorem 6.2, we see that the RKHS Gdetermined by the Gaussian\n",
      "kernel\u0014is the space of pointwise limits of functions of the form\n",
      "g(x)=nX\n",
      "i=1\u000biexp \n",
      "\u00001\n",
      "2kx\u0000xik2\n",
      "\u001b2!\n",
      ":\n",
      "We can think of each point xihaving a feature \u0014xithat is a scaled multivariate Gaussian pdf\n",
      "centered at xi.\n",
      "Example 6.7 (Sinc Kernel) The characteristic function of a Uniform [\u00001;1] random\n",
      "variable (which is symmetric around 0) is  (t)=sinc( t) :=sin(t)=t, so\u0014(x;x0)=sinc( x\u0000x0)\n",
      "is a valid kernel.\n",
      "Inspired by kernel density estimation (Section 4.4), we may be tempted to use the pdf +131\n",
      "of a random variable that is symmetric about the origin to construct a reproducing kernel.\n",
      "However, doing so will not work in general, as the next example illustrates.\n",
      "Example 6.8 (Uniform pdf Does not Construct a Valid Reproducing Kernel) Take\n",
      "the function  (t)=1\n",
      "21fjtj61g, which is the pdf of X\u0018Uniform [\u00001;1]. Unfortunately, the\n",
      "function\u0014(x;x0)= (x\u0000x0) is not positive semideﬁnite, as can be seen for example by\n",
      "constructing the matrix A=[\u0014(ti;tj);i;j=1;2;3] for the points t1=0,t2=0:75, and\n",
      "t3=1:5 as follows:\n",
      "A=0BBBBBBBB@ (0) (\u00000:75) (\u00001:5)\n",
      " (0:75) (0) (\u00000:75)\n",
      " (1:5) (0:75) (0)1CCCCCCCCA=0BBBBBBBB@0:5 0:5 0\n",
      "0:5 0:5 0:5\n",
      "0 0:5 0:51CCCCCCCCA:Chapter 6. Regularization and Kernel Methods 227\n",
      "The eigenvalues of Aaref1=2\u0000p1=2;1=2;1=2+p1=2g\u0019f\u0000 0:2071;0:5;1:2071gand so\n",
      "by Theorem A.9, Ais not a positive semideﬁnite matrix, since it has a negative eigenvalue. +369\n",
      "Consequently, \u0014is not a valid reproducing kernel.\n",
      "One of the reasons why the Gaussian kernel (6.15) is popular is that it enjoys the uni-\n",
      "versal approximation property universal\n",
      "approximation\n",
      "property[88]: the space of functions spanned by the Gaussian kernel\n",
      "is dense in the space of continuous functions with support Z\u001aRp. Naturally, this is a\n",
      "desirable property especially if there is little prior knowledge about the properties of g\u0003.\n",
      "However, note that every function gin the RKHSGassociated with a Gaussian kernel \u0014is\n",
      "inﬁnitely di \u000berentiable. Moreover, a Gaussian RKHS does not contain non-zero constant\n",
      "functions. Indeed, if A\u001aZ is non-empty and open, then the only function of the form\n",
      "g(x)=c1fx2Agcontained inGis the zero function ( c=0).\n",
      "Consequently, if it is known that gis di\u000berentiable only to a certain order, one may\n",
      "prefer the Matérn kernel Mat´ern kernel with parameters \u0017;\u001b> 0:\n",
      "\u0014\u0017(x;x0)=21\u0000\u0017\n",
      "\u0000(\u0017)\u0010p\n",
      "2\u0017kx\u0000x0k=\u001b\u0011\u0017K\u0017\u0010p\n",
      "2\u0017kx\u0000x0k=\u001b\u0011\n",
      "; (6.16)\n",
      "which gives functions that are (weakly) di \u000berentiable to order b\u0017c(but not necessarily to\n",
      "orderd\u0017e). Here, K\u0017denotes the modiﬁed Bessel function of the second kind; see (4.49).\n",
      "The particular form of the Matérn kernel appearing in (6.16) ensures that lim \u0017!1\u0014\u0017(x;x0)= +163\n",
      "\u0014(x;x0), where\u0014is the Gaussian kernel appearing in (6.15).\n",
      "We remark that Sobolev spaces are closely related to the Matérn kernel. Up to constants\n",
      "(which scale the unit ball in the space), in dimension pand for a parameter s>p=2, these\n",
      "spaces can be identiﬁed with  (t)=21\u0000s\n",
      "\u0000(s)ktks\u0000p=2Kp=2\u0000s(ktk), which in turn can be viewed as\n",
      "the characteristic function corresponding to the (radially symmetric) multivariate Student’s\n",
      "tdistribution with sdegrees of freedom: that is, with pdf f(x)/(1+kxk2)\u0000s. +162\n",
      "6.4.3 Reproducing Kernels Using Orthonormal Features\n",
      "We have seen in Sections 6.4.1 and 6.4.2 how to construct reproducing kernels from feature\n",
      "maps and characteristic functions. Another way to construct kernels on a space Xis to work\n",
      "directly from the function class L2(X;\u0016); that is, the set of square-integrable2functions\n",
      "onXwith respect to \u0016; see also Deﬁnition A.4. For simplicity, in what follows, we will +387\n",
      "consider\u0016to be the Lebesgue measure, and will simply write L2(X) rather than L2(X;\u0016).\n",
      "We will also assume that X\u0012Rp.\n",
      "Letf\u00181;\u00182;:::gbe an orthonormal basis of L2(X) and let c1;c2;:::be a sequence of\n",
      "positive numbers. As discussed in Section 6.4.1, the kernel corresponding to a feature map\n",
      "\u001e:X!Rpis\u0014(x;x0)=\u001e(x)>\u001e(x0)=Pp\n",
      "i=1\u001ei(x)\u001ei(x0). Now consider a (possibly inﬁnite)\n",
      "sequence of feature functions \u001ei=ci\u0018i;i=1;2;:::and deﬁne\n",
      "\u0014(x;x0) :=X\n",
      "i>1\u001ei(x)\u001ei(x0)=X\n",
      "i>1\u0015i\u0018i(x)\u0018i(x0); (6.17)\n",
      "1The term radial basis function is sometimes used more generally to mean kernels of the form \u0014(x;x0)=\n",
      "f(kx\u0000x0k) for some function f:R!R.\n",
      "2A function f:X!Ris said to be square-integrable ifR\n",
      "f2(x)\u0016(dx)<1, where\u0016is a measure onX.228 6.4. Construction of Reproducing Kernels\n",
      "where\u0015i=c2\n",
      "i;i=1;2;:::. This is well-deﬁned as long asP\n",
      "i>1\u0015i<1, which we assume\n",
      "from now on. Let Hbe the linear space of functions of the form f=P\n",
      "i>1\u000bi\u0018i, whereP\n",
      "i>1\u000b2\n",
      "i=\u0015i<1. As every function f2L2(X) can be represented as f=P\n",
      "i>1hf;\u0018ii\u0018i, we\n",
      "see thatHis a linear subspace of L2(X). OnHdeﬁne the inner product\n",
      "hf;giH:=X\n",
      "i>1hf;\u0018iihg;\u0018ii\n",
      "\u0015i:\n",
      "With this inner product, the squared norm of f=P\n",
      "i>1\u000bi\u0018iiskfk2\n",
      "H=P\n",
      "i>1\u000b2\n",
      "i=\u0015i<1.\n",
      "We show thatHis actually an RKHS with kernel \u0014by verifying the conditions of Deﬁni-\n",
      "tion 6.1. First,\n",
      "\u0014x=X\n",
      "i>1\u0015i\u0018i(x)\u0018i2H;\n",
      "asP\n",
      "i\u0015i<1by assumption, and so \u0014is ﬁnite. Second, the reproducing property holds.\n",
      "Namely, let f=P\n",
      "i>1\u000bi\u0018i. Then,\n",
      "h\u0014x;fiH=X\n",
      "i>1h\u0014x;\u0018iihf;\u0018ii\n",
      "\u0015i=X\n",
      "i>1\u0015i\u0018i(x)\u000bi\n",
      "\u0015i=X\n",
      "i>1\u000bi\u0018i(x)=f(x):\n",
      "The discussion above demonstrates that kernels can be constructed via (6.17). In fact,\n",
      "(under mild conditions) any given reproducing kernel \u0014can be written in the form (6.17),\n",
      "where this series representation enjoys desirable convergence properties. This result is\n",
      "known as Mercer’s theorem, and is given below. We leave the full proof including the\n",
      "precise conditions to, e.g., [40], but the main idea is that a reproducing kernel \u0014can be\n",
      "thought of as a generalization of a positive semideﬁnite matrix K, and can also be writ-\n",
      "ten in spectral form (see also Section A.6.5). In particular, by Theorem A.9, we can write +369\n",
      "K=VDV>, where Vis a matrix of orthonormal eigenvectors [ v`] and Dthe diagonal\n",
      "matrix of the (positive) eigenvalues [ \u0015`]; that is,\n",
      "K(i;j)=X\n",
      "`>1\u0015`v`(i)v`(j):\n",
      "In (6.18) below, x;x0play the role of i;j, and\u0018`plays the role of v`.\n",
      "Theorem 6.4: Mercer\n",
      "Let\u0014:X\u0002X !Rbe a reproducing kernel for a compact set X \u001aRp. Then\n",
      "(under mild conditions) there exists a countable sequence of non-negative numbers\n",
      "f\u0015`gdecreasing to zero and functions f\u0018`gorthonormal in L2(X) such that\n",
      "\u0014(x;x0)=X\n",
      "`>1\u0015`\u0018`(x)\u0018`(x0); for all x;x02X; (6.18)\n",
      "where (6.18) converges absolutely and uniformly on X\u0002X .\n",
      "Further, if\u0015`>0, then (\u0015`;\u0018`) is an (eigenvalue, eigenfunction) pair for the integral\n",
      "operator K:L2(X)!L2(X) deﬁned by [ K f](x) :=R\n",
      "X\u0014(x;y)f(y) dyforx2X.Chapter 6. Regularization and Kernel Methods 229\n",
      "Theorem 6.4 holds if (i) the kernel \u0014is continuous onX\u0002X , (ii) the function e\u0014(x) :=\n",
      "\u0014(x;x) deﬁned for x2Xis integrable. Extensions of Theorem 6.4 to more general spaces\n",
      "Xand measures \u0016hold; see, e.g., [115] or [40].\n",
      "The key importance of Theorem 6.4 lies in the fact that the series representation (6.18)\n",
      "converges absolutely and uniformly on X\u0002X . The uniform convergence is a much stronger\n",
      "condition than pointwise convergence, and means for instance that properties of the se-\n",
      "quence of partial sums, such as continuity and integrability, are transferred to the limit.\n",
      "Example 6.9 (Mercer) SupposeX=[\u00001;1] and the kernel is \u0014(x;x0)=1+xx0which\n",
      "corresponds to the RKHS Gof a\u000ene functions from X !R. To ﬁnd the (eigenvalue,\n",
      "eigenfunction) pairs for the integral operator appearing in Theorem 6.4, we need to ﬁnd\n",
      "numbersf\u0015`gand orthonormal functions f\u0018`(x)gthat solve\n",
      "Z1\n",
      "\u00001(1+xx0)\u0018`(x0) dx0=\u0015`\u0018`(x);for all x2[\u00001;1]:\n",
      "Consider ﬁrst a constant function \u00181(x)=c. Then, for all x2[\u00001;1], we have that 2 c=\u00151c,\n",
      "and the normalization condition requires thatR1\n",
      "\u00001c2dx=1. Together, these give \u00151=2 and\n",
      "c=\u00061=p\n",
      "2. Next, consider an a \u000ene function \u00182(x)=a+bx. Orthogonality requires that\n",
      "Z1\n",
      "\u00001c(a+bx) dx=0;\n",
      "which implies a=0 (since c,0). Moreover, the normalization condition then requires\n",
      "Z1\n",
      "\u00001b2x2dx=1;\n",
      "or, equivalently, 2 b2=3=1, implying b=\u0006p3=2. Finally, the integral equation reads\n",
      "Z1\n",
      "\u00001(1+xx0)bx0dx0=\u00152bx()2bx\n",
      "3=\u00152bx;\n",
      "implying that \u00152=2=3. We take the positive solutions (i.e., c>0 and b>0), and note that\n",
      "\u00151\u00181(x)\u00181(x0)+\u00152\u00182(x)\u00182(x0)=21p\n",
      "21p\n",
      "2+2\n",
      "3p\n",
      "3p\n",
      "2xp\n",
      "3p\n",
      "2x0=1+xx0=\u0014(x;x0);\n",
      "and so we have found the decomposition appearing in (6.18). As an aside, observe that \u00181\n",
      "and\u00182are orthonormal versions of the ﬁrst two Legendre polynomials. The corresponding +389\n",
      "feature map can be explicitly identiﬁed as \u001e1(x)=p\n",
      "\u00151\u00181(x)=1 and\u001e2(x)=p\u00152\u00182(x)=\n",
      "x.\n",
      "6.4.4 Kernels from Kernels\n",
      "The following theorem lists some useful properties for constructing reproducing kernels\n",
      "from existing reproducing kernels.230 6.4. Construction of Reproducing Kernels\n",
      "Theorem 6.5: Rules for Constructing Kernels from Other Kernels\n",
      "1. If\u0014:Rp\u0002Rp!Ris a reproducing kernel and \u001e:X!Rpis a function, then\n",
      "\u0014(\u001e(x);\u001e(x0)) is a reproducing kernel from X\u0002X!R.\n",
      "2. If\u0014:X\u0002X!Ris a reproducing kernel and f:X!R+is a function, then\n",
      "f(x)\u0014(x;x0)f(x0) is also a reproducing kernel from X\u0002X!R.\n",
      "3. If\u00141and\u00142are reproducing kernels from X\u0002X!R, then so is their sum \u00141+\u00142.\n",
      "4. If\u00141and\u00142are reproducing kernels from X\u0002X!R, then so is their product\n",
      "\u00141\u00142.\n",
      "5. If\u00141and\u00142are reproducing kernels from X\u0002X !RandY\u0002Y ! Rre-\n",
      "spectively, then \u0014+((x;y);(x0;y0)) :=\u00141(x;x0)+\u00142(y;y0) and\u0014\u0002((x;y);(x0;y0)) :=\n",
      "\u00141(x;x0)\u00142(y;y0) are reproducing kernels from ( X\u0002Y )\u0002(X\u0002Y )!R.\n",
      "Proof: For Rules 1, 2, and 3 it is easy to verify that the resulting function is ﬁnite, sym-\n",
      "metric, and positive semideﬁnite, and so is a valid reproducing kernel by Theorem 6.2.\n",
      "For example, for Rule 1 we havePn\n",
      "i=1Pn\n",
      "j=1\u000bi\u0014(yi;yj)\u000bj>0 for every choice of f\u000bign\n",
      "i=1\n",
      "andfyign\n",
      "i=12Rp, since\u0014is a reproducing kernel. In particular, it holds true for yi=\u001e(xi),\n",
      "i=1;:::; n. Rule 4 is easy to show for kernels \u00141;\u00142that admit a representation of the form\n",
      "(6.17), since\n",
      "\u00141(x;x0)\u00142(x;x0)=0BBBBB@X\n",
      "i>1\u001e(1)\n",
      "i(x)\u001e(1)\n",
      "i(x0)1CCCCCA0BBBBBB@X\n",
      "j>1\u001e(2)\n",
      "j(x)\u001e(2)\n",
      "j(x0)1CCCCCCA\n",
      "=X\n",
      "i;j>1\u001e(1)\n",
      "i(x)\u001e(2)\n",
      "j(x)\u001e(1)\n",
      "i(x0)\u001e(2)\n",
      "j(x0)\n",
      "=X\n",
      "k>1\u001ek(x)\u001ek(x0)=:\u0014(x;x0);\n",
      "showing that \u0014=\u00141\u00142also admits a representation of the form (6.17), where the new (pos-\n",
      "sibly inﬁnite) sequence of features ( \u001ek) is identiﬁed in a one-to-one way with the sequence\n",
      "(\u001e(1)\n",
      "i\u001e(2)\n",
      "j). We leave the proof of rule 5 as an exercise (Exercise 8). \u0003\n",
      "Example 6.10 (Polynomial Kernel) Consider x;x02R2with\n",
      "\u0014(x;x0)=(1+hx;x0i)2;\n",
      "wherehx;x0i=x>x0. This is an example of a polynomial kernel polynomial\n",
      "kernel. Combining the fact that\n",
      "sums and products of kernels are again kernels (rules 3 and 4 of Theorem 6.5), we ﬁnd that,\n",
      "sincehx;x0iand the constant function 1 are kernels, so are 1 +hx;x0iand (1 +hx;x0i)2. By\n",
      "writing\n",
      "\u0014(x;x0)=(1+x1x0\n",
      "1+x2x0\n",
      "2)2\n",
      "=1+2x1x0\n",
      "1+2x2x0\n",
      "2+2x1x2x0\n",
      "1x0\n",
      "2+(x1x0\n",
      "1)2+(x2x0\n",
      "2)2;Chapter 6. Regularization and Kernel Methods 231\n",
      "we see that\u0014(x;x0) can be written as the inner product in R6of the two feature vectors \u001e(x)\n",
      "and\u001e(x0), where the feature map \u001e:R2!R6can be explicitly identiﬁed as\n",
      "\u001e(x)=[1;p\n",
      "2x1;p\n",
      "2x2;p\n",
      "2x1x2;x2\n",
      "1;x2\n",
      "2]>:\n",
      "Thus, the RKHS determined by \u0014can be explicitly identiﬁed with the space of functions\n",
      "x7!\u001e(x)>\ffor some\f2R6.\n",
      "In the above example we could explicitly identify the feature map. However, in general\n",
      "a feature map need not be explicitly available. Using a particular reproducing kernel cor-\n",
      "responds to using an implicit (possibly inﬁnite dimensional!) feature map that never needs\n",
      "to be explicitly computed.\n",
      "6.5 Representer Theorem\n",
      "Recall the setting discussed at the beginning of this chapter: we are given training data\n",
      "\u001c=f(xi;yi)gn\n",
      "i=1and a loss function that measures the ﬁt to the data, and we wish to ﬁnd\n",
      "a function gthat minimizes the training loss, with the addition of a regularization term,\n",
      "as described in Section 6.2. To do this, we assume ﬁrst that the class Gof prediction\n",
      "functions can be decomposed as the direct sum of an RKHS H, deﬁned by a kernel function\n",
      "\u0014:X\u0002X!R, and another linear space of real-valued functions H0onX; that is,\n",
      "G=H 0;\n",
      "meaning that any element g2Gcan be written as g=h+h0, with h2H andh02H 0.\n",
      "In minimizing the training loss we wish to penalize the hterm of gbut not the h0term.\n",
      "Speciﬁcally, the aim is to solve the functional optimization problem\n",
      "min\n",
      "g2H 01\n",
      "nnX\n",
      "kgk2oss( yi;g(xi))+\n",
      "H: (6.19)\n",
      "Here, we use a slight abuse of notation: kgkHmeanskhkHifg=h+h0, as above. In this\n",
      "way, we can view H0as the null space of the functional g7!kgkH. This null space may be\n",
      "empty, but typically has a small dimension m; for example it could be the one-dimensional\n",
      "space of constant functions, as in Example 6.2. +217\n",
      "Example 6.11 (Null Space) Consider again the setting of Example 6.2, for which we\n",
      "have feature vectors ex=[1;x>]>andGconsists of functions of the form g:ex7!\f0+x>\f.\n",
      "Each function gcan be decomposed as g=h+h0, where h:ex7!x>\f, and h0:ex7!\f0.\n",
      "Given g2G, we havekgkH=k\fk, and so the null space H0of the functional g7!kgkH\n",
      "(that is, the set of all functions g2Gfor whichkgkH=0) is the set of constant functions\n",
      "here, which has dimension m=1.\n",
      "Regularization favors elements in H0and penalizes large elements in H. As the reg-\n",
      "varies between zero and inﬁnity, solutions to (6.19) vary from\n",
      "“complex” ( g2H 0) to “simple” ( g2H 0).\n",
      "A key reason why RKHSs are so useful is the following. By choosing Hto be an\n",
      "RKHS in (6.19) this functional optimization problem e \u000bectively becomes a parametric232 6.5. Representer Theorem\n",
      "optimization problem. The reason is that any solution to (6.19) can be represented as a\n",
      "ﬁnite-dimensional linear combination of kernel functions, evaluated at the training sample.\n",
      "This is known as the kernel trick kernel trick .\n",
      "Theorem 6.6: Representer Theorem\n",
      "The solution to the penalized optimization problem (6.19) is of the form\n",
      "g(x)=nX\n",
      "i=1\u000bi\u0014(xi;x)+mX\n",
      "j=1\u0011jqj(x); (6.20)\n",
      "wherefq1;:::; qmgis a basis ofH0.\n",
      "Proof: LetF=Spa\u0014xi;i=1;:::; n\t. Clearly,F\u0012H . Then, the Hilbert space Hcan\n",
      "be represented as H=F?, whereF?is the orthogonal complement of F. In other\n",
      "words,F?is the class of functions\n",
      "ff?2H :hf?;fiH=0;f2Fg\u0011f f?:hf?;\u0014xiiH=0;8ig:\n",
      "It follows, by the reproducing kernel property, that for all f?2F?:\n",
      "f?(xi)=hf?;\u0014xiiH=0;i=1;:::; n:\n",
      "Now, take any g2H 0, and write it as g=f+f?+h0, with f2F;f?2F?, and\n",
      "h02H 0. By the deﬁnition of the null space H0, we havekgk2\n",
      "H=kf+f?k2\n",
      "H. Moreover, by\n",
      "Pythagoras’ theorem, the latter is equal to kfk2\n",
      "H+kf?k2\n",
      "H. It follows that\n",
      "1\n",
      "nnX\n",
      "kgk2oss( yi;g(xi))+\n",
      "H=1\n",
      "nnX\n",
      "\u0010=1Loss( yi;f(xi)+h0(xi))+\n",
      "kfk2\n",
      "H+kf?k2\n",
      "H\u0011\n",
      ">1\n",
      "nnX\n",
      "kfk2oss( yi;f(xi)+h0(xi))+\n",
      "H:\n",
      "Since we can obtain equality by taking f?=0, this implies that the minimizer of the pen-\n",
      "alized optimization problem (6.19) lies in the subspace H 0ofG=H 0, and hence\n",
      "is of the form (6.20). \u0003\n",
      "Substituting the representation (6.20) of ginto (6.19) gives the ﬁnite-dimensional op-\n",
      "timization problem:\n",
      "min\n",
      "\u000b2Rn;\u00112Rm1\n",
      "nnX\n",
      "\u000b>K\u000b; (6.21)(K\u000b+Q\u0011)i)+\n",
      "where\n",
      "Kis the n\u0002n(Gram) matrix with entries [ \u0014(xi;xj);i=1;:::; n;j=1;:::; n].\n",
      "Qis the n\u0002mmatrix with entries [ qj(xi);i=1;:::; n;j=1;:::; m].Chapter 6. Regularization and Kernel Methods 233\n",
      "In particular, for the squared-error loss we have\n",
      "min\n",
      "\u000b2Rn;\u00112Rm1\n",
      "\u000b>K\u000b: (6.22)\n",
      "This is a convex optimization problem, and its solution is found by di \u000berentiating (6.22)\n",
      "with respect to \u000band\u0011and equating to zero, leading to the following system of ( n+m)\n",
      "K KQar equations:\"KK>+n\n",
      "Q>K>Q>Q#\"\u000b\n",
      "\u0011#\n",
      "=\"K>\n",
      "Q>#\n",
      "y: (6.23)\n",
      "As long as Qis of full column rank, the minimizing function is unique.\n",
      "Example 6.12 (Ridge Regression (cont.)) We return to Example 6.2 and identify that\n",
      "His the RKHS with linear kernel function \u0014(x;x0)=x>x0andC=H0is the linear space of\n",
      "constant functions. In this case, H0is spanned by the function q1\u00111. Moreover, K=XX>\n",
      "andQ=1.\n",
      "If we appeal to the representer theorem directly, then the problem in (6.6) becomes, as\n",
      "a result of (6.21):\n",
      "min\n",
      "\u000b;\u001101\n",
      "kX>\u000bk2:X>\u000b\n",
      "This is a convex optimization problem, and so the solution follows by taking derivatives\n",
      "and setting them to zero. This gives the equations\n",
      "In)\u000b+\u001101\u0000y\u0001=0;\n",
      "and\n",
      "n\u00110=1>(y\u0000XX>\u000b):\n",
      "Note that these are equivalent to (6.8) and (6.9) (once again assuming that n>pandXhas\n",
      "full rank p). Equivalently, the solution is found by solving (6.23):\n",
      "XX>XX>1+n\n",
      "1>XX>n#\"\u000b\n",
      "\u00110#\n",
      "=\"XX>\n",
      "1>#\n",
      "y:\n",
      "This is a system of ( n+1) linear equations, and is typically of much larger dimension than\n",
      "the ( p+1) linear equations given by (6.8) and (6.9). As such, one may question the prac-\n",
      "ticality of reformulating the problem in this way. However, the beneﬁt of this formulation\n",
      "is that the problem can be expressed entirely through the Gram matrix K, without having\n",
      "to explicitly compute the feature vectors — in turn permitting the (implicit) use of inﬁnite\n",
      "dimensional feature spaces.\n",
      "Example 6.13 (Estimating the Peaks Function) Figure 6.4 shows the surface plot of\n",
      "thepeaks function:\n",
      "f(x1;x2)=3(1\u0000x1)2e\u0000x2\n",
      "1\u0000(x2+1)2\u000010\u0012x1\n",
      "5\u0000x3\n",
      "1\u0000x5\n",
      "2\u0013\n",
      "e\u0000x2\n",
      "1\u0000x2\n",
      "2\u00001\n",
      "3e\u0000(x1+1)2\u0000x2\n",
      "2: (6.24)\n",
      "The goal is to learn the function y=f(x) based on a small set of training data (pairs of\n",
      "(x;y) values). The red dots in the ﬁgure represent data \u001c=f(xi;yi)g20\n",
      "i=1, where yi=f(xi) and\n",
      "thefxighave been chosen in a quasi-random quasi -random way, using Hammersley points (with bases 2234 6.5. Representer Theorem\n",
      "and 3) on the square [ \u00003;3]2. Quasi-random point sets have better space-ﬁlling properties\n",
      "than either a regular grid of points or a set of pseudo-random points. We refer to [71] for\n",
      "details. Note that there is no observation noise in this particular problem.\n",
      "-5\n",
      "2 -20\n",
      "0 05\n",
      "-2 2\n",
      "Figure 6.4: Peaks function sampled at 20 Hammersley points.\n",
      "The purpose of this example is to illustrate how, using the small data set of size n=20,\n",
      "the entire peaks function can be approximated well using kernel methods. In particular, we\n",
      "use the Gaussian kernel (6.15) on R2, and denote byHthe unique RKHS corresponding\n",
      "to this kernel. We omit the regularization term in (6.19), and thus our objective is to ﬁnd\n",
      "the solution to\n",
      "min\n",
      "g2H1\n",
      "nnX\n",
      "i=1(yi\u0000g(xi))2:\n",
      "By the representer theorem, the optimal function is of the form\n",
      "g(x)=nX\n",
      "i=1\u000biexp \n",
      "\u00001\n",
      "2kx\u0000xik2\n",
      "\u001b2!\n",
      ";\n",
      "where\u000b:=[\u000b1;:::;\u000b n]>is, by (6.23), the solution to the set of linear equations KK>\u000b=\n",
      "Ky.\n",
      "Note that we are performing regression over the class of functions Hwith an implicit\n",
      "feature space. Due to the representer theorem, the solution to this problem coincides with\n",
      "the solution to the linear regression problem for which the i-th feature (for i=1;:::; n) is\n",
      "chosen to be the vector [ \u0014(x1;xi);:::;\u0014 (xn;xi)]>.\n",
      "The following code performs these calculations and gives the contour plots of gand\n",
      "thepeaks functions, shown in Figure 6.5. We see that the two are quite close. Code for the\n",
      "generation of Hammersley points is available from the book’s GitHub site as genham.py .\n",
      "peakskernel.py\n",
      "from genham import hammersley\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3DChapter 6. Regularization and Kernel Methods 235\n",
      "from matplotlib import cm\n",
      "from numpy.linalg import norm\n",
      "import numpy as np\n",
      "def peaks(x,y):\n",
      "z = (3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2)\n",
      "- 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2)\n",
      "- 1/3 * np.exp(-(x+1)**2 - y**2))\n",
      "return(z)\n",
      "n = 20\n",
      "x = -3 + 6*hammersley([2,3],n)\n",
      "z = peaks(x[:,0],x[:,1])\n",
      "xx, yy = np.mgrid[-3:3:150j,-3:3:150j]\n",
      "zz = peaks(xx,yy)\n",
      "plt.contour(xx,yy,zz,levels=50)\n",
      "fig=plt.figure()\n",
      "ax = fig.add_subplot(111,projection= '3d')\n",
      "ax.plot_surface(xx,yy,zz,rstride=1,cstride=1,color= 'c',alpha=0.3,\n",
      "linewidth=0)\n",
      "ax.scatter(x[:,0],x[:,1],z,color= 'k',s=20)\n",
      "plt.show()\n",
      "sig2 = 0.3 # kernel parameter\n",
      "def k(x,u):\n",
      "return(np.exp(-0.5*norm(x- u)**2/sig2))\n",
      "K = np.zeros((n,n))\n",
      "for i in range(n):\n",
      "for j in range(n):\n",
      "K[i,j] = k(x[i,:],x[j])\n",
      "alpha = np.linalg.solve(K@K.T, K@z)\n",
      "N, = xx.flatten().shape\n",
      "Kx = np.zeros((n,N))\n",
      "for i in range(n):\n",
      "for j in range(N):\n",
      "Kx[i,j] = k(x[i,:],np.array([xx.flatten()[j],yy.flatten()[j\n",
      "]]))\n",
      "g = Kx.T @ alpha\n",
      "dim = np.sqrt(N).astype(int)\n",
      "yhat = g.reshape(dim,dim)\n",
      "plt.contour(xx,yy,yhat ,levels=50)\n",
      "6.6 Smoothing Cubic Splines\n",
      "A striking application of kernel methods is to ﬁtting “well-behaved” functions to data.\n",
      "Key examples of “well-behaved” functions are those that do not have large second-236 6.6. Smoothing Cubic Splines\n",
      "-2 0 2-3-2-10123\n",
      "-2 0 2\n",
      "Figure 6.5: Contour plots for the prediction function g(left) and the peaks function given\n",
      "in (6.24) (right).\n",
      "order derivatives. Consider functions g: [0;1]!Rthat are twice di \u000berentiable and deﬁne\n",
      "kg00k2:=R1\n",
      "0(g00(x))2dxas a measure of the size of the second derivative.\n",
      "Example 6.14 (Behavior of kg00k2)Intuitively, the larger kg00k2is, the more “wiggly”\n",
      "the function gwill be. As an explicit example, consider g(x)=sin(!x) for x2[0;1], where\n",
      "!is a free parameter. We can explicitly compute g00(x)=\u0000!2sin(!x), and consequently\n",
      "kg00k2=Z1\n",
      "0!4sin2(!x) dx=!4\n",
      "2(1\u0000sinc(2!)):\n",
      "Asj!j!1 , the frequency of gincreases and we have kg00k2!1 .\n",
      "Now, in the context of data ﬁtting, consider the following penalized least-squares op-\n",
      "timization problem on [0 ;1]:\n",
      "min\n",
      "g2G1\n",
      "nnX\n",
      "kg00k2; (6.25)+\n",
      "where we will specify Gin what follows. In order to apply the kernel machinery, we want\n",
      "to write this in the form (6.19), for some RKHS Hand null spaceH0. Clearly, the norm on\n",
      "Hshould be of the form kgkH=kg00kand should be well-deﬁned (i.e., ﬁnite and ensuring\n",
      "gandg0are absolutely continuous). This suggests that we take\n",
      "H=fg2L2[0;1] :kg00k<1;g;g0absolutely continuous ;g(0)=g0(0)=0g;\n",
      "with inner product\n",
      "hf;giH:=Z1\n",
      "0f00(x)g00(x) dx:\n",
      "One rationale for imposing the boundary conditions g(0)=g0(0)=0 is as follows: when\n",
      "expanding gabout the point x=0, Taylor’s theorem (with integral remainder term) states\n",
      "that\n",
      "g(x)=g(0)+g0(0)x+Zx\n",
      "0g00(s) (x\u0000s) ds:Chapter 6. Regularization and Kernel Methods 237\n",
      "Imposing the condition that g(0)=g0(0)=0 for functions in Hwill ensure thatG=\n",
      "H 0where the null space H0contains only linear functions, as we will see.\n",
      "To see that thisHis in fact an RKHS, we derive its reproducing kernel. Using integra-\n",
      "tion by parts (or directly from the Taylor expansion above), write\n",
      "g(x)=Zx\n",
      "0g0(s) ds=Zx\n",
      "0g00(s) (x\u0000s) ds=Z1\n",
      "0g00(s) (x\u0000s)+ds:\n",
      "If\u0014is a kernel, then by the reproducing property it must hold that\n",
      "g(x)=hg;\u0014xiH=Z1\n",
      "0g00(s)\u001400\n",
      "x(s) ds;\n",
      "so that\u0014must satisfy@2\n",
      "@s2\u0014(x;s)=(x\u0000s)+, where y+:=maxfy;0g. Therefore, noting that\n",
      "\u0014(x;u)=h\u0014x;\u0014uiH, we have (see Exercise 15)\n",
      "\u0014(x;u)=Z1\n",
      "0@2\u0014(x;s)\n",
      "@s2@2\u0014(u;s)\n",
      "@s2ds=maxfx;ugminfx;ug2\n",
      "2\u0000minfx;ug3\n",
      "6:\n",
      "The last expression is a cubic function with quadratic and cubic terms that misses the\n",
      "constant and linear monomials. This is not surprising considering the Taylor’s theorem\n",
      "interpretation of a function g2H . If we now takeH0as the space of functions of the\n",
      "following form (having zero second derivative):\n",
      "h0=\u00111+\u00112x;x2[0;1];\n",
      "then (6.25) is exactly of the form (6.19).\n",
      "As a consequence of the representer Theorem 6.6, the optimal solution to (6.25) is a\n",
      "linear combination of piecewise cubic functions:\n",
      "g(x)=\u00111+\u00112x+nX\n",
      "i=1\u000bi\u0014(xi;x): (6.26)\n",
      "Such a function is called a cubic spline cubic spline with n knots (with one knot at each data point xi)\n",
      "— so called, because the piecewise cubic function between knots is required to be “tied\n",
      "together” at the knots. The parameters \u000b;\u0011are determined from (6.21) for instance by\n",
      "solving (6.23) with matrices K=[\u0014(xi;xj)]n\n",
      "i;j=1andQwith i-th row of the form [1 ;xi] for\n",
      "i=1;:::; n.\n",
      "Example 6.15 (Smoothing Spline) Figure 6.6 shows various cubic smoothing splines\n",
      "for the data (0 :05;0:4);(0:2;0:2);(0:5;0:6);(0:75;0:7);(1;1). In the ﬁgure, we use the re-\n",
      ") for the smoothing parameter. Thus r2[0;1], where r=0\n",
      "means an inﬁnite penalty for curvature (leading to the ordinary linear regression solution)\n",
      "andr=1 does not penalize curvature at all and leads to a perfect ﬁt via the so-called nat-\n",
      "ural spline . Of course the latter will generally lead to overﬁtting. For rfrom 0 up to 0.8 the\n",
      "solutions will be close to the simple linear regression line, while only for rvery close to 1,\n",
      "the shape of the curve changes signiﬁcantly.238 6.6. Smoothing Cubic Splines\n",
      "0 0.2 0.4 0.6 0.8 100.20.40.60.81\n",
      ")2gure 6.6: Various cubic smoothing splines for smoothing parameter r=1=(1+n\n",
      "f0:8;0:99;0:999;0:999999g. For r=1, the natural spline through the data points is ob-\n",
      "tained; for r=0, the simple linear regression line is found.\n",
      "The following code ﬁrst computes the matrices KandQ, and then solves the linear\n",
      "system (6.23). Finally, the smoothing curve is determined via (6.26), for selected points,\n",
      "and then plotted. Note that the code plots only a single curve corresponding to the speciﬁed\n",
      "value of p.\n",
      "smoothspline.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "x = np.array([[0.05, 0.2, 0.5, 0.75, 1.]]).T\n",
      "y = np.array([[0.4, 0.2, 0.6, 0.7, 1.]]).T\n",
      "n = x.shape[0]\n",
      "r = 0.999\n",
      "ngamma = (1-r)/r\n",
      "k = lambda x1, x2 : (1/2)* np.max((x1,x2)) * np.min((x1,x2)) ** 2 \\\n",
      "- ((1/6)* np.min((x1,x2))**3)\n",
      "K = np.zeros((n,n))\n",
      "for i in range(n):\n",
      "for j in range(n):\n",
      "K[i,j] = k(x[i], x[j])\n",
      "Q = np.hstack((np.ones((n,1)), x))\n",
      "m1 = np.hstack((K @ K.T + (ngamma * K), K @ Q))\n",
      "m2 = np.hstack((Q.T @ K.T, Q.T @ Q))\n",
      "M = np.vstack((m1,m2))\n",
      "c = np.vstack((K, Q.T)) @ y\n",
      "ad = np.linalg.solve(M,c)Chapter 6. Regularization and Kernel Methods 239\n",
      "# plot the curve\n",
      "xx = np.arange(0,1+0.01,0.01).reshape(-1,1)\n",
      "g = np.zeros_like(xx)\n",
      "Qx = np.hstack((np.ones_like(xx), xx))\n",
      "g = np.zeros_like(xx)\n",
      "N = np.shape(xx)[0]\n",
      "Kx = np.zeros((n,N))\n",
      "for i in range(n):\n",
      "for j in range(N):\n",
      "Kx[i,j] = k(x[i], xx[j])\n",
      "g = g + np.hstack((Kx.T, Qx)) @ ad\n",
      "plt.ylim((0,1.15))\n",
      "plt.plot(xx, g, label = 'r = {} '.format(r), linewidth = 2)\n",
      "plt.plot(x,y, 'b.', markersize=15)\n",
      "plt.xlabel( '$x$')\n",
      "plt.ylabel( '$y$')\n",
      "plt.legend()\n",
      "6.7 Gaussian Process Regression\n",
      "Another application of the kernel machinery is to Gaussian process regression. A Gaussian\n",
      "process Gaussian\n",
      "process(GP) on a spaceXis a stochastic process fZx;x2Xg where, for any choice of\n",
      "indices x1;:::; xn, the vector [ Zx1;:::Zxn]>has a multivariate Gaussian distribution. As\n",
      "such, the distribution of a GP is completely speciﬁed by its mean and covariance functions\n",
      "\u0016:X!Rand\u0014:X\u0002X!R, respectively. The covariance function is a ﬁnite positive\n",
      "semideﬁnite function, and hence, in view of Theorem 6.2, can be viewed as a reproducing\n",
      "kernel onX.\n",
      "As for ordinary regression, the objective of GP regression is to learn a regression func- +168\n",
      "tiongthat predicts a response y=g(x) for each feature vector x. This is done in a Bayesian\n",
      "fashion, by establishing (1) a prior pdf for gand (2) the likelihood of the data, for a given\n",
      "g. From these two we then derive, via Bayes’ formula, the posterior distribution of ggiven\n",
      "the data. We refer to Section 2.9 for the general Bayesian framework. + 47\n",
      "A simple Bayesian model for GP regression is as follows. First, the prior distribution of\n",
      "gis taken to be the distribution of a GP with some known mean function \u0016and covariance\n",
      "function (that is, kernel) \u0014. Most often \u0016is taken to be a constant, and for simplicity of\n",
      "exposition, we take it to be 0. The Gaussian kernel (6.15) is often used for the covariance\n",
      "function. For radial basis function kernels (including the Gaussian kernel), points that are\n",
      "closer will be more highly correlated or “similar” [97], independent of translations in space.\n",
      "Second, similar to standard regression, we view the observed feature vectors x1;:::; xn\n",
      "as ﬁxed and the responses y1;:::; ynas outcomes of random variables Y1;:::; Yn. Speciﬁc-\n",
      "ally, given g, we model thefYigas\n",
      "Yi=g(xi)+\"i;i=1;:::; n; (6.27)240 6.7. Gaussian Process Regression\n",
      "wheref\"igiid\u0018N(0;\u001b2). To simplify the analysis, let us assume that \u001b2is known, so no prior\n",
      "needs to be speciﬁed for \u001b2. Let g=[g(x1);:::; g(xn)]>be the (unknown) vector of re-\n",
      "gression values. Placing a GP prior on the function gis equivalent to placing a multivariate\n",
      "Gaussian prior on the vector g:\n",
      "g\u0018N(0;K); (6.28)\n",
      "where the covariance matrix Kofgis a Gram matrix (implicitly associated with a feature\n",
      "map through the kernel \u0014), given by:\n",
      "K=26666666666666664\u0014(x1;x1)\u0014(x1;x2)::: \u0014 (x1;xn)\n",
      "\u0014(x2;x1)\u0014(x2;x2)::: \u0014 (x2;xn)\n",
      "::::::::::::\n",
      "\u0014(xn;x1)\u0014(xn;x2)::: \u0014 (xn;xn)37777777777777775: (6.29)\n",
      "The likelihood of our data given g, denoted p(yjg), is obtained directly from the model\n",
      "(6.27):\n",
      "(Yjg)\u0018N(g;\u001b2In): (6.30)\n",
      "Solving this Bayesian problem involves deriving the posterior distribution of ( gjY). To\n",
      "do so, we ﬁrst note that since Yhas covariance matrix K+\u001b2In(which can be seen from\n",
      "(6.27)), the joint distribution of Yandgis again normal, with mean 0and covariance\n",
      "matrix:\n",
      "Ky;g=\"K+\u001b2InK\n",
      "K K#\n",
      ": (6.31)\n",
      "The posterior can then be found by conditioning on Y=y, via Theorem C.8, giving +438\n",
      "(gjy)\u0018N\u0010\n",
      "K>(K+\u001b2In)\u00001y;K\u0000K>(K+\u001b2In)\u00001K\u0011\n",
      ":\n",
      "This only gives information about gat the observed points x1;:::; xn. It is more interesting\n",
      "to consider the posterior predictive distribution of eg:=g(ex) for a new input ex. We can ﬁnd\n",
      "the corresponding posterior predictive pdf p(egjy) by integrating out the joint posterior pdf\n",
      "p(eg;gjy), which is equivalent to taking the expectation of p(egjg) when gis distributed\n",
      "according to the posterior pdf p(gjy); that is,\n",
      "p(egjy)=Z\n",
      "p(egjg)p(gjy) dg:\n",
      "To do so more easily than direct evaluation via the above integral representation of p(egjy),\n",
      "we can begin with the joint distribution of [ y>;eg]>, which is multivariate normal with mean\n",
      "0and covariance matrix\n",
      "eK=\"K+\u001b2In\u0014\n",
      "\u0014>\u0014(ex;ex)#\n",
      "; (6.32)\n",
      "where\u0014=[\u0014(ex;x1);:::;\u0014 (ex;xn)]>. It now follows, again by using Theorem C.8, that ( egjy)\n",
      "has a normal distribution with mean and variance given respectively by\n",
      "\u0016(ex)=\u0014>(K+\u001b2In)\u00001y (6.33)\n",
      "and\n",
      "\u001b2(ex)=\u0014(ex;ex)\u0000\u0014>(K+\u001b2In)\u00001\u0014: (6.34)\n",
      "These are sometimes called the predictive predictive mean and variance. It is important to note that\n",
      "we are predicting the expected responseEeY=g(ex) here, and not the actual response eY.Chapter 6. Regularization and Kernel Methods 241\n",
      "Example 6.16 (GP Regression) Suppose the regression function is\n",
      "g(x)=2 sin(2\u0019x);x2[0;1]:\n",
      "We use GP regression to estimate g, using a Gaussian kernel of the form (6.15) with band-\n",
      "width parameter 0 :2. The explanatory variables x1;:::; x30were drawn uniformly on the\n",
      "interval [0;1], and the responses were obtained from (6.27), with noise level \u001b=0:5. Fig-\n",
      "ure 6.7 shows 10 samples from the prior distribution for gas well as the data points and\n",
      "the true sinusoidal regression function g.\n",
      "0 0.2 0.4 0.6 0.8 1\n",
      "x-3-2-10123y\n",
      "0 0.2 0.4 0.6 0.8 1\n",
      "x-3-2-10123y\n",
      "Figure 6.7: Left: samples drawn from the GP prior distribution. Right: the true regression\n",
      "function with the data points.\n",
      "Again assuming that the variance \u001b2, is known, the predictive distribution as determ-\n",
      "ined by (6.33) and (6.34) is shown in Figure 6.8 for bandwidth 0 :2 (left) and 0 :02 (right).\n",
      "Clearly, decreasing the bandwidth leads to the covariance between points xandx0decreas-\n",
      "ing at a faster rate with respect to the squared distance kx\u0000x0k2, leading to a predictive\n",
      "mean that is less smooth.\n",
      "In the above exposition, we have taken the mean function for the prior distribution\n",
      "ofgto be identically zero. If instead we have a general mean function mand write\n",
      "m=[m(x1);:::; m(xn)]>then the predictive variance (6.34) remains unchanged, and the\n",
      "predictive mean (6.33) is modiﬁed to read\n",
      "\u0016(ex)=m(ex)+\u0014>(K+\u001b2In)\u00001(y\u0000m): (6.35)\n",
      "Typically, the variance \u001b2appearing in (6.27) is not known, and the kernel \u0014itself\n",
      "depends on several parameters — for instance a Gaussian kernel (6.15) with an unknown\n",
      "bandwidth parameter. In the Bayesian framework, one typically speciﬁes a hierarchical\n",
      "model by introducing a prior p(\u0012) for the vector \u0012of such hyperparameters hyperparamet -\n",
      "ers. Now, the\n",
      "GP prior ( gj\u0012) (equivalently, specifying p(gj\u0012)) and the model for the likelihood of the\n",
      "data given Yjg;\u0012, namely p(yjg;\u0012), are both dependent on \u0012. The posterior distribution of\n",
      "(gjy;\u0012) is as before.242 6.7. Gaussian Process Regression\n",
      "0 0.2 0.4 0.6 0.8 1\n",
      "x-3-2-10123yg(x)\n",
      "Predictive Mean\n",
      "0 0.2 0.4 0.6 0.8 1\n",
      "x-3-2-10123yg(x)\n",
      "Predictive Mean\n",
      "Figure 6.8: GP regression of synthetic data set with bandwidth 0 :2 (left) and 0.02 (right).\n",
      "The black dots represent the data and the blue curve is the latent function g(x)=2 sin(2\u0019x).\n",
      "The red curve is the mean of the GP predictive distribution given by (6.33), and the shaded\n",
      "region is the 95% conﬁdence band, corresponding to the predictive variance given in (6.34).\n",
      "One approach to setting the hyperparameter \u0012is to determine its posterior p(\u0012jy) and\n",
      "obtain a point estimate, for instance via its maximum a posteriori estimate. However, this\n",
      "can be a computationally demanding exercise. What is frequently done in practice is to\n",
      "consider instead the marginal likelihood p (yj\u0012) and maximize this with respect to \u0012. This\n",
      "procedure is called empirical Bayes empirical Bayes .\n",
      "Considering again the mean function mto be identically zero, from (6.31), we have\n",
      "that ( Yj\u0012) is multivariate normal with mean 0and covariance matrix Ky=K+\u001b2In,\n",
      "immediately giving an expression for the marginal log-likelihood:\n",
      "lnp(yj\u0012)=\u0000n\n",
      "2ln(2\u0019)\u00001\n",
      "2lnjdet(Ky)j\u00001\n",
      "2y>K\u00001\n",
      "yy: (6.36)\n",
      "We notice that only the second and third terms in (6.36) depend on \u0012. Considering a partial\n",
      "derivative of (6.36) with respect to a single element \u0012of the hyperparameter vector \u0012yields\n",
      "@\n",
      "@\u0012lnp(yj\u0012)=\u00001\n",
      "2tr \n",
      "K\u00001\n",
      "y\"@\n",
      "@\u0012Ky#!\n",
      "+1\n",
      "2y>K\u00001\n",
      "y\"@\n",
      "@\u0012Ky#\n",
      "K\u00001\n",
      "yy; (6.37)\n",
      "whereh\n",
      "@\n",
      "@\u0012Kyi\n",
      "is the element-wise derivative of matrix Kywith respect to \u0012. If these partial\n",
      "derivatives can be computed for each hyperparameter \u0012, gradient information could be used\n",
      "when maximizing (6.36).\n",
      "Example 6.17 (GP Regression (cont.)) Continuing Example 6.16, we plot in Fig-\n",
      "ure 6.9 the marginal log-likelihood as a function of the noise level \u001band bandwidth para-\n",
      "meter.\n",
      "The maximum is attained for a bandwidth parameter around 0 :20 and\u001b\u00190:44, which\n",
      "is very close to the left panel of Figure 6.8 for the case where \u001bwas assumed to be known\n",
      "(and equal to 0 :5). We note here that the marginal log-likelihood is extremely ﬂat, perhaps\n",
      "owing to the small number of points.Chapter 6. Regularization and Kernel Methods 243\n",
      "10-210-110010-1100\n",
      "Figure 6.9: Contours of the marginal log-likelihood for the GP regression example. The\n",
      "maximum is denoted by a cross.\n",
      "6.8 Kernel PCA\n",
      "In its basic form, kernel PCA (principal component analysis) can be thought of as PCA in\n",
      "feature space. The main motivation for PCA introduced in Section 4.8 was as a dimension- +153\n",
      "ality reduction technique. There, the analysis rested on an SVD of the matrix b\u0006=1\n",
      "nX>X,\n",
      "where the data in Xwas ﬁrst centered via x0\n",
      "i;j=xi;j\u0000xjwhere xi=1\n",
      "nPn\n",
      "i=1xi;j.\n",
      "What we shall do is to ﬁrst re-cast the problem in terms of the Gram matrix K=XX>=\n",
      "[hxi;xji] (note the di \u000berent order of XandX>), and subsequently replace the inner product\n",
      "hx;x0iwith\u0014(x;x0) for a general reproducing kernel \u0014. To make the link, let us start with\n",
      "an SVD of X>:\n",
      "X>=UDV>: (6.38)\n",
      "The dimensions of X>,U,D, and Vared\u0002n,d\u0002d,d\u0002n, and n\u0002n, respectively. Then an\n",
      "SVD of X>Xis\n",
      "X>X=(UDV>)(UDV>)>=U(DD>)U>\n",
      "and an SVD of Kis\n",
      "K=(UDV>)>(UDV>)=V(D>D)V>:\n",
      "Let\u00151>\u0001\u0001\u0001>\u0015r>0 denote the non-zero eigenvalues of X>X(or, equivalently, of K) and\n",
      "denote the corresponding r\u0002rdiagonal matrix by \u0003. Without loss of generality we can\n",
      "assume that the eigenvector of X>Xcorresponding to \u0015kis the k-th column of Uand that\n",
      "thek-th column of Vis an eigenvector of K. Similar to Section 4.8, let UkandVkcontain +153\n",
      "the ﬁrst kcolumns of UandV, respectively, and let \u0003kbe the corresponding k\u0002ksubmatrix\n",
      "of\u0003,k=1;:::; r.\n",
      "By the SVD (6.38), we have X>Vk=UDV>Vk=Uk\u00031=2\n",
      "k. Next, consider the projection\n",
      "of a point xonto the k-dimensional linear space spanned by the columns of Uk— the ﬁrst\n",
      "kprincipal components. We saw in Section 4.8 that this projection simply is the linear\n",
      "mapping x7!U>\n",
      "kx. Using the fact that Uk=X>Vk\u0003\u00001=2, we ﬁnd that xis projected to a244 6.8. Kernel PCA\n",
      "point zgiven by\n",
      "z=\u0003\u00001=2\n",
      "kV>\n",
      "kXx=\u0003\u00001=2\n",
      "kV>\n",
      "k\u0014x;\n",
      "where we have (suggestively) deﬁned \u0014x:=[hx1;xi;:::;hxn;xi]>. The important point\n",
      "is that zis completely determined by the vector of inner products \u0014xand the kprincipal\n",
      "eigenvalues and (right) eigenvectors of the Gram matrix K. Note that each component zm\n",
      "ofzis of the form\n",
      "zm=nX\n",
      "i=1\u000bm;i\u0014(xi;x);m=1;:::; k: (6.39)\n",
      "The preceding discussion assumed centering of the columns of X. Consider now an\n",
      "uncentered data matrix eX. Then the centered data can be written as X=eX\u00001\n",
      "nEneX, where\n",
      "Enis the n\u0002nmatrix of ones. Consequently,\n",
      "XX>=eXeX>\u00001\n",
      "nEneXeX>\u00001\n",
      "neXeX>En+1\n",
      "n2EneXeX>En;\n",
      "or, more compactly, XX>=HeXeX>H, where H=In\u00001\n",
      "n1n1>\n",
      "n,Inis the n\u0002nidentity matrix,\n",
      "and1nis the n\u00021 vector of ones.\n",
      "To generalize to the kernel setting, we replace eXeX>byK=[\u0014(xi;xj);i;j=1;:::; n]\n",
      "and set\u0014x=[\u0014(x1;x);:::;\u0014 (xn;x)]>, so that \u0003kis the diagonal matrix of the klargest eigen-\n",
      "values of HKH andVkis the corresponding matrix of eigenvectors. Note that the “usual”\n",
      "PCA is recovered when we use the linear kernel \u0014(x;y)=x>y. However, instead of having\n",
      "only kernels that are explicitly inner products of feature vectors, we are now permitted to\n",
      "implicitly use inﬁnite feature maps (functions) by using kernels.\n",
      "Example 6.18 (Kernel PCA) We simulated 200 points, x1;:::; x200, from the uniform\n",
      "distribution on the set B1[(B4\\Bc\n",
      "3), where Br:=f(x;y)2R2:x2+y26r2g(disk with\n",
      "radius r). We apply kernel PCA with Gaussian kernel \u0014(x;x0)=exp\u0010\n",
      "\u0000kx\u0000x0k2\u0011\n",
      "and\n",
      "compute the functions zm(x);m=1;:::; 9 in (6.39). Their density plots are shown in Fig-\n",
      "ure 6.10. The data points are superimposed in each plot. From this we see that the principal\n",
      "components identify the radial structure present in the data. Finally, Figure 6.11 shows\n",
      "the projections [ z1(xi);z2(xi)]>;i=1;:::; 200 of the original data points onto the ﬁrst two\n",
      "principal components. We see that the projected points can be separated by a straight line,\n",
      "whereas this is not possible for the original data; see also, Example 7.6 for a related prob- +274\n",
      "lem.Chapter 6. Regularization and Kernel Methods 245\n",
      "Figure 6.10: First nine eigenfunctions using a Gaussian kernel for the two-dimensional\n",
      "data set formed by the red and cyan points.\n",
      "-0.4 -0.2 0 0.2 0.4 0.6 0.8-0.8-0.6-0.4-0.200.20.40.6\n",
      "Figure 6.11: Projection of the data onto the ﬁrst two principal components. Observe that\n",
      "already the projections of the inner and outer points are well separated.246 Exercises\n",
      "Further Reading\n",
      "For a good overview of the ridge regression and the lasso, we refer the reader to [36, 56].\n",
      "For overviews of the theory of RKHS we refer to [3, 115, 126], and for in-depth background\n",
      "on splines and their connection to RKHSs we refer to [123]. For further details on GP\n",
      "regression we refer to [97] and for kernel PCA in particular we refer to [12, 92]. Finally,\n",
      "many facts about kernels and their corresponding RKHSs can be found in [115].\n",
      "Exercises\n",
      "1. LetGbe an RKHS with reproducing kernel \u0014. Show that \u0014is a positive semideﬁnite\n",
      "function.\n",
      "2. Show that a reproducing kernel, if it exists, is unique.\n",
      "3. LetGbe a Hilbert space of functions g:X!R. Recall that the evaluation func-\n",
      "tional is the map\u000ex:g7!g(x) for a given x2X. Show that evaluation functionals\n",
      "are linear operators.\n",
      "4. LetG0be the pre-RKHS G0constructed in the proof of Theorem 6.2. Thus, g2G 0\n",
      "is of the form g=Pn\n",
      "i=1\u000bi\u0014xiand\n",
      "hg;\u0014xiG0=nX\n",
      "i=1\u000bih\u0014xi;\u0014xiG0=nX\n",
      "i=1\u000bi\u0014(xi;x)=g(x):\n",
      "Therefore, we may write the evaluation functional of g2G 0atxas\u000exg:=hg;\u0014xiG0.\n",
      "<1.G0, for someounded onG0for every x; that is,j\u000exfj<\n",
      "5. Continuing Exercise 4, let ( fn) be a Cauchy sequence in G0such thatjfn(x)j!0 for\n",
      "allx. Show thatkfnkG0!0.\n",
      "6. Continuing Exercises 5 and 4, to show that the inner product (6.14) is well deﬁned,\n",
      "a number of facts have to be checked.\n",
      "(a) Verify that the limit converges.\n",
      "(b) Verify that the limit is independent of the Cauchy sequences used.\n",
      "(c) Verify that the properties of an inner product are satisﬁed. The only non-trivial\n",
      "property to verify is that hf;fiG=0 if and only if f=0.\n",
      "7. Exercises 4–6 show that Gdeﬁned in the proof of Theorem 6.2 is an inner product\n",
      "space. It remains to prove that Gis an RKHS. This requires us to prove that the inner\n",
      "product spaceGis complete (and thus Hilbert), and that its evaluation functionals\n",
      "are bounded and hence continuous (see Theorem A.16). This is done in a number of +391\n",
      "steps.\n",
      "(a) Show thatG0is dense inGin the sense that every f2Gis a limit point (with\n",
      "respect to the norm on G) of a Cauchy sequence ( fn) inG0.Chapter 6. Regularization and Kernel Methods 247\n",
      "(b) Show that every evaluation functional \u000exonGis continuous at the 0 function.\n",
      "That is,\n",
      "8\">0 :9\u000e>0 :8f2G:kfkG<\u000e)jf(x)j<\": (6.40)\n",
      "Continuity of \u000exat all functions g2Gthen follows automatically from linearity.\n",
      "(c) Show thatGis complete; that is, every Cauchy sequence ( fn)2Gconverges in\n",
      "the normjj\u0001jjG.\n",
      "8. If\u00141and\u00142are kernels onXandY, then\u0014+((x;y);(x0;y0)) :=\u00141(x;x0)+\u00142(y;y0)\n",
      "and\u0014\u0002((x;y);(x0;y0) :=\u00141(x;x0)\u00142(y;y0) are kernels on the Cartesian product X\u0002Y .\n",
      "Prove this.\n",
      "9. An RKHS enjoys the following desirable smoothness property: if ( gn) is a sequence\n",
      "belonging to RKHS GonX, andkgn\u0000gkG!0, then g(x)=lim ngn(x) for all x2X.\n",
      "Prove this, using Cauchy–Schwarz.\n",
      "10. Let Xbe anRd-valued random variable that is symmetric about the origin (that is,\n",
      "Xand (\u0000X) are identically distributed). Denote by \u0016is its distribution and  (t)=\n",
      "Eeit>X=R\n",
      "eit>x\u0016(dx) for t2Rdis its characteristic function. Verify that \u0014(x;x0)=\n",
      " (x\u0000x0) is a real-valued positive semideﬁnite function.\n",
      "11. Suppose an RKHS Gof functions from X!R(with kernel \u0014) is invariant under a\n",
      "groupTof transformations T:X!X ; that is, for all f;g2GandT2T, we have\n",
      "(i)f\u000eT2Gand (ii)hf\u000eT;g\u000eTiG=hf;giG. Show that \u0014(Tx;Tx0)=\u0014(x;x0) for\n",
      "allx;x02XandT2T.\n",
      "12. Given two Hilbert spaces HandG, we call a mapping A:H!G aHilbert space\n",
      "isomorphism Hilbert space\n",
      "isomorphismif it is\n",
      "(i) a linear map; that is, A(a f+bg)=aA(f)+bA(g) for any f;g2H anda;b2R.\n",
      "(ii) a surjective map; and\n",
      "(iii) an isometry; that is, for all f;g2H, it holds thathf;giH=hA f;AgiG.\n",
      "LetH=Rp(equipped with the usual Euclidean inner product) and construct its\n",
      "(continuous) dual spaceG, consisting of all continuous linear functions from Rpto\n",
      "R, as follows: (a) For each \f2Rp, deﬁne g\f:Rp!Rviag\f(x)=h\f;xi=\f>x, for\n",
      ".G:=\f>p. (b) EquipGwith the inner product hg\f;g\n",
      "Show that A:H!G deﬁned by A(\f)=g\ffor\f2Rpis a Hilbert space isomorph-\n",
      "ism.\n",
      ">0 is invertible.pmodel matrix. Show that X>X+n\n",
      "14. As Example 6.8 clearly illustrates, the pdf of a random variable that is symmetric\n",
      "about the origin is not in general a valid reproducing kernel. Take two such iid ran-\n",
      "dom variables XandX0with common pdf f, and deﬁne Z=X+X0. Denote by  Z\n",
      "andfZthe characteristic function and pdf of Z, respectively.\n",
      "Show that if  Zis in L1(R),fZis a positive semideﬁnite function. Use this to show\n",
      "that\u0014(x;x0)=fZ(x\u0000x0)=1fjx\u0000x0j62g(1\u0000jx\u0000x0j=2) is a valid reproducing kernel.248 Exercises\n",
      "15. For the smoothing cubic spline of Section 6.6, show that \u0014(x;u)=maxfx;ugminfx;ug2\n",
      "2\u0000\n",
      "minfx;ug3\n",
      "6.\n",
      "16. Let Xbe an n\u0002pmodel matrix and let u2Rpbe the unit-length vector with k-th\n",
      "entry equal to one ( uk=kuk=1). Suppose that the k-th column of Xisvand that it\n",
      "is replaced with a new predictor w, so that we obtain the new model matrix:\n",
      "eX=X+(w\u0000v)u>:\n",
      "(a) Denoting\n",
      "\u000e:=X>(w\u0000v)+kw\u0000vk2\n",
      "2u;\n",
      "show that\n",
      "eX>eX=X>X+u\u000e>+\u000eu>=X>X+(u+\u000e)(u+\u000e)>\n",
      "2\u0000(u\u0000\u000e)(u\u0000\u000e)>\n",
      "2:\n",
      "In other words, eX>eXdi\u000bers from X>Xby a symmetric matrix of rank two.\n",
      "Ip)\u00001is already computed. Explain how the\n",
      "Sherman–Morrison formulas in Theorem A.10 can be applied twice to com- +373\n",
      "IpinO((n+p)p)rse and log-determinant of the matrix eX>eX+n\n",
      "computing time, rather than the usual O((n+p2)p) computing time.3\n",
      "Ip)\u00001when wePython program for updating a matrix B=(X>X+n\n",
      "change the k-th column of X, as shown in the following pseudo-code.\n",
      "Algorithm 6.8.1: Updating via Sherman–Morrison Formula\n",
      "input: Matrices XandB, index k, and replacement wfor the k-th column of X.\n",
      "output: Updated matrices XandB.\n",
      "1Setv2Rnto be the k-th column of X.\n",
      "2Setu2Rpto be the unit-length vector such that uk=kuk=1.\n",
      "3B B\u0000Bu\u000e>B\n",
      "1+\u000e>Bu\n",
      "4B B\u0000B\u000eu>B\n",
      "1+u>B\u000e\n",
      "5Update the k-th column of Xwithw.\n",
      "6return X;B\n",
      "17. Use Algorithm 6.8.1 from Exercise 16 to write Python code that computes the ridge\n",
      "regression coe \u000ecient\fin (6.5) and use it to replicate the results on Figure 6.1. The +217\n",
      "following pseudo-code (with running cost of O((n+p)p2)) may help with the writing\n",
      "of the Python code.\n",
      "3This Sherman–Morrison updating is not always numerically stable. A more numerically stable method\n",
      "Ip.Chapter 6. Regularization and Kernel Methods 249e Cholesky decomposition of X>X+n\n",
      "Algorithm 6.8.2: Ridge Regression Coe \u000ecients via Sherman–Morrison Formula\n",
      ">0.ut: Training setfX;ygand regularization parameter \n",
      "Ip+X>X)\u00001X>y.ion b\f=(n\n",
      "Ip)\u00001.o be an n\u0002pmatrix of zeros and B (n\n",
      "2forj=1;:::; pdo\n",
      "3 Setwto be the j-th column of X.\n",
      "4 UpdatefA;Bgvia Algorithm 6.8.1 with inputs fA;B;j;wg.\n",
      "5b\f B(X>y)\n",
      "6return b\f\n",
      "18. Consider Example 2.10 with D=diag(\u00151;:::;\u0015 p) for some nonnegative vector \u00152 + 55\n",
      "Rp, so that twice the negative logarithm of the model evidence can be written as\n",
      "\u00002 lng(y)=l(\u0015) :=nln[y>(I\u0000X\u0006X>)y]+lnjDj\u0000lnj\u0006j+c;\n",
      "where cis a constant that depends only on n.\n",
      "(a) Use the Woodbury identities (A.15) and (A.16) to show that +373\n",
      "I\u0000X\u0006X>=(I+XDX>)\u00001\n",
      "lnjDj\u0000lnj\u0006j=lnjI+XDX>j:\n",
      "Deduce that l(\u0015)=nln[y>Cy]\u0000lnjCj+c, where C:=(I+XDX>)\u00001.\n",
      "(b) Let [ v1;:::; vp] :=Xdenote the pcolumns /predictors of X. Show that\n",
      "C\u00001=I+pX\n",
      "k=1\u0015kvkv>\n",
      "k:\n",
      "Explain why setting \u0015k=0 has the e \u000bect of excluding the k-th predictor from\n",
      "the regression model. How can this observation be used for model selection?\n",
      "(c) Prove the following formulas for the gradient and Hessian elements of l(\u0015):\n",
      "@l\n",
      "@\u0015i=v>\n",
      "iCvi\u0000n(v>\n",
      "iCy)2\n",
      "y>Cy\n",
      "@2l\n",
      "@\u0015i@\u0015j=(n\u00001)(v>\n",
      "iCvj)2\u0000n266664v>\n",
      "iCvj\u0000(v>\n",
      "iCy)(v>\n",
      "jCy)\n",
      "y>Cy3777752\n",
      ":(6.41)\n",
      "(d) One method to determine which predictors in Xare important is to compute\n",
      "\u0015\u0003:=argmin\n",
      "\u0015>0l(\u0015)\n",
      "using, for example, the interior-point minimization Algorithm B.4.1 with gradi- +421\n",
      "ent and Hessian computed from (6.41). Write Python code to compute \u0015\u0003and\n",
      "use it to select the best polynomial model in Example 2.10.250 Exercises\n",
      "19. (Exercise 18 continued.) Consider again Example 2.10 with D=diag(\u00151;:::;\u0015 p) for +55\n",
      "some nonnegative model-selection parameter \u00152Rp. A Bayesian choice for \u0015is the\n",
      "maximizer of the marginal likelihood g(yj\u0015); that is,\n",
      "\u0015\u0003=argmax\n",
      "\u0015>0\"\n",
      "g(\f;\u001b2;yj\u0015) d\fd\u001b2;\n",
      "where\n",
      "lng(\f;\u001b2;yj\u0015)=\u0000ky\u0000X\fk2+\f>D\u00001\f\n",
      "2\u001b2\u00001\n",
      "2lnjDj\u0000n+p\n",
      "2ln(2\u0019\u001b2)\u0000ln\u001b2:\n",
      "To maximize g(yj\u0015), one can use the EM algorithm with\fand\u001b2acting as latent +128\n",
      "variables in the complete-data log-likelihood lng(\f;\u001b2;yj\u0015). Deﬁne\n",
      "\u0006:=(D\u00001+X>X)\u00001\n",
      "\f:=\u0006X>y\n",
      "b\u001b2:=\u0010\n",
      "kyk2\u0000y>X\f\u0011\u000en:(6.42)\n",
      "(a) Show that the conditional density of the latent variables \fand\u001b2is such that\n",
      "\u0010\n",
      "\u001b\u00002\f\f\f\u0015;y\u0011\n",
      "\u0018Gamma\u0012n\n",
      "2;n\n",
      "2b\u001b2\u0013\n",
      "\u0010\n",
      "\f\f\f\f\u0015;\u001b2;y\u0011\n",
      "\u0018N\u0010\n",
      "\f; \u001b2\u0006\u0011\n",
      ":\n",
      "(b) Use Theorem C.2 to show that the expected complete-data log-likelihood is +432\n",
      "\u0000\f>D\u00001\f\n",
      "2b\u001b2\u0000tr(D\u00001\u0006)+lnjDj\n",
      "2+c1;\n",
      "where c1is a constant that does not depend on \u0015.\n",
      "(c) Use Theorem A.2 to simplify the expected complete-data log-likelihood and to +361\n",
      "show that it is maximized at \u0015i=\u0006ii+(\fi=b\u001b)2fori=1;:::; p:Hence, deduce\n",
      "the following E and M steps in the EM algorithm:\n",
      "E-step. Given\u0015, update ( \u0006;\f;b\u001b2) via the formulas (6.42).\n",
      "M-step. Given ( \u0006;\f;b\u001b2), update\u0015via\u0015i=\u0006ii+(\fi=b\u001b)2;i=1;:::; p:\n",
      "(d) Write Python code to compute \u0015\u0003via the EM algorithm, and use it to select\n",
      "the best polynomial model in Example 2.10. A possible stopping criterion is to\n",
      "terminate the EM iterations when\n",
      "lng(yj\u0015t+1)\u0000lng(yj\u0015t)<\"\n",
      "for some small \">0, where the marginal log-likelihood is\n",
      "lng(yj\u0015)=\u0000n\n",
      "2ln(n\u0019b\u001b2)\u00001\n",
      "2lnjDj+1\n",
      "2lnj\u0006j+ln\u0000(n=2):Chapter 6. Regularization and Kernel Methods 251\n",
      "20. In this exercise we explore how the early stopping of the gradient descent iterations\n",
      "(see Example B.10), +414\n",
      "xt+1=xt\u0000\u000brf(xt);t=0;1;:::;\n",
      "is (approximately) equivalent to the global minimization of f(x)+1\n",
      "kxk2for certain\n",
      " >0 (see Example 6.1). We illustratearameter\n",
      "theearly stopping early stopping idea on the quadratic function f(x)=1\n",
      "2(x\u0000\u0016)>H(x\u0000\u0016), where\n",
      "H2Rn\u0002nis a symmetric positive-deﬁnite (Hessian) matrix with eigenvalues f\u0015kgn\n",
      "k=1.\n",
      "(a) Verify that for a symmetric matrix A2Rnsuch that I\u0000Ais invertible, we have\n",
      "I+A+\u0001\u0001\u0001+At\u00001=(I\u0000At)(I\u0000A)\u00001:\n",
      "(b) Let H=Q\u0003Q>be the diagonalization of Has per Theorem A.8. If x0=0, +368\n",
      "show that the formula for xtis\n",
      "xt=\u0016\u0000Q(I\u0000\u000b\u0003)tQ>\u0016:\n",
      "Hence, deduce that a necessary condition for xtto converge is \u000b<2=max k\u0015k.\n",
      "(c) Show that the minimizer of f(x)+1\n",
      "kxk2can be written as\n",
      "\u00001\u0003)\u00001Q>\u0016:\n",
      "(d) For a ﬁxed value of t, let the learning rate \u000b#0. Using part (b) and (c), show\n",
      "'1=(t\u000b) as\u000b#0, then xt'x\u0003. In other words, xtis approximately\n",
      "is inversely proportional to t\u000b.252 ExercisesCHAPTER7\n",
      "CLASSIFICATION\n",
      "The purpose of this chapter is to explain the mathematical ideas behind well-known\n",
      "classiﬁcation techniques such as the naïve Bayes method, linear and quadratic discrim-\n",
      "inant analysis, logistic /softmax classiﬁcation, the K-nearest neighbors method, and\n",
      "support vector machines.\n",
      "7.1 Introduction\n",
      "Classiﬁcation methods are supervised learning methods in which a categorical response\n",
      "variable Ytakes one of cpossible values (for example whether a person is sick or healthy),\n",
      "which is to be predicted from a vector Xofexplanatory variables (for example, the blood\n",
      "pressure, age, and smoking status of the person), using a prediction function g . In this\n",
      "sense, gclassiﬁes the input Xinto one of the classes, say in the set f0;:::; c\u00001g. For this\n",
      "reason, we will call gaclassiﬁcation function or simply classiﬁer classifier . As with any supervised\n",
      "learning technique (see Section 2.3), the goal is to minimize the expected loss or risk\n",
      "`(g)=ELoss( Y;g(X)) (7.1)\n",
      "for some loss function, Loss( y;by), that quantiﬁes the impact of classifying a response yvia\n",
      "by=g(x). The natural loss function is the zero–one (also written 0–1) or indicator loss indicator loss :\n",
      "Loss( y;by) :=1fy,byg; that is, there is no loss for a correct classiﬁcation ( y=by) and a\n",
      "unit loss for a misclassiﬁcation ( y,by). In this case the optimal classiﬁer g\u0003is given in the\n",
      "following theorem.\n",
      "Theorem 7.1: Optimal classiﬁer\n",
      "For the loss function Loss( y;by)=1fy,byg, an optimal classiﬁcation function is\n",
      "g\u0003(x)=argmax\n",
      "y2f0;:::;c\u00001gP[Y=yjX=x]: (7.2)\n",
      "Proof: The goal is to minimize `(g)=E1fY,g(X)gover all functions gtaking values in\n",
      "f0;:::; c\u00001g. Conditioning on Xgives, by the tower property, `(g)=E(P[Y,g(X)jX] ), +433\n",
      "and so minimizing `(g) with respect to gcan be accomplished by maximizing P[Y=\n",
      "253254 7.1. Introduction\n",
      "g(x)jX=x] with respect to g(x), for every ﬁxed x. In other words, take g(x) to be equal\n",
      "to the class label yfor whichP[Y=yjX=x] is maximal. \u0003\n",
      "The formulation (7.2) allows for “ties”, when there is an equal probability between\n",
      "optimal classes for a feature vector x. Assigning one of these tied classes arbitrarily (or\n",
      "randomly) to xdoes not a \u000bect the loss function and so we assume for simplicity that g\u0003(x)\n",
      "is always a scalar value.\n",
      "Note that, as was the case for the regression (see, e.g., Theorem 2.1), the optimal pre- +21\n",
      "diction function depends on the conditional pdf f(yjx)=P[Y=yjX=x]. However, since\n",
      "we assign xto class yiff(yjx)>f(zjx) for all z, we do not need to learn the entire sur-\n",
      "face of the function f(yjx); we only need to estimate it well enough near the decision\n",
      "boundaryfx:f(yjx)=f(zjx)gfor any choice of classes yandz. This is because the as-\n",
      "signment (7.2) divides the feature space into cregions,Ry=fx:f(yjx)=max zf(zjx)g,\n",
      "y=0;:::; c\u00001.\n",
      "Recall that for any supervised learning problem the smallest possible expected loss\n",
      "(that is, the irreducible risk) is given by `\u0003=`(g\u0003). For the indicator loss, the irreducible\n",
      "risk is equal to P[Y,g\u0003(X)]. This smallest possible probability of misclassiﬁcation is\n",
      "often called the Bayes error rate Bayes error\n",
      "rate.\n",
      "For a given training set \u001c, a classiﬁer is often derived from a pre-classiﬁer g \u001c, which\n",
      "is a prediction function (learner) that can take any real value, rather than only values\n",
      "in the set of class labels. A typical situation is the case of binary classiﬁcation with\n",
      "labels\u00001 and 1, where the prediction function g\u001cis a function taking values in the\n",
      "interval [\u00001;1] and the actual classiﬁer is given by sign( g\u001c). It will be clear from\n",
      "the context whether a prediction function g\u001cshould be interpreted as a classiﬁer or\n",
      "pre-classiﬁer.\n",
      "The indicator loss function may not always be the most appropriate choice of loss\n",
      "function for a given classiﬁcation problem. For example, when diagnosing an illness, the\n",
      "mistake in misclassifying a person as being sick when in fact the person is healthy may\n",
      "be less serious than classifying the person as healthy when in fact the person is sick. In\n",
      "Section 7.2 we consider various classiﬁcation metrics.\n",
      "There are many ways to ﬁt a classiﬁer to a training set \u001c=f(x1;y1);:::; (xn;yn)g. The\n",
      "approach taken in Section 7.3 is to use a Bayesian framework for classiﬁcation. Here the\n",
      "conditional pdf f(yjx) is viewed as a posterior pdf f(yjx)/f(xjy)f(y) for a given class\n",
      "prior f(y) and likelihood f(xjy). Section 7.4 discusses linear and quadratic discriminant\n",
      "analysis for classiﬁcation, which assumes that the class of approximating functions for the\n",
      "conditional pdf f(xjy) is a parametric class Gof Gaussian densities. As a result of this\n",
      "choice ofG, the marginal f(x) is approximated via a Gaussian mixture density.\n",
      "In contrast, in the logistic or soft-max classiﬁcation in Section 7.5, the conditional\n",
      "pdff(yjx) is approximated using a more ﬂexible class of approximating functions. As a\n",
      "result of this, the approximation to the marginal density f(x) does not belong to a simple\n",
      "parametric class (such as a Gaussian mixture). As in unsupervised learning, the cross-\n",
      "entropy loss is the most common choice for training the learner.\n",
      "TheK-nearest neighbors method, discussed in Section 7.6, is yet another approach to\n",
      "classiﬁcation that makes minimal assumptions on the class G. Here the aim is to directlyChapter 7. Classiﬁcation 255\n",
      "estimate the conditional pdf f(yjx) from the training data, using only feature vectors in\n",
      "the neighborhood of x. In Section 7.7 we explain the support vector methodology for clas-\n",
      "siﬁcation; this is based on the same Reproducing Kernel Hilbert Space ideas that proved\n",
      "successful for regression analysis in Section 6.3. Finally, a versatile way to do both clas- +222\n",
      "siﬁcation and regression is to use classiﬁcation and regression trees. This is the topic of\n",
      "Chapter 8. Neural networks (Chapter 9) provide yet another way to perform classiﬁcation. +289\n",
      "+325\n",
      "7.2 Classiﬁcation Metrics\n",
      "The e \u000bectiveness of a classiﬁer gis, theoretically, measured in terms of the risk (7.1), which\n",
      "depends on the loss function used. Fitting a classiﬁer to iid training data \u001c=f(xi;yi)gn\n",
      "i=1is\n",
      "established by minimizing the training loss\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1Loss( yi;g(xi)) (7.3)\n",
      "over some class of functions G. As the training loss is often a poor estimator of the risk,\n",
      "the risk is usually estimated as in (7.3), using instead a test set \u001c0=f(x0\n",
      "i;y0\n",
      "i)gn0\n",
      "i=1gthat is\n",
      "independent of the training set, as explained in Section 2.3. To measure the performance + 23\n",
      "of a classiﬁer on a training or test set, it is convenient to introduce the notion of a loss\n",
      "matrix loss matrix . Consider a classiﬁcation problem with classiﬁer g, loss function Loss, and classes\n",
      "0;:::; c\u00001. If an input feature vector xis classiﬁed as by=g(x) when the observed class\n",
      "isy, the loss incurred is, by deﬁnition, Loss( y;by). Consequently, we may identify the loss\n",
      "function with a matrix L=[Loss( j;k);j;k2f0;:::; c\u00001g]. For the indicator loss function,\n",
      "the matrix Lhas 0s on the diagonal and 1s everywhere else. Another useful matrix is the\n",
      "confusion matrix confusion\n",
      "matrix, denoted by M, where the ( j;k)-th element of Mcounts the number of\n",
      "times that, for the training or test data, the actual (observed) class is jwhereas the predicted\n",
      "class is k. Table 7.1 shows the confusion matrix of some Dog /Cat/Possum classiﬁer.\n",
      "Table 7.1: Confusion matrix for three classes.\n",
      "Predicted\n",
      "Actual Dog Cat Possum\n",
      "Dog 30 2 6\n",
      "Cat 8 22 15\n",
      "Possum 7 4 41\n",
      "We can now express the classiﬁer performance (7.3) in terms of LandMas\n",
      "1\n",
      "nX\n",
      "j;k[L\fM]jk; (7.4)\n",
      "where L\fMis the elementwise product of LandM. Note that for the indicator loss, (7.4)\n",
      "is simply 1\u0000tr(M)=n, and is called the misclassiﬁcation error . The expression (7.4) makesmisclassification\n",
      "error it clear that both the counts and the loss are important in determining the performance of a\n",
      "classiﬁer.256 7.2. Classiﬁcation Metrics\n",
      "In the spirit of Table C.4 for hypothesis testing, it is sometimes useful to divide the +461\n",
      "elements of a confusion matrix into four groups. The diagonal elements are the true positivetrue positivecounts; that is, the numbers of correct classiﬁcations for each class. The true positive counts\n",
      "for the Dog, Cat, and Possum classes in Table 7.1 are 30 ;22, and 41, respectively. Similarly,\n",
      "thetrue negative true negative count for a class is the sum of all matrix elements that do not belong to the\n",
      "row or the column of this particular class. For the Dog class it is 22 +15+4+41=82. The\n",
      "false positive false positive count for a class is the sum of the corresponding column elements without\n",
      "the diagonal element. For the Dog class it is 8 +7=15. Finally, the false negative false negative count\n",
      "for a speciﬁc class, can be calculated by summing over the corresponding row elements\n",
      "(again, without counting the diagonal element). For the Dog class it is 2 +6=8.\n",
      "In terms of the elements of the confusion matrix, we have the following counts for class\n",
      "j=0;:::; c\u00001:\n",
      "True positive tpj=Mj j;\n",
      "False positive fpj=X\n",
      "k,jMk j; (column sum)\n",
      "False negative fn j=X\n",
      "k,jMjk; (row sum)\n",
      "True negative tn j=n\u0000fnj\u0000fpj\u0000tpj:\n",
      "Note that in the binary classiﬁcation case ( c=2), and using the indicator loss function,\n",
      "the misclassiﬁcation error (7.4) can be written as\n",
      "error j=fpj+fnj\n",
      "n: (7.5)\n",
      "This does not depend on which of the two classes is considered, as fp0+fn0=fp1+fn1.\n",
      "Similarly, the accuracy accuracy measures the fraction of correctly classiﬁed objects:\n",
      "accuracyj=1\u0000error j=tpj+tnj\n",
      "n: (7.6)\n",
      "In some cases, classiﬁcation error (or accuracy) alone is not su \u000ecient to adequately\n",
      "describe the e \u000bectiveness of a classiﬁer. As an example, consider the following two classi-\n",
      "ﬁcation problems based on a ﬁngerprint detection system:\n",
      "1. Identiﬁcation of authorized personnel in a top-secret military facility.\n",
      "2. Identiﬁcation to get an online discount for some retail chain.\n",
      "Both problems are binary classiﬁcation problems. However, a false positive in the ﬁrst\n",
      "problem is extremely dangerous, while a false positive in the second problem will make\n",
      "a customer happy. Let us examine a classiﬁer in the top-secret facility. The corresponding\n",
      "confusion matrix is given in Table 7.2.\n",
      "Table 7.2: Confusion matrix for authorized personnel classiﬁcation.\n",
      "Predicted\n",
      "Actual authorized non-authorized\n",
      "authorized 100 400\n",
      "non-authorized 50 100,000Chapter 7. Classiﬁcation 257\n",
      "From (7.6), we conclude that the accuracy of classiﬁcation is equal to\n",
      "accuracy =tp+tn\n",
      "tp+tn+fp+fn=100+100;000\n",
      "100+100;000+50+400\u001999:55%:\n",
      "However, we can see that in this particular case, accuracy is a problematic metric, since the\n",
      "algorithm allowed 50 non-authorized personnel to enter the facility. One way to deal with\n",
      "this issue is to modify the loss function to give a much higher loss to non-authorized access.\n",
      "Thus, instead of an (indicator) loss matrix, we could for example take the loss matrix\n",
      "L= 0 1\n",
      "1000 0!\n",
      ":\n",
      "An alternative approach is to keep the indicator loss function and consider additional clas-\n",
      "siﬁcation metrics. Below we give a list of commonly used metrics. For simplicity we call\n",
      "an object whose actual class is ja “j-object”.\n",
      "The precision precision (also called positive predictive value ) is the fraction of all objects\n",
      "classiﬁed as jthat are actually j-objects. Speciﬁcally,\n",
      "precisionj=tpj\n",
      "tpj+fpj:\n",
      "The recall recall (also called sensitivity ) is the fraction of all j-objects that are correctly\n",
      "classiﬁed as such. That is,\n",
      "recall j=tpj\n",
      "tpj+fnj:\n",
      "Thespeciﬁcity specificity measures the fraction of all non- j-objects that are correctly classiﬁed\n",
      "as such. Speciﬁcally,\n",
      "speciﬁcityj=tnj\n",
      "fpj+tnj:\n",
      "TheF\fscore F\fscore is a combination of the precision and the recall and is used as a single\n",
      "measurement for a classiﬁer’s performance. The F\fscore is given by\n",
      "F\f;j=(\f2+1) tpj\n",
      "(\f2+1) tpj+\f2fnj+fpj:\n",
      "For\f=0 we obtain the precision and for \f!1 we obtain the recall.\n",
      "The particular choice of metric is clearly application dependent. For example, in the\n",
      "classiﬁcation of authorized personnel in a top-secret military facility, suppose we have\n",
      "two classiﬁers. The ﬁrst (Classiﬁer 1) has a confusion matrix given in Table 7.2, and the\n",
      "second (Classiﬁer 2) has a confusion matrix given in Table 7.3. Various metrics for these\n",
      "two classiﬁers are show in Table 7.4. In this case we prefer Classiﬁer 1, which has a much\n",
      "higher precision.258 7.2. Classiﬁcation Metrics\n",
      "Table 7.3: Confusion matrix for authorized personnel classiﬁcation, using a di \u000berent clas-\n",
      "siﬁer (Classiﬁer 2).\n",
      "Predicted\n",
      "Actual Authorized Non-Authorized\n",
      "authorized 50 10\n",
      "non-authorized 450 100,040\n",
      "Table 7.4: Comparing the metrics for the confusion matrices in Tables 7.2 and 7.3.\n",
      "Metric Classiﬁer 1 Classiﬁer 2\n",
      "accuracy 9 :955\u000210\u000019:954\u000210\u00001\n",
      "precision 6 :667\u000210\u000011:000\u000210\u00001\n",
      "recall 2 :000\u000210\u000018:333\u000210\u00001\n",
      "speciﬁcity 9 :995\u000210\u000019:955\u000210\u00001\n",
      "F1 3:077\u000210\u000011:786\u000210\u00001\n",
      "Remark 7.1 (Multilabel and Hierarchical Classiﬁcation) In standard classiﬁcation\n",
      "the classes are assumed to be mutually exclusive. For example a satellite image could\n",
      "be classiﬁed as “cloudy”, “clear”, or “foggy”. In multilabel classiﬁcation multilabel\n",
      "classificationthe classes (often\n",
      "called labels) do not have to be mutually exclusive. In this case the response is a subset\n",
      "Yof some collection of labels f0;:::; c\u00001g. Equivalently, the response can be viewed as\n",
      "a binary vector of length c, where the y-th element is 1 if the response belongs to label y\n",
      "and 0 otherwise. Again, consider the satellite image example and add two labels, such as\n",
      "“road” and “river” to the previous three labels. Clearly, an image can contain both a road\n",
      "and a river. In addition, the image can be clear, cloudy, or foggy.\n",
      "Inhierarchical classiﬁcation hierarchical\n",
      "classificationa hierarchical relation between classes /labels is taken into\n",
      "account during the classiﬁcation process. Usually, the relations are modeled via a tree or a\n",
      "directed acyclic graph. A visual comparison between the hierarchical and non-hierarchical\n",
      "(ﬂat) classiﬁcation tasks for satellite image data is presented in Figure 7.1.\n",
      "root\n",
      "rural\n",
      "farm barnurban\n",
      "skyscraper\n",
      "root\n",
      "rural barn farm urban skyscraper\n",
      "Figure 7.1: Hierarchical (left) and non-hierarchical (right) classiﬁcation schemes. Barns\n",
      "and farms are common in rural areas, while skyscrapers are generally located in cities.\n",
      "While this relation can be clearly observed in the hierarchical model scheme, the connec-\n",
      "tion is missing in the non-hierarchical design.\n",
      "In multilabel classiﬁcation, both the prediction bY:=g(x) and the true response Yare\n",
      "subsets of the label setf0;:::; c\u00001g. A reasonable metric is the so-called exact match ratio exact match\n",
      "ratio,Chapter 7. Classiﬁcation 259\n",
      "deﬁned as\n",
      "exact match ratio =Pn\n",
      "i=11fbYi=Yig\n",
      "n:\n",
      "The exact match ratio is rather stringent, as it requires a full match. In order to consider\n",
      "partial correctness, the following metrics could be used instead.\n",
      "Theaccuracy is deﬁned as the ratio of correctly predicted labels and the total number\n",
      "of predicted and actual labels. The formula is given by\n",
      "accuracy =Pn\n",
      "i=1jYi\\bYij\n",
      "Pn\n",
      "i=1jYi[bYij:\n",
      "Theprecision is deﬁned as the ratio of correctly predicted labels and the total number\n",
      "of predicted labels. Speciﬁcally,\n",
      "precision =Pn\n",
      "i=1jYi\\bYij\n",
      "Pn\n",
      "i=1jbYij: (7.7)\n",
      "Therecall is deﬁned as the ratio of correctly predicted labels and the total number of\n",
      "actual labels. Speciﬁcally,\n",
      "recall =Pn\n",
      "i=1jYi\\bYijPn\n",
      "i=1jYij: (7.8)\n",
      "TheHamming loss counts the average number of incorrect predictions for all classes,\n",
      "calculated as\n",
      "Hamming =1\n",
      "n cnX\n",
      "i=1c\u00001X\n",
      "y=01fy2bYig1fy<Yig+1fy<bYig1fy2Y ig:\n",
      "7.3 Classiﬁcation via Bayes’ Rule\n",
      "We saw from Theorem 7.1 that the optimal classiﬁer for classes 0 ;:::; c\u00001 divides the\n",
      "feature space into cregions, depending on f(yjx): the conditional pdf of the response Y\n",
      "given the feature vector X=x. In particular, if f(yjx)>f(zjx) for all z,y, the feature\n",
      "vector xis classiﬁed as y. Classifying feature vectors on the basis of their conditional class\n",
      "probabilities is a natural thing to do, especially in a Bayesian learning context; see Sec-\n",
      "tion 2.9 for an overview of Bayesian terminology and usage. Speciﬁcally, the conditional + 47\n",
      "probability f(yjx) is interpreted as a posterior probability, of the form\n",
      "f(yjx)/f(xjy)f(y); (7.9)\n",
      "where f(xjy) is the likelihood of obtaining feature vector xfrom class yand f(y) is the\n",
      "prior probability1of class y. By making various modeling assumptions about the prior\n",
      "1Here we have used the Bayesian notation convention of “overloading” the notation f.260 7.3. Classiﬁcation via Bayes’ Rule\n",
      "(e.g., all classes are a priori equally likely) and the likelihood function, one obtains the\n",
      "posterior pdf via Bayes’ formula (7.9). A class byis then assigned to a feature vector x\n",
      "according to the highest posterior probability; that is, we classify according to the Bayes\n",
      "optimal decision rule Bayes optimal\n",
      "decision rule:\n",
      "by=argmax\n",
      "yf(yjx); (7.10)\n",
      "which is exactly (7.2). Since the discrete density f(yjx),y=0;:::; c\u00001 is usually not\n",
      "known, the aim is to approximate it well with a function g(yjx) from some class of func-\n",
      "tionsG. Note that in this context, g(\u0001jx) refers to a discrete density (a probability mass\n",
      "function) for a given x.\n",
      "Suppose a feature vector x=[x1;:::; xp]>ofpfeatures has to be classiﬁed into one of\n",
      "the classes 0 ;:::; c\u00001. For example, the classes could be di \u000berent people and the features\n",
      "could be various facial measurements, such as the width of the eyes divided by the distance\n",
      "between the eyes, or the ratio of the nose height and mouth width. In the naïve Bayes na¨iveBayes\n",
      "method, the class of approximating functions Gis chosen such that g(xjy)=g(x1jy)\u0001\u0001\u0001\n",
      "g(xpjy), that is, conditional on the label, all features are independent. Assuming a uniform\n",
      "prior for y, the posterior pdf can thus be written as\n",
      "g(yjx)/pY\n",
      "j=1g(xjjy);\n",
      "where the marginal pdfs g(xjjy);j=1;:::; pbelong to a given class of approximating\n",
      "functionsG. To classify x, simply take the ythat maximizes the unnormalized posterior\n",
      "pdf.\n",
      "For instance, suppose that the approximating class Gis such that ( Xjjy)\u0018N(\u0016y j;\u001b2),\n",
      "y=0;:::; c\u00001,j=1;:::; p. The corresponding posterior pdf is then\n",
      "g(yj\u0012;x)/exp0BBBBBB@\u00001\n",
      "2pX\n",
      "j=1(xj\u0000\u0016y j)2\n",
      "\u001b21CCCCCCA=exp0BBBB@\u00001\n",
      "2kx\u0000\u0016yk2\n",
      "\u001b21CCCCA;\n",
      "where\u0016y:=[\u0016y1;:::;\u0016 yp]>and\u0012:=f\u00160;:::;\u0016c\u00001;\u001b2gcollects all model parameters. The\n",
      "probability g(yj\u0012;x) is maximal when kx\u0000\u0016ykis minimal. Thus by=argminykx\u0000\u0016ykis\n",
      "the classiﬁer that maximizes the posterior probability. That is, classify xasywhen\u0016yis\n",
      "closest to xin Euclidean distance. Of course, the parameters (here, the f\u0016ygand\u001b2) are\n",
      "unknown and have to be estimated from the training data.\n",
      "We can extend the above idea to the case where also the variance \u001b2depends on the\n",
      "class yand feature j, as in the next example.\n",
      "Example 7.1 (Naïve Bayes Classiﬁcation) Table 7.5 lists the means \u0016and standard de-\n",
      "viations\u001bofp=3 normally distributed features, for c=4 di\u000berent classes. How should\n",
      "a feature vector x=[1:67;2:00;4:23]>be classiﬁed? The posterior pdf is\n",
      "g(yj\u0012;x)/(\u001by1\u001by2\u001by3)\u00001exp0BBBBBB@\u00001\n",
      "23X\n",
      "j=1(xj\u0000\u0016y j)2\n",
      "\u001b2\n",
      "y j1CCCCCCA;\n",
      "where\u0012:=f\u001bj;\u0016jgc\u00001\n",
      "j=0again collects all model parameters. The (unscaled) values for\n",
      "g(yj\u0012;x),y=0;1;2;3 are 53:5, 0:24, 8:37, and 3:5\u000210\u00006, respectively. Hence, the feature\n",
      "vector should be classiﬁed as 0. The code follows.Chapter 7. Classiﬁcation 261\n",
      "Table 7.5: Feature parameters.\n",
      "Feature 1 Feature 2 Feature 3\n",
      "Class \u0016 \u001b \u0016 \u001b \u0016 \u001b\n",
      "0 1.6 0.1 2.4 0.5 4.3 0.2\n",
      "1 1.5 0.2 2.9 0.6 6.1 0.9\n",
      "2 1.8 0.3 2.5 0.3 4.2 0.3\n",
      "3 1.1 0.2 3.1 0.7 5.6 0.3\n",
      "naiveBayes.py\n",
      "import numpy as np\n",
      "x = np.array([1.67,2,4.23]).reshape(1,3)\n",
      "mu = np.array([1.6, 2.4, 4.3,\n",
      "1.5, 2.9, 6.1,\n",
      "1.8, 2.5, 4.2,\n",
      "1.1, 3.1, 5.6]).reshape(4,3)\n",
      "sig = np.array([0.1, 0.5, 0.2,\n",
      "0.2, 0.6, 0.9,\n",
      "0.3, 0.3, 0.3,\n",
      "0.2, 0.7, 0.3]).reshape(4,3)\n",
      "g = lambda y: 1/np.prod(sig[y,:]) * np.exp(\n",
      "-0.5*np.sum((x-mu[y,:])**2/sig[y,:]**2));\n",
      "for y in range(0,4):\n",
      "print( '{:3.2e} '.format(g(y)))\n",
      "5.35e+01\n",
      "2.42e-01\n",
      "8.37e+00\n",
      "3.53e-06\n",
      "7.4 Linear and Quadratic Discriminant Analysis\n",
      "The Bayesian viewpoint for classiﬁcation of the previous section (not limited to naïve\n",
      "Bayes) leads in a natural way to the well-established technique of discriminant analysis discriminant\n",
      "analysis.\n",
      "We discuss the binary classiﬁcation case ﬁrst, with classes 0 and 1.\n",
      "We consider a class of approximating functions Gsuch that, conditional on the class\n",
      "y2f0;1g, the feature vector X=[X1;:::; Xp]>has aN(\u0016y;\u0006y) distribution (see (2.33)): + 45\n",
      "g(xj\u0012;y)=1p(2\u0019)pj\u0006yje\u00001\n",
      "2(x\u0000\u0016y)>\u0006\u00001\n",
      "y(x\u0000\u0016y);x2Rp;y2f0;1g; (7.11)\n",
      "where\u0012=f\u000bj;\u0016j;\u0006jgc\u00001\n",
      "j=0collects all model parameters, including the probability vector \u000b\n",
      "(that is,P\n",
      "i\u000bi=1 and\u000bi>0) which helps deﬁne the prior density: g(yj\u0012)=\u000by;y2f0;1g:\n",
      "Then, the posterior density is\n",
      "g(yj\u0012;x)/\u000by\u0002g(xj\u0012;y);262 7.4. Linear and Quadratic Discriminant Analysis\n",
      "and, according to the Bayes optimal decision rule (7.10), we classify xto come from class\n",
      "0 if\u000b0g(xj\u0012;0)>\u000b 1g(xj\u0012;1) or, equivalently (by taking logarithms) if,\n",
      "ln\u000b0\u00001\n",
      "2lnj\u00060j\u00001\n",
      "2(x\u0000\u00160)>\u0006\u00001\n",
      "0(x\u0000\u00160)>ln\u000b1\u00001\n",
      "2lnj\u00061j\u00001\n",
      "2(x\u0000\u00161)>\u0006\u00001\n",
      "1(x\u0000\u00161):\n",
      "The function\n",
      "\u000ey(x)=ln\u000by\u00001\n",
      "2lnj\u0006yj\u00001\n",
      "2(x\u0000\u0016y)>\u0006\u00001\n",
      "y(x\u0000\u0016y);x2Rp(7.12)\n",
      "is called the quadratic discriminant function quadratic\n",
      "discriminant\n",
      "functionfor class y=0;1. A point xis classiﬁed to\n",
      "class yfor which\u000ey(x) is largest. The function is quadratic in xand so the decision bound-\n",
      "aryfx2Rp:\u000e0(x)=\u000e1(x)gis quadratic as well. An important simpliﬁcation arises for the\n",
      "case where the assumption is made that \u00060=\u00061=\u0006. Now, the decision boundary is the\n",
      "set of xfor which\n",
      "ln\u000b0\u00001\n",
      "2(x\u0000\u00160)>\u0006\u00001(x\u0000\u00160)=ln\u000b1\u00001\n",
      "2(x\u0000\u00161)>\u0006\u00001(x\u0000\u00161):\n",
      "Expanding the above expression shows that the quadratic term in xis eliminated, giving a\n",
      "linear decision boundary in x:\n",
      "ln\u000b0\u00001\n",
      "2\u0016>\n",
      "0\u0006\u00001\u00160+x>\u0006\u00001\u00160=ln\u000b1\u00001\n",
      "2\u0016>\n",
      "1\u0006\u00001\u00161+x>\u0006\u00001\u00161:\n",
      "The corresponding linear discriminant function linear\n",
      "discriminant\n",
      "functionfor class yis\n",
      "\u000ey(x)=ln\u000by\u00001\n",
      "2\u0016>\n",
      "y\u0006\u00001\u0016y+x>\u0006\u00001\u0016y;x2Rp: (7.13)\n",
      "Example 7.2 (Linear Discriminant Analysis) Consider the case where \u000b0=\u000b1=1=2\n",
      "and\n",
      "\u0006=\"2 0:7\n",
      "0:7 2#\n",
      ";\u00160=\"0\n",
      "0#\n",
      ";\u00161=\"2\n",
      "4#\n",
      ":\n",
      "The distribution of Xis a mixture of two bivariate normal distributions. Its pdf, +135\n",
      "1\n",
      "2g(xj\u0012;y=0)+1\n",
      "2g(xj\u0012;y=1);\n",
      "is depicted in Figure 7.2.\n",
      "Figure 7.2: A Gaussian mixture density where the two mixture components have the same\n",
      "covariance matrix.Chapter 7. Classiﬁcation 263\n",
      "We used the following Python code to make this ﬁgure.\n",
      "LDAmixture.py\n",
      "import numpy as np, matplotlib.pyplot as plt\n",
      "from scipy.stats import multivariate_normal\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.colors import LightSource\n",
      "mu0, mu1 = np.array([0,0]), np.array([2,4])\n",
      "Sigma = np.array([[2,0.7],[0.7, 2]])\n",
      "x, y = np.mgrid[-4:6:150j,-5:8:150j]\n",
      "mvn0 = multivariate_normal( mu0, Sigma )\n",
      "mvn1 = multivariate_normal( mu1, Sigma )\n",
      "xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\n",
      "z = 0.5*mvn0.pdf(xy).reshape(x.shape) + 0.5*mvn1.pdf(xy).reshape(x.\n",
      "shape)\n",
      "fig = plt.figure()\n",
      "ax = fig.gca(projection= '3d')\n",
      "ls = LightSource(azdeg=180, altdeg=65)\n",
      "cols = ls.shade(z, plt.cm.winter)\n",
      "surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,\n",
      "antialiased=False , facecolors=cols)\n",
      "plt.show()\n",
      "The following Python code, which imports the previous code, draws a contour plot of\n",
      "the mixture density, simulates 1000 data points from the mixture density, and draws the\n",
      "decision boundary. To compute and display the linear decision boundary, let [ a1;a2]>=\n",
      "2\u0006\u00001(\u00161\u0000\u00160) and b=\u0016>\n",
      "0\u0006\u00001\u00160\u0000\u0016>\n",
      "1\u0006\u00001\u00161. Then, the decision boundary can be written\n",
      "asa1x1+a2x2+b=0 or, equivalently, x2=\u0000(a1x1+b)=a2. We see in Figure 7.3 that the\n",
      "decision boundary nicely separates the two modes of the mixture density.\n",
      "LDA.py\n",
      "from LDAmixture import *\n",
      "from numpy.random import rand\n",
      "from numpy.linalg import inv\n",
      "fig = plt.figure()\n",
      "plt.contourf(x, y,z, cmap=plt.cm.Blues , alpha= 0.9,extend= 'both ')\n",
      "plt.ylim(-5.0,8.0)\n",
      "plt.xlim(-4.0,6.0)\n",
      "M = 1000\n",
      "r = (rand(M,1) < 0.5)\n",
      "for i in range(0,M):\n",
      "if r[i]:\n",
      "u = np.random.multivariate_normal(mu0,Sigma ,1)\n",
      "plt.plot(u[0][0],u[0][1], '.r',alpha = 0.4)\n",
      "else:\n",
      "u = np.random.multivariate_normal(mu1,Sigma ,1)\n",
      "plt.plot(u[0][0],u[0][1], '+k',alpha = 0.6)264 7.4. Linear and Quadratic Discriminant Analysis\n",
      "a = 2*inv(Sigma) @ (mu1-mu0);\n",
      "b = ( mu0.reshape(1,2) @ inv(Sigma) @ mu0.reshape(2,1)\n",
      "- mu1.reshape(1,2) @ inv(Sigma) @mu1.reshape(2,1) )\n",
      "xx = np.linspace(-4,6,100)\n",
      "yy = (-(a[0]*xx +b)/a[1])[0]\n",
      "plt.plot(xx,yy, 'm')\n",
      "plt.show()\n",
      "4\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 4\n",
      " 6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "Figure 7.3: The linear discriminant boundary lies between the two modes of the mixture\n",
      "density and is linear.\n",
      "To illustrate the di \u000berence between the linear and quadratic case, we specify di \u000berent\n",
      "covariance matrices for the mixture components in the next example.\n",
      "Example 7.3 (Quadratic Discriminant Analysis) As in Example 7.2 we consider a\n",
      "mixture of two Gaussians, but now with di \u000berent covariance matrices. Figure 7.4 shows\n",
      "the quadratic decision boundary. The Python code follows.\n",
      "2\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Figure 7.4: A quadratic decision boundary.Chapter 7. Classiﬁcation 265\n",
      "QDA.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import multivariate_normal\n",
      "mu1 = np.array([0,0])\n",
      "mu2 = np.array([2,2])\n",
      "Sigma1 = np.array([[1,0.3],[0.3, 1]])\n",
      "Sigma2 = np.array([[0.3,0.3],[0.3, 1]])\n",
      "x, y = np.mgrid[-2:4:150j,-3:5:150j]\n",
      "mvn1 = multivariate_normal( mu1, Sigma1 )\n",
      "mvn2 = multivariate_normal( mu2, Sigma2 )\n",
      "xy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\n",
      "z = ( 0.5*mvn1.pdf(xy).reshape(x.shape) +\n",
      "0.5*mvn2.pdf(xy).reshape(x.shape) )\n",
      "plt.contour(x,y,z)\n",
      "z1 = ( 0.5*mvn1.pdf(xy).reshape(x.shape) -\n",
      "0.5*mvn2.pdf(xy).reshape(x.shape))\n",
      "plt.contour(x,y,z1, levels=[0],linestyles = 'dashed ',\n",
      "linewidths = 2, colors = 'm')\n",
      "plt.show()\n",
      "Of course, in practice the true parameter \u0012=f\u000bj;\u0006j;\u0016jgc\n",
      "j=1is not known and must be\n",
      "estimated from the training data — for example, by minimizing the cross-entropy training\n",
      "loss(4.4) with respect to \u0012: +123\n",
      "1\n",
      "nnX\n",
      "i=1Loss( f(xi;yi);g(xi;yij\u0012))=\u00001\n",
      "nnX\n",
      "i=1lng(xi;yij\u0012);\n",
      "where\n",
      "lng(x;yj\u0012)=ln\u000by\u00001\n",
      "2lnj\u0006yj\u00001\n",
      "2(x\u0000\u0016y)>\u0006\u00001\n",
      "y(x\u0000\u0016y)\u0000p\n",
      "2ln(2\u0019):\n",
      "The corresponding estimates of the model parameters (see Exercise 2) are:\n",
      "b\u000by=ny\n",
      "n\n",
      "b\u0016y=1\n",
      "nyX\n",
      "i:yi=yxi\n",
      "b\u0006y=1\n",
      "nyX\n",
      "i:yi=y(xi\u0000b\u0016y)(xi\u0000b\u0016y)>(7.14)\n",
      "fory=0;:::; c\u00001, where ny:=Pn\n",
      "i=11fyi=yg. For the case where \u0006y=\u0006for all y, we\n",
      "haveb\u0006=P\n",
      "yb\u000byb\u0006y.\n",
      "When c>2 classes are involved, the classiﬁcation procedure carries through in exactly\n",
      "the same way, leading to quadratic and linear discriminant functions (7.12) and (7.13) for\n",
      "each class. The space Rpnow is partitioned into cregions, determined by the linear or\n",
      "quadratic boundaries determined by each pair of Gaussians.266 7.4. Linear and Quadratic Discriminant Analysis\n",
      "For the linear discriminant case (that is, when \u0006y=\u0006for all y), it is convenient to ﬁrst\n",
      "“whiten” or sphere the data sphere the data as follows. Let Bbe an invertible matrix such that \u0006=BB>,\n",
      "obtained, for example, via the Cholesky method. We linearly transform each data point x +375\n",
      "tox0:=B\u00001xand each mean \u0016yto\u00160\n",
      "y:=B\u00001\u0016y,y=0;:::; c\u00001. Let the random vector X\n",
      "be distributed according to the mixture pdf\n",
      "gX(xj\u0012) :=X\n",
      "y\u000by1p(2\u0019)pj\u0006yje\u00001\n",
      "2(x\u0000\u0016y)>\u0006\u00001\n",
      "y(x\u0000\u0016y):\n",
      "Then, by the transformation Theorem C.4, the vector X0=B\u00001Xhas density +435\n",
      "gX0(x0j\u0012)=gX(xj\u0012)\n",
      "jB\u00001j=c\u00001X\n",
      "y=0\u000byp(2\u0019)pe\u00001\n",
      "2(x\u0000\u0016y)>(BB>)\u00001(x\u0000\u0016y)\n",
      "=c\u00001X\n",
      "y=0\u000byp(2\u0019)pe\u00001\n",
      "2(x0\u0000\u00160\n",
      "y)>(x0\u0000\u00160\n",
      "y)=c\u00001X\n",
      "y=0\u000byp(2\u0019)pe\u00001\n",
      "2kx0\u0000\u00160\n",
      "yk2:\n",
      "This is the pdf of a mixture of standard p-dimensional normal distributions. The name\n",
      "“sphering” derives from the fact that the contours of each mixture component are perfect\n",
      "spheres. Classiﬁcation of the transformed data is now particularly easy: classify xasby:=\n",
      "argminyfkx0\u0000\u00160\n",
      "yk2\u00002 ln\u000byg. Note that this rule only depends on the prior probabilities and\n",
      "the distance from x0to the transformed means f\u00160\n",
      "yg. This procedure can lead to a signiﬁcant\n",
      "dimensionality reduction of the data. Namely, the data can be projected onto the space\n",
      "spanned by the di \u000berences between the mean vectors f\u00160\n",
      "yg. When there are cclasses, this\n",
      "is a ( c\u00001)-dimensional space, as opposed to the p-dimensional space of the original data.\n",
      "We explain the precise ideas via an example.\n",
      "Example 7.4 (Classiﬁcation after Data Reduction) Consider an equal mixture of\n",
      "three 3-dimensional Gaussian distributions with identical covariance matrices. After spher-\n",
      "ing the data, the covariance matrices are all equal to the identity matrix. Suppose the mean\n",
      "vectors of the sphered data are \u00161=[2;1;\u00003]>,\u00162=[1;\u00004;0]>, and\u00163=[2;4;6]>. The\n",
      "left panel of Figure 7.5 shows the 3-dimensional (sphered) data from each of the three\n",
      "classes.\n",
      "4202464202454321012\n",
      "6\n",
      " 4\n",
      " 2\n",
      " 0\n",
      " 2\n",
      " 4\n",
      " 6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Figure 7.5: Left: original data. Right: projected data.\n",
      "The data are stored in three 1000 \u00023 matrices X1,X2, and X3. Here is how the data was\n",
      "generated and plotted.Chapter 7. Classiﬁcation 267\n",
      "datared.py\n",
      "import numpy as np\n",
      "from numpy.random import randn\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "n=1000\n",
      "mu1 = np.array([2,1,-3])\n",
      "mu2 = np.array([1,-4,0])\n",
      "mu3 = np.array([2,4,0])\n",
      "X1 = randn(n,3) + mu1\n",
      "X2 = randn(n,3) + mu2\n",
      "X3 = randn(n,3) + mu3\n",
      "fig = plt.figure()\n",
      "ax = fig.gca(projection= '3d',)\n",
      "ax.plot(X1[:,0],X1[:,1],X1[:,2], 'r.',alpha=0.5,markersize=2)\n",
      "ax.plot(X2[:,0],X2[:,1],X2[:,2], 'b.',alpha=0.5,markersize=2)\n",
      "ax.plot(X3[:,0],X3[:,1],X3[:,2], 'g.',alpha=0.5,markersize=2)\n",
      "ax.set_xlim3d(-4,6)\n",
      "ax.set_ylim3d(-5,5)\n",
      "ax.set_zlim3d(-5,2)\n",
      "plt.show()\n",
      "Since we have equal mixtures, we classify each data point xaccording to the closest\n",
      "distance to\u00161,\u00162, or\u00163. We can achieve a reduction in the dimensionality of the data by\n",
      "projecting the data onto the two-dimensional a \u000ene space spanned by the f\u0016ig; that is, all\n",
      "vectors are of the form\n",
      "\u00161+\f1(\u00162\u0000\u00161)+\f2(\u00163\u0000\u00161); \f 1;\f22R:\n",
      "In fact, one may just as well project the data onto the subspace spanned by the vectors\n",
      "\u001621=\u00162\u0000\u00161and\u001631=\u00163\u0000\u00161. Let W=[\u001621;\u001631] be the 3\u00022 matrix whose columns\n",
      "are\u001621and\u001631. The orthogonal projection matrix onto the subspace Wspanned by the\n",
      "columns of Wis (see Theorem A.4): +364\n",
      "P=WW+=W(W>W)\u00001W>:\n",
      "LetUDV>be the singular value decomposition of W. Then Pcan also be written as\n",
      "P=UD(D>D)\u00001D>U>:\n",
      "Note that Dhas dimension 3\u00022, so is not square. The ﬁrst two columns of U, say u1\n",
      "andu2, form an orthonormal basis of the subspace W. What we want to do is rotate this\n",
      "subspace to the x\u0000yplane, mapping u1andu2to [1;0;0]>and [0;1;0]>, respectively. This\n",
      "is achieved via the rotation matrix U\u00001=U>, giving the skewed projection matrix\n",
      "R=U>P=D(D>D)\u00001D>U>;\n",
      "whose 3rd row only contains zeros. Applying Rto all the data points, and ignoring the\n",
      "3rd component of the projected points (which is 0), gives the right panel of Figure 7.5.\n",
      "We see that the projected points are much better separated than the original ones. We have\n",
      "achieved dimensionality reduction of the data while retaining all the necessary information\n",
      "required for classiﬁcation. Here is the rest of the Python code.268 7.5. Logistic Regression and Softmax Classiﬁcation\n",
      "dataproj.py\n",
      "from datared import *\n",
      "from numpy.linalg import svd, pinv\n",
      "mu21 = (mu2 - mu1).reshape(3,1)\n",
      "mu31 = (mu3 - mu1).reshape(3,1)\n",
      "W = np.hstack((mu21 , mu31))\n",
      "U,_,_ = svd(W) # we only need U\n",
      "P = W @ pinv(W)\n",
      "R = U.T @ P\n",
      "RX1 = (R @ X1.T).T\n",
      "RX2 = (R @ X2.T).T\n",
      "RX3 = (R @ X3.T).T\n",
      "plt.plot(RX1[:,0],RX1[:,1], 'b.',alpha=0.5,markersize=2)\n",
      "plt.plot(RX2[:,0],RX2[:,1], 'g.',alpha=0.5,markersize=2)\n",
      "plt.plot(RX3[:,0],RX3[:,1], 'r.',alpha=0.5,markersize=2)\n",
      "plt.show()\n",
      "7.5 Logistic Regression and Softmax Classiﬁcation\n",
      "In Example 5.10 we introduced the logistic (logit) regression model as a generalized linear +204\n",
      "model where, conditional on a p-dimensonal feature vector x, the random response Yhas\n",
      "aBer(h(x>\f)) distribution with h(u)=1=(1+e\u0000u). The parameter \fwas then learned from\n",
      "the training data by maximizing the likelihood of the training responses or, equivalently,\n",
      "by minimizing the supervised version of the cross-entropy training loss (4.4): +123\n",
      "\u00001\n",
      "nnX\n",
      "i=1lng(yij\f;xi);\n",
      "where g(y=1j\f;x)=1=(1+e\u0000x>\f) and g(y=0j\f;x)=e\u0000x>\f=(1+e\u0000x>\f). In particular,\n",
      "we have\n",
      "lng(y=1j\f;x)\n",
      "g(y=0j\f;x)=x>\f: (7.15)\n",
      "In other words, the log-odds ratio log-odds ratio is a linear function of the feature vector. As a con-\n",
      "sequence, the decision boundary fx:g(y=0j\f;x)=g(y=1j\f;x)gis the hyperplane\n",
      "x>\f=0. Note that xtypically includes the constant feature. If the constant feature is con-\n",
      "sidered separately, that is x=[1;ex>]>, then the boundary is an a \u000ene hyperplane in ex.\n",
      "Suppose that training on \u001c=f(xi;yi)gyields the estimate b\fwith the corresponding\n",
      "learner g\u001c(y=1jx)=1=(1+e\u0000x>b\f). The learner can be used as a pre-classiﬁer from which\n",
      "we obtain the classiﬁer 1fg\u001c(y=1jx)>1=2gor, equivalently,\n",
      "by:=argmax\n",
      "j2f0;1gg\u001c(y=jjx);\n",
      "in accordance with the fundamental classiﬁcation rule (7.2).\n",
      "The above classiﬁcation methodology for the logit model can be generalized to the\n",
      "multi-logit multi -logit model where the response takes values in the set f0;:::; c\u00001g. The key idea isChapter 7. Classiﬁcation 269\n",
      "to replace (7.15) with\n",
      "lng(y=jjW;b;x)\n",
      "g(y=0jW;b;x)=x>\fj;j=1;:::; c\u00001; (7.16)\n",
      "where the matrix W2R(c\u00001)\u0002(p\u00001)and vector b2Rc\u00001reparameterize all \fj2Rpsuch that\n",
      "(recall x=[1;ex>]>):\n",
      "Wex+b=[\f1;:::;\fc\u00001]>x:\n",
      "Observe that the random response Yis assumed to have a conditional probability distri-\n",
      "bution for which the log-odds ratio with respect to class jand a “reference” class (in this\n",
      "case 0) is linear . The separating boundaries between two pairs of classes are again a \u000ene\n",
      "hyperplanes.\n",
      "The model (7.16) completely speciﬁes the distribution of Y, namely:\n",
      "g(yjW;b;x)=exp(zy+1)Pc\n",
      "k=1exp(zk);y=0;:::; c\u00001;\n",
      "where z1is an arbitrary constant, say 0, corresponding to the “reference” class y=0, and\n",
      "[z2;:::; zc]>:=Wex+b:\n",
      "Note that g(yjW;b;x) is the ( y+1)-st component of a=softmax( z), where\n",
      "softmax : z7!exp(z)P\n",
      "kexp(zk)\n",
      "is the softmax softmax function and z=[z1;:::; zc]>. Finally, we can write the classiﬁer as\n",
      "by=argmax\n",
      "j2f0;:::;c\u00001gaj+1:\n",
      "In summary, we have the sequence of mappings transforming the input xinto the output by:\n",
      "x!Wex+b!softmax( z)!argmax\n",
      "j2f0;:::;c\u00001gaj+1!by:\n",
      "In Example 9.4 we will revisit the multi-logit model and reinterpret this sequence of map- +335\n",
      "pings as a neural network . In the context of neural networks, Wis called a weight matrix\n",
      "andbis called a bias vector.\n",
      "The parameters Wandbhave to be learned from the training data, which involves\n",
      "minimization of the supervised version of the cross-entropy training loss (4.4): +123\n",
      "1\n",
      "nnX\n",
      "i=1Loss( f(yijxi);g(yijW;b;xi))=\u00001\n",
      "nnX\n",
      "i=1lng(yijW;b;xi):\n",
      "Using the softmax function, the cross-entropy loss can be simpliﬁed to:\n",
      "Loss( f(yjx);g(yjW;b;x))=\u0000zy+1+lncX\n",
      "k=1exp(zk): (7.17)\n",
      "The discussion on training is postponed until Chapter 9, where we reinterpret the multi-\n",
      "logit model as a neural net, which can be trained using the limited-memory BFGS method\n",
      "(Exercise 11). Note that in the binary case ( c=2), where there is only one vector \fto +354\n",
      "be estimated, Example 5.10 already established that minimization of the cross-entropy\n",
      "training loss is equivalent to likelihood maximization.270 7.6. K-Nearest Neighbors Classiﬁcation\n",
      "7.6 K-Nearest Neighbors Classiﬁcation\n",
      "Let\u001c=f(xi;yi)gn\n",
      "i=1be the training set, with yi2f0;:::; c\u00001g, and let xbe a new feature\n",
      "vector. Deﬁne x(1);x(2);:::; x(n)as the feature vectors ordered by closeness to xin some dis-\n",
      "tance dist( x;xi), e.g., the Euclidean distance kx\u0000x0k. Let\u001c(x) :=f(x(1);y(1)):::;(x(K);y(K))g\n",
      "be the subset of \u001cthat contains Kfeature vectors xithat are closest to x. Then the K-nearest\n",
      "neighbors K-nearest\n",
      "neighborsclassiﬁcation rule classiﬁes xaccording to the most frequently occurring class\n",
      "labels in\u001c(x). If two or more labels receive the same number of votes, the feature vector\n",
      "is classiﬁed by selecting one of these labels randomly with equal probability. For the case\n",
      "K=1 the set\u001c(x) contains only one element, say ( x0;y0), and xis classiﬁed as y0. This\n",
      "divides the space into nregions\n",
      "Ri=fx: dist( x;xi)6dist(x;xj);j,ig;i=1;:::; n:\n",
      "For a feature space Rpwith the Euclidean distance, this gives a V oronoi tessellation of the\n",
      "feature space, similar to what was done for vector quantization in Section 4.6. +142\n",
      "Example 7.5 (Nearest Neighbor Classiﬁcation) The Python program below simulates\n",
      "80 random points above and below the line x2=x1. Points above the line x2=x1have\n",
      "label 0 and points below this line have label 1. Figure 7.6 shows the V oronoi tessellation\n",
      "obtained from the 1-nearest neighbor classiﬁcation.\n",
      "2\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Figure 7.6: The 1-nearest neighbor algorithm divides up the space into V oronoi cells.\n",
      "nearestnb.py\n",
      "import numpy as np\n",
      "from numpy.random import rand ,randn\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.spatial import Voronoi , voronoi_plot_2dChapter 7. Classiﬁcation 271\n",
      "np.random.seed(12345)\n",
      "M = 80\n",
      "x = randn(M,2)\n",
      "y = np.zeros(M) # pre-allocate list\n",
      "for i in range(M):\n",
      "if rand() <0.5:\n",
      "x[i,1], y[i] = x[i,0] + np.abs(randn()), 0\n",
      "else:\n",
      "x[i,1], y[i] = x[i,0] - np.abs(randn()), 1\n",
      "vor = Voronoi(x)\n",
      "plt_options = { 'show_vertices ':False , 'show_points ':False ,\n",
      "'line_alpha ':0.5}\n",
      "fig = voronoi_plot_2d(vor, **plt_options)\n",
      "plt.plot(x[y==0,0], x[y==0,1], 'bo',\n",
      "x[y==1,0], x[y==1,1], 'rs', markersize=3)\n",
      "7.7 Support Vector Machine\n",
      "Suppose we are given the training set \u001c=f(xi;yi)gn\n",
      "i=1, where each response2yitakes either\n",
      "the value\u00001 or 1, and we wish to construct a classiﬁer taking values in f\u00001;1g. As this\n",
      "merely involves a relabeling of the 0–1 classiﬁcation problem in Section 7.1, the optimal\n",
      "classiﬁcation function for the indicator loss, 1fy,byg, is, by Theorem 7.1, equal to\n",
      "g\u0003(x)=8>><>>:1 ifP[Y=1jX=x]>1=2;\n",
      "\u00001 ifP[Y=1jX=x]<1=2:\n",
      "It is not di \u000ecult to show, see Exercise 5, that the function g\u0003can be viewed as the minimizer\n",
      "of the risk for the hinge loss hinge loss function, Loss( y;by)=(1\u0000yby)+:=maxf0;1\u0000ybyg, over all\n",
      "prediction functions g(not necessarily taking values only in the set f\u00001;1g). That is,\n",
      "g\u0003=argmin\n",
      "gE(1\u0000Y g(X))+: (7.18)\n",
      "Given the training set \u001c, we can approximate the risk `(g)=E(1\u0000Y g(X))+with the train-\n",
      "ing loss\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1(1\u0000yig(xi))+;\n",
      "and minimize this over a (smaller) class of functions to obtain the optimal prediction func-\n",
      "tiong\u001c. Finally, as the prediction function g\u001cgenerally is not a classiﬁer by itself (it usually\n",
      "does not only take values \u00001 or 1), we take the classiﬁer\n",
      "sign g\u001c(x):\n",
      "2The reason why we use responses \u00001 and 1 here, instead of 0 and 1, is that the notation becomes easier.272 7.7. Support Vector Machine\n",
      "Therefore, a feature vector xis classiﬁed according to 1 or \u00001 depending on whether\n",
      "g\u001c(x)>0 or<0, respectively. The optimal decision boundary optimal decision\n",
      "boundaryis given by the set of xfor\n",
      "which g\u001c(x)=0.\n",
      "Similar to the cubic smoothing spline or RKHS setting in (6.19), we can consider ﬁnd-\n",
      "ing the best classiﬁer, given the training data, via the penalized goodness-of-ﬁt optimiza-\n",
      "tion:\n",
      "min\n",
      "g2H 01\n",
      "nnX\n",
      "kgk21\u0000yig(xi)]++e\n",
      "H;\n",
      "and to solve convenient to deﬁne  e\n",
      "the equivalent problem\n",
      "min\n",
      "g2H 0nX\n",
      "i=1[1\u0000yig(xi)]++\n",
      "2kgk2\n",
      "H:\n",
      "We know from the Representer Theorem 6.6 that if \u0014is the reproducing kernel cor- +232\n",
      "responding toH, then the solution is of the form (assuming that the null space H0has a\n",
      "constant term only):\n",
      "g(x)=\u000b0+nX\n",
      "i=1\u000bi\u0014(xi;x): (7.19)\n",
      "Substituting into the minimization expression yields the analogue of (6.21): +232\n",
      "min\n",
      "\u000b;\u000b0nX\n",
      "i=1[1\u0000yi(\u000b0+fK\u000bgi)]++\n",
      "2\u000b>K\u000b; (7.20)\n",
      "where Kis the Gram matrix. This is a convex optimization problem, as it is the sum of a\n",
      "\u000bi=yi,i=1;:::; nandd piecewise linear term in \u000b. Deﬁning\u0015i:=\n",
      "\u0015:=[\u00151;:::;\u0015 n]>, we show in Exercise 10 that the optimal \u000band\u000b0in (7.20) can be\n",
      "obtained by solving the “dual” convex optimization problem\n",
      "max\n",
      "\u0015nX\n",
      "i=1\u0015i\u00001\n",
      "nX\n",
      "i=1nX\n",
      "j=1\u0015i\u0015jyiyj\u0014(xi;xj)\n",
      "subject to: \u0015>y=0;06\u001561;(7.21)\n",
      "and\u000b0=yj\u0000P\n",
      "i=1\u000bi\u0014(xi;xj) for any jfor which\u0015j2(0;1). In view of (7.19), the optimal\n",
      "prediction function (pre-classiﬁer) g\u001cis then given by\n",
      "g\u001c(x)=\u000b0+nX\n",
      "i=1\u000bi\u0014(xi;x)=\u000b0+1\n",
      "nX\n",
      "i=1yi\u0015i\u0014(xi;x): (7.22)\n",
      "To mitigate possible numerical problems in the calculation of \u000b0it is customary to take\n",
      "an overall average:\n",
      "\u000b0=1\n",
      "jJjX\n",
      "j2J8>><>>:yj\u0000nX\n",
      "i=1\u000bi\u0014(xi;xj)9>>=>>;;\n",
      "whereJ:=fj:\u0015j2(0;1)g.Chapter 7. Classiﬁcation 273\n",
      "Note that, from (7.22), the optimal pre-classiﬁer g(x) and the classiﬁer sign g(x) only\n",
      "depend on vectors xifor which\u0015i,0. These vectors are called the support vectors support vectors of the\n",
      "support vector machine. It is also important to note that the quadratic function in (7.21)\n",
      ",i=1;:::; n, we can=arization parameter \n",
      "rewrite (7.21) as\n",
      "min\n",
      "\u00171\n",
      "2X\n",
      "i;j\u0017i\u0017jyiyj\u0014(xi;xj)\u0000nX\n",
      "i=1\u0017i\n",
      "subject to:nX\n",
      "=:C;i=1;:::; n:(7.23)\n",
      "For perfectly separable data, that is, data for which an a \u000ene plane can be drawn to perfectly\n",
      "separate the two classes, we may take C=1, as explained below. Otherwise, Cneeds to\n",
      "be chosen via cross-validation or a test data set, for example.\n",
      "Geometric interpretation\n",
      "For the linear kernel function \u0014(x;x0)=x>x0, we have\n",
      "g\u001c(x)=\f0+\f>x;\n",
      "\u00001Pn\f0=\u000b0and\f=\n",
      "i=1\u0015iyixi=Pn\n",
      "i=1\u000bixi, and so the decision boundary is an a \u000ene\n",
      "plane. The situation is illustrated in Figure 7.7. The decision boundary is formed by the\n",
      "points xsuch that g\u001c(x)=0. The two setsfx:g\u001c(x)=\u00001gandfx:g\u001c(x)=1gare called\n",
      "themargins . The distance from the points on a margin to the decision boundary is 1 =k\fk.\n",
      "123\n",
      "Figure 7.7: Classifying two classes (red and blue) using SVM.\n",
      "Based on the “multipliers” f\u0015ig, we can divide the training samples f(xi;yi)ginto three\n",
      "categories (see Exercise 11):\n",
      "Points for which \u0015i2(0;1). These are the support vectors on the margins (green\n",
      "encircled in the ﬁgure) and are correctly classiﬁed.274 7.7. Support Vector Machine\n",
      "Points for which \u0015i=1. These points, which are also support vectors, lie strictly\n",
      "inside the margins (points 1, 2, and 3 in the ﬁgure). Such points may or may not be\n",
      "correctly classiﬁed.\n",
      "Points for which \u0015i=0. These are the non-support vectors, which all lie outside the\n",
      "margins. Every such point is correctly classiﬁed.\n",
      "If the classes of points fxi:yi=1gandfxi:yi=\u00001gare perfectly separable by some\n",
      "a\u000ene plane, then there will be no points strictly inside the margins, so all support vectors\n",
      "will lie exactly on the margins. In this case (7.20) reduces to\n",
      "min\n",
      "\f;\f0k\fk2\n",
      "subject to: yi(\f0+x>\n",
      "i\f)>1;i=1;:::; n;(7.24)\n",
      "using the fact that \u000b0=\f0andK\u000b=XX>\u000b=X\f. We may replace min k\fk2in (7.24) with\n",
      "max 1=k\fk, as this gives the same optimal solution. As 1 =k\fkis equal to half the margin\n",
      "width, the latter optimization problem has a simple interpretation: separate the points via\n",
      "an a\u000ene hyperplane such that the margin width is maximized.\n",
      "Example 7.6 (Support Vector Machine) The data in Figure 7.8 was uniformly gener-\n",
      "ated on the unit disc. Class-1 points (blue dots) have a radius less than 1 /2 (y-values 1) and\n",
      "class-2 points (red crosses) have a radius greater than 1 /2 (y-values\u00001).\n",
      "-1 -0.5 0 0.5 1-0.8-0.6-0.4-0.200.20.40.60.8\n",
      "Figure 7.8: Separate the two classes.\n",
      "Of course it is not possible to separate the two groups of points via a straight line in\n",
      "R2. However, it is possible to separate them in R3by considering three-dimensional feature\n",
      "vectors z=[z1;z2;z3]>=[x1;x2;x2\n",
      "1+x2\n",
      "2]>. For any x2R2, the corresponding feature vec-\n",
      "torzlies on a quadratic surface. In this space it is possible to separate the fzigpoints into\n",
      "two groups by means of a planar surface, as illustrated in Figure 7.9.Chapter 7. Classiﬁcation 275\n",
      "Figure 7.9: In feature space R3the points can be separated by a plane.\n",
      "We wish to ﬁnd a separating plane in R3using the transformed features. The following\n",
      "Python code uses the SVCfunction of the sklearn module to solve the quadratic optimiz-\n",
      "ation problem (7.23) (with C=1). The results are summarized in Table 7.6. The data is\n",
      "available from the book’s GitHub site as svmcirc.csv .\n",
      "svmquad.py\n",
      "import numpy as np\n",
      "from numpy import genfromtxt\n",
      "from sklearn.svm import SVC\n",
      "data = genfromtxt( 'svmcirc.csv ', delimiter= ',')\n",
      "x = data[:,[0,1]] #vectors are rows\n",
      "y = data[:,[2]].reshape(len(x),) #labels\n",
      "tmp = np.sum(np.power(x,2),axis=1).reshape(len(x),1)\n",
      "z = np.hstack((x,tmp))\n",
      "clf = SVC(C = np.inf, kernel= 'linear ')\n",
      "clf.fit(z,y)\n",
      "print(\"Support Vectors \\n\", clf.support_vectors_)\n",
      "print(\"Support Vector Labels \",y[clf.support_])\n",
      "print(\"Nu\",clf.dual_coef_)\n",
      "print(\"Bias\",clf.intercept_)\n",
      "Support Vectors\n",
      "[[ 0.038758 0.53796 0.29090314]\n",
      "[-0.49116 -0.20563 0.28352184]\n",
      "[-0.45068 -0.04797 0.20541358]\n",
      "[-0.061107 -0.41651 0.17721465]]\n",
      "Support Vector Labels [-1. -1. 1. 1.]\n",
      "Nu [[ -46.49249413 -249.01807328 265.31805855 30.19250886]]\n",
      "Bias [5.617891]276 7.7. Support Vector Machine\n",
      "Table 7.6: Optimal support vector machine parameters for the R3data.\n",
      "z>y\u000b=\u0017y\n",
      "0.0388 0.5380 0.2909 \u00001\u000046.4925\n",
      "\u00000.4912\u00000.2056 0.2835 \u00001\u0000249.0181\n",
      "\u00000.4507\u00000.0480 0.2054 1 265.3181\n",
      "\u00000.0611\u00000.4165 0.1772 1 30.1925\n",
      "It follows that the normal vector of the plane is\n",
      "\f=X\n",
      "i2S\u000bizi=[\u00000:9128;0:8917;\u000024:2764]>;\n",
      "whereSis the set of indices of the support vectors. We see that the plane is almost per-\n",
      "pendicular to the z1;z2plane. The bias term \f0can also be found from the table above. In\n",
      "particular, for any x>andyin Table 7.6, we have y\u0000\f>z=\f0=5:6179.\n",
      "To draw the separating boundary in R2we need to project the intersection of the sep-\n",
      "arating plane with the quadratic surface onto the z1;z2plane. That is, we need to ﬁnd all\n",
      "points ( z1;z2) such that\n",
      "5:6179\u00000:9128 z1+0:8917 z2=24:2764 ( z2\n",
      "1+z2\n",
      "2): (7.25)\n",
      "This is the equation of a circle with (approximate) center (0 :019;\u00000:018) and radius 0 :48,\n",
      "which is very close to the true circular boundary between the two groups, with center (0 ;0)\n",
      "and radius 0.5. This circle is drawn in Figure 7.10.\n",
      "-1 0 1-101\n",
      "Figure 7.10: The circular decision boundary can be viewed equivalently as (a) the pro-\n",
      "jection onto the x1;x2plane of the intersection of the separating plane with the quadratic\n",
      "surface (both in R3), or (b) the set of points x=(x1;x2) for which g\u001c(x)=\f0+\f>\u001e(x)=0.\n",
      "An equivalent way to derive this circular separating boundary is to consider the feature\n",
      "map\u001e(x)=[x1;x2;x2\n",
      "1+x2\n",
      "2]>onR2, which deﬁnes a reproducing kernel\n",
      "\u0014(x;x0)=\u001e(x)>\u001e(x0);Chapter 7. Classiﬁcation 277\n",
      "onR2, which in turn gives rise to a (unique) RKHS H. The optimal prediction function\n",
      "(7.19) is now of the form\n",
      "g\u001c(x)=\u000b0+1\n",
      "nX\n",
      "i=1yi\u0015i\u001e(xi)>\u001e(x)=\f0+\f>\u001e(x); (7.26)\n",
      "where\u000b0=\f0and\n",
      "\f=1\n",
      "nX\n",
      "i=1yi\u0015i\u001e(xi):\n",
      "The decision boundary, fx:g\u001c(x)=0g, is again a circle in R2. The following code de-\n",
      "termines the ﬁtted model parameters and the decision boundary. Figure 7.10 shows the\n",
      "optimal decision boundary, which is identical to (7.25). The function mykernel speciﬁes\n",
      "the custom kernel above.\n",
      "svmkern.py\n",
      "import numpy as np, matplotlib.pyplot as plt\n",
      "from numpy import genfromtxt\n",
      "from sklearn.svm import SVC\n",
      "def mykernel(U,V):\n",
      "tmpU = np.sum(np.power(U,2),axis=1).reshape(len(U),1)\n",
      "U = np.hstack((U,tmpU))\n",
      "tmpV = np.sum(np.power(V,2),axis=1).reshape(len(V),1)\n",
      "V = np.hstack((V,tmpV))\n",
      "K = U @ V.T\n",
      "print(K.shape)\n",
      "return K\n",
      "# read in the data\n",
      "inp = genfromtxt( 'svmcirc.csv ', delimiter= ',')\n",
      "data = inp[:,[0,1]] #vectors are rows\n",
      "y = inp[:,[2]].reshape(len(data),) #labels\n",
      "clf = SVC(C = np.inf, kernel=mykernel , gamma= 'auto ') # custom kernel\n",
      "# clf = SVC(C = np.inf, kernel=\"rbf\", gamma= 'scale ') # inbuilt\n",
      "clf.fit(data ,y)\n",
      "print(\"Support Vectors \\n\", clf.support_vectors_)\n",
      "print(\"Support Vector Labels \",y[clf.support_])\n",
      "print(\"Nu \",clf.dual_coef_)\n",
      "print(\"Bias \",clf.intercept_)\n",
      "# plot\n",
      "d = 0.001\n",
      "x_min , x_max = -1,1\n",
      "y_min , y_max = -1,1\n",
      "xx, yy = np.meshgrid(np.arange(x_min , x_max , d), np.arange(y_min ,\n",
      "y_max , d))\n",
      "plt.plot(data[clf.support_ ,0],data[clf.support_ ,1], 'go')\n",
      "plt.plot(data[y==1,0],data[y==1,1], 'b.')\n",
      "plt.plot(data[y==-1,0],data[y==-1,1], 'rx')278 7.7. Support Vector Machine\n",
      "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "Z = Z.reshape(xx.shape)\n",
      "plt.contour(xx, yy, Z,colors =\"k\")\n",
      "plt.show()\n",
      "Finally, we illustrate the use of the Gaussian kernel\n",
      "\u0014(x;x0)=e\u0000ckx\u0000x0k2; (7.27)\n",
      "where c>0 is some tuning constant. This is an example of a radial basis function kernel ,\n",
      "which are reproducing kernels of the form \u0014(x;x0)=f(kx\u0000x0k), for some positive real-\n",
      "valued function f. Each feature vector xis now transformed to a function\u0014x=\u0014(x;\u0001). We\n",
      "can think of it as the (unnormalized) pdf of a Gaussian distribution centered around x, and\n",
      "g\u001cis a (signed) mixture of these pdfs, plus a constant; that is,\n",
      "g\u001c(x)=\u000b0+nX\n",
      "i=1\u000bie\u0000ckxi\u0000xk2:\n",
      "Replacing in Line 2 of the previous code mykernel with 'rbf' produces the SVM\n",
      "parameters given in Table 7.7. Figure 7.11 shows the decision boundary, which is not ex-\n",
      "actly circular, but is close to the true (circular) boundary fx:kxk=1=2g. There are now\n",
      "seven support vectors, rather than the four in Figure 7.10.\n",
      "Table 7.7: Optimal support vector machine parameters for the Gaussian kernel case.\n",
      "x>y\u000b(\u0002109)\n",
      "0.0388 0.5380 \u00001\u00000.0635\n",
      "\u00000.4912\u00000.2056\u00001\u00009.4793\n",
      "0.5086 0.1576 \u00001\u00000.5240\n",
      "\u00000.4507\u00000.0480 1 5.5405x>y\u000b(\u0002109)\n",
      "\u00000.4374 0.3854 \u00001\u00001.4399\n",
      "0.3402\u00000.5740\u00001\u00000.1000\n",
      "\u00000.4098\u00000.1763 1 6.0662\n",
      "-1 0 1-101\n",
      "Figure 7.11: Left: The decision boundary fx:g\u001c(x)=0gis roughly circular, and separates\n",
      "the two classes well. There are seven support vectors, indicated by green circles. Right:\n",
      "The graph of g\u001cis a scaled mixture of Gaussian pdfs plus a constant.Chapter 7. Classiﬁcation 279\n",
      "Remark 7.2 (Scaling and Penalty Parameters) When using a radial basis function in\n",
      "SVCinsklearn , the scaling c(7.27) can be set via the parameter gamma . Note that large\n",
      "values of gamma lead to highly peaked predicted functions, and small values lead to highly\n",
      "in (7.23).redicted functions. The parameter CinSVCrefers C=1=\n",
      "7.8 Classiﬁcation with Scikit-Learn\n",
      "In this section we apply several classiﬁcation methods to a real-world data set, using the\n",
      "Python module sklearn (the package name is Scikit-Learn). Speciﬁcally, the data is ob-\n",
      "tained from UCI’s Breast Cancer Wisconsin data set. This data set, ﬁrst published and\n",
      "analyzed in [118], contains the measurements related to 569 images of 357 benign and\n",
      "212 malignant breast masses. The goal is to classify a breast mass as benign or malig-\n",
      "nant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,\n",
      "Concavity, Concave Points, Symmetry, and Fractal Dimension of each mass. The mean,\n",
      "standard error, and “worst” of these attributes were computed for each image, resulting in\n",
      "30 features. For instance, feature 1 is Mean Radius, feature 11 is Radius SE, feature 21 is\n",
      "Worst Radius.\n",
      "The following Python code reads the data, extracts the response vector and model (fea-\n",
      "ture) matrix and divides the data into a training and test set.\n",
      "skclass1.py\n",
      "from numpy import genfromtxt\n",
      "from sklearn.model_selection import train_test_split\n",
      "url1 = \"http://mlr.cs.umass.edu/ml/machine -learning -databases/\"\n",
      "url2 = \"breast -cancer -wisconsin/\"\n",
      "name = \"wdbc.data\"\n",
      "data = genfromtxt(url1 + url2 + name , delimiter= ',', dtype=str)\n",
      "y = data[:,1] #responses\n",
      "X = data[:,2:].astype( 'float ') #features as an ndarray matrix\n",
      "X_train , X_test , y_train , y_test = train_test_split(\n",
      "X, y, test_size = 0.4, random_state = 1234)\n",
      "To visualize the data we create a 3D scatterplot for the features mean radius , mean\n",
      "texture , and mean concavity , which correspond to the columns 0, 1, and 6 of the model\n",
      "matrix X. Figure 7.12 suggests that the malignant and benign breast masses could be well\n",
      "separated using these three features.\n",
      "skclass2.py\n",
      "from skclass1 import X, y\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import numpy as np\n",
      "Bidx = np.where(y == 'B')\n",
      "Midx= np.where(y == 'M')\n",
      "# plot features Radius (column 0), Texture (1), Concavity (6)280 7.8. Classiﬁcation with Scikit-Learn\n",
      "fig = plt.figure()\n",
      "ax = fig.gca(projection = '3d')\n",
      "ax.scatter(X[Bidx ,0], X[Bidx ,1], X[Bidx ,6],\n",
      "c='r', marker= '^', label= 'Benign ')\n",
      "ax.scatter(X[Midx ,0], X[Midx ,1], X[Midx ,6],\n",
      "c='b', marker= 'o', label= 'Malignant ')\n",
      "ax.legend()\n",
      "ax.set_xlabel( 'Mean Radius ')\n",
      "ax.set_ylabel( 'Mean Texture ')\n",
      "ax.set_zlabel( 'Mean Concavity ')\n",
      "plt.show()\n",
      "Mean Radius10\n",
      "15\n",
      "20\n",
      "25Mean Texture10152025303540Mean Concavity\n",
      "0.00.10.20.30.4\n",
      "Benign\n",
      "Malignant\n",
      "Figure 7.12: Scatterplot of three features of the benign and malignant breast masses.\n",
      "The following code uses various classiﬁers to predict the category of breast masses\n",
      "(benign or malignant). In this case the training set has 341 elements and the test set has 228\n",
      "elements. For each classiﬁer the percentage of correct predictions (that is, the accuracy) in\n",
      "the test set is reported. We see that in this case quadratic discriminant analysis gives the\n",
      "highest accuracy (0 :956). Exercise 18 explores the question whether this metric is the most\n",
      "appropriate for these data.\n",
      "skclass3.py\n",
      "from skclass1 import X_train , y_train , X_test , y_test\n",
      "from sklearn.metrics import accuracy_score\n",
      "import sklearn.discriminant_analysis as DA\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "names = [\"Logit\",\"NBayes\", \"LDA\", \"QDA\", \"KNN\", \"SVM\"]Chapter 7. Classiﬁcation 281\n",
      "classifiers = [LogisticRegression(C=1e5),\n",
      "GaussianNB(),\n",
      "DA.LinearDiscriminantAnalysis(),\n",
      "DA.QuadraticDiscriminantAnalysis(),\n",
      "KNeighborsClassifier(n_neighbors=5),\n",
      "SVC(kernel= 'rbf', gamma = 1e-4)]\n",
      "print( 'Name Accuracy\\n '+14* '-')\n",
      "for name , clf in zip(names , classifiers):\n",
      "clf.fit(X_train , y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print( '{:6} {:3.3f} '.format(name , accuracy_score(y_test ,y_pred)))\n",
      "Name Accuracy\n",
      "--------------\n",
      "Logit 0.943\n",
      "NBayes 0.908\n",
      "LDA 0.943\n",
      "QDA 0.956\n",
      "KNN 0.925\n",
      "SVM 0.939\n",
      "Further Reading\n",
      "An excellent source for understanding various pattern recognition techniques is the book\n",
      "[35] by Duda et al. Theoretical foundations of classiﬁcation, including the Vapnik–\n",
      "Chernovenkis dimension and the fundamental theorem of learning, are discussed in\n",
      "[109, 121, 122]. A popular measure for characterizing the performance of a binary classi-\n",
      "ﬁer is the receiver operating characteristic (ROC) curve [38]. The naïve Bayes classiﬁc-\n",
      "ation paradigm can be extended to handle explanatory variable dependency via graphical\n",
      "models such as Bayesian networks and Markov random ﬁelds [46, 66, 69]. For a detailed\n",
      "discussion on Bayesian decision theory, see [8].\n",
      "Exercises\n",
      "1. Let 06w61. Show that the solution to the convex optimization problem\n",
      "min\n",
      "p1;:::;pnnX\n",
      "i=1p2\n",
      "i\n",
      "subject to:n\u00001X\n",
      "i\u00001pi=wandnX\n",
      "i=1pi=1;(7.28)\n",
      "is given by pi=w=(n\u00001);i=1;:::; n\u00001 and pn=1\u0000w.\n",
      "2. Derive the formulas (7.14) by minimizing the cross-entropy training loss:\n",
      "\u00001\n",
      "nnX\n",
      "i=1lng(xi;yij\u0012);282 Exercises\n",
      "where g(x;yj\u0012) is such that:\n",
      "lng(x;yj\u0012)=ln\u000by\u00001\n",
      "2lnj\u0006yj\u00001\n",
      "2(x\u0000\u0016y)>\u0006\u00001\n",
      "y(x\u0000\u0016y)\u0000p\n",
      "2ln(2\u0019):\n",
      "3. Adapt the code in Example 7.2 to plot the estimated decision boundary instead of the\n",
      "true one in Figure 7.3. Compare the true and estimated decision boundaries.\n",
      "4. Recall from equation (7.16) that the decision boundaries of the multi-logit classiﬁer are\n",
      "linear, and that the pre-classiﬁer can be written as a conditional pdf of the form:\n",
      "g(yjW;b;x)=exp(zy+1)Pc\n",
      "i=1exp(zi);y2f0;:::; c\u00001g;\n",
      "where x>=[1;ex>] and z=Wex+b.\n",
      "(a) Show that the linear discriminant pre-classiﬁer in Section 7.4 can also be written as a\n",
      "conditional pdf of the form ( \u0012=f\u000by;\u0006y;\u0016ygc\u00001\n",
      "y=0):\n",
      "g(yj\u0012;x)=exp(zy+1)Pc\n",
      "i=1exp(zi);y2f0;:::; c\u00001g;\n",
      "where x>=[1;ex>] and z=Wex+b. Find formulas for the corresponding bandW\n",
      "in terms of the linear discriminant parameters f\u000by;\u0016y;\u0006ygc\u00001\n",
      "y=0, where \u0006y=\u0006for all y.\n",
      "(b) Explain which pre-classiﬁer has smaller approximation error: the linear discriminant\n",
      "or multi-logit one? Justify your answer by proving an inequality between the two\n",
      "approximation errors.\n",
      "5. Consider a binary classiﬁcation problem where the response Ytakes values inf\u00001;1g.\n",
      "Show that optimal prediction function for the hinge loss Loss( y;by)=(1\u0000yby)+:=maxf0;1\u0000\n",
      "ybygis the same as the optimal prediction function g\u0003for the indicator loss:\n",
      "g\u0003(x)=8>><>>:1 ifP[Y=1jX=x]>1=2;\n",
      "\u00001 ifP[Y=1jX=x]<1=2:\n",
      "That is, show that\n",
      "E(1\u0000Y h(X))+>E(1\u0000Y g\u0003(X))+ (7.29)\n",
      "for all functions h.\n",
      "6. In Example 4.12, we applied a principal component analysis (PCA) to the iris data, +158\n",
      "but refrained from classifying the ﬂowers based on their feature vectors x. Implement a\n",
      "1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs ( x;y)\n",
      "from the iris data set. How many of the remaining 100 ﬂowers are correctly classiﬁed?\n",
      "Now classify these entries with an o \u000b-the-shelf multi-logit classiﬁer, e.g., such as can be\n",
      "found in the sklearn andstatsmodels packages.\n",
      "7. Figure 7.13 displays two groups of data points, given in Table 7.8. The convex hulls\n",
      "have also been plotted. It is possible to separate the two classes of points via a straight line.Chapter 7. Classiﬁcation 283\n",
      "In fact, many such lines are possible. SVM gives the best separation, in the sense that the\n",
      "gap (margin) between the points is maximal.\n",
      "-3 -2 -1 0 1 2 3 4-2-10123456\n",
      "Figure 7.13: Separate the points by a straight line so that the separation between the two\n",
      "groups is maximal.\n",
      "Table 7.8: Data for Figure 7.13.\n",
      "x1 x2 y\n",
      "2:4524 5 :5673\u00001\n",
      "1:2743 0 :8265 1\n",
      "0:8773\u00000:5478 1\n",
      "1:4837 3 :0464\u00001\n",
      "0:0628 4 :0415\u00001\n",
      "\u00002:4151\u00000:9309 1\n",
      "1:8152 3 :9202\u00001\n",
      "1:8557 2 :7262\u00001\n",
      "\u00000:4239 1 :8349 1\n",
      "1:9630 0 :6942 1x1 x2 y\n",
      "0:5819\u00001:0156 1\n",
      "1:2065 3 :2984\u00001\n",
      "2:6830 0 :4216 1\n",
      "\u00000:0734 1 :3457 1\n",
      "0:0787 0 :6363 1\n",
      "0:3816 5 :2976\u00001\n",
      "0:3386 0 :2882 1\n",
      "\u00000:1493\u00000:7095 1\n",
      "1:5554 4 :9880\u00001\n",
      "3:2031 4 :4614\u00001\n",
      "(a) Identify from the ﬁgure the three support vectors.\n",
      "(b) For a separating boundary (line) given by \f0+\f>x=0, show that the margin width\n",
      "is 2=k\fk.\n",
      "(c) Show that the parameters \f0and\fthat solve the convex optimization problem (7.24)\n",
      "provide the maximal width between the margins.\n",
      "(d) Solve (7.24) using a penalty approach; see Section B.4. In particular, minimize the +417284 Exercises\n",
      "penalty function\n",
      "S(\f;\f0)=k\fk2\u0000CnX\n",
      "i=1minn\n",
      "(\f0+\f>xi)yi\u00001;0o\n",
      "for some positive penalty constant C.\n",
      "(e) Find the solution the dual optimization problem (7.21) by using sklearn ’sSCV\n",
      "method. Note that, as the two point sets are separable, the constraint \u001561 may\n",
      "can be set to 1.the value of \n",
      "8. In Example 7.6 we used the feature map \u001e(x)=[x1;x2;x2\n",
      "1+x2\n",
      "2]>to classify the points.\n",
      "An easier way is to map the points into R1via the feature map \u001e(x)=kxkor any monotone\n",
      "function thereof. Translated back into R2this yields a circular separating boundary. Find\n",
      "the radius and center of this circle, using the fact that here the sorted norms for the two\n",
      "groups are:::;0:4889;0:5528;:::.\n",
      "9. Let Y2f0;1gbe a response variable and let h(x) be the regression function\n",
      "h(x) :=E[YjX=x]=P[Y=1jX=x]:\n",
      "Recall that the Bayes classiﬁer is g\u0003(x)=1fh(x)>1=2g:Letg:R! f0;1gbe any\n",
      "other classiﬁer function. Below, we denote all probabilities and expectations conditional\n",
      "onX=xasPx[\u0001] andEx[\u0001].\n",
      "(a) Show that\n",
      "Px[g(x),Y]=irreducible errorz           }|           {\n",
      "Px[g\u0003(x),Y]+j2h(x)\u00001j1fg(x),g\u0003(x)g:\n",
      "Hence, deduce that for a learner gTconstructed from a training set T, we have\n",
      "E[Px[gT(x),YjT]]=Px[g\u0003(x),Y]+j2h(x)\u00001jP[gT(x),g\u0003(x)];\n",
      "where the ﬁrst expectation and last probability operations are with respect to T.\n",
      "(b) Using the previous result, deduce that for the unconditional error (that is, we no longer\n",
      "condition on X=x), we have\n",
      "P[g\u0003(X),Y]6P[gT(X),Y]:\n",
      "(c) Show that, if gT:=1fhT(x)>1=2gis a classiﬁer function such that as n!1\n",
      "hT(x)d\u0000!Z\u0018N(\u0016(x);\u001b2(x))\n",
      "for some mean and variance functions \u0016(x) and\u001b2(x), respectively, then\n",
      "Px[gT(x),g\u0003(x)]\u0000 sign(1\u00002h(x))(2\u0016(x)\u00001)\n",
      "2\u001b(x)!\n",
      ";\n",
      "whereis the cdf of a standard normal random variable.Chapter 7. Classiﬁcation 285\n",
      "10. The purpose of this exercise is to derive the dual program (7.21) from the primal\n",
      "program (7.20). The starting point is to introduce a vector of auxiliary variables \u0018:=\n",
      "[\u00181;:::;\u0018 n]>and write the primal program as\n",
      "min\n",
      "\u000b;\u000b0;\u0018nX\n",
      "i=1\u0018i+\n",
      "2\u000b>K\u000b\n",
      "subject to: \u0018>0;\n",
      "yi(\u000b0+fK\u000bgi)>1\u0000\u0018i;i=1;:::; n:(7.30)\n",
      "(a) Apply the Lagrangian optimization theory from Section B.2.2 to obtain the Lag- +408\n",
      "rangian functionL(f\u000b0;\u000b;\u0018g;f\u0015;\u0016g), where\u0016and\u0015are the Lagrange multipliers cor-\n",
      "responding to the ﬁrst and second inequality constraints, respectively.\n",
      "(b) Show that the Karush–Kuhn–Tucker (see Theorem B.2) conditions for optimizing L +409\n",
      "are:\n",
      "\u0015>y=0\n",
      "\u000b=y\f\u0015=\n",
      "06\u001561\n",
      "(1\u0000\u0015)\f\u0018=0; \u0015 i(yig(xi)\u00001+\u0018i)=0;i=1;:::; n\n",
      "\u0018>0;yig(xi)\u00001+\u0018i>0;i=1;:::; n:(7.31)\n",
      "Here\fstands for componentwise multiplication; e.g., y\f\u0015=[y1\u00151;:::; yn\u0015n]>, and\n",
      "we have abbreviated \u000b0+fK\u000bgitog(xi), in view of (7.19). [Hint: one of the KKT\n",
      "conditions is \u0015=1\u0000\u0016; thus we can eliminate \u0016.]\n",
      "(c) Using the KKT conditions (7.31), reduce the Lagrange dual function L\u0003(\u0015) :=\n",
      "min\u000b0;\u000b;\u0018L(f\u000b0;\u000b;\u0018g;f\u0015;1\u0000\u0015g) to\n",
      "L\u0003(\u0015)=nX\n",
      "i=1\u0015i\u00001\n",
      "nX\n",
      "i=1nX\n",
      "j=1\u0015i\u0015jyiyj\u0014(xi;xj): (7.32)\n",
      "(d) As a consequence of (7.19) and (a)–(c), show that the optimal prediction function g\u001c\n",
      "is given by\n",
      "g\u001c(x)=\u000b0+1\n",
      "nX\n",
      "i=1yi\u0015i\u0014(xi;x); (7.33)\n",
      "where\u0015is the solution to\n",
      "max\n",
      "\u0015L\u0003(\u0015)\n",
      "subject to: \u0015>y=0;06\u001561;(7.34)\n",
      "and\u000b0=yj\u00001\n",
      "Pn\n",
      "i=1yi\u0015i\u0014(xi;xj) for any jsuch that\u0015j2(0;1).\n",
      "11. Consider SVM classiﬁcation as illustrated in Figure 7.7. The goal of this exercise is to\n",
      "classify the training points f(xi;yi)gbased on the value of the multipliers f\u0015igin Exercise 10.\n",
      "Let\u0018ibe the auxiliary variable in Exercise 10, i=1;:::; n.286 Exercises\n",
      "(a) For\u0015i2(0;1) show that ( xi;yi) lies exactly on the decision border.\n",
      "(b) For\u0015i=1, show that ( xi;yi) lies strictly inside the margins.\n",
      "(c) Show that for \u0015i=0 the point ( xi;yi) lies outside the margins and is correctly classi-\n",
      "ﬁed.\n",
      "12. A well-known data set is the MNIST handwritten digit database, containing many\n",
      "thousands of digitalized numbers (from 0 to 9), each described by a 28 \u000228 matrix of gray\n",
      "scales. A similar but much smaller data set is described in [63]. Here, each handwritten\n",
      "digit is summarized by a 8 \u00028 matrix with integer entries from 0 (white) to 15 (black).\n",
      "Figure 7.14 shows the ﬁrst 50 digitized images. The data set can be accessed with Python\n",
      "using the sklearn package as follows.\n",
      "from sklearn import datasets\n",
      "digits = datasets.load_digits()\n",
      "x_digits = digits.data # explanatory variables\n",
      "y_digits = digits.target # responses\n",
      "Figure 7.14: Classify the digitized images.\n",
      "(a) Divide the data into a 75% training set and 25% test set.\n",
      "(b) Compare the e \u000bectiveness of the K-nearest neighbors and naïve Bayes method to\n",
      "classify the data.\n",
      "(c) Assess which Kto use in the K-nearest neighbors classiﬁcation.\n",
      "13. Download the winequality-red.csv data set from UCI’s wine-quality website.\n",
      "The response here is the wine quality (from 0 to 10) as speciﬁed by a wine “expert”\n",
      "and the explanatory variables are various characteristics such as acidity and sugar con-\n",
      "tent. Use the SVC classiﬁer of sklearn.svm with a linear kernel and penalty para-\n",
      "meter C=1 (see Remark 7.2) to ﬁt the data. Use the method cross_val_score fromChapter 7. Classiﬁcation 287\n",
      "sklearn.model_selection to obtain a ﬁve-fold cross-validation score as an estimate of\n",
      "the probability that the predicted class matches the expert’s class.\n",
      "14. Consider the credit approval data set crx.data from UCI’s credit approval website.\n",
      "The data set is concerned with credit card applications. The last column in the data set\n",
      "indicates whether the application is approved ( +) or not (\u0000). With the view of preserving\n",
      "data privacy, all 15 explanatory variables were anonymized. Note that some explanatory\n",
      "variables are continuous and some are categorical.\n",
      "(a) Load and prepare the data for analysis with sklearn . First, eliminate data\n",
      "rows with missing values. Next, encode categorical explanatory variables using a\n",
      "OneHotEncoder object from sklearn.preprocessing to create a model matrix X\n",
      "with indicator variables for the categorical variables, as described in Section 5.3.5. +177\n",
      "(b) The model matrix should contain 653 rows and 46 columns. The response variable\n",
      "should be a 0 /1 variable (reject /approve). We will consider several classiﬁcation al-\n",
      "gorithms and test their performance (using a zero-one loss) via ten-fold cross valida-\n",
      "tion.\n",
      "i. Write a function which takes 3 parameters: X;y, and a model, and returns the\n",
      "ten-fold cross-validation estimate of the expected generalization risk.\n",
      "ii. Consider the following sklearn classiﬁers: KNeighborsClassifier (k=5),\n",
      "LogisticRegression , and MPLClassifier (multilayer perceptron). Use the\n",
      "function from (i) to identify the best performing classiﬁer.\n",
      "15. Consider a synthetic data set that was generated in the following fashion. The explan-\n",
      "atory variable follows a standard normal distribution. The response label is 0 if the explan-\n",
      "atory variable is between the 0.95 and 0.05 quantiles of the standard normal distribution,\n",
      "and 1, otherwise. The data set was generated using the following code.\n",
      "import numpy as np\n",
      "import scipy.stats\n",
      "# generate data\n",
      "np.random.seed(12345)\n",
      "N = 100\n",
      "X = np.random.randn(N)\n",
      "q = scipy.stats.norm.ppf(0.95)\n",
      "y = np.zeros(N)\n",
      "y[X>=q] = 1\n",
      "y[X<=-q] = 1\n",
      "X = X.reshape(-1,1)\n",
      "Compare the K-nearest neighbors classiﬁer with K=5 and logistic regression classi-\n",
      "ﬁer. Without computation, which classiﬁer is likely to be better for these data? Verify your\n",
      "answer by coding both classiﬁers and printing the corresponding training 0–1 loss.\n",
      "16. Consider the digits data set from Exercise 12. In this exercise, we would like to train\n",
      "a binary classiﬁer for the identiﬁcation of digit 8.\n",
      "(a) Divide the data such that the ﬁrst 1000 rows are used as the training set and the rest\n",
      "are used as the test set.288 Exercises\n",
      "(b) Train the LogisticRegression classiﬁer from the sklearn.linear_model pack-\n",
      "age.\n",
      "(c) “Train” a naïve classiﬁer that always returns 0. That is, the naïve classiﬁer identiﬁes\n",
      "each instance as being not 8.\n",
      "(d) Compare the zero-one test losses of the logistic regression and the naïve classiﬁers.\n",
      "(e) Find the confusion matrix, the precision, and the recall of the logistic regression clas-\n",
      "siﬁer.\n",
      "(f) Find the fraction of eights that are correctly detected by the logistic regression clas-\n",
      "siﬁer.\n",
      "17. Repeat Exercise 16 with the original MNIST data set. Use the ﬁrst 60,000 rows as the\n",
      "train set and the remaining 10,000 rows as the test set. The original data set can be obtained\n",
      "using the following code.\n",
      "from sklearn.datasets import fetch_openml\n",
      "X, y = fetch_openml( 'mnist_784 ', version=1, return_X_y=True)\n",
      "18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is +279\n",
      "the relevant metric to use or if other metrics discussed in Section 7.2 are more appropriate. +255CHAPTER8\n",
      "DECISION TREES AND ENSEMBLE\n",
      "METHODS\n",
      "Statistical learning methods based on decision trees have gained tremendous pop-\n",
      "ularity due to their simplicity, intuitive representation, and predictive accuracy. This\n",
      "chapter gives an introduction to the construction and use of such trees. We also dis-\n",
      "cuss two key ensemble methods, namely bootstrap aggregation and boosting, which\n",
      "can further improve the e \u000eciency of decision trees and other learning methods.\n",
      "8.1 Introduction\n",
      "Tree-based methods provide a simple, intuitive, and powerful mechanism for both regres-\n",
      "sion and classiﬁcation. The main idea is to divide a (potentially complicated) feature space\n",
      "Xinto smaller regions and ﬁt a simple prediction function to each region. For example,\n",
      "in a regression setting, one could take the mean of the training responses associated with\n",
      "the training features that fall in that speciﬁc region. In the classiﬁcation setting, a com-\n",
      "monly used prediction function takes the majority vote among the corresponding response\n",
      "variables. We start with a simple classiﬁcation example.\n",
      "Example 8.1 (Decision Tree for Classiﬁcation) The left panel of Figure 8.1 shows a\n",
      "training set of 15 two-dimensional points (features) falling into two classes (red and blue).\n",
      "How should the new feature vector (black point) be classiﬁed?\n",
      "20 10 0 10 202010010203040\n",
      "20 10 0 10 202010010203040\n",
      "Figure 8.1: Left: training data and a new feature. Right: a partition of the feature space.\n",
      "289290 8.1. Introduction\n",
      "It is not possible to linearly separate the training set, but we can partition the feature\n",
      "spaceX=R2into rectangular regions and assign a class (color) to each region, as shown\n",
      "in the right panel of Figure 8.1. Points in these regions are classiﬁed accordingly as blue\n",
      "or red. The partition thus deﬁnes a classiﬁer (prediction function) gthat assigns to each\n",
      "feature vector xa class “red” or “blue”. For example, for x=[\u000015;0]>(solid black point),\n",
      "g(x)=“blue”, since it belongs to a blue region of the feature space.\n",
      "Both the classiﬁcation procedure and the partitioning of the feature space can be con-\n",
      "veniently represented by a binary decision tree decision tree . This is a tree where each node vcorres-\n",
      "ponds to a region (subset) Rvof the feature space X— the root node corresponding to the\n",
      "feature space itself.\n",
      "x2≤12.0\n",
      "x1≤ −20.5\n",
      "x1≤20.0\n",
      "x1≤2.5\n",
      "x1≤ −5.0True\n",
      "TrueFalse\n",
      "True\n",
      "True\n",
      "True FalseFalseFalseFalse\n",
      "Figure 8.2: The decision-\n",
      "tree that corresponds to the\n",
      "partition in Figure 8.1.Each internal node vcontains a logical condition that di-\n",
      "videsRvinto two disjoint subregions. The leaf nodes (the ter-\n",
      "minal nodes of the tree) are not subdivided, and their corres-\n",
      "ponding regions form a partition of X, as they are disjoint and\n",
      "their union isX. Associated with each leaf node wis also a\n",
      "regional prediction function gwonRw.\n",
      "The partitioning of Figure 8.1 was obtained from\n",
      "the decision tree shown in Figure 8.2. As an illustra-\n",
      "tion of the decision procedure, consider again the input\n",
      "x=[x1;x2]>=[\u000015;0]>. The classiﬁcation process starts\n",
      "from the tree root, which contains the condition x2612:0. As\n",
      "the second component of xis 0, the root condition is satisﬁed\n",
      "and we proceed to the left child, which contains the condition\n",
      "x16\u000020:5. The next step is similar. As 0 >\u000020:5, the condi-\n",
      "tion is not satisﬁed and we proceed to the right child. Such an\n",
      "evaluation of logical conditions along the tree path will even-\n",
      "tually bring us to a leaf node and its associated region. In this\n",
      "case the process terminates in a leaf that corresponds to the\n",
      "left blue region in the right-hand panel of Figure 8.1.\n",
      "More generally, a binary tree Twill partition the feature space Xinto as many regions\n",
      "as there are leaf nodes. Denote the set of leaf nodes by W. The overall prediction function\n",
      "gthat corresponds to the tree can then be written as\n",
      "g(x)=X\n",
      "w2Wgw(x)1fx2R wg; (8.1)\n",
      "where 1denotes the indicator function. The representation (8.1) is very general and de-\n",
      "pends on (1) how the regions fRwgare constructed via the logical conditions in the decision\n",
      "tree, as well as (2) how the regional prediction functions regional\n",
      "prediction\n",
      "functionsof the leaf nodes are deﬁned.\n",
      "Simple logical conditions of the form xj6\u0018split a Euclidean feature space into rect-\n",
      "angles aligned with the axes. For example, Figure 8.2 partitions the feature space into six\n",
      "rectangles: two blue and four red rectangles.\n",
      "In a classiﬁcation setting, the regional prediction function gwcorresponding to a leaf\n",
      "node wtakes values in the set of possible class labels. In most cases, as in Example 8.1, it\n",
      "is taken to be constant on the corresponding region Rw. In a regression setting, gwis real-\n",
      "valued and also usually takes only one value. That is, every feature vector in Rwleads toChapter 8. Decision Trees and Ensemble Methods 291\n",
      "the same predicted value. Of course, di \u000berent regions will usually have di \u000berent predicted\n",
      "values.\n",
      "Constructing a tree with a training set \u001c=f(xi;yi)ggn\n",
      "i=1amounts to minimizing the\n",
      "training loss\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1Loss( yi;g(xi)) (8.2)\n",
      "for some loss function; see Chapter 2. With gof the form (8.1), we can write + 19\n",
      "`\u001c(g)=1\n",
      "nnX\n",
      "i=1Loss( yi;g(xi))=1\n",
      "nnX\n",
      "i=1X\n",
      "w2W1fxi2R wgLoss( yi;g(xi)) (8.3)\n",
      "=X\n",
      "w2W1\n",
      "nnX\n",
      "i=11fxi2R wgLoss( yi;gw(xi))\n",
      "|                                    {z                                    }\n",
      "(\u0003); (8.4)\n",
      "where (\u0003) is the contribution by the regional prediction function gwto the overall training\n",
      "loss. In the case where all fxigare di \u000berent, ﬁnding a decision tree Tthat gives a zero\n",
      "squared-error or zero–one training loss is easy, see Exercise 1, but such an “overﬁtted” tree\n",
      "will have poor predictive behavior, expressed in terms of the generalization risk. Instead\n",
      "we consider a restricted class of decision trees and aim to minimize the training loss within\n",
      "that class. It is common to use a top-down greedy approach, which can only achieve an\n",
      "approximate minimization of the training loss.\n",
      "8.2 Top-Down Construction of Decision Trees\n",
      "Let\u001c=f(xi;yi)gn\n",
      "i=1be the training set. The key to constructing a binary decision tree T\n",
      "is to specify a splitting rule splitting rule for each node v, which can be deﬁned as a logical function\n",
      "s:X!f False;Truegor, equivalently, a binary function s:X!f 0;1g. For example,\n",
      "in the decision tree of Figure 8.2 the root node has splitting rule x7!1fx2612:0g, in\n",
      "correspondence with the logical condition fx2612:0g. During the construction of the tree,\n",
      "each node vis associated with a speciﬁc region Rv\u0012X and therefore also the training\n",
      "subsetf(x;y)2\u001c:x2R vg\u0012\u001c. Using a splitting rule s, we can divide any subset \u001bof the\n",
      "training set \u001cinto two sets:\n",
      "\u001bT:=f(x;y)2\u001b:s(x)=Truegand\u001bF:=f(x;y)2\u001b:s(x)=Falseg: (8.5)\n",
      "Starting from an empty tree and the initial data set \u001c, a generic decision tree con-\n",
      "struction takes the form of the recursive Algorithm 8.2.1. Here we use the notation\n",
      "Tvfor a subtree of Tstarting from node v. The ﬁnal tree Tis thus obtained via T=\n",
      "Construct_Subtree (v0;\u001c), where v0is the root of the tree.292 8.2. Top-Down Construction of Decision Trees\n",
      "Algorithm 8.2.1: Construct_Subtree\n",
      "Input: A node vand a subset of the training data: \u001b\u0012\u001c.\n",
      "Output: A (sub) decision tree Tv.\n",
      "1iftermination criterion is met then //vis a leaf node\n",
      "2 Train a regional prediction function gvusing the training data \u001b.\n",
      "3else // split the node\n",
      "4 Find the best splitting rule svfor node v.\n",
      "5 Create successors vTandvFofv.\n",
      "6\u001bT f(x;y)2\u001b:sv(x)=Trueg\n",
      "7\u001bF f(x;y)2\u001b:sv(x)=Falseg\n",
      "8TvT Construct_Subtree (vT;\u001bT) // left branch\n",
      "9TvF Construct_Subtree (vF;\u001bF) // right branch\n",
      "10returnTv\n",
      "The splitting rule svdivides the region Rvinto two disjoint parts, say RvTandRvF. The\n",
      "corresponding prediction functions, gTandgF, satisfy\n",
      "gv(x)=gT(x)1fx2R vTg+gF(x)1fx2R vFg;x2R v:\n",
      "In order to implement the procedure described in Algorithm 8.2.1, we need to address\n",
      "the construction of the regional prediction functions gvat the leaves (Line 2), the speciﬁc-\n",
      "ation of the splitting rule (Line 4), and the termination criterion (Line 1). These important\n",
      "aspects are detailed in the following Sections 8.2.1, 8.2.2, and 8.2.3, respectively.\n",
      "8.2.1 Regional Prediction Functions\n",
      "In general, there is no restriction on how to choose the prediction function gwfor a leaf\n",
      "node v=win Line 2 of Algorithm 8.2.1. In principle we can train any model from the\n",
      "data; e.g., via linear regression. However, in practice very simple prediction functions are\n",
      "used. Below, we detail a popular choice for classiﬁcation, as well as one for regression.\n",
      "1. In the classiﬁcation setting with class labels 0 ;:::; c\u00001, the regional prediction\n",
      "function gwfor leaf node wis usually chosen to be constant and equal to the most\n",
      "common class label of the training data in the associated region Rw(ties can be\n",
      "broken randomly). More precisely, let nwbe the number of feature vectors in region\n",
      "Rwand let\n",
      "pw\n",
      "z=1\n",
      "nwX\n",
      "f(x;y)2\u001c:x2Rwg1fy=zg;\n",
      "be the proportion of feature vectors in Rwthat have class label z=0;:::; c\u00001. The\n",
      "regional prediction function for node wis chosen to be the constant\n",
      "gw(x)=argmax\n",
      "z2f0;:::;c\u00001gpw\n",
      "z: (8.6)\n",
      "2. In the regression setting, gwis usually chosen as the mean response in the region;\n",
      "that is,\n",
      "gw(x)=yRw:=1\n",
      "nwX\n",
      "f(x;y)2\u001c:x2Rwgy; (8.7)Chapter 8. Decision Trees and Ensemble Methods 293\n",
      "where nwis again the number of feature vectors in Rw. It is not di \u000ecult to show that\n",
      "gw(x)=yRwminimizes the squared-error loss with respect to all constant functions,\n",
      "in the regionRw; see Exercise 2.\n",
      "8.2.2 Splitting Rules\n",
      "In Line 4 in Algorithm 8.2.1, we divide region Rvinto two sets, using a splitting rule\n",
      "(function) sv. Consequently, the data set \u001bassociated with node v(that is, the subset of the\n",
      "original data set \u001cwhose feature vectors lie in Rv), is also split — into \u001bTand\u001bF. What is\n",
      "the beneﬁt of such a split in terms of a reduction in the training loss? If vwere set to a leaf\n",
      "node, its contribution to the training loss would be (see (8.4)):\n",
      "1\n",
      "nnX\n",
      "i=11f(x;y)2\u001bgLoss( yi;gv(xi)): (8.8)\n",
      "Ifvwere to be split instead, its contribution to the overall training loss would be:\n",
      "1\n",
      "nnX\n",
      "i=11f(x;y)2\u001bTgLoss( yi;gT(xi))+1\n",
      "nnX\n",
      "i=11f(x;y)2\u001bFgLoss( yi;gF(xi)); (8.9)\n",
      "where gTandgFare the prediction functions belonging to the child nodes vTandvF. A\n",
      "greedy heuristic is to pretend that the tree construction algorithm immediately terminates\n",
      "after the split, in which case vTandvFare leaf nodes, and gTandgFare readily evaluated\n",
      "— e.g., as in Section 8.2.1. Note that for any splitting rule the contribution (8.8) is always\n",
      "greater than or equal to (8.9). It therefore makes sense to choose the splitting rule such that\n",
      "(8.9) is minimized. Moreover, the termination criterion may involve comparing (8.9) with\n",
      "(8.8). If their di \u000berence is too small it may not be worth further splitting the feature space.\n",
      "As an example, suppose the feature space is X=Rpand we consider splitting rules of\n",
      "the form\n",
      "s(x)=1fxj6\u0018g; (8.10)\n",
      "for some 16j6pand\u00182R, where we identify 0 with False and 1 with True . Due to the\n",
      "computational and interpretative simplicity, such binary splitting rules are implemented in\n",
      "many software packages and are considered to be the de facto standard. As we have seen,\n",
      "these rules divide up the feature space into rectangles, as in Figure 8.1. It is natural to ask\n",
      "how jand\u0018should be chosen so as to minimize (8.9). For a regression problem, using a\n",
      "squared-error loss and a constant regional prediction function as in (8.7), the sum (8.9) is\n",
      "given by\n",
      "1\n",
      "nX\n",
      "(x;y)2\u001c:xj6\u0018\u0000y\u0000yT\u00012+1\n",
      "nX\n",
      "(x;y)2\u001c:xj>\u0018\u0000y\u0000yF\u00012; (8.11)\n",
      "where yTandyFare the average responses for the \u001bTand\u001bFdata, respectively. Let fxj;kgm\n",
      "k=1\n",
      "denote the possible values of xj;j=1;:::; pwithin the training subset \u001b(with m6n\n",
      "elements). Note that, for a ﬁxed j, (8.11) is a piecewise constant function of \u0018, and that its\n",
      "minimal value is attained at some value xj;k. As a consequence, to minimize (8.11) over\n",
      "alljand\u0018, it su \u000eces to evaluate (8.11) for each of the m\u0002pvalues xj;kand then take the\n",
      "minimizing pair ( j;xj;k).294 8.2. Top-Down Construction of Decision Trees\n",
      "For a classiﬁcation problem, using the indicator loss and a constant regional prediction\n",
      "function as in (8.6), the aim is to choose a splitting rule that minimizes\n",
      "1\n",
      "nX\n",
      "(x;y)2\u001bT1fy,y\u0003\n",
      "Tg+1\n",
      "nX\n",
      "(x;y)2\u001bF1fy,y\u0003\n",
      "Fg; (8.12)\n",
      "where y\u0003\n",
      "T=gT(x) is the most prevalent class (majority vote) in the data set \u001bTandy\u0003\n",
      "Fis the\n",
      "most prevalent class in \u001bF. If the feature space is X=Rpand the splitting rules are of the\n",
      "form (8.10), then the optimal splitting rule can be obtained in the same way as described\n",
      "above for the regression case; the only di \u000berence is that (8.11) is replaced with (8.12).\n",
      "We can view the minimization of (8.12) as minimizing a weighted average of “impur-\n",
      "ities” of nodes \u001bTand\u001bF. Namely, for an arbitrary training subset \u001b\u0012\u001c, ify\u0003is the most\n",
      "prevalent label, then\n",
      "1\n",
      "j\u001bjX\n",
      "(x;y)2\u001b1fy,y\u0003g=1\u00001\n",
      "j\u001bjX\n",
      "(x;y)2\u001b1fy=y\u0003g=1\u0000py\u0003=1\u0000max\n",
      "z2f0;:::;c\u00001gpz;\n",
      "where pzis the proportion of data points in \u001bthat have class label z,z=0;:::; c\u00001. The\n",
      "quantity\n",
      "1\u0000max\n",
      "z2f0;:::;c\u00001gpz\n",
      "measures the diversity of the labels in \u001band is called the misclassiﬁcation impurity . Con-misclassification\n",
      "impurity sequently, (8.12) is the weighted sum of the misclassiﬁcation impurities of \u001bTand\u001bF, with\n",
      "weights byj\u001bTj=nandj\u001bFj=n, respectively. Note that the misclassiﬁcation impurity only\n",
      "depends on the label proportions rather than on the individual responses. Instead of using\n",
      "the misclassiﬁcation impurity to decide if and how to split a data set \u001b, we can use other\n",
      "impurity measures that only depend on the label proportions. Two popular choices are the\n",
      "entropy impurity entropy\n",
      "impurity:\n",
      "\u0000c\u00001X\n",
      "z=0pzlog2(pz)\n",
      "and the Gini impurity Gini impurity :\n",
      "1\n",
      "20BBBBBB@1\u0000c\u00001X\n",
      "z=0p2\n",
      "z1CCCCCCA:\n",
      "All of these impurities are maximal when the label proportions are equal to 1 =c. Typical\n",
      "shapes of the above impurity measures are illustrated in Figure 8.3 for the two-label case,\n",
      "with class probabilities pand 1\u0000p. We see here the similarity of the di \u000berent impurity\n",
      "measures. Note that impurities can be arbitrarily scaled, and so using ln( pz)=log2(pz) ln(2)\n",
      "instead of log2(pz) above gives an equivalent entropy impurity.\n",
      "8.2.3 Termination Criterion\n",
      "When building a tree, one can deﬁne various types of termination conditions. For example,\n",
      "we might stop when the number of data points in the tree node (the size of the input \u001bset\n",
      "in Algorithm 8.2.1) is less than or equal to some predeﬁned number. Or we might choose\n",
      "the maximal depth of the tree in advance. Another possibility is to stop when there is noChapter 8. Decision Trees and Ensemble Methods 295\n",
      "0 0 .2 0 .4 0 .6 0 .8 100.20.40.6\n",
      "pimpuritycross-entropy\n",
      "Gini index\n",
      "misclassiﬁcation\n",
      "Figure 8.3: Entropy, Gini, and misclassiﬁcation impurities for binary classiﬁcation, with\n",
      "class frequencies p1=pandp2=1\u0000p. The entropy impurity was normalized (divided by\n",
      "2), to ensure that all impurity measures attain the same maximum value of 1 /2 atp=1=2.\n",
      "signiﬁcant advantage, in terms of training loss, to split regions. Ultimately, the quality of a\n",
      "tree is determined by its predictive performance (generalization risk) and the termination\n",
      "condition should aim to strike a balance between minimizing the approximation error and\n",
      "minimizing the statistical error, as discussed in Section 2.4. + 31\n",
      "Example 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-\n",
      "alization risk, consider Figure 8.4, which shows the typical behavior of the cross-validation\n",
      "loss as a function of the tree depth. Recall that the cross-validation loss is an estimate of the\n",
      "expected generalization risk. Complicated (deep) trees tend to overﬁt the training data by\n",
      "producing many divisions of the feature space. As we have seen, this overﬁtting problem is\n",
      "typical of all learning methods; see Chapter 2 and in particular Example 2.1. To conclude, + 26\n",
      "increasing the maximal depth does not necessarily result in better performance.\n",
      "0 5 10 15 20 25 300.30.350.40.45\n",
      "tree depthloss\n",
      "Figure 8.4: The ten-fold cross-validation loss as a function of the maximal tree depth for a\n",
      "classiﬁcation problem. The optimal maximal tree depth is here 6.296 8.2. Top-Down Construction of Decision Trees\n",
      "To create Figure 8.4 we used1the Python method make_blobs from the sklearn\n",
      "module to produce a training set of size n=5000 with ten-dimensional feature vectors +492\n",
      "(thus, p=10 andX=R10), each of which is classiﬁed into one of c=3 classes. The full\n",
      "code is given below.\n",
      "TreeDepthCV.py\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import zero_one_loss\n",
      "import matplotlib.pyplot as plt\n",
      "def ZeroOneScore(clf, X, y):\n",
      "y_pred = clf.predict(X)\n",
      "return zero_one_loss(y, y_pred)\n",
      "# Construct the training set\n",
      "X, y = make_blobs(n_samples=5000, n_features=10, centers=3,\n",
      "random_state=10, cluster_std=10)\n",
      "# construct a decision tree classifier\n",
      "clf = DecisionTreeClassifier(random_state=0)\n",
      "# Cross -validation loss as a function of tree depth (1 to 30)\n",
      "xdepthlist = []\n",
      "cvlist = []\n",
      "tree_depth = range(1,30)\n",
      "for d in tree_depth:\n",
      "xdepthlist.append(d)\n",
      "clf.max_depth=d\n",
      "cv = np.mean(cross_val_score(clf, X, y, cv=10, scoring=\n",
      "ZeroOneScore))\n",
      "cvlist.append(cv)\n",
      "plt.xlabel( 'tree depth ', fontsize=18, color= 'black ')\n",
      "plt.ylabel( 'loss ', fontsize=18, color= 'black ')\n",
      "plt.plot(xdepthlist , cvlist , '-*', linewidth=0.5)\n",
      "The code above relies heavily on sklearn and hides the implementation details. To\n",
      "show how decision trees are actually constructed using the previous theory, we proceed\n",
      "with a very basic implementation.\n",
      "8.2.4 Basic Implementation\n",
      "In this section we implement a regression tree, step by step. To run the program, amalgam-\n",
      "ate the code snippets below into one ﬁle, in the order presented. First, we import various\n",
      "packages and deﬁne a function to generate the training and test data.\n",
      "1The data used for Figure 8.1 was produced in a similar way.Chapter 8. Decision Trees and Ensemble Methods 297\n",
      "BasicTree.py\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_friedman1\n",
      "from sklearn.model_selection import train_test_split\n",
      "def makedata():\n",
      "n_points = 500 # number of samples\n",
      "X, y = make_friedman1(n_samples=n_points , n_features=5,\n",
      "noise=1.0, random_state =100)\n",
      "return train_test_split(X, y, test_size=0.5, random_state=3)\n",
      "The “main” method calls the makedata method, uses the training data to build a regres-\n",
      "sion tree, and then predicts the responses of the test set and reports the mean squared-error\n",
      "loss.\n",
      "def main():\n",
      "X_train , X_test , y_train , y_test = makedata()\n",
      "maxdepth = 10 # maximum tree depth\n",
      "# Create tree root at depth 0\n",
      "treeRoot = TNode(0, X_train ,y_train)\n",
      "# Build the regression tree with maximal depth equal to max_depth\n",
      "Construct_Subtree(treeRoot , maxdepth)\n",
      "# Predict\n",
      "y_hat = np.zeros(len(X_test))\n",
      "for i in range(len(X_test)):\n",
      "y_hat[i] = Predict(X_test[i],treeRoot)\n",
      "MSE = np.mean(np.power(y_hat - y_test ,2))\n",
      "print(\"Basic tree: tree loss = \", MSE)\n",
      "The next step is to specify a tree node as a Python class. Each node has a number of\n",
      "attributes, including the features and the response data ( Xandy) and the depth at which\n",
      "the node is placed in the tree. The root node has depth 0. Each node wcan calculate its\n",
      "contribution to the squared-error training lossPn\n",
      "i=11fxi2Rwg(yi\u0000gw(xi))2. Note that we\n",
      "have omitted the constant 1 =nterm when training the tree, which simply scales the loss\n",
      "(8.2).\n",
      "class TNode:\n",
      "def __init__(self , depth , X, y):\n",
      "self.depth = depth\n",
      "self.X = X # matrix of features\n",
      "self.y = y # vector of response variables\n",
      "# initialize optimal split parameters\n",
      "self.j = None\n",
      "self.xi = None\n",
      "# initialize children to be None\n",
      "self.left = None\n",
      "self.right = None\n",
      "# initialize the regional predictor298 8.2. Top-Down Construction of Decision Trees\n",
      "self.g = None\n",
      "def CalculateLoss(self):\n",
      "if(len(self.y)==0):\n",
      "return 0\n",
      "return np.sum(np.power(self.y - self.y.mean() ,2))\n",
      "The function below implements the training (tree-building) Algorithm 8.2.1.\n",
      "def Construct_Subtree(node , max_depth):\n",
      "if(node.depth == max_depth or len(node.y) == 1):\n",
      "node.g = node.y.mean()\n",
      "else:\n",
      "j, xi = CalculateOptimalSplit(node)\n",
      "node.j = j\n",
      "node.xi = xi\n",
      "Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)\n",
      "if(len(yt)>0):\n",
      "node.left = TNode(node.depth+1,Xt,yt)\n",
      "Construct_Subtree(node.left , max_depth)\n",
      "if(len(yf)>0):\n",
      "node.right = TNode(node.depth+1, Xf,yf)\n",
      "Construct_Subtree(node.right , max_depth)\n",
      "return node\n",
      "This requires an implementation of the CalculateOptimalSplit function. To start,\n",
      "we implement a function DataSplit that splits the data according to s(x)=1fxj6\u0018g.\n",
      "def DataSplit(X,y,j,xi):\n",
      "ids = X[:,j]<=xi\n",
      "Xt = X[ids == True ,:]\n",
      "Xf = X[ids == False ,:]\n",
      "yt = y[ids == True]\n",
      "yf = y[ids == False]\n",
      "return Xt, yt, Xf, yf\n",
      "TheCalculateOptimalSplit method runs through the possible splitting thresholds\n",
      "\u0018from the setfxj;kgand ﬁnds the optimal split.\n",
      "def CalculateOptimalSplit(node):\n",
      "X = node.X\n",
      "y = node.y\n",
      "best_var = 0\n",
      "best_xi = X[0,best_var]\n",
      "best_split_val = node.CalculateLoss()\n",
      "m, n = X.shape\n",
      "for j in range(0,n):Chapter 8. Decision Trees and Ensemble Methods 299\n",
      "for i in range(0,m):\n",
      "xi = X[i,j]\n",
      "Xt, yt, Xf, yf = DataSplit(X,y,j,xi)\n",
      "tmpt = TNode(0, Xt, yt)\n",
      "tmpf = TNode(0, Xf, yf)\n",
      "loss_t = tmpt.CalculateLoss()\n",
      "loss_f = tmpf.CalculateLoss()\n",
      "curr_val = loss_t + loss_f\n",
      "if (curr_val < best_split_val):\n",
      "best_split_val = curr_val\n",
      "best_var = j\n",
      "best_xi = xi\n",
      "return best_var , best_xi\n",
      "Finally, we implement the recursive method for prediction.\n",
      "def Predict(X,node):\n",
      "if(node.right == None and node.left != None):\n",
      "return Predict(X,node.left)\n",
      "if(node.right != None and node.left == None):\n",
      "return Predict(X,node.right)\n",
      "if(node.right == None and node.left == None):\n",
      "return node.g\n",
      "else:\n",
      "if(X[node.j] <= node.xi):\n",
      "return Predict(X,node.left)\n",
      "else:\n",
      "return Predict(X,node.right)\n",
      "Running the main function deﬁned above gives a similar2result to what one would\n",
      "achieve with the sklearn package, using the DecisionTreeRegressor method.\n",
      "main() # run the main program\n",
      "# compare with sklearn\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "X_train , X_test , y_train , y_test = makedata() # use the same data\n",
      "regTree = DecisionTreeRegressor(max_depth = 10, random_state=0)\n",
      "regTree.fit(X_train ,y_train)\n",
      "y_hat = regTree.predict(X_test)\n",
      "MSE2 = np.mean(np.power(y_hat - y_test ,2))\n",
      "print(\"DecisionTreeRegressor: tree loss = \", MSE2)\n",
      "Basic tree: tree loss = 9.067077996170276\n",
      "DecisionTreeRegressor: tree loss = 10.197991295531748\n",
      "2After establishing a best split \u0018=xj;k,sklearn assigns the corresponding feature vector randomly to\n",
      "one of the two child nodes, rather than to the True child.300 8.3. Additional Considerations\n",
      "8.3 Additional Considerations\n",
      "8.3.1 Binary Versus Non-Binary Trees\n",
      "While it is possible to split a tree node into more than two groups (multiway splits), it\n",
      "generally produces inferior results compared to the simple binary split. The major reason\n",
      "is that multiway splits can lead to too many nodes near the tree root that have only a\n",
      "few data points, thus leaving insu \u000ecient data for later splits. As multiway splits can be\n",
      "represented as several binary splits, the latter is preferred [55].\n",
      "8.3.2 Data Preprocessing\n",
      "Sometimes, it can be beneﬁcial to preprocess the data prior to the tree construction. For\n",
      "example, PCA can be used with a view to identify the most important dimensions, which +153\n",
      "in turn will lead to simpler and possibly more informative splitting rules in the internal\n",
      "nodes.\n",
      "8.3.3 Alternative Splitting Rules\n",
      "We restricted our attention to splitting rules of the type s(x)=1fxj6\u0018g, where j2\n",
      "f1;:::; pgand\u00182R. These types of rules may not always result in a simple partition\n",
      "of the feature space, as illustrated by the binary data in Figure 8.5. In this case, the feature\n",
      "space could have been partitioned into just two regions, separated by a straight line.\n",
      "Figure 8.5: The two groups of points can here be separated by a straight line. Instead, the\n",
      "classiﬁcation tree divides up the space into many rectangles, leading to an unnecessarily\n",
      "complicated classiﬁcation procedure.\n",
      "In this case many classiﬁcation methods discussed in Chapter 7, such as linear discrim-\n",
      "inant analysis (Section 7.4), will work very well, whereas the classiﬁcation tree is rather +261\n",
      "elaborate, dividing the feature set into too many regions. An obvious remedy is to use\n",
      "splitting rules of the form\n",
      "s(x)=1fa>x6\u0018g:Chapter 8. Decision Trees and Ensemble Methods 301\n",
      "In some cases, such as the one just discussed, it may be useful to use a splitting rule\n",
      "that involves several variables, as opposed to a single one. The decision regarding the split\n",
      "type clearly depends on the problem domain. For example, for logical (binary) variables\n",
      "our domain knowledge may indicate that a di \u000berent behavior is expected when both xiand\n",
      "xj(i,j) are True . In this case, we will naturally introduce a decision rule of the form:\n",
      "s(x)=1fxi=True andxj=Trueg:\n",
      "8.3.4 Categorical Variables\n",
      "When an explanatory variable is categorical with labels (levels) say f1;:::; kg, the split-\n",
      "ting rule is generally deﬁned via a partition of the label set f1;:::; kginto two subsets.\n",
      "Speciﬁcally, let LandRbe a partition off1;:::; kg. Then, the splitting rule is deﬁned via\n",
      "s(x)=1fxj2Lg:\n",
      "For the general supervised learning case, ﬁnding the optimal partition in the sense of min-\n",
      "imal loss requires one to consider 2ksubsets off1;:::; kg. Consequently, ﬁnding a good\n",
      "splitting rule for categorical variables can be challenging when the number of labels pis\n",
      "large.\n",
      "8.3.5 Missing Values\n",
      "Missing data is present in many real-life problems. Generally, when working with incom-\n",
      "plete feature vectors, where one or more values are missing, it is typical to either com-\n",
      "pletely delete the feature vector from the data (which may distort the data) or to impute\n",
      "(guess) its missing values from the available data; see e.g., [120]. Tree methods, however,\n",
      "allow an elegant approach for handling missing data. Speciﬁcally, in the general case, the\n",
      "missing data problem can be handled via surrogate splitting rules [20].\n",
      "When dealing with categorical (factor) features, we can introduce an additional cat-\n",
      "egory “missing” for the absent data.\n",
      "The main idea of surrogate rules is as follows. First, we construct a decision (regression\n",
      "or a classiﬁcation) tree via Algorithm 8.2.1. During this construction process, the solution\n",
      "of the optimization problem (8.9) is calculated only over the observations that are not\n",
      "missing a particular variable. Suppose that a tree node vhas a splitting rule s\u0003(x)=1fxj\u00036\n",
      "\u0018\u0003gfor some 16j\u00036pand threshold \u0018\u0003.\n",
      "For the node vwe can introduce a set of alternative splitting rules that resemble the\n",
      "original splitting rule, sometimes called the primary splitting rule, using di \u000berent variables\n",
      "and thresholds. Namely, we look for a binary splitting rule s(xjj;\u0018),j,j\u0003such that the\n",
      "data split introduced by swill be similar to the original data split from s\u0003. The similarity is\n",
      "generally measured via a binary misclassiﬁcation loss, where the true classes of observa-\n",
      "tions are determined by the primary splitting rule and the surrogate splitting rules serve as\n",
      "classiﬁers. Consider, for example, the data in Table 8.1 and suppose that the primary split-\n",
      "ting rule at node vis1fAge625g. That is, the ﬁve data points are split such that the left\n",
      "and the right child of vcontains two and three data points, respectively. Next, the following\n",
      "surrogate splitting rules can be considered:302 8.4. Controlling the Tree Shape\n",
      "1.1fSalary61500g, and\n",
      "2.1fHeight6173g.\n",
      "Table 8.1: Example data with three variables (Age, Height, and Salary).\n",
      "Id Age Height Salary\n",
      "1 20 173 1000\n",
      "2 25 168 1500\n",
      "3 38 191 1700\n",
      "4 49 170 1900\n",
      "5 62 182 2000\n",
      "The1fSalary61500gsurrogate rule completely mimics the primary rule, in the sense\n",
      "that the data splits induced by these rules are identical. Namely, both rules partition the\n",
      "data into two sets (by Id) f1;2gandf3;4;5g. On the other hand, the 1fHeight6173grule\n",
      "is less similar to the primary rule, since it causes the di \u000berent partitionf1;2;4gandf3;5g.\n",
      "It is up to the user to deﬁne the number of surrogate rules for each tree node. As soon as\n",
      "these surrogate rules are available, we can use them to handle a new data point, even if the\n",
      "main rule cannot be applied due to a missing value of the primary variable xj\u0003. Speciﬁcally,\n",
      "if the observation is missing the primary split variable, we apply the ﬁrst (best) surrogate\n",
      "rule. If the ﬁrst surrogate variable is also missing, we apply the second best surrogate rule,\n",
      "and so on.\n",
      "8.4 Controlling the Tree Shape\n",
      "Eventually, we are interested in getting the right-size tree. Namely, a tree that shows good\n",
      "generalization properties. It was already discussed in Section 8.2.3 (Figure 8.4) that shal-\n",
      "low trees tend to underﬁt and deep trees tend to overﬁt the data. Basically, a shallow tree\n",
      "does not produce a su \u000ecient number of splits and a deep tree will produce many partitions\n",
      "and thus many leaf nodes. If we grow the tree to a su \u000ecient depth, each training sample\n",
      "will occupy a separate leaf and we will observe a zero loss with respect to the training data.\n",
      "The above phenomenon is illustrated in Figure 8.6, which presents the cross-validation loss\n",
      "and the training loss as a function of the tree depth.\n",
      "In order to overcome the under- and the overﬁtting problem, Breiman et al. [20] ex-\n",
      "amined the possibility of stopping the tree from growing as soon as the decrease in loss\n",
      "due to a split of node v, as expressed in the di \u000berence of (8.8) and (8.9), is smaller than\n",
      "some predeﬁned parameter \u000e2R. Under this setting, the tree construction process will\n",
      "terminate when no leaf node can be split such that the contribution to the training loss after\n",
      "this split is greater than \u000e.\n",
      "The authors found that this approach was unsatisfactory. Speciﬁcally, it was noted that a\n",
      "very small\u000eleads to an excessive amount of splitting and thus causes overﬁtting. Increasing\n",
      "\u000edid not work either. The problem is that the nature of the proposed rule is one-step-look-\n",
      "ahead . To see this, consider a tree node for which the best possible decrease in loss isChapter 8. Decision Trees and Ensemble Methods 303\n",
      "0 5 10 15 2000.10.20.30.4\n",
      "tree depthlosstrain\n",
      "CV\n",
      "Figure 8.6: The cross-validation and the training loss as a function of the tree depth for a\n",
      "binary classiﬁcation problem.\n",
      "smaller than \u000e. According to the proposed procedure, this node will not be split further. This\n",
      "may, however, be sub-optimal, because it could happen that one of the node’s descendants ,\n",
      "if split, could lead to a major decrease in loss.\n",
      "To address these issues, a so-called pruning tree pruning routine can be employed. The idea is as\n",
      "follows. We ﬁrst grow a very deep tree and then prune (remove nodes) it upwards until we\n",
      "reach the root node. Consequently, the pruning process causes the number of tree nodes\n",
      "to decrease. While the tree is being pruned, the generalization risk gradually decreases up\n",
      "to the point where it starts increasing again, at which point the pruning is stopped. This\n",
      "decreasing /increasing behavior is due to the bias–variance tradeo \u000b(2.22).\n",
      "We next describe the details. To start with, let vandv0be tree nodes. We say that v0is\n",
      "adescendant ofvif there is a path down the tree, which leads from vtov0. If such a path\n",
      "exists, we also say that vis an ancestor ofv0. Consider the tree in Figure 8.7.\n",
      "To formally deﬁne pruning, we will require the following Deﬁnition 8.1. An example\n",
      "of pruning is demonstrated in Figure 8.8.\n",
      "Deﬁnition 8.1: Branches and Pruning\n",
      "1. A tree branch tree branch Tvof the treeTis a sub-tree of Trooted at node v2T.\n",
      "2. The pruning of branch Tvfrom a treeTis performed via deletion of the entire\n",
      "branchTvfromTexcept the branch’s root node v. The resulting pruned tree is\n",
      "denoted byT\u0000Tv.\n",
      "3. A sub-tree T\u0000Tvis called a pruned sub-tree of T. We indicate this with the\n",
      "notationT\u0000Tv\u001eTorT\u001fT\u0000Tv.\n",
      "A basic decision tree pruning procedure is summarized in Algorithm 8.4.1.304 8.4. Controlling the Tree Shape\n",
      "v1\n",
      "v2\n",
      "v4\n",
      "v7v5\n",
      "v8 v9v3\n",
      "v6\n",
      "v10 v11\n",
      "Figure 8.7: The node v9is a descendant of v2, and v2is an ancestor offv4;v5;v7;v8;v9g, but\n",
      "v6is not a descendant of v2.\n",
      "v1\n",
      "v2\n",
      "v4\n",
      "v7v5\n",
      "v8 v9v3\n",
      "v6\n",
      "v10 v11\n",
      "(a)T\n",
      "v2\n",
      "v4\n",
      "v7v5\n",
      "v8 v9 (b)Tv2\n",
      "v1\n",
      "v2 v3\n",
      "v6\n",
      "v10 v11 (c)T\u0000Tv2\n",
      "Figure 8.8: The pruned tree T\u0000Tv2in (c) is the result of pruning the Tv2branch in (b) from\n",
      "the original tree Tin (a).\n",
      "Algorithm 8.4.1: Decision Tree Pruning\n",
      "Input: Training set \u001c.\n",
      "Output: Sequence of decision trees T0\u001fT1\u001f\u0001\u0001\u0001\n",
      "1Build a large decision tree T0via Algorithm 8.2.1. [A possible termination\n",
      "criterion for that algorithm is to have some small predetermined number of data\n",
      "points at each terminal node of T0.]\n",
      "2T0 T0\n",
      "3k 0\n",
      "4whileT0has more than one node do\n",
      "5 k k+1\n",
      "6 Choose v2T0.\n",
      "7 Prune the branch rooted at vfromT0.\n",
      "8Tk T0\u0000TvandT0 Tk.\n",
      "9returnT0;T1;:::;TkChapter 8. Decision Trees and Ensemble Methods 305\n",
      "LetT0be the initial (deep) tree and let Tkbe the tree obtained after the k-th pruning\n",
      "operation, for k=1;:::; K. As soon as the sequence of trees T0\u001fT1\u001f\u0001\u0001\u0001\u001fTKis avail-\n",
      "able, one can choose the best tree of fTkgK\n",
      "k=1according to the smallest generalization risk.\n",
      "Speciﬁcally, we can split the data into training and validation sets. In this case, Algorithm\n",
      "8.4.1 is executed using the training set and the generalization risks of fTkgK\n",
      "k=1are estimated\n",
      "via the validation set.\n",
      "While Algorithm 8.4.1 and the corresponding best tree selection process look appeal-\n",
      "ing, there is still an important question to consider; namely, how to choose the node vand\n",
      "the corresponding branch Tvin Line 6 of the algorithm. In order to overcome this problem,\n",
      "Breiman proposed a method called cost complexity pruning , which we discuss next.\n",
      "8.4.1 Cost-Complexity Pruning\n",
      "LetT\u001eT0be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)\n",
      "nodes ofTbyW. The number of leaves jWj is a measure for the complexity of the tree;\n",
      "recall thatjWj is the number of regions fRwgin the partition of X. Corresponding to each\n",
      "treeTis a prediction function g, as in (8.1). In cost-complexity pruning cost-complexity\n",
      "pruningthe objective is to\n",
      "ﬁnd a prediction function g(or, equivalently, tree T) that minimizes the training loss `\u001c(g)\n",
      "while taking into account the complexity of the tree. The idea is to regularize the training\n",
      "loss, similar to what was done in Chapter 6, by adding a penalty term for the complexity\n",
      "of the tree. This leads to the following deﬁnition.\n",
      "Deﬁnition 8.2: Cost-Complexity Measure\n",
      "Let\u001c=f(xi;yi)gn\n",
      ">0 be a real number. For a given tree T, the\n",
      "cost-complexity measure cost-complexity\n",
      ";T) is deﬁned as:\n",
      ";T) :=1\n",
      "nX\n",
      "w2W0BBBBB@nX\n",
      "jWj (8.13)wgLoss( yi;gw(xi))1CCCCCA+\n",
      "jWj;g)+\n",
      "where`\u001c(g)is the training loss (8.2).\n",
      "result in a small penalty for the tree complexity jWj, and thus large\n",
      ";T). In particular,entire training data well) will minimize the measure C\u001c(\n",
      ";T). On the other hand, large values of \n",
      "will prefer smaller trees or, more precisely, trees with fewer leaves. For su \u000eciently large\n",
      ", the solution Twill collapse to a single (root) node.\n",
      ", there exists a smallest minimizing sub-\n",
      "is selected viact to the cost-complexity measure. In practice, a suitable \n",
      "observing the performance of the learner on the validation set or by cross-validation.\n",
      "These advantages and the corresponding limitations are detailed next.306 8.4. Controlling the Tree Shape\n",
      "8.4.2 Advantages and Limitations of Decision Trees\n",
      "We list a number of advantages and disadvantages of decision trees, as compared with\n",
      "other supervised learning methods such as were discussed in Chapters 5, 6, and 7.\n",
      "Advantages\n",
      "1. The tree structure can handle both categorical and numerical features in a natural\n",
      "and straightforward way. Speciﬁcally, there is no need to pre-process categorical\n",
      "features, say via the introduction of dummy variables.\n",
      "2. The ﬁnal tree obtained after the training phase can be compactly stored for the pur-\n",
      "pose of making predictions for new feature vectors. The prediction process only\n",
      "involves a single tree traversal from the tree root to a leaf.\n",
      "3. The hierarchical nature of decision trees allows for an e \u000ecient encoding of the fea-\n",
      "ture’s conditional information. Speciﬁcally, after an internal split of a feature xjvia\n",
      "the standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of\n",
      "data that were constructed based on this split, thus implicitly exploiting the corres-\n",
      "ponding conditional information from the initial split of xj.\n",
      "4. The tree structure can be easily understood and interpreted by domain experts with\n",
      "little statistical knowledge, since it is essentially a logical decision ﬂow diagram.\n",
      "5. The sequential decision tree growth procedure in Algorithm 8.2.1, and in particular\n",
      "the fact that the tree has been split using the most important features, provides an\n",
      "implicit step-wise variable elimination procedure. In addition, the partition of the\n",
      "variable space into smaller regions results in simpler prediction problems in these\n",
      "regions.\n",
      "6. Decision trees are invariant under monotone transformations of the data. To see this,\n",
      "consider the (optimal) splitting rule s(x)=1fx362g, where x3is a positive feature.\n",
      "Suppose that x3is transformed to x0\n",
      "3=x2\n",
      "3. Now, the optimal splitting rule will take\n",
      "the form s(x)=1fx0\n",
      "364g.\n",
      "7. In the classiﬁcation setting, it is common to report not only the predicted value of a\n",
      "feature vector, e.g., as in (8.6), but also the respective class probabilities. Decision\n",
      "trees handle this task without any additional e \u000bort. Speciﬁcally, consider a new fea-\n",
      "ture vector. During the estimation process, we will perform a tree traversal and the\n",
      "point will end up in a certain leaf w. The probability of this feature vector lying in\n",
      "class zcan be estimated as the proportion of training points in wthat are in class z.\n",
      "8. As each training point is treated equally in the construction of a tree, their structure\n",
      "of the tree will be relatively robust to outliers. In a way, trees exhibit a similar kind\n",
      "of robustness as the sample median does for real-valued data.Chapter 8. Decision Trees and Ensemble Methods 307\n",
      "Limitations\n",
      "Despite the fact that the decision trees are extremely interpretable, the predictive accuracy\n",
      "is generally inferior to other established statistical learning methods. In addition, decision\n",
      "trees, and in particular very deep trees that were not subject to pruning, are heavily reliant\n",
      "on their training set. A small change in the training set can result in a dramatic change of the\n",
      "resulting decision tree. Their inferior predictive accuracy, however, is a direct consequence\n",
      "of the bias–variance tradeo \u000b. Speciﬁcally, a decision tree model generally exhibits a high\n",
      "variance. To overcome the above limitations, several promising approaches such as bag-\n",
      "ging,random forest , and boosting are introduced below.\n",
      "The bagging approach was initially introduced in the context of an ensemble of\n",
      "decision trees. However, both the bagging and the boosting methods can be applied\n",
      "to improve the accuracy of general prediction functions.\n",
      "8.5 Bootstrap Aggregation\n",
      "The major idea of the bootstrap aggregation orbagging bagging method is to combine prediction\n",
      "functions learned from multiple data sets, with a view to improving overall prediction\n",
      "accuracy. Bagging is especially beneﬁcial when dealing with predictors that tend to overﬁt\n",
      "the data, such as in decision trees, where the (unpruned) tree structure is very sensitive to\n",
      "small changes in the training set [37, 55].\n",
      "To start with, consider an idealized setting for a regression tree, where we have access\n",
      "toBiid copies3T1;:::;TBof a training setT. Then, we can train Bseparate regression\n",
      "models ( Bdi\u000berent decision trees) using these sets, giving learners gT1;:::; gTB, and take\n",
      "their average:\n",
      "gavg(x)=1\n",
      "BBX\n",
      "b=1gTb(x): (8.14)\n",
      "By the law of large numbers, as B! 1 , the average prediction function converges to +447\n",
      "the expected prediction function gy:=EgT. The following result shows that using gyas\n",
      "a prediction function (if it were known) would result in an expected squared-error gen-\n",
      "eralization risk that is less than or equal to the expected generalization risk for a general + 24\n",
      "prediction function gT. It thus suggests that taking an average of prediction functions may\n",
      "lead to a better expected squared-error generalization risk.\n",
      "Theorem 8.1: Expected Squared-Error Generalization Risk\n",
      "LetTbe a random training set and let X;Ybe a random feature vector and response\n",
      "that are independent of T. Then,\n",
      "E\u0012\n",
      "Y\u0000gT(X)\u00132\n",
      ">E\u0010\n",
      "Y\u0000gy(X)\u00112:\n",
      "3In this sectionTkmeans the k-th training set, not a training set of size k.308 8.5. Bootstrap Aggregation\n",
      "Proof: We have\n",
      "E\"\u0012\n",
      "Y\u0000gT(X)\u00132\f\f\f\f\fX;Y#\n",
      ">\u0012\n",
      "E[YjX;Y]\u0000E[gT(X)jX;Y]\u00132\n",
      "=\u0012\n",
      "Y\u0000gy(X)\u00132\n",
      ";\n",
      "where the inequality follows from EU2>(EU)2for any (conditional) expectation. Con-\n",
      "sequently, by the tower property, +433\n",
      "E\u0012\n",
      "Y\u0000gT(X)\u00132\n",
      "=Eh\n",
      "Eh\u0000Y\u0000gT(X)\u00012jX;Yii\n",
      ">E\u0012\n",
      "Y\u0000gy(X)\u00132\n",
      ":\n",
      "\u0003\n",
      "Unfortunately, multiple independent data sets are rarely available. But we can substi-\n",
      "tute them by bootstrapped ones. Speciﬁcally, instead of the T1;:::;TBsets, we can obtain\n",
      "random training sets T\u0003\n",
      "1;:::;T\u0003\n",
      "Bby resampling them from a single (ﬁxed) training set \u001c, +76\n",
      "similar to Algorithm 3.2.6, and use them to train Bseparate models. By model averaging\n",
      "as in (8.14) we obtain the bootstrapped aggregated estimator or bagged estimator bagged\n",
      "estimatorof the\n",
      "form:\n",
      "gbag(x)=1\n",
      "BBX\n",
      "b=1gT\u0003\n",
      "b(x): (8.15)\n",
      "Algorithm 8.5.1: Bootstrap Aggregation Sampling\n",
      "Input: Training set \u001c=f(xi;yi)gn\n",
      "i=1and resample size B.\n",
      "Output: Bootstrapped data sets.\n",
      "1forb=1toBdo\n",
      "2T\u0003\n",
      "b ;\n",
      "3 fori=1tondo\n",
      "4 Draw U\u0018U(0;1)\n",
      "5 I dnUe // select random index\n",
      "6T\u0003\n",
      "b T\u0003\n",
      "b[f(xI;yI)g.\n",
      "7returnT\u0003\n",
      "b;b=1;:::; B.\n",
      "Remark 8.1 (Bootstrap Aggregation for Classiﬁcation Problems) Note that (8.15)\n",
      "is suitable for handling regression problems. However, the bagging idea can be readily\n",
      "extended to handle classiﬁcation settings as well. For example, gbagcan take the majority\n",
      "vote amongfgT\u0003\n",
      "bg;b=1;:::; B; that is, to accept the most frequent class among Bpredict-\n",
      "ors.\n",
      "While bagging can be applied for any statistical model (such as decision trees, neural\n",
      "networks, linear regression, K-nearest neighbors, and so on), it is most e \u000bective for pre-\n",
      "dictors that are sensitive to small changes in the training set. The reason becomes clear\n",
      "when we decompose the expected generalization risk as\n",
      "E`(gT)=`\u0003+E(E[gT(X)jX]\u0000g\u0003(X))2\n",
      "|                           {z                           }\n",
      "expected squared bias+E[Var[gT(X)jX]]|                 {z                 }\n",
      "expected variance; (8.16)Chapter 8. Decision Trees and Ensemble Methods 309\n",
      "similar to (2.22). Compare this with the same decomposition for the average prediction + 35\n",
      "function gbagin (8.14). As Egbag(x)=EgT(x), we see that any possible improvement in\n",
      "the generalization risk must be due to the expected variance term. Averaging and bagging\n",
      "are thus only useful for predictors with a large expected variance, relative to the other two\n",
      "terms. Examples of such “unstable” predictors include decision trees, neural networks, and\n",
      "subset selection in linear regression [22]. On the other hand, “stable” predictors are in-\n",
      "sensitive to small data changes, an example being the K-nearest neighbors method. Note\n",
      "that for independent training sets T1;:::;TBa reduction of the variance by a factor Bis\n",
      "achieved:Vargbag(x)=B\u00001VargT(x). Again, it depends on the squared bias and irredu-\n",
      "cible loss how signiﬁcant this reduction is for the generalization risk.\n",
      "Remark 8.2 (Limitations of Bagging) It is important to remember that gbagis not ex-\n",
      "actly equal to gavg, which in turn is not exactly gy. Speciﬁcally, gbagis constructed from the\n",
      "bootstrap approximation of the sampling pdf f. As a consequence, for stable predictors,\n",
      "it can happen that gbagwill perform worse than gT. In addition to the deterioration of the\n",
      "bagging performance for stable procedures, it can also happen that gThas already achieved\n",
      "a near optimal predictive accuracy given the available training data. In this case, bagging\n",
      "will not introduce a signiﬁcant improvement.\n",
      "The bagging process provides an opportunity to estimate the generalization risk of\n",
      "the bagged model without an additional test set. Speciﬁcally, recall that we obtain the\n",
      "T\u0003\n",
      "1;:::;T\u0003\n",
      "Bsets from a single training set \u001cby sampling via Algorithm 8.5.1, and use them\n",
      "to train Bseparate models. It can be shown (see Exercise 8) that, for large sample sizes, on\n",
      "average about a third (more precisely, a fraction e\u00001\u00190:37) of the original sample points\n",
      "are not included in bootstrapped set T\u0003\n",
      "bfor 16b6B. Therefore, these samples can be\n",
      "used for the loss estimation. These samples are called out-of-bag out-of-bag (OOB) observations.\n",
      "Speciﬁcally, for each sample from the original data set, we calculate the OOB loss using\n",
      "predictors that were trained without this particular sample. The estimation procedure is\n",
      "summarized in Algorithm 8.5.2. Hastie et al. [55] observe that, under certain conditions, the\n",
      "OOB loss is almost identical to the n-fold cross-validation loss. In addition, the OOB loss\n",
      "can be used to determine the number of trees required. Speciﬁcally, we can train predictors\n",
      "until the OOB loss stops changing. Namely, decision trees are added until the OOB loss\n",
      "stabilizes.\n",
      "Algorithm 8.5.2: Out-of-Bag Loss Estimation\n",
      "Input: The original data set \u001c=f(x1;y1);:::; (xn;yn)g, the bootstrapped data sets\n",
      "fT\u0003\n",
      "1;:::;T\u0003\n",
      "Bg, and the trained predictorsn\n",
      "gT\u0003\n",
      "1;:::; gT\u0003\n",
      "Bo\n",
      ".\n",
      "Output: Out-of-bag loss for the averaged model.\n",
      "1fori=1tondo\n",
      "2Ci ; // Indices of predictors not depending on (xi;yi)\n",
      "3 forb=1toBdo\n",
      "4 if(xi;yi)<T\u0003\n",
      "bthenCi C i[fbg\n",
      "5 Y0\n",
      "i jC ij\u00001P\n",
      "b2CigT\u0003\n",
      "b(xi)\n",
      "6 Li Loss\u0010\n",
      "yi;Y0\n",
      "i\u0011\n",
      "7LOOB 1\n",
      "nPn\n",
      "i=1Li\n",
      "8return LOOB.310 8.5. Bootstrap Aggregation\n",
      "Example 8.3 (Bagging for a Regression Tree) We next proceed with a basic bagging\n",
      "example for a regression tree, in which we compare the decision tree estimator with the\n",
      "corresponding bagged estimator. We use the R2metric (coe \u000ecient of determination) for\n",
      "comparison.\n",
      "BaggingExample.py\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_friedman1\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "np.random.seed(100)\n",
      "# create regression problem\n",
      "n_points = 1000 # points\n",
      "x, y = make_friedman1(n_samples=n_points , n_features=15,\n",
      "noise=1.0, random_state =100)\n",
      "# split to train/test set\n",
      "x_train , x_test , y_train , y_test = \\\n",
      "train_test_split(x, y, test_size=0.33, random_state =100)\n",
      "# training\n",
      "regTree = DecisionTreeRegressor(random_state =100)\n",
      "regTree.fit(x_train ,y_train)\n",
      "# test\n",
      "yhat = regTree.predict(x_test)\n",
      "# Bagging construction\n",
      "n_estimators=500\n",
      "bag = np.empty((n_estimators), dtype=object)\n",
      "bootstrap_ds_arr = np.empty((n_estimators), dtype=object)\n",
      "for i in range(n_estimators):\n",
      "# sample bootstrapped data set\n",
      "ids = np.random.choice(range(0,len(x_train)),size=len(x_train),\n",
      "replace=True)\n",
      "x_boot = x_train[ids]\n",
      "y_boot = y_train[ids]\n",
      "bootstrap_ds_arr[i] = np.unique(ids)\n",
      "bag[i] = DecisionTreeRegressor()\n",
      "bag[i].fit(x_boot ,y_boot)\n",
      "# bagging prediction\n",
      "yhatbag = np.zeros(len(y_test))\n",
      "for i in range(n_estimators):\n",
      "yhatbag = yhatbag + bag[i].predict(x_test)\n",
      "yhatbag = yhatbag/n_estimators\n",
      "# out of bag loss estimationChapter 8. Decision Trees and Ensemble Methods 311\n",
      "oob_pred_arr = np.zeros(len(x_train))\n",
      "for i in range(len(x_train)):\n",
      "x = x_train[i].reshape(1, -1)\n",
      "C = []\n",
      "for b in range(n_estimators):\n",
      "if(np.isin(i, bootstrap_ds_arr[b])==False):\n",
      "C.append(b)\n",
      "for pred in bag[C]:\n",
      "oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/len(C))\n",
      "L_oob = r2_score(y_train , oob_pred_arr)\n",
      "print(\"DecisionTreeRegressor R^2 score = \",r2_score(y_test , yhat),\n",
      "\"\\nBagging R^2 score = \", r2_score(y_test , yhatbag),\n",
      "\"\\nBagging OOB R^2 score = \",L_oob)\n",
      "DecisionTreeRegressor R^2 score = 0.575438224929718\n",
      "Bagging R^2 score = 0.7612121189201985\n",
      "Bagging OOB R^2 score = 0.7758253149069059\n",
      "The decision tree bagging improves the test-set R2score by about 32% (from 0 :575\n",
      "to 0:761). Moreover, the OOB score (0 :776) is very close to the true generalization risk\n",
      "(0:761) of the bagged estimator.\n",
      "The bagging procedure can be further enhanced by introducing random forests, which\n",
      "is discussed next.\n",
      "8.6 Random Forests\n",
      "In Section 8.5, we discussed the intuition behind the prediction averaging procedure. Spe-\n",
      "ciﬁcally, for some feature vector xletZb=gTb(x);b=1;2;:::; Bbe iid prediction val-\n",
      "ues, obtained from independent training sets T1;:::;TB. Suppose that VarZb=\u001b2for all\n",
      "b=1;:::; B. Then the variance of the average prediction value ZBis equal to\u001b2=B. How-\n",
      "ever, if bootstrapped data sets fT\u0003\n",
      "bgare used instead, the corresponding random variables\n",
      "fZbgwill be correlated . In particular, Zb=gT\u0003\n",
      "b(x) for b=1;:::; Bare identically distrib-\n",
      "uted (but not independent) with some positive pairwise correlation %. It then holds that (see\n",
      "Exercise 9)\n",
      "VarZB=%\u001b2+\u001b2(1\u0000%)\n",
      "B: (8.17)\n",
      "While the second term of (8.17) goes to zero as the number of observation Bincreases, the\n",
      "ﬁrst term remains constant.\n",
      "This issue is particularly relevant for bagging with decision trees. For example, con-\n",
      "sider a situation in which there exists a feature that provides a very good split of the data.\n",
      "Such a feature will be selected and split for every fgT\u0003\n",
      "bgB\n",
      "b=1at the root level and we will\n",
      "consequently end up with highly correlated predictions. In such a situation, prediction\n",
      "averaging will not introduce the desired improvement in the performance of the bagged\n",
      "predictor.312 8.6. Random Forests\n",
      "The major idea of random forests is to perform bagging in combination with a “decor-\n",
      "relation” of the trees by including only a subset of features during the tree construction. For\n",
      "each bootstrapped training set T\u0003\n",
      "bwe build a decision tree using a randomly selected subset\n",
      "ofm6pfeatures for the splitting rules. This simple but powerful idea will decorrelate the\n",
      "trees, since strong predictors will have a smaller chance to be considered at the root levels.\n",
      "Consequentially, we can expect to improve the predictive performance of the bagged\n",
      "estimator. The resulting predictor (random forest) construction is summarized in Algorithm\n",
      "8.6.1.\n",
      "Algorithm 8.6.1: Random Forest Construction\n",
      "Input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, the number of trees in the forest B, and the\n",
      "number m6pof features to be included, where pis the total number of\n",
      "features in x.\n",
      "Output: Ensemble of trees.\n",
      "1Generate bootstrapped training sets fT\u0003\n",
      "1;:::;T\u0003\n",
      "Bgvia Algorithm 8.5.1.\n",
      "2forb=1toBdo\n",
      "3 Train a decision tree gT\u0003\n",
      "bvia Algorithm 8.2.1, where each split is performed\n",
      "using mrandomly selected features out of p.\n",
      "4returnfgT\u0003\n",
      "bgB\n",
      "b=1.\n",
      "For regression problems, the output of Algorithm 8.6.1 is combined to yield the random\n",
      "forest prediction function:\n",
      "gRF(x)=1\n",
      "BBX\n",
      "b=1gT\u0003\n",
      "b(x):\n",
      "In the classiﬁcation setting, similar to Remark 8.1, we take instead the majority vote from\n",
      "thefgT\u0003\n",
      "bg.\n",
      "Example 8.4 (Random Forest for a Regression Tree) We continue with the basic\n",
      "bagging Example 8.3 for a regression tree, in which we compared the decision tree es-\n",
      "timator with the corresponding bagged estimator. Here, however, we use the random forest\n",
      "with B=500 trees and a subset size m=8. It can be seen that the random forest’s R2score\n",
      "is outperforming that of the bagged estimator.\n",
      "BaggingExampleRF.py\n",
      "from sklearn.datasets import make_friedman1\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "# create regression problem\n",
      "n_points = 1000 # points\n",
      "x, y = make_friedman1(n_samples=n_points , n_features=15,\n",
      "noise=1.0, random_state =100)\n",
      "# split to train/test set\n",
      "x_train , x_test , y_train , y_test = \\\n",
      "train_test_split(x, y, test_size=0.33, random_state =100)\n",
      "rf = RandomForestRegressor(n_estimators=500, oob_score = True ,Chapter 8. Decision Trees and Ensemble Methods 313\n",
      "max_features=8,random_state=100)\n",
      "rf.fit(x_train ,y_train)\n",
      "yhatrf = rf.predict(x_test)\n",
      "print(\"RF R^2 score = \", r2_score(y_test , yhatrf),\n",
      "\"\\nRF OOB R^2 score = \", rf.oob_score_)\n",
      "RF R^2 score = 0.8106589580845707\n",
      "RF OOB R^2 score = 0.8260541058404149\n",
      "Remark 8.3 (The Optimal Number of Subset Features m)The default values for m\n",
      "arebp=3candjppk\n",
      "for regression and classiﬁcation setting, respectively. However, the\n",
      "standard practice is to treat mas a hyperparameter that requires tuning, depending on the\n",
      "speciﬁc problem at hand [55].\n",
      "Note that the procedure of bagging decision trees is a special case of a random forest\n",
      "construction (see Exercise 11). Consequently, the OOB loss is readily available for random\n",
      "forests.\n",
      "While the advantage of bagging in the sense of enhanced accuracy is clear, we should\n",
      "also consider its negative aspects and, in particular, the loss of interpretability. Speciﬁcally\n",
      "a random forest consists of many trees, thus making the prediction process both hard to\n",
      "visualize and interpret. For example, given a random forest, it is not easy to determine a\n",
      "subset of features that are essential for accurate prediction.\n",
      "The feature importance measure intends to address this issue. The idea is as follows.\n",
      "Each internal node of a decision tree induces a certain decrease in the training loss; see\n",
      "(8.9). Let us denote this decrease in the training loss by \u0001Loss(v), where vis not a leaf node\n",
      "ofT. In addition, recall that for splitting rules of the type 1fxj6\u0018g(16j6p), each node\n",
      "vis associated with a feature xjthat determines the split. Using the above deﬁnitions, we\n",
      "can deﬁne the feature importance feature\n",
      "importanceofxjas\n",
      "IT(xj)=X\n",
      "vinternal2T\u0001Loss(v)1fxjis associated with vg;16j6p: (8.18)\n",
      "While (8.18) is deﬁned for a single tree, it can be readily extended to random forests.\n",
      "Speciﬁcally, the feature importance in that case will be averaged over all trees of the forest;\n",
      "that is, for a forest consisting of BtreesfT1;:::;TBg, the feature importance measure is:\n",
      "IRF(xj)=1\n",
      "BBX\n",
      "b=1ITb(xj);16j6p: (8.19)\n",
      "Example 8.5 (Feature Importance) We consider a classiﬁcation problem with 15 fea-\n",
      "tures. The data is speciﬁcally designed to contain only 5 informative features out of 15.\n",
      "In the code below, we apply the random forest procedure and calculate the corresponding\n",
      "feature importance measures, which are summarized in Figure 8.9.314 8.6. Random Forests\n",
      "VarImportance.py\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import matplotlib.pyplot as plt, pylab\n",
      "n_points = 1000 # create regression data with 1000 data points\n",
      "x, y = make_classification(n_samples=n_points , n_features=15,\n",
      "n_informative=5, n_redundant=0, n_repeated=0, random_state=100,\n",
      "shuffle=False)\n",
      "rf = RandomForestClassifier(n_estimators=200, max_features=\"log2\")\n",
      "rf.fit(x,y)\n",
      "importances = rf.feature_importances_\n",
      "indices = np.argsort(importances)[::-1]\n",
      "for f in range(15):\n",
      "print(\"Feature %d (%f)\" % (indices[f]+1, importances[indices[f\n",
      "]]))\n",
      "std = np.std([rf.feature_importances_ for tree in rf.estimators_],\n",
      "axis=0)\n",
      "f = plt.figure()\n",
      "plt.bar(range(x.shape[1]), importances[indices],\n",
      "color=\"b\", yerr=std[indices], align=\"center\")\n",
      "plt.xticks(range(x.shape[1]), indices+1)\n",
      "plt.xlim([-1, x.shape[1]])\n",
      "pylab.xlabel(\"feature index\")\n",
      "pylab.ylabel(\"importance\")\n",
      "plt.show()\n",
      "5 1 2 4 3 7 11 13 6 9 15 8 14 10 1200.10.2\n",
      "feature indeximportance\n",
      "Figure 8.9: Importance measure for the 15-feature data set with only 5 informative features\n",
      "x1;x2;x3;x4, and x5.Chapter 8. Decision Trees and Ensemble Methods 315\n",
      "Clearly, it is hard to visualize and understand the prediction process based on 200 trees.\n",
      "However, Figure 8.9 shows that the features x1;x2;x3;x4;andx5were correctly identiﬁed\n",
      "as being important.\n",
      "8.7 Boosting\n",
      "Boosting is a powerful idea that aims to improve the accuracy of any learning algorithm,\n",
      "especially when involving weak learners weak learners — simple prediction functions that exhibit per-\n",
      "formance slightly better than random guessing. Shallow decision trees typically yield weak\n",
      "learners.\n",
      "Originally, boosting was developed for binary classiﬁcation tasks, but it can be readily\n",
      "extended to handle general classiﬁcation and regression problems. The boosting approach\n",
      "has some similarity with the bagging method in the sense that boosting uses an ensemble of\n",
      "prediction functions. Despite this similarity, there exists a fundamental di \u000berence between\n",
      "these methods. Speciﬁcally, while bagging involves the ﬁtting of prediction functions to\n",
      "bootstrapped data, the predicting functions in boosting are learned sequentially . That is,\n",
      "each learner uses information from previous learners.\n",
      "The idea is to start with a simple model (weak learner) g0for the data \u001c=f(xi;yi)gn\n",
      "i=1\n",
      "and then to improve or “boost” this learner to a learner g1:=g0+h1. Here, the function h1\n",
      "is found by minimizing the training loss for g0+h1over all functions hin some class of\n",
      "functionsH. For example,Hcould be the set of prediction functions that can be obtained\n",
      "via a decision tree of maximal depth 2. Given a loss function Loss, the function h1is thus\n",
      "obtained as the solution to the optimization problem\n",
      "h1=argmin\n",
      "h2H1\n",
      "nnX\n",
      "i=1Loss (yi;g0(xi)+h(xi)): (8.20)\n",
      "This process can be repeated for g1to obtain g2=g1+h2, and so on, yielding the boosted\n",
      "prediction function\n",
      "gB(x)=g0(x)+BX\n",
      "b=1hb(x): (8.21)\n",
      "Instead of using the updating step gb=gb\u00001+hb, one prefers to use the smooth updating\n",
      ". As we shall seeably chosen step-size parameter \n",
      "shortly, this helps reduce overﬁtting.\n",
      "Boosting can be used for regression and classiﬁcation problems. We start with a simple\n",
      "regression setting, using the squared-error loss; thus, Loss( y;by)=(y\u0000by)2. In this case, it\n",
      "is common to start with g0(x)=n\u00001Pn\n",
      "i=1yi, and each hbforb=1;:::; Bis chosen as a\n",
      "learner for the data set \u001cbof residuals corresponding to gb\u00001. That is,\u001cb:=n\u0010\n",
      "xi;e(b)\n",
      "i\u0011on\n",
      "i=1,\n",
      "with\n",
      "e(b)\n",
      "i:=yi\u0000gb\u00001(xi): (8.22)\n",
      "This leads to the following boosting procedure for regression with squared-error loss.316 8.7. Boosting\n",
      "Algorithm 8.7.1: Regression Boosting with Squared-Error Loss\n",
      "Input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, the number of boosting rounds B, and a\n",
      ".hrinkage step-size parameter \n",
      "Output: Boosted prediction function.\n",
      "1Setg0(x) n\u00001Pn\n",
      "i=1yi.\n",
      "2forb=1toBdo\n",
      "3 Sete(b)\n",
      "i yi\u0000gb\u00001(xi) for i=1;:::; n, and let\u001cb n\u0010\n",
      "xi;e(b)\n",
      "i\u0011on\n",
      "i=1.\n",
      "4 Fit a prediction function hbon the training data \u001cb.\n",
      "hb(x).b(x) gb\u00001(x)+\n",
      "6return gB.\n",
      " step-sizeze parameter \n",
      "introduced in Algorithm 8.7.1 controls the speed of the\n",
      ", boosting takes smaller steps to-mall values of \n",
      "is of great practical import-ization. The step-size \n",
      "ance, since it helps the boosting algorithm to avoid overﬁtting. This phenomenon is\n",
      "demonstrated in Figure 8.10.\n",
      "−2−1 0 1 2−50050train data\n",
      "g1000 , γ= 1\n",
      "−2−1 0 1 2−50050train data\n",
      "g1000 , γ= 0 .005\n",
      "Figure 8.10: The left and the right panels show the ﬁtted boosting regression model g1000\n",
      "=0:005, respectively. Note the overﬁtting on the left.\n",
      "A very basic implementation of Algorithm 8.7.1 which reproduces Figure 8.10 is\n",
      "provided below.\n",
      "RegressionBoosting.py\n",
      "import numpy as np\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.datasets import make_regression\n",
      "import matplotlib.pyplot as plt\n",
      "def TrainBoost(alpha ,BoostingRounds ,x,y):\n",
      "g_0 = np.mean(y)Chapter 8. Decision Trees and Ensemble Methods 317\n",
      "residuals = y-alpha*g_0\n",
      "# list of basic regressor\n",
      "g_boost = []\n",
      "for i in range(BoostingRounds):\n",
      "h_i = DecisionTreeRegressor(max_depth=1)\n",
      "h_i.fit(x,residuals)\n",
      "residuals = residuals - alpha*h_i.predict(x)\n",
      "g_boost.append(h_i)\n",
      "return g_0, g_boost\n",
      "def Predict(g_0, g_boost ,alpha , x):\n",
      "yhat = alpha*g_0*np.ones(len(x))\n",
      "for j in range(len(g_boost)):\n",
      "yhat = yhat+alpha*g_boost[j].predict(x)\n",
      "return yhat\n",
      "np.random.seed(1)\n",
      "sz = 30\n",
      "# create data set\n",
      "x,y = make_regression(n_samples=sz, n_features=1, n_informative=1,\n",
      "noise=10.0)\n",
      "# boosting algorithm\n",
      "BoostingRounds = 1000\n",
      "alphas = [1, 0.005]\n",
      "for alpha in alphas:\n",
      "g_0, g_boost = TrainBoost(alpha ,BoostingRounds ,x,y)\n",
      "yhat = Predict(g_0, g_boost , alpha , x)\n",
      "# plot\n",
      "tmpX = np.reshape(np.linspace(-2.5,2,1000) ,(1000,1))\n",
      "yhatX = Predict(g_0, g_boost , alpha , tmpX)\n",
      "f = plt.figure()\n",
      "plt.plot(x,y, '*')\n",
      "plt.plot(tmpX ,yhatX)\n",
      "plt.show()\n",
      "can be viewed as a step size made in the direction of the negative\n",
      "gradient of the squared-error training loss. To see this, note that the negative gradient\n",
      "\u0000@Loss (yi;z)\n",
      "@z\f\f\f\f\fz=gb\u00001(xi)=\u0000@(yi\u0000z)2\n",
      "@z\f\f\f\f\f\fz=gb\u00001(xi)=2(yi\u0000gb\u00001(xi))\n",
      "is two times the residual e(b)\n",
      "igiven in (8.22) that is used in Algorithm 8.7.1 to ﬁt the pre-\n",
      "diction function hb.\n",
      "In fact, one of the major advances in the theory of boosting was the recognition that\n",
      "one can use a similar gradient descent method for any di \u000berentiable loss function. The318 8.7. Boosting\n",
      "resulting algorithm is called gradient boosting gradient\n",
      "boosting. The general gradient boosting algorithm is\n",
      "summarized in Algorithm 8.7.2. The main idea is to mimic a gradient descent algorithm\n",
      "in the following sense. At each stage of the boosting procedure, we calculate a negative +414\n",
      "gradient on ntraining points x1;:::; xn(Lines 3–4). Then, we ﬁt a simple model (such as\n",
      "a shallow decision tree) to approximate the gradient (Line 5) for any feature x. Finally,\n",
      "-sized step in the direction of thehod, we make a \n",
      "negative gradient (Line 6).\n",
      "Algorithm 8.7.2: Gradient Boosting\n",
      "Input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, the number of boosting rounds B, a\n",
      ".i\u000berentiable loss function Loss( y;by), and a gradient step-size parameter \n",
      "Output: Gradient boosted prediction function.\n",
      "1Setg0(x) 0.\n",
      "2forb=1toBdo\n",
      "3 fori=1tondo\n",
      "4 Evaluate the negative gradient of the loss at ( xi;yi) via\n",
      "r(b)\n",
      "i \u0000@Loss (yi;z)\n",
      "@z\f\f\f\f\fz=gb\u00001(xi)i=1;:::; n:\n",
      "5 Approximate the negative gradient by solving\n",
      "hb=argmin\n",
      "h2H1\n",
      "nnX\n",
      "i=0\u0010\n",
      "r(b)\n",
      "i\u0000h(xi)\u00112: (8.23)\n",
      "hb(x).b(x) gb\u00001(x)+\n",
      "7return gB\n",
      "Example 8.6 (Gradient Boosting for a Regression Tree) Let us continue with the ba-\n",
      "sic bagging and random forest examples for a regression tree (Examples 8.3 and 8.4), where\n",
      "we compared the standard decision tree estimator with the corresponding bagging and ran-\n",
      "dom forest estimators. Now, we use the gradient boosting estimator from Algorithm 8.7.2,\n",
      "=0:1 and perform B=100 boosting rounds. As\n",
      "a prediction function hbforb=1;:::; Bwe use small regression trees of depth at most\n",
      "3. Note that such individual trees do not usually give good performance; that is, they are\n",
      "weak prediction functions. We can see that the resulting boosting prediction function gives\n",
      "theR2score equal to 0 :899, which is better than R2scores of simple decision tree (0 :5754),\n",
      "the bagged tree (0 :761), and the random forest (0 :8106).\n",
      "GradientBoostingRegression.py\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_friedman1\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_scoreChapter 8. Decision Trees and Ensemble Methods 319\n",
      "# create regression problem\n",
      "n_points = 1000 # points\n",
      "x, y = make_friedman1(n_samples=n_points , n_features=15,\n",
      "noise=1.0, random_state =100)\n",
      "# split to train/test set\n",
      "x_train , x_test , y_train , y_test = \\\n",
      "train_test_split(x, y, test_size=0.33, random_state =100)\n",
      "# boosting sklearn\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "breg = GradientBoostingRegressor(learning_rate=0.1,\n",
      "n_estimators=100, max_depth =3, random_state=100)\n",
      "breg.fit(x_train ,y_train)\n",
      "yhat = breg.predict(x_test)\n",
      "print(\"Gradient Boosting R^2 score = \",r2_score(y_test , yhat))\n",
      "Gradient Boosting R^2 score = 0.8993055635639531\n",
      "We proceed with the classiﬁcation setting and consider the original boosting algorithm:\n",
      "AdaBoost AdaBoost . The inventors of the AdaBoost method considered a binary classiﬁcation prob-\n",
      "lem, where the response variable belongs to the f\u00001;1gset. The idea of AdaBoost is similar\n",
      "to the one presented in the regression setting, that is, AdaBoost ﬁts a sequence of prediction\n",
      "functions g0;g1=g0+h1;g2=g0+h1+h2;:::with ﬁnal prediction function\n",
      "gB(x)=g0(x)+BX\n",
      "b=1hb(x); (8.24)\n",
      "where each function hbis of the form hb(x)=\u000bbcb(x), with\u000bb2R+and where cbis a\n",
      "proper (but weak) classiﬁer in some class C. Thus, cb(x)2f\u00001;1g. Exactly as in (8.20), we\n",
      "solve at each boosting iteration the optimization problem\n",
      "(\u000bb;cb)=argmin\n",
      "\u000b>0;c2C1\n",
      "nnX\n",
      "i=1Loss (yi;gb\u00001(xi)+\u000bc(xi)): (8.25)\n",
      "However, in this case the loss function is deﬁned as Loss( y;by)=e\u0000yby. The algorithm starts\n",
      "with a simple model g0:=0 and for each successive iteration b=1;:::; Bsolves (8.25).\n",
      "Thus,\n",
      "(\u000bb;cb)=argmin\n",
      "\u000b>0;c2CnX\n",
      "i=1e\u0000yigb\u00001(xi)|    {z    }\n",
      "w(b)\n",
      "ie\u0000yi\u000bc(xi)=argmin\n",
      "\u000b>0;c2CnX\n",
      "i=1w(b)\n",
      "ie\u0000yi\u000bc(xi);\n",
      "where w(b)\n",
      "i:=expf\u0000yigb\u00001(xi)gdoes not depend on \u000borc. It follows that\n",
      "(\u000bb;cb)=argmin\n",
      "\u000b>0;c2Ce\u0000\u000bnX\n",
      "i=1w(b)\n",
      "i1fc(xi)=yig+e\u000bnX\n",
      "i=1w(b)\n",
      "i1fc(xi),yig\n",
      "=argmin\n",
      "\u000b>0;c2C(e\u000b\u0000e\u0000\u000b)`(b)\n",
      "\u001c(c)+e\u0000\u000b; (8.26)320 8.7. Boosting\n",
      "where\n",
      "`(b)\n",
      "\u001c(c) :=Pn\n",
      "i=1w(b)\n",
      "i1fc(xi),yig\n",
      "Pn\n",
      "i=1w(b)\n",
      "i\n",
      "can be interpreted as the weighted zero–one training loss at iteration b.\n",
      "For any\u000b>0, the program (8.26) is minimized by a classiﬁer c2Cthat minimizes\n",
      "this weighted training loss; that is,\n",
      "cb(x)=argmin\n",
      "c2C`(b)\n",
      "\u001c: (8.27)\n",
      "Substituting (8.27) into (8.26) and solving for the optimal \u000bgives\n",
      "\u000bb=1\n",
      "2ln 1\u0000`(b)\n",
      "\u001c(cb)\n",
      "`(b)\n",
      "\u001c(cb)!\n",
      ": (8.28)\n",
      "This gives the AdaBoost algorithm, summarized below.\n",
      "Algorithm 8.7.3: AdaBoost\n",
      "Input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, and the number of boosting rounds B.\n",
      "Output: AdaBoost prediction function.\n",
      "1Setg0(x) 0.\n",
      "2fori=1tondo\n",
      "3 w(1)\n",
      "i 1=n\n",
      "4forb=1toBdo\n",
      "5 Fit a classiﬁer cbon the training set \u001cby solving\n",
      "cb=argmin\n",
      "c2C`(b)\n",
      "\u001c(c)=argmin\n",
      "c2CPn\n",
      "i=1w(b)\n",
      "i1fc(xi),yig\n",
      "Pn\n",
      "i=1w(b)\n",
      "i:\n",
      "6 Set\u000bb 1\n",
      "2ln 1\u0000`(b)\n",
      "\u001c(cb)\n",
      "`(b)\n",
      "\u001c(cb)!\n",
      ". // Update weights\n",
      "7 fori=1tondo\n",
      "8 w(b+1)\n",
      "i w(b)\n",
      "iexpf\u0000yi\u000bbcb(xi)g.\n",
      "9return gB(x) :=PB\n",
      "b=1\u000bbcb(x).\n",
      "Algorithm 8.7.3 is quite intuitive. At the ﬁrst step ( b=1), AdaBoost assigns an equal\n",
      "weight w(1)\n",
      "i=1=nto each training sample ( xi;yi) in the set \u001c=f(xi;yi)gn\n",
      "i=1. Note that, in\n",
      "this case, the weighted zero–one training loss is equal to the regular zero–one training loss.\n",
      "At each successive step b>1, the weights of observations that were incorrectly classiﬁed\n",
      "by the previous boosting prediction function gbare increased, and the weights of correctly\n",
      "classiﬁed observations are decreased. Due to the use of the weighted zero–one loss, the set\n",
      "of incorrectly classiﬁed training samples will receive an extra weight and thus have a better\n",
      "chance of being classiﬁed correctly by the next classiﬁer cb+1. As soon as the AdaBoost\n",
      "algorithm ﬁnds the prediction function gB, the ﬁnal classiﬁcation is delivered via\n",
      "sign0BBBBB@BX\n",
      "b=1\u000bbcb(x)1CCCCCA:Chapter 8. Decision Trees and Ensemble Methods 321\n",
      "The step-size parameter \u000bbfound by the AdaBoost algorithm in Line 6 can be\n",
      "viewed as an optimal step-size in the sense of training loss minimization. How-\n",
      "ever, similar to the regression setting, one can slow down the AdaBoost algorithm\n",
      ". As usual, when the latter is done in \u000bb=\n",
      "practice, it is tackling the problem of overﬁtting.\n",
      "We consider an implementation of Algorithm 8.7.3 for a binary classiﬁcation problem.\n",
      "Speciﬁcally, during all boosting rounds, we use simple decision trees of depth 1 (also called\n",
      "decision tree stumps stumps ) as weak learners. The exponential and zero–one training losses as a\n",
      "function of the number of boosting rounds are presented in Figure 8.11.\n",
      "AdaBoost.py\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import zero_one_loss\n",
      "import numpy as np\n",
      "def ExponentialLoss(y,yhat):\n",
      "n = len(y)\n",
      "loss = 0\n",
      "for i in range(n):\n",
      "loss = loss+np.exp(-y[i]*yhat[i])\n",
      "loss = loss/n\n",
      "return loss\n",
      "# create binary classification problem\n",
      "np.random.seed(100)\n",
      "n_points = 100 # points\n",
      "x, y = make_blobs(n_samples=n_points , n_features=5, centers=2,\n",
      "cluster_std=20.0, random_state =100)\n",
      "y[y==0]=-1\n",
      "# AdaBoost implementation\n",
      "BoostingRounds = 1000\n",
      "n = len(x)\n",
      "W = 1/n*np.ones(n)\n",
      "Learner = []\n",
      "alpha_b_arr = []\n",
      "for i in range(BoostingRounds):\n",
      "clf = DecisionTreeClassifier(max_depth=1)\n",
      "clf.fit(x,y, sample_weight=W)\n",
      "Learner.append(clf)\n",
      "train_pred = clf.predict(x)\n",
      "err_b = 0322 8.7. Boosting\n",
      "for i in range(n):\n",
      "if(train_pred[i]!=y[i]):\n",
      "err_b = err_b+W[i]\n",
      "err_b = err_b/np.sum(W)\n",
      "alpha_b = 0.5*np.log((1-err_b)/err_b)\n",
      "alpha_b_arr.append(alpha_b)\n",
      "for i in range(n):\n",
      "W[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])\n",
      "yhat_boost = np.zeros(len(y))\n",
      "for j in range(BoostingRounds):\n",
      "yhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)\n",
      "yhat = np.zeros(n)\n",
      "yhat[yhat_boost >=0] = 1\n",
      "yhat[yhat_boost <0] = -1\n",
      "print(\"AdaBoost Classifier exponential loss = \", ExponentialLoss(y,\n",
      "yhat_boost))\n",
      "print(\"AdaBoost Classifier zero --one loss = \",zero_one_loss(y,yhat))\n",
      "AdaBoost Classifier exponential loss = 0.004224013663777142\n",
      "AdaBoost Classifier zero --one loss = 0.0\n",
      "200 400 600 800 1 ,00000.20.40.60.81\n",
      "Blossexponential loss\n",
      "zero-one loss\n",
      "Figure 8.11: Exponential and zero–one training loss as a function of the number of boosting\n",
      "rounds Bfor a binary classiﬁcation problem.Chapter 8. Decision Trees and Ensemble Methods 323\n",
      "Further Reading\n",
      "Breiman’s book on decision trees, [20], serves as a great starting point. Some additional\n",
      "advances can be found in [62, 96]. From the computational point of view, there exists\n",
      "an e\u000ecient recursive procedure for tree pruning; see Chapters 3 and 10 in [20]. Several\n",
      "advantages and disadvantages of using decision trees are debated in [37, 55]. A detailed\n",
      "discussion on bagging and random forests can be found in [21] and [23], respectively.\n",
      "Freund and Schapire [44] provide the ﬁrst boosting algorithm, the AdaBoost. While Ad-\n",
      "aBoost was developed in the context of the computational complexity of learning, it was\n",
      "later discovered by Friedman [45] that AdaBoost is a special case of an additive model.\n",
      "In addition, it was shown that for any di \u000berentiable loss function, there exists an e \u000ecient\n",
      "boosting procedure which mimics the gradient descent algorithm. The foundation of the\n",
      "resulting gradient boosting method is detailed in [45]. Python packages that implement\n",
      "gradient boosting include XGBoost andLightGBM .\n",
      "Exercises\n",
      "1. Show that any training set \u001c=f(x;yi);i=1;:::; ngcan be ﬁtted via a tree with zero\n",
      "training loss.\n",
      "2. Suppose during the construction of a decision tree we wish to specify a constant re-\n",
      "gional prediction function gwon the regionRw, based on the training data in Rw, say\n",
      "f(x1;y1);:::; (xk;yk)g. Show that gw(x) :=k\u00001Pk\n",
      "i=1yiminimizes the squared-error loss.\n",
      "3. Using the program from Section 8.2.4, write a basic implementation of a decision tree\n",
      "for a binary classiﬁcation problem. Implement the misclassiﬁcation, Gini index, and en-\n",
      "tropy impurity criteria to split nodes. Compare the results.\n",
      "4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in\n",
      "a certain tree region. Calculate the misclassiﬁcation impurity, the Gini impurity, and the\n",
      "entropy impurity. Repeat these calculations for 2 blue and 3 red data points.\n",
      "5. Consider the procedure of ﬁnding the best splitting rule for a categorical variable with\n",
      "klabels from Section 8.3.4. Show that one needs to consider 2ksubsets off1;:::; kgto ﬁnd\n",
      "the optimal partition of labels.\n",
      "6. Reproduce Figure 8.6 using the following classiﬁcation data.\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y = make_blobs(n_samples=5000, n_features=10, centers=3,\n",
      "random_state=10, cluster_std=10)\n",
      "7. Prove (8.13); that is, show that\n",
      "X\n",
      "w2W0BBBBB@nX\n",
      "i=11fxi2R wgLoss( yi;gw(xi))1CCCCCA=n`\u001c(g):324 Exercises\n",
      "8. Suppose \u001cis a training set with nelements and \u001c\u0003, also of size n, is obtained from \u001c\n",
      "by bootstrapping; that is, resampling with replacement. Show that for large n,\u001c\u0003does not\n",
      "contain a fraction of about e\u00001\u00190:37 of the points from \u001c.\n",
      "9. Prove Equation (8.17).\n",
      "10. Consider the following training /test split of the data. Construct a random forest re-\n",
      "gressor and identify the optimal subset size min the sense of R2score (see Remark 8.3).\n",
      "import numpy as np\n",
      "from sklearn.datasets import make_friedman1\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "# create regression problem\n",
      "n_points = 1000 # points\n",
      "x, y = make_friedman1(n_samples=n_points , n_features=15,\n",
      "noise=1.0, random_state =100)\n",
      "# split to train/test set\n",
      "x_train , x_test , y_train , y_test = \\\n",
      "train_test_split(x, y, test_size=0.33, random_state =100)\n",
      "11. Explain why bagging decision trees are a special case of random forests.\n",
      "12. Show that (8.28) holds.\n",
      "13. Consider the following classiﬁcation data and module imports:\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.metrics import zero_one_loss\n",
      "from sklearn.model_selection import train_test_split\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "X_train , y_train = make_blobs(n_samples=5000, n_features=10,\n",
      "centers=3, random_state=10, cluster_std=5)\n",
      "Using the gradient boosting algorithm with B=100 rounds, plot the training loss as a\n",
      "=0:1;0:3;0:5;0:7;1. What is your conclusion regarding the relation\n",
      "?CHAPTER9and\n",
      "DEEPLEARNING\n",
      "In this chapter, we show how one can construct a rich class of approximating func-\n",
      "tions called neural networks. The learners belonging to the neural-network class of\n",
      "functions have attractive properties that have made them ubiquitous in modern machine\n",
      "learning applications — their training is computationally feasible and their complexity\n",
      "is easy to control and ﬁne-tune.\n",
      "9.1 Introduction\n",
      "In Chapter 2 we described the basic supervised learning task; namely, we wish to predict a\n",
      "random output Yfrom a random input X, using a prediction function g:x7!ythat belongs\n",
      "to a suitably chosen class of approximating functions G. More generally, we may wish to\n",
      "predict a vector-valued output yusing a prediction function g:x7!yfrom classG.\n",
      "In this chapter ydenotes the vector-valued output for a given input x. This di \u000bers\n",
      "from our previous use (e.g., in Table 2.1), where ydenotes a vector of scalar outputs.\n",
      "In the machine learning context, the class Gis sometimes referred to as the hypothesis\n",
      "space or the universe of possible models , and the representational capacity of a hypothesisrepresentational\n",
      "capacity spaceGis simply its complexity.\n",
      "Suppose that we have a class of functions GL, indexed by a parameter Lthat controls\n",
      "the complexity of the class, so that GL\u001a G L+1\u001a G L+2\u001a \u0001\u0001\u0001 . In selecting a suitable\n",
      "class of functions, we have to be mindful of the approximation–estimation tradeo \u000b. On the + 31\n",
      "one hand, the class GLmust be complex (rich) enough to accurately represent the optimal\n",
      "unknown prediction function g\u0003, which may require a very large L. On the other hand, the\n",
      "learners in the class GLmust be simple enough to train with small estimation error and\n",
      "with minimal demands on computer memory, which may necessitate a small L.\n",
      "In balancing these competing objectives, it helps if the more complex class GL+1is\n",
      "easily constructed from an already existing and simpler GL. The simpler class of functions\n",
      "GLmay itself be constructed by modifying an even simpler class GL\u00001, and so on.\n",
      "A class of functions that permits such a natural hierarchical construction is the class of\n",
      "neural networks . Conceptually, a neural network with Llayers is a nonlinear parametricneural\n",
      "networks regression model whose representational capacity can easily be controlled by L.\n",
      "+188325326 9.1. Introduction\n",
      "Alternatively, in (9.3) we will deﬁne the output of a neural network as the repeated\n",
      "composition of linear and (componentwise) nonlinear functions. As we shall see, this rep-\n",
      "resentation of the output will provide a ﬂexible class of nonlinear functions that can be\n",
      "easily di \u000berentiated. As a result, the training of learners via gradient optimization methods\n",
      "involves mostly standard matrix operations that can be performed very e \u000eciently. +414\n",
      "Historically, neural networks were originally intended to mimic the workings of the\n",
      "human brain, with the network nodes modeling neurons and the network links modeling\n",
      "the axons connecting neurons. For this reason, rather than using the terminology of the\n",
      "regression models in Chapter 5, we prefer to use a nomenclature inspired by the apparent\n",
      "resemblance of neural networks to structures in the human brain.\n",
      "We note, however, that the attempts at building e \u000ecient machine learning algorithms by\n",
      "mimicking the functioning of the human brain have been as unsuccessful as the attempts\n",
      "at building ﬂying aircraft by mimicking the ﬂapping of birds’ wings. Instead, many ef-\n",
      "fective machine algorithms have been inspired by age-old mathematical ideas for function\n",
      "approximation. One such idea is the following fundamental result (see [119] for a proof).\n",
      "Theorem 9.1: Kolmogorov (1957)\n",
      "Every continuous function g\u0003: [0;1]p7!Rwith p>2 can be written as\n",
      "g\u0003(x)=2p+1X\n",
      "j=1hj0BBBBB@pX\n",
      "i=1hi j(xi)1CCCCCA;\n",
      "wherefhj;hi jgis a set of univariate continuous functions that depend on g\u0003.\n",
      "This result tells us that any continuous high-dimensional map can be represented as\n",
      "the function composition of much simpler (one-dimensional) maps. The composition of\n",
      "the maps needed to compute the output g\u0003(x) for a given input x2Rpare depicted in\n",
      "Figure 9.1, showing a directed graph or neural network with three layers, denoted as l=\n",
      "0;1;2.\n",
      "hijxi\n",
      "xpzjhj→ ajz1h1→ a1\n",
      "zqhq→ aqg∗(x)x1\n",
      "hpqh11\n",
      "Figure 9.1: Every continuous function g\u0003: [0;1]p7!Rcan be represented by a neural net-\n",
      "work with one hidden layer ( l=1), an input layer ( l=0), and an output layer ( l=2).Chapter 9. Deep Learning 327\n",
      "In particular, each of the pcomponents of the input xis represented as a node in the\n",
      "input layer (l=0). In the hidden layer (l=1) there are q:=2p+1 nodes, each of whichhidden layeris associated with a pair of variables ( z;a) with values\n",
      "zj:=pX\n",
      "i=1hi j(xi) and aj:=hj(zj):\n",
      "A link between nodes ( zj;aj) and xiwith weight hi jsigniﬁes that the value of zjdepends\n",
      "on the value of xivia the function hi j. Finally, the output layer (l=2) represents the value\n",
      "g\u0003(x)=Pq\n",
      "j=1aj. Note that the arrows on the graph remind us that the sequence of the\n",
      "computations is executed from left to right, or from the input layer l=0 through to the\n",
      "output layer l=2.\n",
      "In practice, we do not know the collection of functions fhj;hi jg, because they depend\n",
      "on the unknown g\u0003. In the unlikely event that g\u0003is linear, then all of the (2 p+1)(p+1)\n",
      "one-dimensional functions will be linear as well. However, in general, we should expect\n",
      "that each of the functions in fhj;hi jgis nonlinear.\n",
      "Unfortunately, Theorem 9.1 only asserts the existence of fhj;hi jg, and does not tell us\n",
      "how to construct these nonlinear functions. One way out of this predicament is to replace\n",
      "these (2 p+1)(p+1) unknown functions with a much larger number of known nonlinear\n",
      "functions called activation functions .1For example, a logistic activation function isactivation\n",
      "functionsS(z)=(1+exp(\u0000z))\u00001:\n",
      "We then hope that such a network, built from a su \u000eciently large number of activation\n",
      "functions, will have similar representational capacity as the neural network in Figure 9.1\n",
      "with (2 p+1)(p+1) functions.\n",
      "In general, we wish to use the simplest activation functions that will allow us to build\n",
      "a learner with large representational capacity and low training cost. The logistic function\n",
      "is merely one possible choice for an activation function from among inﬁnite possibilit-\n",
      "ies. Figure 9.2 shows a small selection of activation functions with di \u000berent regularity or\n",
      "smoothness properties.\n",
      "Heaviside or unit step rectiﬁed linear unit (ReLU) logistic\n",
      "-2 0 200.51\n",
      "-2 0 20123\n",
      "-2 0 200.51\n",
      "1fz>0g z\u00021fz>0g (1+exp(\u0000z))\u00001\n",
      "Figure 9.2: Some common activation functions S(z) with their deﬁning formulas and plots.\n",
      "The logistic function is an example of a sigmoid (that is, an S-shaped) function. Some\n",
      "books deﬁne the logistic function as 2 S(z)\u00001 (in terms of our deﬁnition).\n",
      "In addition to choosing the type and number of activation functions in a neural network,\n",
      "we can improve its representational capacity in another important way: introduce more\n",
      "hidden layers. In the next section we explore this possibility in detail.\n",
      "1Activation functions derive their name from models of a neuron’s response when exposed to chemical\n",
      "or electric stimuli.328 9.2. Feed-Forward Neural Networks\n",
      "9.2 Feed-Forward Neural Networks\n",
      "In a neural network with L+1 layers, the zero or input layer ( l=0) encodes the input feature\n",
      "vector x, and the last or output layer ( l=L) encodes the (multivalued) output function g(x).\n",
      "The remaining layers are called hidden layers . Each layer has a number of nodes, say pl\n",
      "nodes for layer l=0;:::; L. In this notation, p0is the dimension of the input feature vector\n",
      "xand, for example, pL=1 signiﬁes that g(x) is a scalar output. All nodes in the hidden\n",
      "layers ( l=1;:::; L\u00001) are associated with a pair of variables ( z;a), which we gather\n",
      "into pl-dimensional column vectors zlandal. In the so-called feed-forward feed-forward networks, the\n",
      "variables in any layer lare simple functions of the variables in the preceding layer l\u00001. In\n",
      "particular, zlandal\u00001are related via the linear relation zl=Wlal\u00001+bl, for some weight\n",
      "matrix Wlandbias vector bl.weight matrix\n",
      "bias vector Within any hidden layer l=1;:::; L\u00001, the components of the vectors zlandal\n",
      "are related via al=Sl(zl), where Sl:Rpl7!Rplis a nonlinear multivalued function. All of\n",
      "these multivalued functions are typically of the form\n",
      "Sl(z)=[S(z1);:::; S(zdim(z))]>;l=1;:::; L\u00001; (9.1)\n",
      "where Sis an activation function common to all hidden layers. The function SL:RpL\u000017!\n",
      "RpLin the output layer is more general and its speciﬁcation depends, for example, on\n",
      "whether the network is used for classiﬁcation or for the prediction of a continuous output\n",
      "Y. A four-layer ( L=3) network is illustrated in Figure 9.3.\n",
      "Input layer Hidden layers Output layer\n",
      "bias\n",
      "weightb1,j\n",
      "z3,mS3 w1,jiz2,kS→a2,kb2,kb3,m\n",
      "z1,jS→a1,jw2,k j w3,mkg(x) xi\n",
      "Figure 9.3: A neural network with L=3: the l=0 layer is the input layer, followed by two\n",
      "hidden layers, and the output layer. Hidden layers may have di \u000berent numbers of nodes.\n",
      "The output of this neural network is determined by the input vector x, (nonlinear)\n",
      "functionsfSlg, as well as weight matrices Wl=[wl;i j] and bias vectors bl=[bl;j] for\n",
      "l=1;2;3.Chapter 9. Deep Learning 329\n",
      "Here, the ( i;j)-th element of the weight matrix Wl=[wl;i j] is the weight that con-\n",
      "nects the j-th node in the ( l\u0000l)-st layer with the i-th node in the l-th layer.\n",
      "The name given to L(the number of layers without the input layer) is the network depthnetwork depthand max lplis called the network width . While we mostly study networks that have an equalnetwork widthnumber of nodes in the hidden layers ( p1=\u0001\u0001\u0001=pL\u00001), in general there can be di \u000berent\n",
      "numbers of nodes in each hidden layer.\n",
      "The output g(x) of a multiple-layer neural network is obtained from the input xvia the\n",
      "following sequence of computations:\n",
      "x|{z}\n",
      "a0!W1a0+b1|       {z       }\n",
      "z1!S1(z1)|{z}\n",
      "a1!W2a1+b2|       {z       }\n",
      "z2!S2(z2)|{z}\n",
      "a2!\u0001\u0001\u0001\n",
      "!WLaL\u00001+bL|          {z          }\n",
      "zL!SL(zL)|{z}\n",
      "aL=g(x):(9.2)\n",
      "Denoting the function z7!Wlz+blbyMl, the output g(x) can thus be written as the\n",
      "function composition\n",
      "g(x)=SL\u000eML\u000e\u0001\u0001\u0001\u000e S2\u000eM2\u000eS1\u000eM1(x): (9.3)\n",
      "The algorithm for computing the output g(x) for an input xis summarized next. Note\n",
      "that we leave open the possibility that the activation functions fSlghave di \u000berent deﬁnitions\n",
      "for each layer. In some cases, Slmay even depend on some or all of the already computed\n",
      "z1;z2;:::anda1;a2;:::.\n",
      "Algorithm 9.2.1: Feed-Forward Propagation for a Neural Network\n",
      "input: Feature vector x; weightsfwl;i jg, biasesfbl;igfor each layer l=1;:::; L.\n",
      "output: The value of the prediction function g(x).\n",
      "1a0 x // the zero or input layer\n",
      "2forl=1toLdo\n",
      "3 Compute the hidden variable zl;ifor each node iin layer l:\n",
      "zl Wlal\u00001+bl\n",
      "4 Compute the activation function al;ifor each node iin layer l:\n",
      "al Sl(zl)\n",
      "5return g(x) aL // the output layer330 9.2. Feed-Forward Neural Networks\n",
      "Example 9.1 (Nonlinear Multi-Output Regression) Given the input x2Rp0and an\n",
      "activation function S:R7!R, the output g(x) :=[g1(x);:::; gp2(x)]>of anonlinear multi-\n",
      "output regression model can be computed via a neural network with: +213\n",
      "z1=W1x+b1;where W12Rp1\u0002p0;b12Rp1;\n",
      "a1;k=S(z1;k);k=1;:::; p1;\n",
      "g(x)=W2a1+b2;where W22Rp2\u0002p1;b22Rp2;\n",
      "which is a neural network with one hidden layer and output function S2(z)=z. In the\n",
      "special case where p1=p2=1,b2=0;W2=1, and we collect all parameters into the\n",
      "vector\u0012>=[b1;W1]2Rp0+1, the neural network can be interpreted as a generalized linear\n",
      "model withE[YjX=x]=h([1;x>]\u0012) for some activation function h. +204\n",
      "Example 9.2 (Multi-Logit Classiﬁcation) Suppose that, for a classiﬁcation problem,\n",
      "an input xhas to be classiﬁed into one of cclasses, labeled 0 ;:::; c\u00001. We can perform the\n",
      "classiﬁcation via a neural network with one hidden layer, with p1=cnodes. In particular,\n",
      "we have\n",
      "z1=W1x+b1;a1=S1(z1);\n",
      "where S1is the softmax function:softmax\n",
      "softmax : z7!exp(z)P\n",
      "kexp(zk):\n",
      "For the output, we take g(x)=[g1(x);:::; gc(x)]>=a1, which can then be used as a\n",
      "pre-classiﬁer ofx. The actual classiﬁer of xinto one of the categories 0 ;1;:::; c\u00001 is then +254\n",
      "argmax\n",
      "k2f0;:::;c\u00001ggk+1(x):\n",
      "This is equivalent to the multi-logit classiﬁer in Section 7.5. Note, however, that there we +268\n",
      "used a slightly di \u000berent notation, with exinstead of xand we have a reference class; see\n",
      "Exercise 13.\n",
      "In practical implementations, the softmax function can cause numerical over- and\n",
      "under-ﬂow errors when either one of the exp( zk) happens to be extremely large orP\n",
      "kexp(zk) happens to be very small. In such cases we can exploit the invariance\n",
      "property (Exercise 1):\n",
      "softmax( z)=softmax( z+c\u00021) for any constant c:\n",
      "Using this property, we can compute softmax( z) with greater numerical stability via\n",
      "softmax( z\u0000max kfzkg\u00021).\n",
      "When neural networks are used for classiﬁcation into cclasses and the number of out-\n",
      "put nodes is c\u00001, then the gi(x) may be viewed as nonlinear discriminant functions . +262Chapter 9. Deep Learning 331\n",
      "Example 9.3 (Density Estimation) Estimating the density fof some random feature\n",
      "X2Ris the prototypical unsupervised learning task, which we tackled in Section 4.5.2 us-\n",
      "ing Gaussian mixture models. We can view a Gaussian mixture model with p1components +137\n",
      "and a common scale parameter \u001b > 0 as a neural network with two hidden layers, similar\n",
      "to the one on Figure 9.3. In particular, if the activation function in the ﬁrst hidden layer,\n",
      "S1, is of the form (9.1) with S(z) :=exp(\u0000z2=(2\u001b2))=p\n",
      "2\u0019\u001b2, then the density value g(x) is\n",
      "computed via:\n",
      "z1=W1x+b1;a1=S1(z1);\n",
      "z2=W2a1+b2;a2=S2(z2);\n",
      "g(x)=a>\n",
      "1a2;\n",
      "where W1=1is ap1\u00021 column vector of ones, W2=Ois ap1\u0002p1matrix of zeros, and S2\n",
      "is the softmax function. We identify the column vector b1with the p1location parameters,\n",
      "[\u00161;:::;\u0016 p1]>of the Gaussian mixture and b22Rp1with the p1weights of the mixture.\n",
      "Note the unusual activation function of the output layer — it requires the value of a1from\n",
      "the ﬁrst hidden layer and a2from the second hidden layer.\n",
      "There are a number of key design characteristics of a feed-forward network. First, we\n",
      "need to choose the activation function(s). Second, we need to choose the loss function for\n",
      "the training of the network. As we shall explain in the next section, the most common\n",
      "choices are the ReLU activation function and the cross-entropy loss. Crucially, we need\n",
      "to carefully construct the network architecture — the number of connections among thenetwork\n",
      "architecture nodes in di \u000berent layers and the overall number of layers of the network.\n",
      "For example, if the connections from one layer to the next are pruned (called sparse\n",
      "connectivity ) and the links share the same weight values fwl;i jg(called parameter sharing )\n",
      "for allf(i;j) :ji\u0000jj=0;1;:::g, then the weight matrices will be sparse and Toeplitz . +381\n",
      "Intuitively, the parameter sharing and sparse connectivity can speed up the training of\n",
      "the network, because there are fewer parameters to learn, and the Toeplitz structure permits\n",
      "quick computation of the matrix-vector products in Algorithm 9.2.1. An important example\n",
      "of such a network is the convolution neural network (CNN), in which some or all of theconvolution\n",
      "neural\n",
      "networknetwork layers encode the linear operation of convolution :\n",
      "Wlal\u00001=wl\u0003al\u00001;\n",
      "where [ x\u0003y]i:=P\n",
      "kxkyi\u0000k+1. As discussed in Example A.10, a convolution matrix is a +382\n",
      "special type of sparse Toeplitz matrix, and its action on a vector of learning parameters can\n",
      "be evaluated quickly via the fast Fourier transform . +396\n",
      "CNNs are particularly suited to image processing problems, because their convolution\n",
      "layers closely mimic the neurological properties of the visual cortex. In particular, the\n",
      "cortex partitions the visual ﬁeld into many small regions and assigns a group of neurons to\n",
      "every such region. Moreover, some of these groups of neurons respond only to the presence\n",
      "of particular features (for example, edges).\n",
      "This neurological property is naturally modeled via convolution layers in the neural\n",
      "network. Speciﬁcally, suppose that the input image is given by an m1\u0002m2matrix of pixels.\n",
      "Now, deﬁne a k\u0002kmatrix (sometimes called a kernel , where kis generally taken to be 3\n",
      "or 5). Then, the convolution layer output can be calculated using the discrete convolution332 9.3. Back-Propagation\n",
      "of all possible k\u0002kinput matrix regions and the kernel matrix; (see Example A.10). In\n",
      "particular, by noting that there are ( m1\u0000k+1)\u0002(m2\u0000k+1) possible regions in the original\n",
      "image, we conclude that the convolution layer output size is ( m1\u0000k+1)\u0002(m2\u0000k+1).\n",
      "In practice, we frequently deﬁne several kernel matrices, giving an output layer of size\n",
      "(m1\u0000k+1)\u0002(m2\u0000k+1)\u0002(the number of kernels). Figure 9.4 shows a 5 \u00025 input image\n",
      "and a 2\u00022 kernel with a 4 \u00024 output matrix. An example of using a CNN for image\n",
      "classiﬁcation is given in Section 9.5.2.\n",
      "Figure 9.4: An example 5 \u00025 input image and a 2 \u00022 kernel. The kernel is applied to every\n",
      "2\u00022 region of the original image.\n",
      "9.3 Back-Propagation\n",
      "The training of neural networks is a major challenge that requires both ingenuity and much\n",
      "experimentation. The algorithms for training neural networks with great depth are collect-\n",
      "ively referred to as deep learning methods. One of the simplest and most e \u000bective methodsdeep learningfor training is via steepest descent and its variations.+414Steepest descent requires computation of the gradient with respect to all bias vectors\n",
      "and weight matrices. Given the potentially large number of parameters (weight and bias\n",
      "terms) in a neural network, we need to ﬁnd an e \u000ecient method to calculate this gradient.\n",
      "To illustrate the nature of the gradient computations, let \u0012=fWl;blgbe a column vec-\n",
      "tor of length dim( \u0012)=PL\n",
      "l=1(pl\u00001pl+pl) that collects all the weight parameters (number-\n",
      "ingPL\n",
      "l=1pl\u00001pl) and bias parameters (numberingPL\n",
      "l=1pl) of a multiple-layer network with\n",
      "training loss:\n",
      "`\u001c(g(\u0001j\u0012)) :=1\n",
      "nnX\n",
      "i=1Loss( yi;g(xij\u0012)):\n",
      "Writing Ci(\u0012) :=Loss( yi;g(xij\u0012)) for short (using Cforcost), we have\n",
      "`\u001c(g(\u0001j\u0012))=1\n",
      "nnX\n",
      "i=1Ci(\u0012); (9.4)\n",
      "so that obtaining the gradient of `\u001crequires computation of @Ci=@\u0012for every i. For ac-\n",
      "tivation functions of the form (9.1), deﬁne Dlas the diagonal matrix with the vector of\n",
      "derivatives\n",
      "S0(z) :=[S0(zl;1);:::; S0(zl;pl)]>\n",
      "down its main diagonal; that is,\n",
      "Dl:=diag( S0(zl;1);:::; S0(zl;pl));l=1;:::; L\u00001:Chapter 9. Deep Learning 333\n",
      "The following theorem provides us with the formulas needed to compute the gradient of a\n",
      "typical Ci(\u0012).\n",
      "Theorem 9.2: Gradient of Training Loss\n",
      "For a given (input, output) pair ( x;y), let g(xj\u0012) be the output of Algorithm 9.2.1,\n",
      "and let C(\u0012)=Loss( y;g(xj\u0012)) be an almost-everywhere di \u000berentiable loss func-\n",
      "tion. Supposefzl;algL\n",
      "l=1are the vectors obtained during the feed-forward propagation\n",
      "(a0=x;aL=g(xj\u0012)). Then, we have for l=1;:::; L:\n",
      "@C\n",
      "@Wl=\u000ela>\n",
      "l\u00001and@C\n",
      "@bl=\u000el;\n",
      "where\u000el:=@C=@zlis computed recursively for l=L;:::; 2:\n",
      "\u000el\u00001=Dl\u00001W>\n",
      "l\u000elwith\u000eL=@SL\n",
      "@zL@C\n",
      "@g: (9.5)\n",
      "Proof: The scalar value Cis obtained from the transitions (9.2), followed by the mapping\n",
      "g(xj\u0012)7!Loss( y;g(xj\u0012)). Using the chain rule (see Appendix B.1.2), we have +402\n",
      "\u000eL=@C\n",
      "@zL=@g(x)\n",
      "@zL@C\n",
      "@g(x)=@SL\n",
      "@zL@C\n",
      "@g:\n",
      "Recall that the vector /vector derivative of a linear mapping z7!Wzis given by W>; see\n",
      "(B.5). It follows that, since zl=Wlal\u00001+blandal=S(zl), the chain rule gives +401\n",
      "@zl\n",
      "@zl\u00001=@al\u00001\n",
      "@zl\u00001@zl\n",
      "@al\u00001=Dl\u00001W>\n",
      "l:\n",
      "Hence, the recursive formula (9.5):\n",
      "\u000el\u00001=@C\n",
      "@zl\u00001=@zl\n",
      "@zl\u00001@C\n",
      "@zl=Dl\u00001W>\n",
      "l\u000el;l=L;:::; 3;2:\n",
      "Using thef\u000elg, we can now compute the derivatives with respect to the weight matrices\n",
      "and the biases. In particular, applying the “scalar /matrix” di \u000berentiation rule (B.10) to\n",
      "zl=Wlal\u00001+blgives:\n",
      "@C\n",
      "@Wl=@C\n",
      "@zl@zl\n",
      "@Wl=\u000ela>\n",
      "l\u00001;l=1;:::; L\n",
      "and\n",
      "@C\n",
      "@bl=@zl\n",
      "@bl@C\n",
      "@zl=\u000el;l=1;:::; L:\n",
      "\u0003\n",
      "From the theorem we can see that for each pair ( x;y) in the training set, we can compute the\n",
      "gradient@C=@\u0012in a sequential manner, by computing \u000eL;:::;\u000e1. This procedure is called\n",
      "back-propagation back-\n",
      "propagation. Since back-propagation mostly involves simple matrix multiplication, it334 9.3. Back-Propagation\n",
      "can be e \u000eciently implemented using dedicated computing hardware such as graphical pro-\n",
      "cessor units (GPUs) and other parallel computing architecture. Note also that many matrix\n",
      "computations that run in quadratic time can be replaced with linear-time componentwise\n",
      "multiplication. Speciﬁcally, multiplication of a vector with a diagonal matrix is equivalent\n",
      "to componentwise multiplication:\n",
      "A|{z}\n",
      "diag( a)b=a\fb:\n",
      "Consequently, we can write \u000el\u00001=Dl\u00001W>\n",
      "l\u000elas:\u000el\u00001=S0(zl\u00001)\fW>\n",
      "l\u000el;l=L;:::; 3;2.\n",
      "We now summarize the back-propagation algorithm for the computation of a typical\n",
      "@C=@\u0012. In the following algorithm, Lines 1 to 5 are the feed-forward part of the algorithm,\n",
      "and Lines 7 to 10 are the back-propagation part of the algorithm.\n",
      "Algorithm 9.3.1: Computing the Gradient of a Typical C(\u0012)\n",
      "input: Training example ( x;y), weight matrices and bias vectors fWl;blgL\n",
      "l=1=:\u0012,\n",
      "activation functions fSlgL\n",
      "l=1.\n",
      "output: The derivatives with respect to all weight matrices and bias vectors.\n",
      "1a0 x\n",
      "2forl=1;:::; Ldo // feed-forward\n",
      "3 zl Wlal\u00001+bl\n",
      "4 al Sl(zl)\n",
      "5\u000eL @SL\n",
      "@zL@C\n",
      "@g\n",
      "6z0 0 // arbitrary assignment needed to finish the loop\n",
      "7forl=L;:::; 1do // back-propagation\n",
      "8@C\n",
      "@bl \u000el\n",
      "9@C\n",
      "@Wl \u000ela>\n",
      "l\u00001\n",
      "10\u000el\u00001 S0(zl\u00001)\fW>\n",
      "l\u000el\n",
      "11return@C\n",
      "@Wland@C\n",
      "@blfor all l=1;:::; Land the value g(x) aL(if needed)\n",
      "Note that for the gradient of C(\u0012) to exist at every point, we need the activation func-\n",
      "tions to be di \u000berentiable everywhere. This is the case, for example, for the logistic activa-\n",
      "tion function in Figure 9.2. It is not the case for the ReLU function, which is di \u000berentiable\n",
      "everywhere, except at z=0. However, in practice, the kink of the ReLU function at z=0\n",
      "is unlikely to trip the back-propagation algorithm, because rounding errors and the ﬁnite-\n",
      "precision computer arithmetic make it extremely unlikely that we will need to evaluate the\n",
      "ReLU at precisely z=0. This is the reason why in Theorem 9.2 we merely required that\n",
      "C(\u0012) is almost-everywhere di \u000berentiable.\n",
      "In spite of its kink at the origin, the ReLU has an important advantage over the logistic\n",
      "function. While the derivative of the logistic function decays exponentially fast to zero as\n",
      "we move away from the origin, a phenomenon referred to as saturation , the derivative ofsaturationthe ReLU function is always unity for positive z. Thus, for large positive z, the derivative of\n",
      "the logistic function does not carry any useful information, but the derivative of the ReLU\n",
      "can help guide a gradient optimization algorithm. The situation for the Heaviside function\n",
      "in Figure 9.2 is even worse, because its derivative is completely noninformative for any\n",
      "z,0. In this respect, the lack of saturation of the ReLU function for z>0 makes it a\n",
      "desirable activation function for training a network via back-propagation.Chapter 9. Deep Learning 335\n",
      "Finally, note that to obtain the gradient @`\u001c=@\u0012of the training loss, we simply need to\n",
      "loop Algorithm 9.3.1 over all the ntraining examples, as follows.\n",
      "Algorithm 9.3.2: Computing the Gradient of the Training Loss\n",
      "input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, weight matrices and bias vectors\n",
      "fWl;blgL\n",
      "l=1=:\u0012, activation functions fSlgL\n",
      "l=1.\n",
      "output: The gradient of the training loss.\n",
      "1fori=1;:::; ndo // loop over all training examples\n",
      "2 Run Algorithm 9.3.1 with input ( xi;yi) to computen@Ci\n",
      "@Wl;@Ci\n",
      "@bloL\n",
      "l=1\n",
      "3return@C\n",
      "@Wl=1\n",
      "nPn\n",
      "i=1@Ci\n",
      "@Wland@C\n",
      "@bl=1\n",
      "nPn\n",
      "i=1@Ci\n",
      "@blfor all l=1;:::; L\n",
      "Example 9.4 (Squared-Error and Cross-Entropy Loss) The back-propagation Al-\n",
      "gorithm 9.3.1 requires a formula for \u000eLin line 5. In particular, to execute line 5 we need to\n",
      "specify both a loss function and an SLthat deﬁnes the output layer: g(xj\u0012)=aL=SL(zL).\n",
      "For instance, in the multi-logit classiﬁcation of inputs xinto pLcategories labeled +268\n",
      "0;1;:::; (pL\u00001), the output layer is deﬁned via the softmax function:\n",
      "SL:zL7!exp(zL)PpL\n",
      "k=1exp(zL;k):\n",
      "In other words, g(xj\u0012) is a probability vector such that its ( y+1)-st component gy+1(xj\u0012)=\n",
      "g(yj\u0012;x) is the estimate or prediction of the true conditional probability f(yjx). Combin-\n",
      "ing the softmax output with the cross-entropy loss, as was done in (7.17), yields: +269\n",
      "Loss( f(yjx);g(yj\u0012;x))=\u0000lng(yj\u0012;x)\n",
      "=\u0000lngy+1(xj\u0012)\n",
      "=\u0000zy+1+lnPpL\n",
      "k=1exp(zk):\n",
      "Hence, we obtain the vector \u000eLwith components ( k=1;:::; pL)\n",
      "\u000eL;k=@\n",
      "@zk\u0010\n",
      "\u0000zy+1+lnPpL\n",
      "k=1exp(zk)\u0011\n",
      "=gk(xj\u0012)\u00001fy=k\u00001g:\n",
      "Note that we can remove a node from the ﬁnal layer of the multi-logit network, be-\n",
      "cause g1(xj\u0012) (which corresponds to the y=0 class) can be eliminated, using the fact\n",
      "thatg1(xj\u0012)=1\u0000PpL\n",
      "k=2gk(xj\u0012). For a numerical comparison, see Exercise 13.\n",
      "As another example, in nonlinear multi-output regression (see Example 9.1), the out-\n",
      "put function SLis typically of the form (9.1), so that @SL=@z=diag( S0\n",
      "L(z1);:::; S0\n",
      "L(zpL)).\n",
      "Combining the output g(xj\u0012)=SL(zL) with the squared-error loss yields:\n",
      "Loss( y;g(xj\u0012))=ky\u0000g(xj\u0012)k2=pLX\n",
      "j=1(yj\u0000gj(xj\u0012))2:\n",
      "Hence, line 5 in Algorithm 9.3.1 simpliﬁes to:\n",
      "\u000eL=@SL\n",
      "@z@C\n",
      "@g=S0\n",
      "L(zL)\f2(g(xj\u0012)\u0000y):336 9.4. Methods for Training\n",
      "9.4 Methods for Training\n",
      "Neural networks have been studied for a long time, yet it is only recently that there have\n",
      "been su \u000ecient computational resources to train them e \u000bectively. The training of neural\n",
      "networks requires minimization of a training loss, `\u001c(g(\u0001j\u0012))=1\n",
      "nPn\n",
      "i=1Ci(\u0012), which is typ-\n",
      "ically a di \u000ecult high-dimensional optimization problem with multiple local minima. We\n",
      "next consider a number of simple training methods.\n",
      "In this section, the vectors \u000etand1tuse the notation of Section B.3.2 and should not\n",
      "be confused with the derivative \u000eand the prediction function g, respectively.\n",
      "9.4.1 Steepest Descent\n",
      "If we can compute the gradient of `\u001c(g(\u0001j\u0012)) via back-propagation, then we can apply the\n",
      "steepest descent algorithm, which reads as follows. Starting from a guess \u00121, we iterate the +414\n",
      "following step until convergence:\n",
      "\u0012t+1=\u0012t\u0000\u000btut; t=1;2;:::; (9.6)\n",
      "where ut:=@`\u001c\n",
      "@\u0012(\u0012t) and\u000btis the learning rate learning rate .\n",
      "Observe that, rather than operating directly on the weights and biases, we operate in-\n",
      "stead on\u0012:=fWl;blgL\n",
      "l=1— a column vector of lengthPL\n",
      "l=1(pl\u00001pl+pl) that stores all the\n",
      "weight and bias parameters. The advantage of organizing the computations in this way is\n",
      "that we can easily compute the learning rate \u000bt; for example, via the Barzilai–Borwein\n",
      "formula in (B.26). +415\n",
      "Algorithm 9.4.1: Training via Steepest Descent\n",
      "input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, initial weight matrices and bias vectors\n",
      "fWl;blgL\n",
      "l=1=:\u00121, activation functions fSlgL\n",
      "l=1.\n",
      "output: The parameters of the trained learner.\n",
      "1t 1;\u000e 0:1\u00021;ut\u00001 0; \u000b 0:1 // initialization\n",
      "2while stopping condition is not met do\n",
      "3 compute the gradient ut=@`\u001c\n",
      "@\u0012(\u0012t) using Algorithm 9.3.2\n",
      "41 ut\u0000ut\u00001\n",
      "5 if\u000e>1>0then // check if Hessian is positive-definite\n",
      "6\u000b \u000e>1\u000ek1k2// Barzilai-Borwein\n",
      "7 else\n",
      "8\u000b 2\u0002\u000b // failing positivity, do something heuristic\n",
      "9\u000e \u0000\u000but\n",
      "10\u0012t+1 \u0012t+\u000e\n",
      "11 t t+1\n",
      "12return\u0012tas the minimizer of the training loss\n",
      "Typically, we initialize the algorithm with small random values for \u00121, while being\n",
      "careful to avoid saturating the activation function. For example, in the case of the ReLUChapter 9. Deep Learning 337\n",
      "activation function, we will use small positive values to ensure that its derivative is not\n",
      "zero. A zero derivative of the activation function prevents the propagation of information\n",
      "useful for computing a good search direction.\n",
      "Recall that computation of the gradient of the training loss via Algorithm 9.3.2 requires\n",
      "averaging over all training examples. When the size nof the training set \u001cnis too large,\n",
      "computation of the gradient @`\u001cn=@\u0012via Algorithm 9.3.2 may be too costly. In such cases,\n",
      "we may employ the stochastic gradient descent stochastic\n",
      "gradient\n",
      "descentalgorithm. In this algorithm, we view the\n",
      "training loss as an expectation that can be approximated via Monte Carlo sampling. In\n",
      "particular, if Kis a random variable with distribution P[K=k]=1=nfork=1;:::; n, then\n",
      "we can write\n",
      "`\u001c(g(\u0001j\u0012))=1\n",
      "nnX\n",
      "k=1Loss( yk;g(xkj\u0012))=ELoss( yK;g(xKj\u0012)):\n",
      "We can thus approximate `\u001c(g(\u0001j\u0012)) via a Monte Carlo estimator using Niid copies of K:\n",
      "b`\u001c(g(\u0001j\u0012)) :=1\n",
      "NNX\n",
      "i=1Loss( yKi;g(xKij\u0012)):\n",
      "The iid Monte Carlo sample K1;:::; KNis called a minibatch minibatch (see also Exercise 3). Typic-\n",
      "ally, n\u001dNso that the probability of observing ties in a minibatch of size Nis negligible.\n",
      "Finally, note that if the learning rate of the stochastic gradient descent algorithm sat-\n",
      "isﬁes the conditions in (3.30), then the stochastic gradient descent algorithm is simply a +106\n",
      "version of the stochastic approximation Algorithm 3.4.5.\n",
      "9.4.2 Levenberg–Marquardt Method\n",
      "Since a neural network with squared-error loss is a special type of nonlinear regression\n",
      "model, it is possible to train it using classical nonlinear least-squares minimization meth-\n",
      "ods, such as the Levenberg–Marquardt algorithm. +417\n",
      "For simplicity of notation, suppose that the output of the net for an input xis ascalar\n",
      "g(x). For a given input parameter \u0012of dimension d=dim(\u0012), the Levenberg–Marquardt\n",
      "Algorithm B.3.3 requires computation of the following vector of outputs:\n",
      "g(\u001cj\u0012) :=[g(x1j\u0012);:::; g(xnj\u0012)]>;\n",
      "as well as the n\u0002dmatrix of Jacobi, G, ofgat\u0012. To compute these quantities, we can\n",
      "again use the back-propagation Algorithm 9.3.1, as follows.\n",
      "Algorithm 9.4.2: Output for Training via Levenberg–Marquardt\n",
      "input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, parameter\u0012.\n",
      "output: Vector g(\u001cj\u0012) and matrix of Jacobi Gfor use in Algorithm B.3.3.\n",
      "1fori=1;:::; ndo // loop over all training examples\n",
      "2 Run Algorithm 9.3.1 with input ( xi;yi) (using@C\n",
      "@g=1 in line 5) to compute\n",
      "g(xij\u0012) and@g(xij\u0012)\n",
      "@\u0012.\n",
      "3g(\u001cj\u0012) [g(x1j\u0012);:::; g(xnj\u0012)]>\n",
      "4G h@g(x1j\u0012)\n",
      "@\u0012;\u0001\u0001\u0001;@g(xnj\u0012)\n",
      "@\u0012i>\n",
      "5return g(\u001cj\u0012) and G338 9.4. Methods for Training\n",
      "The Levenberg–Marquardt algorithm is not suitable for networks with a large number\n",
      "of parameters, because the cost of the matrix computations becomes prohibitive. For in-\n",
      "stance, obtaining the Levenberg–Marquardt search direction in (B.28) usually incurs an\n",
      "O(d3) cost. In addition, the Levenberg–Marquardt algorithm is applicable only when we\n",
      "wish to train the network using the squared-error loss. Both of these shortcomings are\n",
      "mitigated to an extent with the quasi-Newton or adaptive gradient methods described next.\n",
      "9.4.3 Limited-Memory BFGS Method\n",
      "All the methods discussed so far have been ﬁrst-order optimization methods, that is, meth-\n",
      "ods that only use the gradient vector ut:=@`\u001c\n",
      "@\u0012(\u0012t) at the current (and /or immediate past) can-\n",
      "didate solution \u0012t. In trying to design a more e \u000ecient second-order optimization method,\n",
      "we may be tempted to use Newton’s method with a search direction: +412\n",
      "\u0000H\u00001\n",
      "tut;\n",
      "where Htis the d\u0002dmatrix of second-order partial derivatives of `\u001c(g(\u0001j\u0012)) at\u0012t.\n",
      "There are two problems with this approach. First, while the computation of utvia Al-\n",
      "gorithm 9.3.2 typically costs O(d), the computation of HtcostsO(d2). Second, even if we\n",
      "have somehow computed Htvery fast, computing the search direction H\u00001\n",
      "tutstill incurs an\n",
      "O(d3) cost. Both of these considerations make Newton’s method impractical for large d.\n",
      "Instead, a practical alternative is to use a quasi-Newton method , in which we directlyquasi -newton\n",
      "method aim to approximate H\u00001\n",
      "tvia a matrix Ctthat satisﬁes the secant condition :\n",
      "+413Ct1t=\u000et;\n",
      "where\u000et:=\u0012t\u0000\u0012t\u00001and1t:=ut\u0000ut\u00001.\n",
      "An ingenious formula that generates a suitable sequence of approximating matrices\n",
      "fCtg(each satisfying the secant condition) is the BFGS updating formula (B.23), which\n",
      "can be written as the recursion (see Exercise 9):\n",
      "Ct=\u0010\n",
      "I\u0000\u001dt1t\u000e>\n",
      "t\u0011>Ct\u00001\u0010\n",
      "I\u0000\u001dt1t\u000e>\n",
      "t\u0011\n",
      "+\u001dt\u000et\u000e>\n",
      "t; \u001d t:=(1>\n",
      "t\u000et)\u00001: (9.7)\n",
      "This formula allows us to update Ct\u00001toCtand then compute CtutinO(d2) time. While\n",
      "this quasi-Newton approach is better than the O(d3) cost of Newton’s method, it may be\n",
      "still too costly in large-scale applications.\n",
      "Instead, an approximate or limited memory BFGS limited memory\n",
      "bfgsupdating can be achieved in O(d)\n",
      "time. The idea is to store a few of the most recent pairs f\u000et;1tgin order to evaluate its action\n",
      "on a vector utwithout explicitly constructing and storing Ctin computer memory. This is\n",
      "possible, because updating C0toC1in (9.7) requires only the pair \u000e1;11, and similarly\n",
      "computing Ctfrom C0only requires the history of the updates \u000e1;11:::;\u000et;1t, which can\n",
      "be shown as follows.\n",
      "Deﬁne the matrices At;:::; A0via the backward recursion ( j=1;:::; t):\n",
      "At:=I;Aj\u00001:=\u0010\n",
      "I\u0000\u001dj1j\u000e>\n",
      "j\u0011\n",
      "Aj;\n",
      "and observe that all matrix vector products: Aju=:qj;forj=0;:::; tcan be computed\n",
      "e\u000eciently via the backward recursion starting with qt=u:\n",
      "\u001cj:=\u000e>\n",
      "jqj;qj\u00001=qj\u0000\u001dj\u001cj1j;j=t;t\u00001;:::; 1: (9.8)Chapter 9. Deep Learning 339\n",
      "In addition tofqjg, we will make use of the vectors frjgdeﬁned via the recursion:\n",
      "r0:=C0q0;rj=rj\u00001+\u001dj\u0010\n",
      "\u001cj\u00001>\n",
      "jrj\u00001\u0011\n",
      "\u000ej;j=1;:::; t: (9.9)\n",
      "At the ﬁnal iteration t, the BFGS updating formula (9.7) can be rewritten in the form:\n",
      "Ct=A>\n",
      "t\u00001Ct\u00001At\u00001+\u001dt\u000et\u000e>\n",
      "t:\n",
      "By iterating the recursion (9.7) backwards to C0, we can write:\n",
      "Ct=A>\n",
      "0C0A0+tX\n",
      "j=1\u001djA>\n",
      "j\u000ej\u000e>\n",
      "jAj;\n",
      "that is, we can express Ctin terms of the initial C0and the entire history of all BFGS values\n",
      "f\u000ej;1jg, as claimed. Further, with the fqj;rjgcomputed via (9.8) and (9.9), we can write:\n",
      "Ctu=A>\n",
      "0C0q0+tX\n",
      "j=1\u001dj\u0010\n",
      "\u000e>\n",
      "jqj\u0011\n",
      "A>\n",
      "j\u000ej\n",
      "=A>\n",
      "0r0+\u001d1\u001c1A>\n",
      "1\u000e1+tX\n",
      "j=2\u001dj\u001cjA>\n",
      "j\u000ej\n",
      "=A>\n",
      "1\u0002\u0000I\u0000\u001d1\u000e11>\n",
      "1\u0001r0+\u001d1\u001c1\u000e1\u0003+tX\n",
      "j=2\u001dj\u001cjA>\n",
      "j\u000ej:\n",
      "Hence, from the deﬁnition of the frjgin (9.9), we obtain\n",
      "Ctu=A>\n",
      "1r1+tX\n",
      "j=2\u001dj\u001cjA>\n",
      "j\u000ej\n",
      "=A>\n",
      "2r2+tX\n",
      "j=3\u001dj\u001cjA>\n",
      "j\u000ej\n",
      "=\u0001\u0001\u0001=A>\n",
      "trt+0=rt:\n",
      "Given C0and the history of all recent BFGS values f\u000ej;1jgh\n",
      "j=1, the computation of the quasi-\n",
      "Newton search direction d=\u0000Chucan be accomplished via the recursions (9.8) and (9.9)\n",
      "as summarized in Algorithm 9.4.3.\n",
      "Note that if C0is a diagonal matrix, say the identity matrix, then C0qis cheap to\n",
      "compute and the cost of running Algorithm 9.4.3 is O(h d). Thus, for a ﬁxed length of the\n",
      "BFGS history, the cost of the limited-memory BFGS updating grows linearly in d, making\n",
      "it a viable optimization algorithm in large-scale applications.340 9.4. Methods for Training\n",
      "Algorithm 9.4.3: Limited-Memory BFGS Update\n",
      "input: BFGS history list f\u000ej;1jgh\n",
      "j=1, initial C0, and input u.\n",
      "output: d=\u0000Chu, where Ct=\u0000I\u0000\u001dt\u000et1>\n",
      "t\u0001Ct\u00001\u0010\n",
      "I\u0000\u001dt1t\u000e>\n",
      "t\u0011\n",
      "+\u001dt\u000et\u000e>\n",
      "t.\n",
      "1q u\n",
      "2fori=h;h\u00001;:::; 1do // backward recursion to compute A0u\n",
      "3\u001di \u0010\n",
      "\u000e>\n",
      "i1i\u0011\u00001\n",
      "4\u001ci \u000e>\n",
      "iq\n",
      "5 q q\u0000\u001di\u001ci1i\n",
      "6q C0q // compute C0(A0u)\n",
      "7fori=1;:::; hdo // compute recursion (9.9)\n",
      "8 q q+\u001di(\u001ci\u00001>\n",
      "iq)\u000ei\n",
      "9return d \u0000q,the value of\u0000Chu\n",
      "In summary, a quasi-Newton algorithm with limited-memory BFGS updating reads as\n",
      "follows.\n",
      "Algorithm 9.4.4: Quasi-Newton Minimization with Limited-Memory BFGS\n",
      "input: Training set \u001c=f(xi;yi)gn\n",
      "i=1, initial weight matrices and bias vectors\n",
      "fWl;blgL\n",
      "l=1=:\u00121, activation functions fSlgL\n",
      "l=1, and history parameter h.\n",
      "output: The parameters of the trained learner.\n",
      "1t 1;\u000e 0:1\u00021;ut\u00001 0 // initialization\n",
      "2while stopping condition is not met do\n",
      "3 Compute`value=`\u001c(g(\u0001j\u0012t)) and ut=@`\u001c\n",
      "@\u0012(\u0012t) via Algorithm 9.3.2.\n",
      "41 ut\u0000ut\u00001\n",
      "5 Add (\u000e;1) to the BFGS history as the newest BFGS pair.\n",
      "6 ifthe number of pairs in the BFGS history is greater than hthen\n",
      "7 remove the oldest pair from the BFGS history\n",
      "8 Compute dvia Algorithm 9.4.3 using the BFGS history, C0=I, and ut.\n",
      "9\u000b 1\n",
      "10 while`\u001c(g(\u0001j\u0012t+\u000bd))>`value+10\u00004\u000bd>utdo\n",
      "11\u000b \u000b=1:5 // line-search along quasi-Newton direction\n",
      "12\u000e \u000bd\n",
      "13\u0012t+1 \u0012t+\u000e\n",
      "14 t t+1\n",
      "15return\u0012tas the minimizer of the training loss\n",
      "9.4.4 Adaptive Gradient Methods\n",
      "Recall that the limited-memory BFGS method in the previous section determines a search\n",
      "direction using the recent history of previously computed gradients futgand input paramet-\n",
      "ersf\u0012tg. This is because the BFGS pairs f\u000et;1tgcan be easily constructed from the identities:\n",
      "\u000et=\u0012t\u0000\u0012t\u00001and1t=ut\u0000ut\u00001. In other words, using only past gradient computations and\n",
      "with little extra computation, it is possible to infer some of the second-order informationChapter 9. Deep Learning 341\n",
      "contained in the Hessian matrix of `\u001c(\u0012). In addition to the BFGS method, there are other\n",
      "ways in which we can exploit the history of past gradient computations.\n",
      "One approach is to use the normal approximation method , in which the Hessian of `\u001c +416\n",
      "at\u0012tis approximated via\n",
      "I+1=\n",
      "htX\n",
      "i=t\u0000h+1uiu>\n",
      "i; (9.10)\n",
      "is a tuning parameterre the hmost recently computed gradients and \n",
      "=1=h). The search direction is then given by\n",
      "\u0000bH\u00001\n",
      "tut;\n",
      "which can be computed quickly in O(h2d) time either using the QR decomposition (Exer-\n",
      "cises 5 and 6), or the Sherman–Morrison Algorithm A.6.1. This approach requires that we +375\n",
      "store the last hgradient vectors in memory.\n",
      "Another approach that completely bypasses the need to invert a Hessian approximation\n",
      "is the Adaptive Gradient orAdaGrad AdaGrad method, in which we only store the diagonal of bHt\n",
      "and use the search direction:\n",
      "\u0000diag(bHt)\u00001=2ut:\n",
      "We can avoid storing any of the gradient history by instead using the slightly di \u000berent\n",
      "search direction2\n",
      "\u0000ut.p\n",
      "\u00021;\n",
      "where the vector vtis updated recursively via\n",
      "vt= \n",
      "1\u00001\n",
      "h!\n",
      "vt\u00001+1\n",
      "hut\fut:\n",
      "\u00021and the diagonal of vt, the di \u000berence between the vector vt+\n",
      "the Hessian bHtwill be negligible.\n",
      "A more sophisticated version of AdaGrad is the adaptive moment estimation orAdam Adam\n",
      "method, in which we not only average the vectors fvtg, but also average the gradient vectors\n",
      "futg, as follows.\n",
      "Algorithm 9.4.5: Updating of Search Direction at Iteration tviaAdam\n",
      "input: ut,but\u00001,vt\u00001,\u0012t, and parameters ( \u000b;hv;hu), equal to, e.g., (10\u00003;103;10).\n",
      "output: but,vt,\u0012t+1.\n",
      "1but \u0010\n",
      "1\u00001\n",
      "hu\u0011\n",
      "but\u00001+1\n",
      "huut\n",
      "2vt \u0010\n",
      "1\u00001\n",
      "hv\u0011\n",
      "vt\u00001+1\n",
      "hvut\fut\n",
      "3u\u0003\n",
      "t but.\u0010\n",
      "1\u0000(1\u0000h\u00001\n",
      "u)t\u0011\n",
      "4v\u0003\n",
      "t vt.\u0010\n",
      "1\u0000(1\u0000h\u00001\n",
      "v)t\u0011\n",
      "5\u0012t+1 \u0012t\u0000\u000bu\u0003\n",
      "t.\u0010p\n",
      "v\u0003\n",
      "t+10\u00008\u00021\u0011\n",
      "6return but,vt,\u0012t+1\n",
      "2Here we divide two vectors componentwise.342 9.5. Examples in Python\n",
      "Yet another computationally cheap approach is the momentum method , in which themomentum\n",
      "method steepest descent iteration (9.6) is modiﬁed to\n",
      "\u000et;1=\u0012t\u0000\u000btut+\n",
      "is a tuning parameter. This strategy frequently performs better\n",
      "than the “vanilla” steepest descent method, because the search direction is less likely to\n",
      "change abruptly.\n",
      "Numerical experience suggests that the vanilla steepest-descent Algorithm 9.4.1 and\n",
      "the Levenberg–Marquardt Algorithm B.3.3 are e \u000bective for networks with shallow archi-\n",
      "tectures, but not for networks with deep architectures. In comparison, the stochastic gradi-\n",
      "ent descent method, the limited-memory BFGS Algorithm 9.4.4, or any of the adaptive\n",
      "gradient methods in this section, can frequently handle networks with many hidden lay-\n",
      "ers (provided that any tuning parameters and initialization values are carefully chosen via\n",
      "experimentation).\n",
      "9.5 Examples in Python\n",
      "In this section we provide two numerical examples in Python. In the ﬁrst example, we\n",
      "train a neural network with the stochastic gradient descent method using the polynomial\n",
      "regression data from Example 2.1, and without using any specialized Python packages. +26\n",
      "In the second example, we consider a realistic application of a neural network to image\n",
      "recognition and classiﬁcation. Here we use the specialized open-source Python package\n",
      "Pytorch .\n",
      "9.5.1 Simple Polynomial Regression\n",
      "Consider again the polynomial regression data set depicted in Figure 2.4. We use a network\n",
      "with architecture\n",
      "[p0;p1;p2;p3]=[1;20;20;1]:\n",
      "In other words, we have two hidden layers with 20 neurons, resulting in a learner with a\n",
      "total of dim( \u0012)=481 parameters. To implement such a neural network, we ﬁrst import the\n",
      "numpy and the matplotlib packages, then read the regression problem data and deﬁne\n",
      "the feed-forward neural network layers.\n",
      "NeuralNetPurePython.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "#%%\n",
      "# import data\n",
      "data = np.genfromtxt( 'polyreg.csv ',delimiter= ',')\n",
      "X = data[:,0].reshape(-1,1)\n",
      "y = data[:,1].reshape(-1,1)\n",
      "# Network setup\n",
      "p = [X.shape[1],20,20,1] # size of layers\n",
      "L = len(p)-1 # number of layersChapter 9. Deep Learning 343\n",
      "Next, the initialize method generates random initial weight matrices and bias vec-\n",
      "torsfWl;blgL\n",
      "l=1. Speciﬁcally, all parameters are initialized with values distributed according\n",
      "to the standard normal distribution.\n",
      "def initialize(p, w_sig = 1):\n",
      "W, b = [[]]*len(p), [[]]*len(p)\n",
      "for l in range(1,len(p)):\n",
      "W[l]= w_sig * np.random.randn(p[l], p[l-1])\n",
      "b[l]= w_sig * np.random.randn(p[l], 1)\n",
      "return W,b\n",
      "W,b = initialize(p) # initialize weight matrices and bias vectors\n",
      "The following code implements the ReLU activation function from Figure 9.2 and the\n",
      "squared error loss. Note that these functions return both the function values and the corres-\n",
      "ponding gradients.\n",
      "def RELU(z,l): # RELU activation function: value and derivative\n",
      "if l == L: return z, np.ones_like(z)\n",
      "else:\n",
      "val = np.maximum(0,z) # RELU function element -wise\n",
      "J = np.array(z>0, dtype = float) # derivative of RELU\n",
      "element -wise\n",
      "return val, J\n",
      "def loss_fn(y,g):\n",
      "return (g - y)**2, 2 * (g - y)\n",
      "S = RELU\n",
      "Next, we implement the feed-forward and backward-propagation Algorithm 9.3.1.\n",
      "Here, we have implemented Algorithm 9.3.2 inside the backward-propagation loop.\n",
      "def feedforward(x,W,b):\n",
      "a, z, gr_S = [0]*(L+1), [0]*(L+1), [0]*(L+1)\n",
      "a[0] = x.reshape(-1,1)\n",
      "for l in range(1,L+1):\n",
      "z[l] = W[l] @ a[l-1] + b[l] # affine transformation\n",
      "a[l], gr_S[l] = S(z[l],l) # activation function\n",
      "return a, z, gr_S\n",
      "def backward(W,b,X,y):\n",
      "n =len(y)\n",
      "delta = [0]*(L+1)\n",
      "dC_db , dC_dW = [0]*(L+1), [0]*(L+1)\n",
      "loss=0\n",
      "for i in range(n): # loop over training examples\n",
      "a, z, gr_S = feedforward(X[i,:].T, W, b)\n",
      "cost , gr_C = loss_fn(y[i], a[L]) # cost i and gradient wrt g\n",
      "loss += cost/n344 9.5. Examples in Python\n",
      "delta[L] = gr_S[L] @ gr_C\n",
      "for l in range(L,0,-1): # l = L,...,1\n",
      "dCi_dbl = delta[l]\n",
      "dCi_dWl = delta[l] @ a[l-1].T\n",
      "# ---- sum up over samples ----\n",
      "dC_db[l] = dC_db[l] + dCi_dbl/n\n",
      "dC_dW[l] = dC_dW[l] + dCi_dWl/n\n",
      "# -----------------------------\n",
      "delta[l-1] = gr_S[l-1] * W[l].T @ delta[l]\n",
      "return dC_dW , dC_db , loss\n",
      "As explained in Section 9.4, it is sometimes more convenient to collect all the weight\n",
      "matrices and bias vectors fWl;blgL\n",
      "l=1into a single vector \u0012. Consequently, we code two\n",
      "functions that map the weight matrices and the bias vectors into a single parameter vector,\n",
      "and vice versa.\n",
      "def list2vec(W,b):\n",
      "# converts list of weight matrices and bias vectors into\n",
      "# one column vector\n",
      "b_stack = np.vstack([b[i] for i in range(1,len(b))] )\n",
      "W_stack = np.vstack(W[i].flatten().reshape(-1,1) for i in range\n",
      "(1,len(W)))\n",
      "vec = np.vstack([b_stack , W_stack])\n",
      "return vec\n",
      "#%%\n",
      "def vec2list(vec, p):\n",
      "# converts vector to weight matrices and bias vectors\n",
      "W, b = [[]]*len(p) ,[[]]*len(p)\n",
      "p_count = 0\n",
      "for l in range(1,len(p)): # construct bias vectors\n",
      "b[l] = vec[p_count:(p_count+p[l])].reshape(-1,1)\n",
      "p_count = p_count + p[l]\n",
      "for l in range(1,len(p)): # construct weight matrices\n",
      "W[l] = vec[p_count:(p_count + p[l]*p[l-1])].reshape(p[l], p[\n",
      "l-1])\n",
      "p_count = p_count + (p[l]*p[l-1])\n",
      "return W, b\n",
      "Finally, we run the stochastic gradient descent for 104iterations using a minibatch of\n",
      "size 20 and a constant learning rate of \u000bt=0:005.\n",
      "batch_size = 20\n",
      "lr = 0.005\n",
      "beta = list2vec(W,b)\n",
      "loss_arr = []Chapter 9. Deep Learning 345\n",
      "n = len(X)\n",
      "num_epochs = 10000\n",
      "print(\"epoch | batch loss\")\n",
      "print(\"----------------------------\")\n",
      "for epoch in range(1,num_epochs+1):\n",
      "batch_idx = np.random.choice(n,batch_size)\n",
      "batch_X = X[batch_idx].reshape(-1,1)\n",
      "batch_y=y[batch_idx].reshape(-1,1)\n",
      "dC_dW , dC_db , loss = backward(W,b,batch_X ,batch_y)\n",
      "d_beta = list2vec(dC_dW ,dC_db)\n",
      "loss_arr.append(loss.flatten()[0])\n",
      "if(epoch==1 or np.mod(epoch ,1000)==0):\n",
      "print(epoch ,\": \",loss.flatten()[0])\n",
      "beta = beta - lr*d_beta\n",
      "W,b = vec2list(beta ,p)\n",
      "# calculate the loss of the entire training set\n",
      "dC_dW , dC_db , loss = backward(W,b,X,y)\n",
      "print(\"entire training set loss = \",loss.flatten()[0])\n",
      "xx = np.arange(0,1,0.01)\n",
      "y_preds = np.zeros_like(xx)\n",
      "for i in range(len(xx)):\n",
      "a, _, _ = feedforward(xx[i],W,b)\n",
      "y_preds[i], = a[L]\n",
      "plt.plot(X,y, 'r.', markersize = 4,label = 'y')\n",
      "plt.plot(np.array(xx), y_preds , 'b',label = 'fit')\n",
      "plt.legend()\n",
      "plt.xlabel( 'x')\n",
      "plt.ylabel( 'y')\n",
      "plt.show()\n",
      "plt.plot(np.array(loss_arr), 'b')\n",
      "plt.xlabel( 'iteration ')\n",
      "plt.ylabel( 'Training Loss ')\n",
      "plt.show()\n",
      "epoch | batch loss\n",
      "----------------------------\n",
      "1 : 158.6779278688539\n",
      "1000 : 54.52430507401445\n",
      "2000 : 38.346572088604965\n",
      "3000 : 31.02036319180713\n",
      "4000 : 22.91114276931535\n",
      "5000 : 27.75810262906341\n",
      "6000 : 22.296907007032928\n",
      "7000 : 17.337367420038046\n",
      "8000 : 19.233689945334195\n",
      "9000 : 39.54261478969857\n",
      "10000 : 14.754724387604416\n",
      "entire training set loss = 28.904957963612727\n",
      "The left panel of Figure 9.5 shows a trained neural network with a training loss of\n",
      "approximately 28 :9. As seen from the right panel of Figure 9.5, the algorithm initially\n",
      "makes rapid progress until it settles down into a stationary regime after 400 iterations.346 9.5. Examples in Python\n",
      "0.00\n",
      " 0.25\n",
      " 0.50\n",
      " 0.75\n",
      " 1.00\n",
      "input u\n",
      "0\n",
      "20\n",
      "40output y\n",
      "ﬁt\n",
      "y\n",
      "0\n",
      " 500\n",
      " 1000\n",
      " 1500\n",
      " 2000\n",
      "iteration\n",
      "0\n",
      "100\n",
      "200\n",
      "300Batch Loss\n",
      "Figure 9.5: Left panel: The ﬁtted neural network with training loss of `\u001c(g\u001c)\u001928:9. Right\n",
      "panel: The evolution of the estimated loss, b`\u001c(g\u001c(\u0001j\u0012)), over the steepest-descent iterations.\n",
      "9.5.2 Image Classiﬁcation\n",
      "In this section, we will use the package Pytorch , which is an open-source machine learn-\n",
      "ing library for Python. Pytorch can easily exploit any graphics processing unit (GPU)\n",
      "for accelerated computation. As an example, we consider the Fashion-MNIST data set\n",
      "from https://www.kaggle.com/zalando-research/fashionmnist . The Fashion-\n",
      "MNIST data set contains 28 \u000228 gray-scale images of clothing. Our task is to classify\n",
      "each image according to its label. Speciﬁcally, the labels are: T-Shirt, Trouser, Pullover,\n",
      "Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. Figure 9.6 depicts a typical\n",
      "ankle boot in the left panel and a typical dress in the right panel. To start with, we import\n",
      "the required libraries and load the Fashion-MNIST data set.\n",
      "Figure 9.6: Left: an ankle boot. Right: a dress.\n",
      "ImageClassificationPytorch.py\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "import pandas as pd\n",
      "import numpy as npChapter 9. Deep Learning 347\n",
      "import matplotlib.pyplot as plt\n",
      "from torch.utils.data import Dataset , DataLoader\n",
      "from PIL import Image\n",
      "import torch.nn.functional as F\n",
      "#################################################################\n",
      "# data loader class\n",
      "#################################################################\n",
      "class LoadData(Dataset):\n",
      "def __init__(self , fName , transform=None):\n",
      "data = pd.read_csv(fName)\n",
      "self.X = np.array(data.iloc[:, 1:], dtype=np.uint8).reshape\n",
      "(-1, 1, 28, 28)\n",
      "self.y = np.array(data.iloc[:, 0])\n",
      "def __len__(self):\n",
      "return len(self.X)\n",
      "def __getitem__(self , idx):\n",
      "img = self.X[idx]\n",
      "lbl = self.y[idx]\n",
      "return (img, lbl)\n",
      "# load the image data\n",
      "train_ds = LoadData( 'fashionmnist/fashion -mnist_train.csv ')\n",
      "test_ds = LoadData( 'fashionmnist/fashion -mnist_test.csv ')\n",
      "# set labels dictionary\n",
      "labels = {0 : 'T-Shirt ', 1 : 'Trouser ', 2 : 'Pullover ',\n",
      "3 : 'Dress ', 4 : 'Coat ', 5 : 'Sandal ', 6 : 'Shirt ',\n",
      "7 : 'Sneaker ', 8 : 'Bag', 9 : 'Ankle Boot '}\n",
      "Since an image input data is generally memory intensive, it is important to partition\n",
      "the data set into (mini-)batches. The code below deﬁnes a batch size of 100 images and\n",
      "initializes the Pytorch data loader objects. These objects will be used for e \u000ecient iteration\n",
      "over the data set.\n",
      "# load the data in batches\n",
      "batch_size = 100\n",
      "train_loader = torch.utils.data.DataLoader(dataset=train_ds ,\n",
      "batch_size=batch_size ,\n",
      "shuffle=True)\n",
      "test_loader = torch.utils.data.DataLoader(dataset=test_ds ,\n",
      "batch_size=batch_size ,\n",
      "shuffle=True)\n",
      "Next, to deﬁne the network architecture in Pytorch all we need to do is deﬁne an\n",
      "instance of the torch.nn.Module class. Choosing a network architecture with good gen-\n",
      "eralization properties can be a di \u000ecult task. Here, we use a network with two convolution\n",
      "layers (deﬁned in the cnn_layer block), a 3\u00023 kernel, and three hidden layers (deﬁned in\n",
      "theflat_layer block). Since there are ten possible output labels, the output layer has ten\n",
      "nodes. More speciﬁcally, the ﬁrst and the second convolution layers have 16 and 32 output\n",
      "channels. Combining this with the deﬁnition of the 3 \u00023 kernel, we conclude that the size348 9.5. Examples in Python\n",
      "of the ﬁrst ﬂat hidden layer should be:\n",
      "0BBBBBBBB@second convolution layerz                      }|                      {\n",
      "(28\u00003+1)|        {z        }\n",
      "ﬁrst convolution layer\u00003+11CCCCCCCCA2\n",
      "\u000232=18432;\n",
      "where the multiplication by 32 follows from the fact that the second convolution layer has\n",
      "32 output channels. Having said that, the flat_fts variable determines the number of\n",
      "output layers of the convolution block. This number is used to deﬁne the size of the ﬁrst\n",
      "hidden layer of the flat_layer block. The rest of the hidden layers have 100 neurons and\n",
      "we use the ReLU activation function for all layers. Finally, note that the forward method\n",
      "in the CNNclass implements the forward pass.\n",
      "# define the network\n",
      "class CNN(nn.Module):\n",
      "def __init__(self):\n",
      "super(CNN, self).__init__()\n",
      "self.cnn_layer = nn.Sequential(\n",
      "nn.Conv2d(1, 16, kernel_size=3, stride=(1,1)),\n",
      "nn.ReLU(),\n",
      "nn.Conv2d(16, 32, kernel_size=3, stride=(1,1)),\n",
      "nn.ReLU(),\n",
      ")\n",
      "self.flat_fts = (((28-3+1) -3+1)**2)*32\n",
      "self.flat_layer = nn.Sequential(\n",
      "nn.Linear(self.flat_fts , 100),\n",
      "nn.ReLU(),\n",
      "nn.Linear(100, 100),\n",
      "nn.ReLU(),\n",
      "nn.Linear(100, 100),\n",
      "nn.ReLU(),\n",
      "nn.Linear(100, 10))\n",
      "def forward(self , x):\n",
      "out = self.cnn_layer(x)\n",
      "out = out.view(-1, self.flat_fts)\n",
      "out = self.flat_layer(out)\n",
      "return out\n",
      "Next, we specify how the network will be trained. We choose the device type, namely,\n",
      "the central processing unit (CPU) or the GPU (if available), the number of training itera-\n",
      "tions (epochs), and the learning rate. Then, we create an instance of the proposed convolu-\n",
      "tion network and send it to the predeﬁned device (CPU or GPU). Note how easily one can\n",
      "switch between the CPU or the GPU without major changes to the code.\n",
      "In addition to the speciﬁcations above, we need to choose an appropriate loss function\n",
      "and training algorithm. Here, we use the cross-entropy loss and the Adam adaptive gradi- +269\n",
      "ent Algorithm 9.4.5. Once these parameters are set, the learning proceeds to evaluate the\n",
      "gradient of the loss function via the back-propagation algorithm.Chapter 9. Deep Learning 349\n",
      "# learning parameters\n",
      "num_epochs = 50\n",
      "learning_rate = 0.001\n",
      "#device = torch.device ( 'cpu') # use this to run on CPU\n",
      "device = torch.device ( 'cuda ') # use this to run on GPU\n",
      "#instance of the Conv Net\n",
      "cnn = CNN()\n",
      "cnn.to(device=device)\n",
      "#loss function and optimizer\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
      "# the learning loop\n",
      "losses = []\n",
      "for epoch in range(1,num_epochs+1):\n",
      "for i, (images , labels) in enumerate(train_loader):\n",
      "images = Variable(images.float()).to(device=device)\n",
      "labels = Variable(labels).to(device=device)\n",
      "optimizer.zero_grad()\n",
      "outputs = cnn(images)\n",
      "loss = criterion(outputs , labels)\n",
      "loss.backward()\n",
      "optimizer.step()\n",
      "losses.append(loss.item())\n",
      "if(epoch==1 or epoch % 10 == 0):\n",
      "print (\"Epoch : \", epoch , \", Training Loss: \", loss.item())\n",
      "# evaluate on the test set\n",
      "cnn.eval()\n",
      "correct = 0\n",
      "total = 0\n",
      "for images , labels in test_loader:\n",
      "images = Variable(images.float()).to(device=device)\n",
      "outputs = cnn(images)\n",
      "_, predicted = torch.max(outputs.data , 1)\n",
      "total += labels.size(0)\n",
      "correct += (predicted.cpu() == labels).sum()\n",
      "print(\"Test Accuracy of the model on the 10,000 training test images\n",
      ": \", (100 * correct.item() / total),\"%\")\n",
      "# plot\n",
      "plt.rc( 'text ', usetex=True)\n",
      "plt.rc( 'font ', family= 'serif ',size=20)\n",
      "plt.tight_layout()\n",
      "plt.plot(np.array(losses)[10:len(losses)])\n",
      "plt.xlabel(r '{iteration} ',fontsize=20)\n",
      "plt.ylabel(r '{Batch Loss} ',fontsize=20)\n",
      "plt.subplots_adjust(top=0.8)\n",
      "plt.show()350 Exercises\n",
      "Epoch : 1 , Training Loss: 0.412550151348114\n",
      "Epoch : 10 , Training Loss: 0.05452106520533562\n",
      "Epoch : 20 , Training Loss: 0.07233225554227829\n",
      "Epoch : 30 , Training Loss: 0.01696968264877796\n",
      "Epoch : 40 , Training Loss: 0.0008199119474738836\n",
      "Epoch : 50 , Training Loss: 0.006860652007162571\n",
      "Test Accuracy of the model on the 10,000 training test images: 91.02 %\n",
      "Finally, we evaluate the network performance using the test data set. A typical mini-\n",
      "batch loss as a function of iteration is shown in Figure 9.7 and the proposed neural network\n",
      "achieves about 91% accuracy on the test set.\n",
      "0\n",
      " 200\n",
      " 400\n",
      " 600\n",
      " 800\n",
      " 1000\n",
      "iteration\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0Batch Loss\n",
      "Figure 9.7: The batch loss history.\n",
      "Further Reading\n",
      "A popular book written by some of the pioneers of deep learning is [53]. For an excellent\n",
      "and gentle introduction to the intuition behind neural networks, we recommend [94]. A\n",
      "summary of many e \u000bective gradient descent methods for training of deep networks is given\n",
      "in [105]. An early resource on the limited-memory BFGS method is [81], and a more recent\n",
      "resource includes [13], which makes recommendations on the best choice for the length of\n",
      "the BFGS history (that is, the value of the parameter h).\n",
      "Exercises\n",
      "1. Show that the softmax function\n",
      "softmax : z7!exp(z)P\n",
      "kexp(zk)\n",
      "satisﬁes the invariance property:\n",
      "softmax( z)=softmax( z+c\u00021);for any constant c:Chapter 9. Deep Learning 351\n",
      "2.Projection pursuit projection\n",
      "pursuitis a network with one hidden layer that can be written as:\n",
      "g(x)=S(!>x);\n",
      "where Sis a univariate smoothing cubic spline . If we use squared-error loss with \u001cn= +235\n",
      "fyi;xign\n",
      "i=1, we need to minimize the training loss:\n",
      "1\n",
      "nnX\n",
      "i=1\u0000yi\u0000S(!>xi)\u00012\n",
      "with respect to !and all cubic smoothing splines. This training of the network is typically\n",
      "tackled iteratively in a manner similar to the EM algorithm . In particular, we iterate ( t= +139\n",
      "1;2;:::) the following steps until convergence.\n",
      "(a) Given the missing data !t, compute the spline Stby training a cubic smoothing spline\n",
      "onfyi;!>\n",
      "txig. The smoothing coe \u000ecient of the spline may be determined as part of\n",
      "this step.\n",
      "(b) Given the spline function St, compute the next projection vector !t+1viaiterative\n",
      "reweighted least squares :iterative\n",
      "reweighted\n",
      "least squares\n",
      "!t+1=argmin\n",
      "\f(et\u0000X\f)>\u0006t(et\u0000X\f); (9.11)\n",
      "where\n",
      "et;i:=!>\n",
      "txi+yi\u0000St(!>\n",
      "txi)\n",
      "S0\n",
      "t(!>\n",
      "txi);i=1;:::; n\n",
      "is the adjusted response, and \u00061=2\n",
      "t=diag( S0\n",
      "t(!>\n",
      "tx1);:::; S0\n",
      "t(!>\n",
      "txn)) is a diagonal mat-\n",
      "rix.\n",
      "Apply Taylor’s Theorem B.1 to the function Stand derive the iterative reweighted +402\n",
      "least-squares optimization program (9.11).\n",
      "3. Suppose that in the stochastic gradient descent method we wish to repeatedly draw +337\n",
      "minibatches of size Nfrom\u001cn, where we assume that N\u0002m=nfor some large integer m.\n",
      "Instead of repeatedly resampling from \u001cn, an alternative is to reshu \u000fe\u001cnvia a random per-\n",
      "mutation \u0005and then advance sequentially through the reshu \u000fed training set to construct +115\n",
      "mnon-overlapping minibatches. A single traversal of such a reshu \u000fed training set is called\n",
      "anepoch epoch . The following pseudo-code describes the procedure.352 Exercises\n",
      "Algorithm 9.5.1: Stochastic Gradient Descent with Reshu \u000fing\n",
      "input: Training set \u001cn=f(xi;yi)gn\n",
      "i=1, initial weight matrices and bias vectors\n",
      "fWl;blgL\n",
      "l=1!\u00121, activation functions fSlgL\n",
      "l=1, learning ratesf\u000b1;\u000b2;:::g.\n",
      "output: The parameters of the trained learner.\n",
      "1t 1 and epoch 0\n",
      "2while stopping condition is not met do\n",
      "3 Draw U1;:::; Uniid\u0018U(0;1).\n",
      "4 Let\u0005be the permutation of f1;:::; ngthat satisﬁes U\u00051<\u0001\u0001\u0001<U\u0005n:\n",
      "5 (xi;yi) (x\u0005i;y\u0005i) for i=1;:::; n // reshuffle \u001cn\n",
      "6 forj=1;:::; mdo\n",
      "7 b`\u001c 1\n",
      "NPjN\n",
      "i=(j\u00001)N+1Loss( yi;g(xij\u0012))\n",
      "8\u0012t+1 \u0012t\u0000\u000bt@b`\u001c\n",
      "@\u0012(\u0012t)\n",
      "9 t t+1\n",
      "10 epoch epoch +1 // number of reshuffles or epochs\n",
      "11return\u0012tas the minimizer of the training loss\n",
      "Write Python code that implements the stochastic gradient descent with data reshu \u000fing,\n",
      "and use it to train the neural net in Section 9.5.1. +342\n",
      "4. Denote the pdf of the N(0;\u0006) distribution by '\u0006(\u0001), and let\n",
      "D(\u00160;\u00060j\u00161;\u00061)=Z\n",
      "Rd'\u00060(x\u0000\u00160) ln'\u00060(x\u0000\u00160)\n",
      "'\u00061(x\u0000\u00161)dx\n",
      "be the Kullback–Leibler divergence between the densities of the N(\u00160;\u00060) andN(\u00161;\u00061) +42\n",
      "distributions on Rd. Show that\n",
      "2D(\u00160;\u00060j\u00161;\u00061)=tr(\u0006\u00001\n",
      "1\u00060)\u0000lnj\u0006\u00001\n",
      "1\u00060j+(\u00161\u0000\u00160)>\u0006\u00001\n",
      "1(\u00161\u0000\u00160)\u0000d:\n",
      "Hence, deduce the formula in (B.22).\n",
      "5. Suppose that we wish to compute the inverse and log-determinant of the matrix\n",
      "In+UU>;\n",
      "where Uis an n\u0002hmatrix with h\u001cn. Show that\n",
      "(In+UU>)\u00001=In\u0000QnQ>\n",
      "n;\n",
      "where Qncontains the ﬁrst nrows of the ( n+h)\u0002hmatrix Qin the QR factorization of +377\n",
      "the (n+h)\u0002hmatrix:\"U\n",
      "Ih#\n",
      "=QR:\n",
      "In addition, show that ln jIn+UU>j=Ph\n",
      "i=1lnr2\n",
      "ii, wherefriigare the diagonal elements of\n",
      "theh\u0002hmatrix R.Chapter 9. Deep Learning 353\n",
      "6. Suppose that\n",
      "U=[u0;u1;:::; uh\u00001];\n",
      "where all u2Rnare column vectors and we have computed ( In+UU>)\u00001via the QR\n",
      "factorization method in Exercise 5. If the columns of matrix Uare updated to\n",
      "[u1;:::; uh\u00001;uh];\n",
      "show that the inverse ( In+UU>)\u00001can be updated in O(h n) time (rather than computed\n",
      "from scratch inO(h2n) time). Deduce that the computing cost of updating the Hessian\n",
      "approximation (9.10) is the same as that for the limited-memory BFGS Algorithm 9.4.3.\n",
      "In your solution you may use the following facts from [29]. Suppose we are given the\n",
      "QandRfactors in the QR factorization of a matrix A2Rn\u0002h. If a row /column is added to\n",
      "matrix A, then the QandRfactors need not be recomputed from scratch (in O(h2n) time),\n",
      "but can be updated e \u000eciently inO(h n) time. Similarly, if a row /column is removed from\n",
      "matrix A, then the QandRfactors can be updated in O(h2) time.\n",
      "7. Suppose that U2Rn\u0002hhas its k-th column vreplaced with w, giving the updated eU.\n",
      "(a) If e2Rhdenotes the unit-length vector such that ek=kek=1 and\n",
      "r\u0006:=p\n",
      "2\n",
      "2U>(w\u0000v)+p\n",
      "2kw\u0000vk2\n",
      "4e\u0006p\n",
      "2\n",
      "2e;\n",
      "show that\n",
      "eU>eU=U>U+r+r>\n",
      "+\u0000r\u0000r>\n",
      "\u0000:\n",
      "[Hint: You may ﬁnd Exercise 16 in Chapter 6 useful.] +248\n",
      "(b) Let B:=(Ih+U>U)\u00001. Use the Woodbury identity (A.15) to show that +373\n",
      "(In+eUeU>)\u00001=In\u0000eU\u0010\n",
      "B\u00001+r+r>\n",
      "+\u0000r\u0000r>\n",
      "\u0000\u0011\u00001eU>:\n",
      "(c) Suppose that we have stored Bin computer memory. Use Algorithm 6.8.1 and parts\n",
      "(a) and (b) to write pseudo-code that updates ( In+UU>)\u00001to (In+eUeU>)\u00001inO((n+h)h)\n",
      "computing time.\n",
      "8. Equation (9.7) gives the rank-two BFGS update of the inverse Hessian Ct\u00001toCt. In-\n",
      "stead of using a two-rank update, we can consider a one-rank update, in which Ct\u00001is\n",
      "updated to Ctby the general rank-one formula:\n",
      "Ct=Ct\u00001+\u001dtrtr>\n",
      "t:\n",
      "Find values for the scalar \u001dtand vector rt, such that Ctsatisﬁes the secant condition Ct1t=\n",
      "\u000et.\n",
      "9. Show that the BFGS formula (B.23) can be written as:\n",
      "C \u0010\n",
      "I\u0000\u001d1\u000e>\u0011>C\u0010\n",
      "I\u0000\u001d1\u000e>\u0011\n",
      "+\u001d\u000e\u000e>;\n",
      "where\u001d:=(1>\u000e)\u00001.354 Exercises\n",
      "10. Show that the BFGS formula (B.23) is the solution to the constrained optimization\n",
      "problem:\n",
      "CBFGS= argmin\n",
      "Asubject to A1=\u000e;A=A>D(0;Cj0;A);\n",
      "whereDis the Kullback–Leibler discrepancy deﬁned in (B.22). On the other hand, show\n",
      "that the DFP formula (B.24) is the solution to the constrained optimization problem:\n",
      "CDFP= argmin\n",
      "Asubject to A1=\u000e;A=A>D(0;Aj0;C):\n",
      "11. Consider again the logistic regression model in Exercise 5.18, which used iterative +213\n",
      "reweighted least squares for training the learner. Repeat all the computations, but this\n",
      "time using the limited-memory BFGS Algorithm 9.4.4. Which training algorithm converges\n",
      "faster to the optimal solution?\n",
      "12. Download the seeds_dataset.txt data set from the book’s GitHub site, which con-\n",
      "tains 210 independent examples. The categorical output (response) here is the type of wheat\n",
      "grain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c=3. The seven con-\n",
      "tinuous features (explanatory variables) are measurements of the geometrical properties of\n",
      "the grain (area, perimeter, compactness, length, width, asymmetry coe \u000ecient, and length\n",
      "of kernel groove). Thus, x2R7(which does not include the constant feature 1) and the\n",
      "multi-logit pre-classiﬁer in Example 9.2 can be written as g(x)=softmax( Wx+b), where +330\n",
      "W2R3\u00027andb2R3. Implement and train this pre-classiﬁer on the ﬁrst n=105 examples\n",
      "of the seeds data set using, for example, Algorithm 9.4.1. Use the remaining n0=105\n",
      "examples in the data set to estimate the generalization risk of the learner using the cross-\n",
      "entropy loss. [Hint: Use the cross-entropy loss formulas from Example 9.4.]\n",
      "13. In Exercise 12 above, we train the multi-logit classiﬁer using a weight matrix W2R3\u00027\n",
      "and bias vector b2R3. Repeat the training of the multi-logit model, but this time keeping z1\n",
      "as an arbitrary constant (say z1=0), and thus setting c=0 to be a “reference” class. This\n",
      "has the e \u000bect of removing a node from the output layer of the network, giving a weight\n",
      "matrix W2R2\u00027and bias vector b2R2of smaller dimensions than in (7.16). +269\n",
      "14. Consider again Example 9.4, where we used a softmax output function SLin con- +335\n",
      "junction with the cross-entropy loss: C(\u0012)=\u0000lngy+1(xj\u0012):Find formulas for@C\n",
      "@gand@SL\n",
      "@zL.\n",
      "Hence, verify that:\n",
      "@SL\n",
      "@zL@C\n",
      "@g=g(xj\u0012)\u0000ey+1;\n",
      "where eiis the unit length vector with an entry of 1 in the i-th position.\n",
      "15. Derive the formula (B.25) for a diagonal Hessian update in a quasi-Newton method +414\n",
      "for minimization. In other words, given a current minimizer xtoff(x), a diagonal matrix\n",
      "Cof approximating the Hessian of f, and a gradient vector u=rf(xt), ﬁnd the solution\n",
      "to the constrained optimization program:\n",
      "min\n",
      "AD(xt;Cjxt\u0000Au;A)\n",
      "subject to: A1>\u000e;Ais diagonal;\n",
      "whereDis the Kullback–Leibler distance deﬁned in (B.22) (see Exercise 4).Chapter 9. Deep Learning 355\n",
      "16. Consider again the Python implementation of the polynomial regression in Sec-\n",
      "tion 9.5.1, where the stochastic gradient descent was used for training.\n",
      "Using the polynomial regression data set, implement and run the following four altern-\n",
      "ative training methods:\n",
      "(a) the steepest-descent Algorithm 9.4.1;\n",
      "(b) the Levenberg–Marquardt Algorithm B.3.3, in conjunction with Algorithm 9.4.2 for +417\n",
      "computing the matrix of Jacobi;\n",
      "(c) the limited-memory BFGS Algorithm 9.4.4;\n",
      "(d) the Adam Algorithm 9.4.5, which uses past gradient values to determine the next\n",
      "search direction.\n",
      "For each training algorithm, using trial and error, tune any algorithmic parameters so that\n",
      "the network training is as fast as possible. Comment on the relative advantages and disad-\n",
      "vantages of each training /optimization method. For example, comment on which optimiz-\n",
      "ation method makes rapid initial progress, but gets trapped in a suboptimal solution, and\n",
      "which method is slower, but more consistent in ﬁnding good optima.\n",
      "17. Consider again the Pytorch code in Section 9.5.2. Repeat all the computations, but\n",
      "this time using the momentum method for training of the network. Comment on which\n",
      "method is preferable: the momentum or the Adam method?356APPENDIXA\n",
      "LINEAR ALGEBRA AND FUNCTIONAL\n",
      "ANALYSIS\n",
      "The purpose of this appendix is to review some important topics in linear algebra\n",
      "and functional analysis. We assume that the reader has some familiarity with matrix\n",
      "and vector operations, including matrix multiplication and the computation of determ-\n",
      "inants.\n",
      "A.1 Vector Spaces, Bases, and Matrices\n",
      "Linear algebra is the study of vector spaces and linear mappings. Vectors are, by deﬁni-\n",
      "tion, elements of some vector space vector space Vand satisfy the usual rules of addition and scalar\n",
      "multiplication, e.g.,\n",
      "ifx2V andy2V, then\u000bx+\fy2V for all\u000b;\f2R(orC).\n",
      "We will be dealing mostly with vectors in the Euclidean vector space Rnfor some n. That\n",
      "is, we view the points of Rnas objects that can be added up and multiplied with a scalar,\n",
      "e.g., ( x1;x2)+(y1;y2)=(x1+y1;x2+y2) for points in R2. Sometimes it is convenient to\n",
      "work with the complex vector space Cninstead ofRn; see also Section A.3.\n",
      "Vectors v1;:::; vkare called linearly independent linearly\n",
      "independentif none of them can be expressed as\n",
      "a linear combination of the others; that is, if \u000b1v1+\u0001\u0001\u0001+\u000bnvn=0, then it must hold that\n",
      "\u000bi=0 for all i=1;:::; n.\n",
      "Deﬁnition A.1: Basis of a Vector Space\n",
      "A set of vectorsB=fv1;:::; vngis called a basis basis of the vector space Vif every\n",
      "vector x2V can be written as a unique linear combination of the vectors in B:\n",
      "x=\u000b1v1+\u0001\u0001\u0001+\u000bnvn:\n",
      "The (possibly inﬁnite) number nis called the dimension dimension ofV.\n",
      "357358 A.1. Vector Spaces, Bases, and Matrices\n",
      "Using a basisBofV, we can thus represent each vector x2V as a row or column of\n",
      "numbers\n",
      "[\u000b1;:::;\u000b n] or26666666664\u000b1\n",
      ":::\n",
      "\u000bn37777777775: (A.1)\n",
      "Typically, vectors in Rnare represented via the standard basis standard basis , consisting of unit\n",
      "vectors (points) e1=(1;0;:::; 0);:::; en=(0;0;:::; 0;1). As a consequence, any point\n",
      "(x1;:::; xn)2Rncan be represented, using the standard basis, as a row or column vec-\n",
      "tor of the form (A.1) above, with \u000bi=xi;i=1;:::; n. We will also write [ x1;x2;:::; xn]>,\n",
      "for the corresponding column vector, where>denotes the transpose transpose .\n",
      "To avoid confusion, we will use the convention from now on that a generic vector x\n",
      "is always represented via the standard basis as a column vector. The corresponding\n",
      "row vector is denoted by x>.\n",
      "Amatrix can be viewed as an array of mrows and ncolumns that deﬁnes a linearmatrixtransformation fromRntoRm(or for complex matrices, from CntoCm). The matrix is saidlinear\n",
      "transformation to be square ifm=n. Ifa1;a2;:::; anare the columns of A, that is, A=[a1;a2;:::; an],\n",
      "and if x=[x1;:::; xn]>, then Ax=x1a1+\u0001\u0001\u0001+xnan:In particular, the standard basis\n",
      "vector ekis mapped to the vector ak,k=1;:::; n. We sometimes use the notation A=[ai j],\n",
      "to denote a matrix whose ( i;j)-th element is ai j. When we wish to emphasize that a matrix\n",
      "Ais real-valued with mrows and ncolumns, we write A2Rm\u0002n. The rank rank of a matrix is the\n",
      "number of linearly independent rows or, equivalently, the number of linearly independent\n",
      "columns.\n",
      "Example A.1 (Linear Transformation) Take the matrix\n",
      "A=\"1 1\n",
      "\u00000:5\u00002#\n",
      ":\n",
      "It transforms the two basis vectors [1 ;0]>and [0;1]>, shown in red and blue in the left panel\n",
      "of Figure A.1, to the vectors [1 ;\u00000:5]>and [1;\u00002]>, shown on the right panel. Similarly,\n",
      "the points on the unit circle are transformed to an ellipse.\n",
      "-1 -0.5 0 0.5 1\n",
      "x-1-0.500.51y\n",
      "-2 0 2\n",
      "x-3-2-10123y\n",
      "Figure A.1: A linear transformation of the unit circle.Appendix A. Linear Algebra and Functional Analysis 359\n",
      "Suppose A=[a1;:::; an], where theA=faigform a basis of Rn. Take any vector x=\n",
      "[x1;:::; xn]>\n",
      "Ewith respect to the standard basis E(we write subscript Eto stress this). Then\n",
      "the representation of this vector with respect to Ais simply\n",
      "y=A\u00001x;\n",
      "where A\u00001is the inverse inverse ofA; that is, the matrix such that AA\u00001=A\u00001A=In, where\n",
      "Inis the n-dimensional identity matrix. To see this, note that A\u00001aigives the i-th unit\n",
      "vector representation, for i=1;:::; n, and recall that each vector in Rnis a unique linear\n",
      "combination of these basis vectors.\n",
      "Example A.2 (Basis Representation) Consider the matrix\n",
      "A=\"1 2\n",
      "3 4#\n",
      "with inverse A\u00001=\"\u00002 1\n",
      "3=2\u00001=2#\n",
      ": (A.2)\n",
      "The vector x=[1;1]>\n",
      "Ein the standard basis has representation y=A\u00001x=[\u00001;1]>\n",
      "Ain the\n",
      "basis consisting of the columns of A. Namely,\n",
      "Ay=\u0000\"1\n",
      "3#\n",
      "+\"2\n",
      "4#\n",
      "=\"1\n",
      "1#\n",
      ":\n",
      "Thetranspose transpose of a matrix A=[ai j] is the matrix A>=[aji]; that is, the ( i;j)-th element\n",
      "ofA>is the ( j;i)-th element of A. The trace trace of a square matrix is the sum of its diagonal\n",
      "elements. A useful result is the following cyclic property.\n",
      "Theorem A.1: Cyclic Property\n",
      "The trace is invariant under cyclic permutations: tr( ABC )=tr(BCA )=tr(CAB ).\n",
      "Proof: It su\u000eces to show that tr( DE) is equal to tr( ED) for any m\u0002nmatrix D=[di j]\n",
      "andn\u0002mmatrix E=[ei j]. The diagonal elements of DEarePn\n",
      "j=1di jeji;i=1;:::; mand\n",
      "the diagonal elements of EDarePm\n",
      "i=1ejidi j;j=1;:::; n. They sum up to the same numberPm\n",
      "i=1Pn\n",
      "j=1di jeji. \u0003\n",
      "A square matrix has an inverse if and only if its columns (or rows) are linearly in-\n",
      "dependent. This is the same as the matrix being of full rank ; that is, its rank is equal to\n",
      "the number of columns. An equivalent statement is that its determinant is not zero. The\n",
      "determinant determinant of an n\u0002nmatrix A=[ai;j] is deﬁned as\n",
      "det(A) :=X\n",
      "\u0019(\u00001)\u0010(\u0019)nY\n",
      "i=1a\u0019i;i; (A.3)\n",
      "where the sum is over all permutations \u0019=(\u00191;:::;\u0019 n) of (1;:::; n), and\u0010(\u0019) is the num-\n",
      "ber of pairs ( i;j) for which i<jand\u0019i> \u0019 j. For example, \u0010(2;3;4;1)=3 for the pairs\n",
      "(1;4);(2;4);(3;4). The determinant of a diagonal matrix diagonal matrix — a matrix with only zero ele-\n",
      "ments o \u000bthe diagonal — is simply the product of its diagonal elements.360 A.1. Vector Spaces, Bases, and Matrices\n",
      "Geometrically, the determinant of a square matrix A=[a1;:::; an] is the (signed)\n",
      "volume of the parallelepiped ( n-dimensional parallelogram) deﬁned by the columns\n",
      "a1;:::; an; that is, the set of points x=Pn\n",
      "i=1\u000biai, where 06\u000bi61;i=1;:::; n.\n",
      "The easiest way to compute a determinant of a general matrix is to apply simple op-\n",
      "erations to the matrix that potentially reduce its complexity (as in the number of non-zero\n",
      "elements, for example), while retaining its determinant:\n",
      "Adding a multiple of one column (or row) to another, does not change the determin-\n",
      "ant.\n",
      "Multiplying a column (or row) with a number multiplies the determinant by the same\n",
      "number.\n",
      "Swapping two rows changes the sign of the determinant.\n",
      "By applying these rules repeatedly one can reduce any matrix to a diagonal matrix.\n",
      "It follows then that the determinant of the original matrix is equal to the product of the\n",
      "diagonal elements of the resulting diagonal matrix multiplied by a known constant.\n",
      "Example A.3 (Determinant and Volume) Figure A.2 illustrates how the determinant\n",
      "of a matrix can be viewed as a signed volume, which can be computed by repeatedly apply-\n",
      "ing the ﬁrst rule above. Here, we wish to compute the area of red parallelogram determined\n",
      "by the matrix Agiven in (A.2). In particular, the corner points of the parallelogram corres-\n",
      "pond to the vectors [0 ;0]>;[1;3]>;[2;4]>, and [3;7]>.\n",
      "0 0.5 1 1.5 2 2.5 3-202468\n",
      "Figure A.2: The volume of the red parallelogram can be obtained by a number of shear\n",
      "operations that do not change the volume.\n",
      "Adding\u00002 times the ﬁrst column of Ato the second column gives the matrix\n",
      "B=\"1 0\n",
      "3\u00002#\n",
      ";Appendix A. Linear Algebra and Functional Analysis 361\n",
      "corresponding to the blue parallelogram. The linear operation that transforms the red to the\n",
      "blue parallelogram can be thought of as a succession of two linear transformations. The\n",
      "ﬁrst is to transform the coordinates of points on the red parallelogram (in standard basis)\n",
      "to the basis formed by the columns of A. Second, relative to this new basis, we apply the\n",
      "matrix Babove. Note that the input of this matrix is with respect to the new basis, whereas\n",
      "the output is with respect to the standard basis. The matrix for the combined operation is\n",
      "now\n",
      "BA\u00001=\"1 0\n",
      "3\u00002# \"\u00002 1\n",
      "3=2\u00001=2#\n",
      "=\"\u00002 1\n",
      "\u00009 4#\n",
      ";\n",
      "which maps [1 ;3]>to [1;3]>(does not change) and [2 ;4]>to [0;\u00002]>. We say that we\n",
      "apply a shear shear in the direction [1 ;3]>. The signiﬁcance of such an operation is that a shear\n",
      "does not alter the volume of the parallelogram . The second (blue) parallelogram has an\n",
      "easier form, because one of the sides is parallel to the y-axis. By applying another shear,\n",
      "in the direction [0 ;\u00002]>, we can obtain a simple (green) rectangle, whose volume is 2. In\n",
      "matrix terms, we add 3 /2 times the second column of Bto the ﬁrst column of B, to obtain\n",
      "the matrix\n",
      "C=\"1 0\n",
      "0\u00002#\n",
      ";\n",
      "which is a diagonal matrix, whose determinant is \u00002, corresponding to the volume 2 of all\n",
      "the parallelograms.\n",
      "Theorem A.2 summarizes a number of useful matrix rules for the concepts that we have\n",
      "discussed so far. We leave the proofs, which typically involves “writing out” the equations,\n",
      "as an exercise for the reader; see also [116].\n",
      "Theorem A.2: Useful Matrix Rules\n",
      "1. (AB)>=B>A>\n",
      "2. (AB)\u00001=B\u00001A\u00001\n",
      "3. (A\u00001)>=(A>)\u00001=:A\u0000>\n",
      "4. det( AB)=det(A) det( B)\n",
      "5.x>Ax=tr\u0000Axx>\u0001\n",
      "6. det( A)=Q\n",
      "iaiiifA=[ai j] is triangular\n",
      "Next, consider an n\u0002pmatrix Afor which the matrix inverse fails to exist. That is, A\n",
      "is either non-square ( n,p) or its determinant is 0. Instead of the inverse, we can use its\n",
      "so-called pseudo-inverse, which always exists.362 A.2. Inner Product\n",
      "Deﬁnition A.2: Moore–Penrose Pseudo-Inverse\n",
      "The Moore–Penrose pseudo-inverse Moore –Penrose\n",
      "pseudo -inverseof a real matrix A2Rn\u0002pis deﬁned as the\n",
      "unique matrix A+2Rp\u0002nthat satisﬁes the conditions:\n",
      "1.AA+A=A\n",
      "2.A+AA+=A+\n",
      "3. (AA+)>=AA+\n",
      "4. (A+A)>=A+A\n",
      "We can write A+explicitly in terms of Awhen Ahas a full column or row rank. For\n",
      "example, we always have\n",
      "A>AA+=A>(AA+)>=((AA+)A)>=(A)>=A>: (A.4)\n",
      "IfAhas a full column rank p, then ( A>A)\u00001exists, so that from (A.4) it follows that\n",
      "A+=(A>A)\u00001A>. This is referred to as the left pseudo-inverse left\n",
      "pseudo -inverse, asA+A=Ip. Similarly, if\n",
      "Ahas a full row rank n, that is, ( AA>)\u00001exists, then it follows from\n",
      "A+AA>=(A+A)>A>=(A(A+A))>=A>\n",
      "thatA+=A>(AA>)\u00001. This is the right pseudo-inverse right\n",
      "pseudo -inverse, asAA+=In. Finally, if Ais of full\n",
      "rank and square, then A+=A\u00001.\n",
      "A.2 Inner Product\n",
      "The (Euclidean) inner product inner product of two real vectors x=[x1;:::; xn]>andy=[y1;:::; yn]>\n",
      "is deﬁned as the number\n",
      "hx;yi=nX\n",
      "i=1xiyi=x>y:\n",
      "Here x>yis the matrix multiplication of the (1 \u0002n) matrix x>and the ( n\u00021) matrix y.\n",
      "The inner product induces a geometry on the linear space Rn, allowing for the deﬁnition of\n",
      "length, angle, and so on. The inner product satisﬁes the following properties:\n",
      "1.h\u000bx+\fy;zi=\u000bhx;zi+\fhy;zi;\n",
      "2.hx;yi=hy;xi;\n",
      "3.hx;xi>0;\n",
      "4.hx;xi=0 if and only if x=0.\n",
      "Vectors xandyare called perpendicular (ororthogonal orthogonal ) ifhx;yi=0. The Euclidean\n",
      "norm Euclidean norm (or length) of a vector xis deﬁned as\n",
      "jjxjj=q\n",
      "x2\n",
      "1+\u0001\u0001\u0001+x2\n",
      "n=p\n",
      "hx;xi:Appendix A. Linear Algebra and Functional Analysis 363\n",
      "Ifxandyare perpendicular, then Pythagoras’ theorem Pythagoras ’\n",
      "theoremholds:\n",
      "jjx+yjj2=hx+y;x+yi=hx;xi+2hx;yi+hy;yi=jjxjj2+jjyjj2: (A.5)\n",
      "A basisfv1;:::; vngofRnin which all the vectors are pairwise perpendicular and have\n",
      "norm 1 is called an orthonormal orthonormal (short for orthogonal and normalized) basis. For example,\n",
      "the standard basis is orthonormal.\n",
      "Theorem A.3: Orthonormal Basis Representation\n",
      "Iffv1;:::; vngis an orthonormal basis of Rn, then any vector x2Rncan be expressed\n",
      "as\n",
      "x=hx;v1iv1+\u0001\u0001\u0001+hx;vnivn: (A.6)\n",
      "Proof: Observe that, because the fvigform a basis, there exist unique \u000b1;:::;\u000b nsuch that\n",
      "x=\u000b1v1+\u0001\u0001\u0001+\u000bnvn. By the linearity of the inner product and the orthonormality of the\n",
      "fvigit follows thathx;vji=hP\n",
      "i\u000bivi;vji=\u000bj. \u0003\n",
      "Ann\u0002nmatrix Vwhose columns form an orthonormal basis is called an orthogonal\n",
      "matrix orthogonal\n",
      "matrix.1Note that for an orthogonal matrix V=[v1;:::; vn], we have\n",
      "V>V=26666666666666664v>\n",
      "1\n",
      "v>\n",
      "2:::\n",
      "v>\n",
      "n37777777777777775[v1;v2;:::; vn]=26666666664v>\n",
      "1v1v>\n",
      "1v2:::v>\n",
      "1vn\n",
      "::::::::::::\n",
      "v>\n",
      "nv1v>\n",
      "nv2:::v>\n",
      "nvn37777777775=In:\n",
      "Hence, V\u00001=V>. Note also that an orthogonal transformation is length preserving length\n",
      "preserving; that\n",
      "is,Vxhas the same length as x. This follows from\n",
      "jjVxjj2=hVx;Vxi=x>V>Vx=x>x=jjxjj2:\n",
      "A.3 Complex Vectors and Matrices\n",
      "Instead of the vector space Rnofn-dimensional real vectors, it is sometimes useful to\n",
      "consider the vector space Cnofn-dimensional complex vectors. In this case the adjoint adjoint\n",
      "orconjugate transpose operation (\u0003) replaces the transpose operation ( >). This involves\n",
      "the usual transposition of the matrix or vector with the additional step that any complex\n",
      "number z=x+iyis replaced by its complex conjugate z=x\u0000iy. For example, if\n",
      "x=\"a1+ib1\n",
      "a2+ib2#\n",
      "and A=\"a11+ib11a12+ib12\n",
      "a21+ib21a22+ib22#\n",
      ";\n",
      "then\n",
      "x\u0003=[a1\u0000ib1;a2\u0000ib2] and A\u0003=\"a11\u0000ib11a21\u0000ib21\n",
      "a12\u0000ib12a22\u0000ib22#\n",
      ":\n",
      "1The qualiﬁer “orthogonal” for such matrices has been ﬁxed by history. A better term would have been\n",
      "“orthonormal”.364 A.4. Orthogonal Projections\n",
      "The (Euclidean) inner product of xandy(viewed as column vectors) is now deﬁned as\n",
      "hx;yi=y\u0003x=nX\n",
      "i=1xiyi;\n",
      "which is no longer symmetric: hx;yi=hy;xi. Note that this generalizes the real-valued\n",
      "inner product. The determinant of a complex matrix Ais deﬁned exactly as in (A.3). As a\n",
      "consequence, det( A\u0003)=det(A).\n",
      "A complex matrix is said to be Hermitian orself-adjoint ifA\u0003=A, and unitary ifHermitian\n",
      "unitaryA\u0003A=I(that is, if A\u0003=A\u00001). For real matrices “Hermitian” is the same as “symmetric”,\n",
      "and “unitary” is the same as “orthogonal”.\n",
      "A.4 Orthogonal Projections\n",
      "Letfu1;:::; ukgbe a set of linearly independent vectors in Rn. The set\n",
      "V=Spanfu1;:::; ukg=f\u000b1u1+\u0001\u0001\u0001+\u000bkuk; \u000b1;:::;\u000b k2Rg;\n",
      "is called the linear subspace spanned by fu1;:::; ukg. The orthogonal complement ofV,linear subspace\n",
      "orthogonal\n",
      "complementdenoted byV?, is the set of all vectors wthat are orthogonal to V, in the sense that\n",
      "hw;vi=0 for all v2V. The matrix Psuch that Px=x, for all x2V, and Px=0, for all\n",
      "x2V?is called the orthogonal projection matrix orthogonal\n",
      "projection\n",
      "matrixontoV. Suppose that U=[u1;:::; uk]\n",
      "has full rank, in which case U>Uis an invertible matrix. The orthogonal projection matrix\n",
      "PontoV=Spanfu1;:::; ukgis then given by\n",
      "P=U(U>U)\u00001U>:\n",
      "Namely, since PU=U, the matrix Pprojects any vector in Vonto itself. Moreover, P\n",
      "projects any vector in V?onto the zero vector. Using the pseudo-inverse, it is possible to\n",
      "specify the projection matrix also for the case where Uis not of full rank, leading to the\n",
      "following theorem.\n",
      "Theorem A.4: Orthogonal Projection\n",
      "LetU=[u1;:::; uk]. Then, the orthogonal projection matrix PontoV=Spanfu1;\n",
      ":::;ukgis given by\n",
      "P=U U+; (A.7)\n",
      "where U+is the (right) pseudo-inverse of U.\n",
      "Proof: By Property 1 of Deﬁnition A.2 we have PU=UU+U=U, so that Pprojects any\n",
      "vector inVonto itself. Moreover, Pprojects any vector in V?onto the zero vector. \u0003\n",
      "Note that in the special case where u1;:::; ukabove form an orthonormal basis of V,\n",
      "then the projection onto Vis very simple to describe, namely we have\n",
      "Px=UU>x=kX\n",
      "i=1hx;uiiui: (A.8)Appendix A. Linear Algebra and Functional Analysis 365\n",
      "For any point x2Rn, the point inVthat is closest to xis its orthogonal projection Px,\n",
      "as the following theorem shows.\n",
      "Theorem A.5: Orthogonal Projection and Minimal Distance\n",
      "Letfu1;:::; ukgbe an orthonormal basis of subspace Vand let Pbe the orthogonal\n",
      "projection matrix onto V. The solution to the minimization program\n",
      "min\n",
      "y2Vkx\u0000yk2\n",
      "isy=Px. That is, Px2V is closest to x.\n",
      "Proof: We can write each point y2V asy=Pk\n",
      "i=1\u000biui. Consequently,\n",
      "kx\u0000yk2=\u001c\n",
      "x\u0000kX\n",
      "i=1\u000biui;x\u0000kX\n",
      "i=1\u000biui\u001d\n",
      "=kxk2\u00002kX\n",
      "i=1\u000bihx;uii+kX\n",
      "i=1\u000b2\n",
      "i:\n",
      "Minimizing this with respect to the f\u000biggives\u000bi=hx;uii;i=1;:::; k. In view of (A.8),\n",
      "the optimal yis thus Px. \u0003\n",
      "A.5 Eigenvalues and Eigenvectors\n",
      "LetAbe an n\u0002nmatrix. If Av=\u0015vfor some number \u0015and non-zero vector v, then\u0015is\n",
      "called an eigenvalue ofAwith eigenvector v.eigenvalue\n",
      "eigenvector If (\u0015;v) is an (eigenvalue, eigenvector) pair, the matrix \u0015I\u0000Amaps any multiple of v\n",
      "to the zero vector. Consequently, the columns of \u0015I\u0000Aare linearly dependent , and hence\n",
      "its determinant is 0. This provides a way to identify the eigenvalues, namely as the r6n\n",
      "di\u000berent roots\u00151;:::;\u0015 rof the characteristic polynomial characteristic\n",
      "polynomial\n",
      "det(\u0015I\u0000A)=(\u0015\u0000\u00151)\u000b1\u0001\u0001\u0001(\u0015\u0000\u0015r)\u000br;\n",
      "where\u000b1+\u0001\u0001\u0001+\u000br=n. The integer \u000biis called the algebraic multiplicity of\u0015i. Thealgebraic\n",
      "multiplicity eigenvectors that correspond to an eigenvalue \u0015ilie in the kernel ornull space of the matrix\n",
      "null space \u0015iI\u0000A; that is, the linear space of vectors vsuch that (\u0015iI\u0000A)v=0. This space is called\n",
      "theeigenspace of\u0015i. Its dimension, di2f1;:::; ng, is called the geometric multiplicity ofgeometric\n",
      "multiplicity \u0015i. It always holds that di6\u000bi. IfP\n",
      "idi=n, then we can construct a basis for Rnconsisting\n",
      "of eigenvectors, as illustrated next.\n",
      "Example A.4 (Linear Transformation (cont.)) We revisit the linear transformation in\n",
      "Figure A.1, where\n",
      "A=\"1 1\n",
      "\u00001=2\u00002#\n",
      ":\n",
      "The characteristic polynomial is ( \u0015\u00001)(\u0015+2)+1=2, with roots \u00151=\u00001=2\u0000p\n",
      "7=2\u0019\n",
      "\u00001:8229 and\u00152=\u00001=2+p\n",
      "7=2\u00190:8229. The corresponding unit eigenvectors are v1\u0019\n",
      "[0:3339;\u00000:9426]>andv2\u0019[0:9847;\u00000:1744]>. The eigenspace corresponding to \u00151is366 A.5. Eigenvalues and Eigenvectors\n",
      "V1=Spanfv1g=f\fv1:\f2Rgand the eigenspace corresponding to \u00152isV2=Spanfv2g.\n",
      "The algebraic and geometric multiplicities are 1 in this case. Any pair of vectors taken\n",
      "fromV1andV2forms a basis for R2. Figure A.3 shows how v1andv2are transformed to\n",
      "Av12V 1andAv22V 2, respectively.\n",
      "-2 0 2\n",
      "x-3-2-10123y\n",
      "Figure A.3: The dashed arrows are the unit eigenvectors v1(blue) and v2(red) of matrix A.\n",
      "Their transformed values Av1andAv2are indicated by solid arrows.\n",
      "A matrix for which the algebraic and geometric multiplicities of all its eigenvalues\n",
      "are the same is called semi-simple . This is equivalent to the matrix being diagonalizable ,semi-simple\n",
      "diagonalizable meaning that there is a matrix Vand a diagonal matrix Dsuch that\n",
      "A=VDV\u00001:\n",
      "To see that this so-called eigen-decomposition eigen -\n",
      "decompositionholds, suppose Ais a semi-simple matrix\n",
      "with eigenvalues\n",
      "\u00151;:::;\u0015 1|     {z     }\n",
      "d1;\u0001\u0001\u0001;\u0015r;:::;\u0015 r|     {z     }\n",
      "dr:\n",
      "LetDbe the diagonal matrix whose diagonal elements are the eigenvalues of A, and let V\n",
      "be a matrix whose columns are linearly independent eigenvectors corresponding to these\n",
      "eigenvalues. Then, for each (eigenvalue, eigenvector) pair ( \u0015;v), we have Av=\u0015v. Hence,\n",
      "in matrix notation, we have A V=VD, and so A=VDV\u00001.\n",
      "A.5.1 Left- and Right-Eigenvectors\n",
      "The eigenvector as deﬁned in the previous section is called a right -eigenvector, as it lies on\n",
      "the right of Ain the equation Av=\u0015v.\n",
      "IfAis a complex matrix with an eigenvalue \u0015, then the eigenvalue’s complex conjugate\n",
      "\u0015is an eigenvalue of A\u0003. To see this, deﬁne B:=\u0015I\u0000AandB\u0003:=\u0015I\u0000A\u0003. Since\u0015is\n",
      "an eigenvalue, we have det( B)=0. Applying the identity det( B)=det(B\u0003), we see thatAppendix A. Linear Algebra and Functional Analysis 367\n",
      "therefore det( B\u0003)=0, and hence that \u0015is an eigenvalue of A\u0003. Let wbe an eigenvector\n",
      "corresponding to \u0015. Then, A\u0003w=\u0015wor, equivalently,\n",
      "w\u0003A=\u0015w\u0003:\n",
      "For this reason, we call w\u0003theleft-eigenvector left-\n",
      "eigenvectorofAfor eigenvalue \u0015. Ifvis a (right-) ei-\n",
      "genvector of A, then its adjoint v\u0003is usually nota left-eigenvector, unless A\u0003A=AA\u0003\n",
      "(such matrices are called normal ; normal matrix a real symmetric matrix is normal). However, the im-\n",
      "portant property holds that left- and right-eigenvectors belonging to di\u000berent eigenvalues\n",
      "areorthogonal . Namely, if w\u0003is a left-eigenvalue of \u00151andva right-eigenvalue of \u00152,\u00151,\n",
      "then\n",
      "\u00151w\u0003v=w\u0003Av=\u00152w\u0003v;\n",
      "which can only be true if w\u0003v=0.\n",
      "Theorem A.6: Schur Triangulation\n",
      "For any complex matrix A, there exists a unitary matrix Usuch that T=U\u00001AUis\n",
      "upper triangular.\n",
      "Proof: The proof is by induction on the dimension nof the matrix. Clearly, the statement\n",
      "is true for n=1, as Ais simply a complex number and we can take Uequal to 1. Suppose\n",
      "that the result is true for dimension n. We wish to show that it also holds for dimension\n",
      "n+1. Any matrix Aalways has at least one eigenvalue \u0015with eigenvector v, normalized\n",
      "to have length 1. Let Ube any unitary matrix whose ﬁrst column is v. Such a matrix can\n",
      "always be constructed2. AsUis unitary, the ﬁrst row of U\u00001isv\u0003, and U\u00001AUis of the form\n",
      "\"v\u0003\n",
      "\u0003#\n",
      "Ah\n",
      "v\u0003i\n",
      "|   {z   }\n",
      "U=\"\u0015\u0003\n",
      "0B#\n",
      ";\n",
      "for some matrix B. By the induction hypothesis, there exists a unitary matrix Wand an\n",
      "upper triangular matrix Tsuch that W\u00001BW=T. Now, deﬁne\n",
      "V:=\"10>\n",
      "0W#\n",
      ":\n",
      "Then,\n",
      "V\u00001\u0010\n",
      "U\u00001AU\u0011\n",
      "V=\"10>\n",
      "0W\u00001#\"\u0015\u0003\n",
      "0B#\"10>\n",
      "0W#\n",
      "=\"\u0015\u0003\n",
      "0W\u00001BW#\n",
      "=\"\u0015\u0003\n",
      "0T#\n",
      ";\n",
      "which is upper triangular of dimension n+1. Since UVis unitary, this completes the\n",
      "induction, and hence the result is true for all n. \u0003\n",
      "The theorem above can be used to prove an important property of Hermitian matrices,\n",
      "i.e., matrices for which A\u0003=A.\n",
      "2After specifying vwe can complete the rest of the unitary matrix via the Gram–Schmidt procedure, for\n",
      "example; see Section A.6.4.368 A.5. Eigenvalues and Eigenvectors\n",
      "Theorem A.7: Eigenvalues of a Hermitian Matrix\n",
      "Any n\u0002nHermitian matrix has real eigenvalues. The corresponding matrix of nor-\n",
      "malized eigenvectors is a unitary matrix.\n",
      "Proof: LetAbe a Hermitian matrix. By Theorem A.6 there exists a unitary matrix Usuch\n",
      "thatU\u00001AU=T, where Tis upper triangular. It follows that the adjoint ( U\u00001AU)\u0003=T\u0003\n",
      "is lower triangular. However, ( U\u00001AU)\u0003=U\u00001AU, since A\u0003=AandU\u0003=U\u00001. Hence,\n",
      "TandT\u0003must be the same, which can only be the case if Tis areal diagonal matrix D.\n",
      "Since AU=DU, the diagonal elements are exactly the eigenvalues and the corresponding\n",
      "eigenvectors are the columns of U. \u0003\n",
      "In particular, the eigenvalues of a real symmetric matrix are real. We can now repeat\n",
      "the proof of Theorem A.6 with real eigenvalues and eigenvectors, so that there exists an\n",
      "orthogonal matrix Qsuch that Q\u00001AQ=Q>AQ=D. The eigenvectors can be chosen as\n",
      "the columns of Q, which form an orthonormal basis. This proves the following theorem.\n",
      "Theorem A.8: Real Symmetric Matrices are Orthogonally Diagonizable\n",
      "Any real symmetric matrix Acan be written as\n",
      "A=QDQ>;\n",
      "where Dis the diagonal matrix of (real) eigenvalues and Qis an orthogonal matrix\n",
      "whose columns are eigenvectors of A.\n",
      "Example A.5 (Real Symmetric Matrices and Ellipses) As we have seen, linear trans-\n",
      "formations map circles into ellipses. We can use the above theory for real symmetric\n",
      "matrices to identify the principal axes. Consider, for example, the transformation with mat-\n",
      "rixA=[1;1;\u00001=2;\u00002] in (A.1). A point xon the unit circle is mapped to a point y=Ax.\n",
      "Since for such points kxk2=x>x=1, we have that ysatisﬁes y>(A\u00001)>A\u00001y=1, which\n",
      "gives the equation for the ellipse\n",
      "17y2\n",
      "1\n",
      "9+20y1y2\n",
      "9+8y2\n",
      "2\n",
      "9=1:\n",
      "LetQbe the orthogonal matrix of eigenvectors of the symmetric matrix ( A\u00001)>A\u00001=\n",
      "(AA>)\u00001, soQ>(AA>)\u00001Q=Dfor some diagonal matrix D. Taking the inverse on both\n",
      "sides of the previous equation, we have Q>AA>Q=D\u00001, which shows that Qis also the\n",
      "matrix of eigenvectors of AA>. These eigenvectors point precisely in the direction of the\n",
      "principal axes, as shown in Figure A.4. It turns out, see Section A.6.5, that the square roots\n",
      "of the eigenvalues of AA>, here approximately 2 :4221 and 0:6193, correspond to the sizes\n",
      "of the principal axes of the ellipse, as illustrated in Figure A.4.Appendix A. Linear Algebra and Functional Analysis 369\n",
      "-1 0 1-2-1012\n",
      "Figure A.4: The eigenvectors and eigenvalues of AA>determine the principal axes of the\n",
      "ellipse.\n",
      "The following deﬁnition generalizes the notion of positivity of a real variable to that of a\n",
      "(Hermitian) matrix, providing a crucial concept for multivariate di \u000berentiation and optim-\n",
      "ization; see Appendix B. +399\n",
      "Deﬁnition A.3: Positive (Semi)Deﬁnite Matrix\n",
      "A Hermitian matrix Ais called positive semideﬁnite positive\n",
      "semidefinite(we write A\u00170) ifhAx;xi>0\n",
      "for all x. It is called positive deﬁnite (we write A\u001f0) ifhAx;xi>0 for all x,0.\n",
      "The positive (semi)deﬁniteness of a matrix can be directly related to the positivity of\n",
      "its eigenvalues, as follows:\n",
      "Theorem A.9: Eigenvalues of a Positive Semideﬁnite Matrix\n",
      "All eigenvalues of a positive semideﬁnite matrix are non-negative and all eigenval-\n",
      "ues of a positive deﬁnite matrix are strictly positive.\n",
      "Proof: LetAbe a positive semideﬁnite matrix. By Theorem A.7, the eigenvalues of Aare\n",
      "allreal. Suppose\u0015is an eigenvalue with eigenvector v. As Ais positive semideﬁnite, we\n",
      "have\n",
      "06hAv;vi=\u0015hv;vi=\u0015kvk2;\n",
      "which can only be true if \u0015>0. Similarly, for a positive deﬁnite matrix, \u0015must be strictly\n",
      "greater than 0. \u0003\n",
      "Corollary A.1 Any real positive semideﬁnite matrix Acan be written as\n",
      "A=BB>\n",
      "for some real matrix B. Conversely, for any real matrix B, the matrix BB>is positive\n",
      "semideﬁnite.370 A.6. Matrix Decompositions\n",
      "Proof: The matrix Ais both Hermitian (by deﬁnition) and real (by assumption) and hence\n",
      "it is symmetric. By Theorem A.8, we can write A=QDQ>, where Dis the diagonal\n",
      "matrix of (real) eigenvalues of A. By Theorem A.9 all eigenvalues are non-negative, and\n",
      "thus their square root is real-valued. Now, deﬁne B=Qp\n",
      "D, wherep\n",
      "Dis deﬁned as the\n",
      "diagonal matrix whose diagonal elements are the square roots of the eigenvalues of A.\n",
      "Then, BB>=Qp\n",
      "D(p\n",
      "D)>Q>=QDQ>=A. The converse statement follows from the\n",
      "fact that x>BB>x=kB>xk2>0 for all x. \u0003\n",
      "A.6 Matrix Decompositions\n",
      "Matrix decompositions are frequently used in linear algebra to simplify proofs, avoid nu-\n",
      "merical instability, and to speed up computations. We mention three important matrix de-\n",
      "compositions: (P)LU, QR, and SVD.\n",
      "A.6.1 (P)LU Decomposition\n",
      "Every invertible matrix Acan be written as the product of three matrices:\n",
      "A=PLU; (A.9)\n",
      "where Lis a lower triangular matrix, Uan upper triangular matrix, and Papermutation\n",
      "matrix permutation\n",
      "matrix. A permutation matrix is a square matrix with a single 1 in each row and column,\n",
      "and zeros otherwise. The matrix product PBsimply permutes the rows of a matrix Band,\n",
      "likewise, BPpermutes its columns. A decomposition of the form (A.9) is called a PLU\n",
      "decomposition PLU\n",
      "decomposition. As a permutation matrix is orthogonal, its transpose is equal to its inverse,\n",
      "and so we can write (A.9) as\n",
      "P>A=LU:\n",
      "The decomposition is not unique, and in many cases Pcan be taken to be the identity\n",
      "matrix, in which case we speak of the LU decomposition ofA, also called the LR for\n",
      "left–right (triangular) decomposition.\n",
      "A PLU decomposition of an invertible n\u0002nmatrix A0can be obtained recursively as\n",
      "follows. The ﬁrst step is to swap the rows of A0such that the element in the ﬁrst column and\n",
      "ﬁrst row of the pivoted matrix is as large as possible in absolute value. Write the resulting\n",
      "matrix as\n",
      "eP0A0=\"a1b>\n",
      "1\n",
      "c1D1#\n",
      ";\n",
      "whereeP0is the permutation matrix that swaps the ﬁrst and k-th row, where kis the row\n",
      "that contains the largest element in the ﬁrst column. Next, add the matrix \u0000c1[1;b>\n",
      "1=a1] to\n",
      "the last n\u00001 rows of eP0A0, to obtain the matrix\n",
      "\"a1 b>\n",
      "1\n",
      "0 D 1\u0000c1b>\n",
      "1=a1#\n",
      "=:\"a1b>\n",
      "1\n",
      "0 A 1#\n",
      ":\n",
      "In e\u000bect, we add some multiple of the ﬁrst row to each of the remaining rows in order to\n",
      "obtain zeros in the ﬁrst column, except for the ﬁrst element.\n",
      "We now apply the same procedure to A1as we did to A0and then to subsequent smaller\n",
      "matrices A2;:::; An\u00001:Appendix A. Linear Algebra and Functional Analysis 371\n",
      "1. Swap the ﬁrst row with the row having the maximal absolute value element in the\n",
      "ﬁrst column.\n",
      "2. Make every other element in the ﬁrst column equal to 0 by adding appropriate mul-\n",
      "tiples of the ﬁrst row to the other rows.\n",
      "Suppose that Athas a PLU decomposition PtLtUt. Then it is easy to check that\n",
      "eP>\n",
      "t\u00001\"10>\n",
      "0 P t#\n",
      "|        {z        }\n",
      "Pt\u00001\"1 0>\n",
      "P>\n",
      "tct=atLt#\n",
      "|           {z           }\n",
      "Lt\u00001\"atb>\n",
      "t\n",
      "0 U t#\n",
      "|   {z   }\n",
      "Ut\u00001(A.10)\n",
      "is a PLU decomposition of At\u00001. Since the PLU decomposition for the scalar An\u00001is trivial,\n",
      "by working backwards we obtain a PLU decomposition P0L0U0ofA.\n",
      "Example A.6 (PLU Decomposition) Take\n",
      "A=26666666640\u00001 7\n",
      "3 2 0\n",
      "1 1 13777777775:\n",
      "Our goal is to modify Avia Steps 1 and 2 above so as to obtain an upper triangular matrix\n",
      "with maximal elements on the diagonal. We ﬁrst swap the ﬁrst and second row. Next, we\n",
      "add\u00001=3 times the ﬁrst row to the third row and 1 =3 times the second row to the third row:\n",
      "26666666640\u00001 7\n",
      "3 2 0\n",
      "1 1 13777777775\u0000!26666666643 2 0\n",
      "0\u00001 7\n",
      "1 1 13777777775\u0000!26666666643 2 0\n",
      "0\u00001 7\n",
      "0 1=3 13777777775\u0000!26666666643 2 0\n",
      "0\u00001 7\n",
      "0 0 10=33777777775:\n",
      "The ﬁnal matrix is U0, and in the process we have applied the permutation matrices\n",
      "eP0=26666666640 1 0\n",
      "1 0 0\n",
      "0 0 13777777775;eP1=\"1 0\n",
      "0 1#\n",
      ":\n",
      "Using the recursion (A.10) we can now recover P0andL0. Namely, at the ﬁnal iteration\n",
      "we have P2=1;L2=1, and U2=10=3. And subsequently,\n",
      "P1=\"1 0\n",
      "0 1#\n",
      ";L1=\"1 0\n",
      "\u00001=3 1#\n",
      ";P0=26666666640 1 0\n",
      "1 0 0\n",
      "0 0 13777777775;L0=26666666641 0 0\n",
      "0 1 0\n",
      "1=3\u00001=3 13777777775;\n",
      "observing that a1=3;c1=[0;1]>,a2=\u00001, and c2=1=3.\n",
      "PLU decompositions can be used to solve large systems of linear equations of the form\n",
      "Ax=be\u000eciently, especially when such an equation has to be solved for many di \u000berent b.\n",
      "This is done by ﬁrst decomposing AintoPLU , and then solving two triangular systems:\n",
      "1.Ly=P>b.\n",
      "2.Ux=y.372 A.6. Matrix Decompositions\n",
      "The ﬁrst equation can be solved e \u000eciently via forward substitution , and the second viaforward\n",
      "substitution backward substitution , as illustrated in the following example.\n",
      "backward\n",
      "substitution Example A.7 (Solving Linear Equations with an LU Decomposition) LetA=PLU\n",
      "be the same as in Example A.6. We wish to solve Ax=[1;2;3]>. First, solving\n",
      "26666666641 0 0\n",
      "0 1 0\n",
      "1=3\u00001=3 137777777752666666664y1\n",
      "y2\n",
      "y33777777775=26666666642\n",
      "1\n",
      "33777777775\n",
      "gives, y1=2,y2=1 and y3=3\u00002=3+1=3=8=3, by forward substitution. Next,\n",
      "26666666643 2 0\n",
      "0\u00001 7\n",
      "0 0 10=337777777752666666664x1\n",
      "x2\n",
      "x33777777775=26666666642\n",
      "1\n",
      "8=33777777775\n",
      "gives x3=4=5;x2=\u00001+28=5=23=5, and x1=2(1\u000023=5)=3=\u000012=5, so x=\n",
      "[\u000012;23;4]>=5.\n",
      "A.6.2 Woodbury Identity\n",
      "LU (or more generally PLU) decompositions can also be applied to block matrices. A\n",
      "starting point is the following LU decomposition for a general 2 \u00022 matrix:\n",
      "\"a b\n",
      "c d#\n",
      "=\"a 0\n",
      "c d\u0000bc=a#\"1b=a\n",
      "0 1#\n",
      ";\n",
      "which holds as long as a,0; this can be seen by simply writing out the matrix product.\n",
      "The block matrix generalization for matrices A2Rn\u0002n;B2Rn\u0002k;C2Rk\u0002n;D2Rk\u0002kis\n",
      "\u0006:=\"A B\n",
      "C D#\n",
      "=\"A O n\u0002k\n",
      "C D\u0000CA\u00001B#\"InA\u00001B\n",
      "Ok\u0002n Ik#\n",
      "; (A.11)\n",
      "provided that Ais invertible (again, write out the block matrix product). Here, we use the\n",
      "notation Op\u0002qto denote the p\u0002qmatrix of zeros. We can further rewrite this as:\n",
      "\u0006=\"In On\u0002k\n",
      "CA\u00001Ik#\"A O n\u0002k\n",
      "Ok\u0002nD\u0000CA\u00001B#\"InA\u00001B\n",
      "Ok\u0002n Ik#\n",
      ":\n",
      "Thus, inverting both sides, we obtain\n",
      "\u0006\u00001=\"InA\u00001B\n",
      "Ok\u0002n Ik#\u00001\"A O n\u0002k\n",
      "Ok\u0002nD\u0000CA\u00001B#\u00001\"In On\u0002k\n",
      "CA\u00001Ik#\u00001\n",
      ":\n",
      "Inversion of the above block matrices gives (again write out)\n",
      "\u0006\u00001=\"In\u0000A\u00001B\n",
      "Ok\u0002n Ik#\"A\u00001On\u0002k\n",
      "Ok\u0002n(D\u0000CA\u00001B)\u00001#\"In On\u0002k\n",
      "\u0000CA\u00001Ik#\n",
      ": (A.12)\n",
      "Assuming that Dis invertible, we could also perform a block UL (as opposed to LU)\n",
      "decomposition:\n",
      "\u0006=\"A\u0000BD\u00001C B\n",
      "Ok\u0002n D#\"In On\u0002k\n",
      "D\u00001C I k#\n",
      "; (A.13)Appendix A. Linear Algebra and Functional Analysis 373\n",
      "which, after a similar calculation as the one above, yields\n",
      "\u0006\u00001=\"In On\u0002k\n",
      "\u0000D\u00001C I k#\"(A\u0000BD\u00001C)\u00001On\u0002k\n",
      "Ok\u0002n D\u00001#\"In\u0000BD\u00001\n",
      "Ok\u0002n Ik#\n",
      ": (A.14)\n",
      "The upper-left block of \u0006\u00001from (A.14) must be the same as the upper-left block of \u0006\u00001\n",
      "from (A.12), leading to the Woodbury identity Woodbury\n",
      "identity:\n",
      "(A\u0000BD\u00001C)\u00001=A\u00001+A\u00001B(D\u0000CA\u00001B)\u00001CA\u00001: (A.15)\n",
      "From (A.11) and the fact that the determinant of a product is the product of the determ-\n",
      "inants, we see that det( \u0006)=det(A) det( D\u0000CA\u00001B). Similarly, from (A.13) we have\n",
      "det(\u0006)=det(A\u0000BD\u00001C) det( D), leading to the identity\n",
      "det(A\u0000BD\u00001C) det( D)=det(A) det( D\u0000CA\u00001B): (A.16)\n",
      "The following special cases of (A.16) and (A.15) are of particular importance.\n",
      "Theorem A.10: Sherman–Morrison Formula\n",
      "Suppose that A2Rn\u0002nis invertible and x;y2Rn. Then,\n",
      "det(A+xy>)=det(A)(1+y>A\u00001x):\n",
      "If in addition y>A\u00001x,\u00001, then the Sherman–Morrison formula Sherman –\n",
      "Morrison\n",
      "formulaholds:\n",
      "(A+xy>)\u00001=A\u00001\u0000A\u00001xy>A\u00001\n",
      "1+y>A\u00001x:\n",
      "Proof: Take B=x,C=\u0000y>, and D=1 in (A.16) and (A.15). \u0003\n",
      "One important application of the Sherman–Morrison formula is in the e \u000ecient solution\n",
      "of the linear system Ax=b, where Ais an n\u0002nmatrix of the form:\n",
      "A=A0+pX\n",
      "j=1aja>\n",
      "j\n",
      "for some column vectors a1;:::; ap2Rnandn\u0002ndiagonal (or otherwise easily invertible)\n",
      "matrix A0. Such linear systems arise, for example, in the context of ridge regression and +217\n",
      "optimization. +416\n",
      "To see how the Sherman–Morrison formula can be exploited, deﬁne the matrices\n",
      "A0;:::; Apvia the recursion:\n",
      "Ak=Ak\u00001+aka>\n",
      "k;k=1;:::; p:\n",
      "Application of Theorem A.10 for k=1;:::; pyields the identities:3\n",
      "A\u00001\n",
      "k=A\u00001\n",
      "k\u00001\u0000A\u00001\n",
      "k\u00001aka>\n",
      "kA\u00001\n",
      "k\u00001\n",
      "1+a>\n",
      "kA\u00001\n",
      "k\u00001ak\n",
      "jAkj=jAk\u00001j\u0002\u0010\n",
      "1+a>\n",
      "kA\u00001\n",
      "k\u00001ak\u0011\n",
      ":\n",
      "3HerejAjis a shorthand notation for det( A).374 A.6. Matrix Decompositions\n",
      "Therefore, by evolving the recursive relationships up until k=p, we obtain:\n",
      "A\u00001\n",
      "p=A\u00001\n",
      "0\u0000pX\n",
      "j=1A\u00001\n",
      "j\u00001aja>\n",
      "jA\u00001\n",
      "j\u00001\n",
      "1+a>\n",
      "jA\u00001\n",
      "j\u00001aj\n",
      "jApj=jA0j\u0002pY\n",
      "j=1\u0010\n",
      "1+a>\n",
      "jA\u00001\n",
      "j\u00001aj\u0011\n",
      ":\n",
      "These expressions will allow us to easily compute A\u00001=A\u00001\n",
      "pandjAj=jApjprovided the\n",
      "following quantities are available:\n",
      "ck;j:=A\u00001\n",
      "k\u00001aj;k=1;:::; p\u00001;j=k+1;:::; p:\n",
      "Since, by Theorem A.10, we can write:\n",
      "A\u00001\n",
      "k\u00001aj=A\u00001\n",
      "k\u00002aj\u0000A\u00001\n",
      "k\u00002ak\u00001a>\n",
      "k\u00001A\u00001\n",
      "k\u00002\n",
      "1+a>\n",
      "k\u00001A\u00001\n",
      "k\u00002ak\u00001aj;\n",
      "the quantitiesfck;jgcan be computed from the recursion:\n",
      "c1;j=A\u00001\n",
      "0aj;j=1;:::; p\n",
      "ck;j=ck\u00001;j\u0000a>\n",
      "k\u00001ck\u00001;j\n",
      "1+a>\n",
      "k\u00001ck\u00001;k\u00001ck\u00001;k\u00001;k=2;:::; p;j=k;:::; p:(A.17)\n",
      "Observe that this recursive computation takes O(p2n) time and that once fck;jgare available,\n",
      "we can express A\u00001andjAjas:\n",
      "A\u00001=A\u00001\n",
      "0\u0000pX\n",
      "j=1cj;jc>\n",
      "j;j\n",
      "1+a>\n",
      "jcj;j\n",
      "jAj=jA0j\u0002pY\n",
      "j=1\u0010\n",
      "1+a>\n",
      "jcj;j\u0011\n",
      ":\n",
      "In summary, we have proved the following.\n",
      "Theorem A.11: Sherman–Morrison Recursion\n",
      "The inverse and determinant of the n\u0002nmatrix A=A0+Pp\n",
      "k=1aka>\n",
      "kare given\n",
      "respectively by:\n",
      "A\u00001=A\u00001\n",
      "0\u0000CD\u00001C>\n",
      "det(A)=det(A0) det( D);\n",
      "where C2Rn\u0002pandD2Rp\u0002pare the matrices\n",
      "C:=h\n",
      "c1;1;:::; cp;pi\n",
      ";D:=diag\u0010\n",
      "1+a>\n",
      "1c1;1;\u0001\u0001\u0001;1+a>\n",
      "pcp;p\u0011\n",
      ";\n",
      "and all thefcj;kgare computed from the recursion (A.17) in O(p2n) time.Appendix A. Linear Algebra and Functional Analysis 375\n",
      "As a consequence of Theorem A.11, the solution to the linear system Ax=bcan be\n",
      "computed inO(p2n) time via:\n",
      "x=A\u00001\n",
      "0b\u0000CD\u00001[C>b]:\n",
      "Ifn>p, the Sherman–Morrison recursion can frequently be much faster than the O(n3)\n",
      "direct solution via the LU decomposition method in Section A.6.1. +370\n",
      "In summary, the following algorithm computes the matrices CandDin Theorem A.11\n",
      "via the recursion (A.17).\n",
      "Algorithm A.6.1: Sherman–Morrison Recursion\n",
      "input: Easily invertible matrix A0and column vectors a1;:::; ap.\n",
      "output: Matrices CandDsuch that CD\u00001C>=A\u00001\n",
      "0\u0000\u0010\n",
      "A0+P\n",
      "jaja>\n",
      "j\u0011\u00001.\n",
      "1ck A\u00001\n",
      "0akfork=1;:::; p(assuming A0is diagonal or easily invertible matrix)\n",
      "2fork=1;:::; p\u00001do\n",
      "3 dk 1+a>\n",
      "kck\n",
      "4 forj=k+1;:::; pdo\n",
      "5 cj cj\u0000a>\n",
      "kcj\n",
      "dkck\n",
      "6dp 1+a>\n",
      "pcp\n",
      "7C [c1;:::; cp]\n",
      "8D diag( d1;:::; dp)\n",
      "9return C andD\n",
      "Finally, note that if A0is a diagonal matrix and we only store the diagonal elements of\n",
      "DandA0(as opposed to storing the full matrices DandA0), then the storage or memory\n",
      "requirements of Algorithm A.6.1 are only O(p n).\n",
      "A.6.3 Cholesky Decomposition\n",
      "IfAis a real-valued positive deﬁnite matrix (and therefore symmetric), e.g., a covariance\n",
      "matrix, then an LU decomposition can be achieved with matrices LandU=L>.\n",
      "Theorem A.12: Cholesky Decomposition\n",
      "A real-valued positive deﬁnite matrix A=[ai j]2Rn\u0002ncan be decomposed as\n",
      "A=LL>;\n",
      "where the real n\u0002nlower triangular matrix L=[lk j] satisﬁes the recursive formula\n",
      "lk j=ak j\u0000Pj\u00001\n",
      "i=1ljilkiq\n",
      "aj j\u0000Pj\u00001\n",
      "i=1l2\n",
      "ji;where0X\n",
      "i=1ljilki:=0 (A.18)\n",
      "fork=1;:::; nandj=1;:::; k.376 A.6. Matrix Decompositions\n",
      "Proof: The proof is by inductive construction. For k=1;:::; n, let Akbe the left-upper\n",
      "k\u0002ksubmatrix of A=An. With e1:=[1;0;:::; 0]>, we have A1=a11=e>\n",
      "1Ae1>0 by the\n",
      "positive-deﬁniteness of A. It follows that l11=pa11. Suppose that Ak\u00001has a Cholesky fac-\n",
      "torization Lk\u00001L>\n",
      "k\u00001with Lk\u00001having strictly positive diagonal elements, we can construct\n",
      "a Cholesky factorization of Akas follows. First write\n",
      "Ak=\"Lk\u00001L>\n",
      "k\u00001ak\u00001\n",
      "a>\n",
      "k\u00001akk#\n",
      "and propose Lkto be of the form\n",
      "Lk=\"Lk\u000010\n",
      "l>\n",
      "k\u00001lkk#\n",
      "for some vector lk\u000012Rk\u00001and scalar lkk, for which it must hold that\"Lk\u00001L>\n",
      "k\u00001ak\u00001\n",
      "a>\n",
      "k\u00001akk#\n",
      "=\"Lk\u000010\n",
      "l>\n",
      "k\u00001lkk#\"L>\n",
      "k\u00001lk\u00001\n",
      "0>lkk#\n",
      ":\n",
      "To establish that such an lk\u00001andlkkexist, we must verify that the set of equations\n",
      "Lk\u00001lk\u00001=ak\u00001\n",
      "l>\n",
      "k\u00001lk\u00001+l2\n",
      "kk=akk(A.19)\n",
      "has a solution. The system Lk\u00001lk\u00001=ak\u00001has a unique solution, because (by assump-\n",
      "tion) Lk\u00001is lower diagonal with strictly positive entries down the main diagonal and we\n",
      "can solve for lk\u00001using forward substitution: lk\u00001=L\u00001\n",
      "k\u00001ak\u00001. We can solve the second\n",
      "equation as lkk=p\n",
      "akk\u0000klkk2, provided that the term within the square root is positive.\n",
      "We demonstrate this using the fact that Ais a positive deﬁnite matrix. In particular, for\n",
      "x2Rnof the form [ x>\n",
      "1;x2;0>]>, where x1is a non-zero ( k\u00001)-dimensional vector and x2\n",
      "a non-zero number, we have\n",
      "0<x>Ax=\u0002x>\n",
      "1;x2\u0003\"Lk\u00001L>\n",
      "k\u00001ak\u00001\n",
      "a>\n",
      "k\u00001akk#\"x1\n",
      "x2#\n",
      "=kL>\n",
      "k\u00001x1k2+2x>\n",
      "1ak\u00001x2+akkx2\n",
      "2:\n",
      "Now take x1=\u0000x2L\u0000>\n",
      "k\u00001lk\u00001to obtain 0 <x>Ax=x2\n",
      "2(akk\u0000klk\u00001k2). Therefore, (A.19)\n",
      "can be uniquely solved. As we have already solved it for k=1, we can solve it for any\n",
      "k=1;:::; n, leading to the recursive formula (A.18) and Algorithm A.6.2 below. \u0003\n",
      "An implementation of Cholesky’s decomposition that uses the notation in the proof of\n",
      "Theorem A.6.3 is the following algorithm, whose running cost is O(n3).\n",
      "Algorithm A.6.2: Cholesky Decomposition\n",
      "input: Positive-deﬁnite n\u0002nmatrix Anwith entriesfai jg.\n",
      "output: Lower triangular Lnsuch that LnL>\n",
      "n=An.\n",
      "1L1 pa11\n",
      "2fork=2;:::; ndo\n",
      "3 ak\u00001 [a1k;:::; ak\u00001;k]>\n",
      "4 lk\u00001 L\u00001\n",
      "k\u00001ak\u00001(computed inO(k2) time via forward substitution)\n",
      "5 lkk p\n",
      "akk\u0000l>\n",
      "k\u00001lk\u00001\n",
      "6 Lk \"Lk\u000010\n",
      "l>\n",
      "k\u00001lkk#\n",
      "7return L nAppendix A. Linear Algebra and Functional Analysis 377\n",
      "A.6.4 QR Decomposition and the Gram–Schmidt Procedure\n",
      "LetAbe an n\u0002pmatrix, where p6n. Then, there exists a matrix Q2Rn\u0002psatisfying\n",
      "Q>Q=Ip, and an upper triangular matrix R2Rp\u0002p, such that\n",
      "A=QR:\n",
      "This is the QR decomposition for real-valued matrices. When Ahas full column rank, such\n",
      "a decomposition can be obtained via the Gram–Schmidt procedure, which constructs anGram–Schmidtorthonormal basis fu1;:::; upgof the column space of Aspanned byfa1;:::; apg, in the\n",
      "following way (see also Figure A.5):\n",
      "1. Take u1=a1=ka1k.\n",
      "2. Let p1be the projection of a2onto Spanfu1g. That is, p1=hu1;a2iu1. Now take\n",
      "u2=(a2\u0000p1)=ka2\u0000p1k. This vector is perpendicular to u1and has unit length.\n",
      "3. Let p2be the projection of a3onto Spanfu1;u2g. That is, p2=hu1;a3iu1+hu2;a3iu2.\n",
      "Now take u3=(a3\u0000p2)=ka3\u0000p2k. This vector is perpendicular to both u1andu2\n",
      "and has unit length.\n",
      "4. Continue this process to obtain u4;:::; up.\n",
      "0 1 2-10123\n",
      "p1a1\n",
      "a2a2!p1u1\n",
      "u2\n",
      "Figure A.5: Illustration of the Gram–Schmidt procedure.\n",
      "At the end of the procedure, a set fu1;:::; upgofporthonormal vectors are obtained.\n",
      "Consequently, as a result of Theorem A.3,\n",
      "aj=jX\n",
      "i=1haj;uii| {z }\n",
      "ri jui;j=1;:::; p;378 A.6. Matrix Decompositions\n",
      "for some numbers ri j;j=1;:::; i;i=1;:::; p. Denoting the corresponding upper triangu-\n",
      "lar matrix [ ri j] byR, we have in matrix notation:\n",
      "QR=[u1;:::; up]26666666666666664r11r12r13::: r1p\n",
      "0r22r23::: r2p\n",
      ":::0:::::::::\n",
      "0 0 0 ::: rpp37777777777777775=[a1;:::; ap]=A;\n",
      "which yields a QR decomposition. The QR decomposition can be used to e \u000eciently solve\n",
      "least-squares problems; this will be shown shortly. It can also be used to calculate the\n",
      "determinant of the matrix A, whenever Ais square. Namely, det( A)=det(Q) det( R)=\n",
      "det(R); and since Ris triangular, its determinant is the product of its diagonal elements.\n",
      "There exist various improvements of the Gram–Schmidt process (for example, the House-\n",
      "holder transformation [52]) that not only improve the numerical stability of the QR de-\n",
      "composition, but also can be applied even when Ais not full rank.\n",
      "An important application of the QR decomposition is found in solving the least-squares\n",
      "problem inO(p2n) time:\n",
      "min\n",
      "\f2RpkX\f\u0000yk2\n",
      "for some X2Rn\u0002p(model) matrix. Using the deﬁning properties of the pseudo-inverse in\n",
      "Deﬁnition A.2, one can show that kXX+y\u0000yk26kX\f\u0000yk2for any\f. In other words, b\f:= +362\n",
      "X+yminimizeskX\f\u0000yk. If we have the QR decomposition X=QR, then a numerically\n",
      "stable way to calculate b\fwith anO(p2n) cost is via\n",
      "b\f=(QR)+y=R+Q+y=R+Q>y:\n",
      "IfXhas full column rank, then R+=R\u00001.\n",
      "Note that while the QR decomposition is the method of choice for solving the ordinary\n",
      "least-squares regression problem, the Sherman–Morrison recursion is the method of choice +374\n",
      "for solving the regularized least-squares (or ridge) regression problem. +217\n",
      "A.6.5 Singular Value Decomposition\n",
      "One of the most useful matrix decompositions is the singular value decomposition singular value\n",
      "decomposition(SVD).\n",
      "Theorem A.13: Singular Value Decomposition\n",
      "Any (complex) matrix m\u0002nmatrix Aadmits a unique decomposition\n",
      "A=U\u0006V\u0003;\n",
      "where UandVare unitary matrices of dimension mandn, respectively, and \u0006is a\n",
      "realm\u0002ndiagonal matrix. If Ais real, then UandVare both orthogonal matrices.\n",
      "Proof: Without loss of generality we can assume that m>n(otherwise consider the trans-\n",
      "pose of A). Then A\u0003Ais a positive semideﬁnite Hermitian matrix, because hA\u0003Av;vi=\n",
      "v\u0003A\u0003Av=kAvk2>0 for all v. Hence, A\u0003Ahas non-negative real eigenvalues, \u00151>\u00152>Appendix A. Linear Algebra and Functional Analysis 379\n",
      "\u0001\u0001\u0001>\u0015n>0. By Theorem A.7 the matrix V=[v1;:::; vn] of right-eigenvectors is a unit-\n",
      "ary matrix. Deﬁne the i-thsingular value singular value as\u001bi=p\u0015i,i=1;:::; nand suppose \u00151;:::;\u0015 r\n",
      "are all greater than 0, and \u0015r+1;:::;\u0015 n=0. In particular, Avi=0fori=r+1;:::; n. Let\n",
      "ui=Avi=\u001bi,i=1;:::; r. Then, for i;j6r,\n",
      "hui;uji=u\u0003\n",
      "jui=v\u0003\n",
      "jA\u0003Avi\n",
      "\u001bi\u001bj=\u0015i1fi=jg\n",
      "\u001bi\u001bj=1fi=jg:\n",
      "We can extend u1;:::; urto an orthonormal basis fu1;:::; umgofCm(e.g., using the Gram–\n",
      "Schmidt procedure). Let U=[u1;:::; un] be the corresponding unitary matrix. Deﬁning \u0006\n",
      "to be the m\u0002ndiagonal matrix with diagonal ( \u001b1;:::;\u001b r;0;:::; 0), we have,\n",
      "U\u0006=[Av1;:::; Avr;0;:::; 0]=A V;\n",
      "and hence A=U\u0006V\u0003. \u0003\n",
      "Note that\n",
      "AA\u0003=U\u0006V\u0003V\u0006\u0003U\u0003=U\u0006\u0006>U\u0003and A\u0003A=V\u0006\u0003U\u0003U\u0006V\u0003=V\u0006>\u0006V\u0003:\n",
      "So,Uis a unitary matrix whose columns are eigenvectors of AA\u0003andVis a unitary matrix\n",
      "whose columns are eigenvectors of A\u0003A.\n",
      "The SVD makes it possible to write the matrix Aas a sum of rank-1 matrices, weighted\n",
      "by the singular values f\u001big:\n",
      "A=h\n",
      "u1;u2;:::; umi266666666666666666666664\u001b10::: ::: 0\n",
      "0:::0::: 0\n",
      "0::: \u001b r::: 0\n",
      "0::: ::: 0::: 0\n",
      "0::: ::: ::::::037777777777777777777777526666666666666664v\u0003\n",
      "1\n",
      "v\u0003\n",
      "2:::\n",
      "v\u0003\n",
      "n37777777777777775=rX\n",
      "i=1\u001biuiv\u0003\n",
      "i; (A.20)\n",
      "which is called the dyade orspectral representation spectral\n",
      "representationofA.\n",
      "For real-valued matrices, the SVD has a nice geometric interpretation, illustrated in\n",
      "Figure A.6. The linear mapping deﬁned by matrix Acan be thought of as a succession of\n",
      "three linear operations: (1) an orthogonal transformation (i.e., a rotation with a possible\n",
      "ﬂipping of some axes), corresponding to matrix V>, followed by (2) a simple scaling of\n",
      "the unit vectors, corresponding to \u0006, followed by (3) another orthogonal transformation,\n",
      "corresponding to U.\n",
      "-4 -2 0 2 4-202\n",
      "-4 -2 0 2 4-202\n",
      "-4 -2 0 2 4-202\n",
      "-4 -2 0 2 4-202\n",
      "Figure A.6: The ﬁgure shows how the unit circle and unit vectors (ﬁrst panel) are ﬁrst\n",
      "rotated (second panel), then scaled (third panel), and ﬁnally rotated and ﬂipped.380 A.6. Matrix Decompositions\n",
      "Example A.8 (Ellipses) We continue Example A.5. Using the svdmethod of the mod-\n",
      "ulenumpy.linalg , we obtain the following SVD matrices for matrix A:\n",
      "U=\"\u00000:5430 0:8398\n",
      "0:8398 0:5430#\n",
      ";\u0006=\"2:4221 0\n",
      "0 0:6193#\n",
      ";and V=\"\u00000:3975 0:9176\n",
      "\u00000:9176\u00000:3975#\n",
      ":\n",
      "Figure A.4 shows the columns of the matrix U\u0006as the two principal axes of the ellipse that +369\n",
      "is obtained by applying matrix Ato the points of the unit circle.\n",
      "A practical method to compute the pseudo-inverse of a real-valued matrix Ais via the\n",
      "singular value decomposition A=U\u0006V>, where \u0006is the diagonal matrix collecting all the\n",
      "positive singular values, say \u001b1;:::;\u001b r, as in Theorem A.13. In this case, A+=V\u0006+U>,\n",
      "where \u0006+is the n\u0002mdiagonal (pseudo-inverse) matrix:\n",
      "\u0006+=266666666666666666666664\u001b\u00001\n",
      "10::: ::: 0\n",
      "0:::0::: 0\n",
      "0::: \u001b\u00001\n",
      "r::: 0\n",
      "0::: ::: 0::: 0\n",
      "0::: ::: ::::::0377777777777777777777775:\n",
      "We conclude with a typical application of the pseudo-inverse for a least-squares optim-\n",
      "ization problem from data science.\n",
      "Example A.9 (Rank-Deﬁcient Least Squares) Given is an n\u0002pdata matrix\n",
      "X=26666666666666664x11x12\u0001\u0001\u0001 x1p\n",
      "x21x22\u0001\u0001\u0001 x2p\n",
      "::::::::::::\n",
      "xn1xn2\u0001\u0001\u0001 xnp37777777777777775:\n",
      "It is assumed that the matrix is of full row rank (all rows of Xare linearly independent) and\n",
      "that the number of rows is less than the number of columns: n<p. Under this setting, any\n",
      "solution to the equation X\f=yprovides a perfect ﬁt to the data and minimizes (to 0) the\n",
      "least-squares problem\n",
      "b\f=argmin\n",
      "\f2RpkX\f\u0000yk2: (A.21)\n",
      "In particular, if \f\u0003minimizeskX\f\u0000yk2then so does \f\u0003+ufor all uin the null space\n",
      "NX:=fu:Xu=0g, which has dimension p\u0000n. To cope with the non-uniqueness of\n",
      "solutions, a possible approach is to solve instead the following optimization problem:\n",
      "minimize\n",
      "\f2Rp\f>\f\n",
      "subject to X\f\u0000y=0:\n",
      "That is, we are interested in a solution \fwith the smallest squared norm (or, equival-\n",
      "ently, the smallest norm). The solution can be obtained via Lagrange’s method (see Sec-\n",
      "tion B.2.2). Speciﬁcally, set L(\f;\u0015)=\f>\f\u0000\u0015>(X\f\u0000y), and solve +408\n",
      "r\fL(\f;\u0015)=2\f\u0000X>\u0015=0; (A.22)Appendix A. Linear Algebra and Functional Analysis 381\n",
      "and\n",
      "r\u0015L(\f;\u0015)=X\f\u0000y=0: (A.23)\n",
      "From (A.22) we get \f=X>\u0015=2. By substituting it in (A.23), we arrive at \u0015=2(XX>)\u00001y,\n",
      "and hence\fis given by\n",
      "\f=X>\u0015\n",
      "2=X>2(XX>)\u00001y\n",
      "2=X>(XX>)\u00001y=X+y:\n",
      "An example Python code is given below.\n",
      "svdexample.py\n",
      "from numpy import diag , zeros ,vstack\n",
      "from numpy.random import rand , seed\n",
      "from numpy.linalg import svd, pinv\n",
      "seed(12345)\n",
      "n = 5\n",
      "p = 8\n",
      "X = rand(n,p)\n",
      "y = rand(n,1)\n",
      "U,S,VT = svd(X)\n",
      "SI = diag(1/S)\n",
      "# compute pseudo inverse\n",
      "pseudo_inv = VT.T @ vstack((SI, zeros((p-n,n)))) @ U.T\n",
      "b = pseudo_inv @ y\n",
      "#b = pinv(X) @ y #remove comment for the built -in pseudo inverse\n",
      "print(X @ b - y)\n",
      "[[5.55111512e-16]\n",
      "[1.11022302e-16]\n",
      "[5.55111512e-16]\n",
      "[8.60422844e-16]\n",
      "[2.22044605e-16]]\n",
      "A.6.6 Solving Structured Matrix Equations\n",
      "For a general matrix A2Cn\u0002n, performing matrix–vector multiplications takes O(n2) op-\n",
      "erations; and solving linear systems Ax=b, and carrying out LU decompositions takes\n",
      "O(n3) operations. However, when Aissparse (i.e., has relatively few non-zero elements)\n",
      "or has a special structure, the computational complexity for these operations can often be\n",
      "reduced. Matrices Athat are “structured” in this way often satisfy a Sylvester equation Sylvester\n",
      "equation, of\n",
      "the form\n",
      "M1A\u0000AM\u0003\n",
      "2=G1G\u0003\n",
      "2; (A.24)\n",
      "where Mi2Cn\u0002n;i=1;2 are sparse matrices and Gi2Cn\u0002r,i=1;2 are matrices of rank\n",
      "r\u001cn. The elements of Amust be easy to recover from these matrices, e.g., with O(1) op-\n",
      "erations. A typical example is a (square) Toeplitz matrix Toeplitz matrix , which has the following structure:382 A.6. Matrix Decompositions\n",
      "A=266666666666666666666664a0a\u00001\u0001\u0001\u0001 a\u0000(n\u00002)a\u0000(n\u00001)\n",
      "a1 a0a\u00001 a\u0000(n\u00002)\n",
      "::: a1a0::::::\n",
      "an\u00002:::::: a\u00001\n",
      "an\u00001an\u00002\u0001\u0001\u0001 a1 a0377777777777777777777775:\n",
      "A general square Toeplitz matrix Ais completely determined by the 2 n\u00001 elements along\n",
      "its ﬁrst row and column. If Ais also Hermitian (i.e., A\u0003=A), then clearly it is determined\n",
      "by only nelements. If we deﬁne the matrices:\n",
      "M1=2666666666666666666666640 0\u0001\u0001\u0001 0 1\n",
      "1 0 0 0\n",
      ":::1 0::::::\n",
      "0::::::0\n",
      "0 0\u0001\u0001\u0001 1 0377777777777777777777775and M2=2666666666666666666666640 1\u0001\u0001\u0001 0 0\n",
      "0 0 1 0\n",
      ":::0 0::::::\n",
      "0::::::1\n",
      "\u00001 0\u0001\u0001\u0001 0 0377777777777777777777775;\n",
      "then (A.24) is satisﬁed with\n",
      "G1G\u0003\n",
      "2:=26666666666666666666641 0\n",
      "0a1+a\u0000(n\u00001)\n",
      "0a2+a\u0000(n\u00002)\n",
      "::::::\n",
      "0an\u00001+a\u000013777777777777777777775\"an\u00001\u0000a\u00001an\u00002\u0000a\u00002::: a1\u0000a\u0000(n\u00001)2a0\n",
      "0 0 ::: 0 1#\n",
      "=266666666666666666666664an\u00001\u0000a\u00001an\u00002\u0000a\u00002::: a1\u0000a\u0000(n\u00001) 2a0\n",
      "0 0 ::: 0 a1+a\u0000(n\u00001)\n",
      ":::::: :::::: a2+a\u0000(n\u00002)\n",
      ":::::: :::::::::\n",
      "0 0 ::: 0 an\u00001+a\u00001377777777777777777777775;\n",
      "which has rank r62.\n",
      "Example A.10 (Discrete Convolution of Vectors) The convolution of two vectors\n",
      "can be represented as multiplication of one of the vectors by a Toeplitz matrix. Sup-\n",
      "pose a=[a1;:::; an]>andb=[b1;:::; bn]>are two complex-valued vectors. Then, their\n",
      "convolution convolution is deﬁned as the vector a\u0003bwith i-th element\n",
      "[a\u0003b]i=nX\n",
      "k=1akbi\u0000k+1;i=1;:::; n;\n",
      "where bj:=0 for j60. It is easy to verify that the convolution can be written as\n",
      "a\u0003b=Ab;Appendix A. Linear Algebra and Functional Analysis 383\n",
      "where, denoting the d-dimensional column vector of zeros by 0d, we have that\n",
      "A=26666666666666666666666664a 0\n",
      "0n\u00001a:::\n",
      "0n\u00002:::0n\u00002\n",
      ":::a0n\u00001\n",
      "0 a37777777777777777777777775:\n",
      "Clearly, the matrix Ais a (sparse) Toeplitz matrix.\n",
      "Acirculant matrix circulant\n",
      "matrixis a special Toeplitz matrix which is obtained from a vector cby\n",
      "circularly permuting its indices as follows:\n",
      "C=266666666666666666666664c0cn\u00001::: c2c1\n",
      "c1 c0cn\u00001 c2\n",
      "::: c1 c0::::::\n",
      "cn\u00002::::::cn\u00001\n",
      "cn\u00001cn\u00002::: c1c0377777777777777777777775: (A.25)\n",
      "Note that Cis completely determined by the nelements of its ﬁrst column, c.\n",
      "To illustrate how structured matrices allow for faster matrix computations, consider\n",
      "solving the n\u0002nlinear system:\n",
      "Anxn=an\n",
      "forxn=[x1;:::; xn]>, where an=[a1;:::; an]>, and\n",
      "An:=266666666666666666666666641 a1::: an\u00002an\u00001\n",
      "a1 1::: an\u00002\n",
      ":::::::::::::::\n",
      "an\u00002::::::a1\n",
      "an\u00001an\u00002\u0001\u0001\u0001 a1 137777777777777777777777775(A.26)\n",
      "is a real-valued symmetric positive-deﬁnite Toeplitz matrix (so that it is invertible). Note\n",
      "that the entries of Anare completely determined by the right-hand side of the linear equa-\n",
      "tion: vector an. As we shall see shortly in Example A.11, the solution to the more general\n",
      "linear equation Anxn=bn, where bnis arbitrary, can be e \u000eciently computed using the\n",
      "solution to this speciﬁc system Anxn=an, obtained via a special recursive algorithm\n",
      "(Algorithm A.6.3 below).\n",
      "For every k=1;:::; nthek\u0002kToeplitz matrix Aksatisﬁes\n",
      "Ak=PkAkPk;\n",
      "where Pkis a permutation matrix that “ﬂips” the order of elements — rows when pre-\n",
      "multiplying and columns when post-multiplying. For example,\n",
      "\"1 2 3 4 5\n",
      "6 7 8 9 10#\n",
      "P5=\"5 4 3 2 1\n",
      "10 9 8 7 6#\n",
      ";where P5=266666666666666666640 0 0 0 1\n",
      "0 0 0 1 0\n",
      "0 0 1 0 0\n",
      "0 1 0 0 0\n",
      "1 0 0 0 037777777777777777775:384 A.6. Matrix Decompositions\n",
      "Clearly, Pk=P>\n",
      "kandPkPk=Ikhold, so that in fact Pkis an orthogonal matrix.\n",
      "We can solve the n\u0002nlinear system Anxn=aninO(n2) time recursively, as follows.\n",
      "Assume that we have somehow solved for the upper k\u0002kblock Akxk=akand now we\n",
      "wish to solve for the ( k+1)\u0002(k+1) block:\n",
      "Ak+1xk+1=ak+1()\"AkPkak\n",
      "a>\n",
      "kPk 1#\"z\n",
      "\u000b#\n",
      "=\"ak\n",
      "ak+1#\n",
      ":\n",
      "Therefore,\n",
      "\u000b=ak+1\u0000a>\n",
      "kPkz\n",
      "Akz=ak\u0000\u000bPkak:\n",
      "Since A\u00001\n",
      "kPk=PkA\u00001\n",
      "k, the second equation above simpliﬁes to\n",
      "z=A\u00001\n",
      "kak\u0000\u000bA\u00001\n",
      "kPkak\n",
      "=xk\u0000\u000bPkxk:\n",
      "Substituting z=xk\u0000\u000bPkxkinto\u000b=ak+1\u0000a>\n",
      "kPkzand solving for \u000byields:\n",
      "\u000b=ak+1\u0000a>\n",
      "kPkxk\n",
      "1\u0000a>\n",
      "kxk:\n",
      "Finally, with the value of \u000bcomputed above, we have\n",
      "xk+1=\"xk\u0000\u000bPkxk\n",
      "\u000b#\n",
      ":\n",
      "This gives the following Levinson–Durbin Levinson –\n",
      "Durbinrecursive algorithm for solving Anxn=an.\n",
      "Algorithm A.6.3: Levinson–Durbin Recursion for Solving Anxn=an\n",
      "input: First row [1 ;a1;:::; an\u00001]=[1;a>\n",
      "n\u00001] of matrix An.\n",
      "output: Solution xn=A\u00001\n",
      "nan.\n",
      "1x1 a1\n",
      "2fork=1;:::; n\u00001do\n",
      "3\fk 1\u0000a>\n",
      "kxk\n",
      "4 ˘x [xk;k;xk;k\u00001;:::; xk;1]>\n",
      "5\u000b (ak+1\u0000a>\n",
      "k˘x)=\fk\n",
      "6 xk+1 \"xk\u0000\u000b˘x\n",
      "\u000b#\n",
      "7return xn\n",
      "In the algorithm above, we have identiﬁed xk=[xk;1;xk;2;:::; xk;k]>. The advantage of\n",
      "the Levinson–Durbin algorithm is that its running cost is O(n2), instead of the usual O(n3).\n",
      "Using thefxk;\fkgcomputed in Algorithm A.6.3, we construct the following lower tri-\n",
      "angular matrix recursively, setting L1=1 and\n",
      "Lk+1=\"Lk 0k\n",
      "\u0000(Pkxk)>1#\n",
      "; k=1;:::; n\u00001: (A.27)Appendix A. Linear Algebra and Functional Analysis 385\n",
      "Then, we have the following factorization of An.\n",
      "Theorem A.14: Diagonalization of Toeplitz Correlation Matrix A n\n",
      "For a real-valued symmetric positive-deﬁnite Toeplitz matrix Anof the form (A.26),\n",
      "we have\n",
      "LnAnL>\n",
      "n=Dn;\n",
      "where Lnis a the lower diagonal matrix (A.27) and Dn:=diag(1;\f1;:::;\f n\u00001) is a\n",
      "diagonal matrix.\n",
      "Proof: We give a proof by induction. Obviously, L1A1L>\n",
      "1=1\u00011\u00011=1=D1is true. Next,\n",
      "assume that the factorization LkAkL>\n",
      "k=Dkholds for a given k. Observe that\n",
      "Lk+1Ak+1=\"Lk 0k\n",
      "\u0000(Pkxk)>1#\"AkPkak\n",
      "a>\n",
      "kPk 1#\n",
      "=\"LkAk; LkPkak\n",
      "\u0000(Pkxk)>Ak+a>\n",
      "kPk;\u0000(Pkxk)>Pkak+1#\n",
      ":\n",
      "It is straightforward to verify that [ \u0000(Pkxk)>Ak+a>\n",
      "kPk;\u0000(Pkxk)>Pkak+1]=[0>\n",
      "k;\fk],\n",
      "yielding the recursion\n",
      "Lk+1Ak+1=\"LkAkLkPkak\n",
      "0>\n",
      "k\fk#\n",
      ":\n",
      "Secondly, observe that\n",
      "Lk+1Ak+1L>\n",
      "k+1=\"LkAkLkPkak\n",
      "0>\n",
      "k\fk#\"L>\n",
      "k\u0000Pkxk\n",
      "0>\n",
      "k 1#\n",
      "=\"LkAkL>\n",
      "k;\u0000LkAkPkxk+LkPkak\n",
      "0>\n",
      "k; \f k#\n",
      ":\n",
      "By noting that AkPkxk=PkPkAkPkxk=PkAkxk=Pkak, we obtain:\n",
      "Lk+1Ak+1L>\n",
      "k+1=\"LkAkL>\n",
      "k0k\n",
      "0>\n",
      "k\fk#\n",
      ":\n",
      "Hence, the result follows by induction. \u0003\n",
      "Example A.11 (Solving A nxn=bninO(n2)Time) One application of the factoriza-\n",
      "tion in Theorem A.14 is in the fast solution of a linear system Anxn=bn, where the right-\n",
      "hand side is an arbitrary vector bn. Since the solution xncan be written as\n",
      "xn=A\u00001\n",
      "nbn=L>\n",
      "nD\u00001\n",
      "nLnbn;\n",
      "we can compute xninO(n2) time, as follows.\n",
      "Algorithm A.6.4: Solving Anxn=bnfor a General Right-Hand Side\n",
      "input: First row [1 ;a>\n",
      "n\u00001] of matrix Anand right-hand side bn.\n",
      "output: Solution xn=A\u00001\n",
      "nbn.\n",
      "1Compute Lnin (A.27) and the numbers \f1;:::;\f n\u00001via Algorithm A.6.3.\n",
      "2[x1;:::; xn]> Lnbn(computed inO(n2) time)\n",
      "3xi xi=\fi\u00001fori=2;:::; n(computed inO(n) time)\n",
      "4[x1;:::; xn] [x1;:::; xn]Ln(computed inO(n2) time)\n",
      "5return xn [x1;:::; xn]>386 A.7. Functional Analysis\n",
      "Note that it is possible to avoid the explicit construction of the lower triangular matrix\n",
      "in (A.27) via the following modiﬁcation of Algorithm A.6.3, which only stores an extra\n",
      "vector yat each recursive step of the Levinson–Durbin algorithm.\n",
      "Algorithm A.6.5: Solving Anxn=bnwithO(n) Memory Cost\n",
      "input: First row [1 ;a>\n",
      "n\u00001] of matrix Anand right-hand side bn.\n",
      "output: Solution xn=A\u00001\n",
      "nbn.\n",
      "1x b1\n",
      "2y a1\n",
      "3fork=1;:::; n\u00001do\n",
      "4 ˘x [xk;xk\u00001;:::; x1]\n",
      "5 ˘y [yk;yk\u00001;:::; y1]\n",
      "6\f 1\u0000a>\n",
      "ky\n",
      "7\u000bx (bk+1\u0000b>\n",
      "k˘x)=\f\n",
      "8\u000by (ak+1\u0000a>\n",
      "k˘y)=\f\n",
      "9 x [x\u0000\u000bx˘x;\u000bx]>\n",
      "10 y [y\u0000\u000by˘y;\u000by]>\n",
      "11return x\n",
      "A.7 Functional Analysis\n",
      "Much of the previous theory on Euclidean vector spaces can be generalized to vector spaces\n",
      "offunctions . Every element of a (real-valued) function spaceHis a function from somefunction spacesetXtoR, and elements can be added and scalar multiplied as if they were vectors. In\n",
      "other words, if f2H andg2H, then\u000bf+\fg2H for all\u000b;\f2R. OnHwe can impose\n",
      "an inner product as a mapping h\u0001;\u0001ifromH\u0002H toRthat satisﬁes\n",
      "1.h\u000bf1+\ff2;gi=\u000bhf1;gi+\fhf2;gi;\n",
      "2.hf;gi=hg;fi;\n",
      "3.hf;fi>0;\n",
      "4.hf;fi=0 if and only if f=0 (the zero function).\n",
      "We focus on real-valued function spaces, although the theory for complex-valued\n",
      "function spaces is similar (and sometimes easier), under suitable modiﬁcations (e.g.,\n",
      "hf;gi=hg;fi).\n",
      "Similar to the linear algebra setting in Section A.2, we say that two elements fandg\n",
      "inHareorthogonal to each other with respect to this inner product if hf;gi=0. Given an\n",
      "inner product, we can measure distances between elements of the function space Husing\n",
      "thenorm norm\n",
      "kfk:=p\n",
      "hf;fi:\n",
      "For example, the distance between two functions fmandfnis given bykfm\u0000fnk. The space\n",
      "His said to be complete complete if every sequence of functions f1;f2;:::2H for which\n",
      "kfm\u0000fnk! 0 asm;n!1; (A.28)Appendix A. Linear Algebra and Functional Analysis 387\n",
      "converges to some f2H; that is,kf\u0000fnk! 0 asn!1 . A sequence that satisﬁes (A.28)\n",
      "is called a Cauchy sequence .Cauchy\n",
      "sequence A complete inner product space is called a Hilbert space . The most fundamental Hilbert\n",
      "Hilbert space space of functions is the space L2. An in-depth introduction to L2requires some measure\n",
      "theory [6]. For our purposes, it su \u000eces to assume that X\u0012Rdand that onXameasure measure \u0016is\n",
      "deﬁned which assigns to each suitable4setAa positive number \u0016(A)>0 (e.g., its volume).\n",
      "In many cases of interest \u0016is of the form\n",
      "\u0016(A)=Z\n",
      "Aw(x) dx (A.29)\n",
      "where w>0 is a positive function on Xwhich is called the density density of\u0016with respect to\n",
      "the Lebesgue measure (the natural volume measure on Rd). We write \u0016(dx)=w(x) dxto\n",
      "indicate that \u0016has density w. Another important case is where\n",
      "\u0016(A)=X\n",
      "x2A\\Zdw(x); (A.30)\n",
      "where w>0 is again called the density of \u0016, but now with respect to the counting measure\n",
      "onZd(which counts the points of Zd). Integrals with respect to measures \u0016in (A.29) and\n",
      "(A.30) can now be deﬁned as\n",
      "Z\n",
      "f(x)\u0016(dx)=Z\n",
      "f(x)w(x) dx;\n",
      "and Z\n",
      "f(x)\u0016(dx)=X\n",
      "xf(x)w(x);\n",
      "respectively. We assume for simplicity that \u0016has the form (A.29). For measures of the\n",
      "form (A.30) (so-called discrete measures), replace integrals by sums in what follows.\n",
      "Deﬁnition A.4: L2Space\n",
      "LetXbe a subset of Rdwith measure \u0016(dx)=w(x) dx. The Hilbert space L2(X;\u0016)\n",
      "is the linear space of functions from XtoRthat satisfy\n",
      "Z\n",
      "Xf(x)2w(x) dx<1; (A.31)\n",
      "and with inner product\n",
      "hf;gi=Z\n",
      "Xf(x)g(x)w(x) dx: (A.32)\n",
      "LetHbe a Hilbert space. A set of functions ffi;i2Ig is called an orthonormal system orthonormal\n",
      "systemif\n",
      "4Not all sets have a measure. Suitable sets are Borel sets, which can be thought of as countable unions\n",
      "of rectangles.388 A.7. Functional Analysis\n",
      "1. the norm of every fiis 1; that is,hfi;fii=1 for all i2I,\n",
      "2. theffigare orthogonal; that is, hfi;fji=0 for i,j.\n",
      "It follows then that the ffigare linearly independent; that is, the only linear combinationP\n",
      "j\u000bjfj(x) that is equal to fi(x) for all xis the one where \u000bi=1 and\u000bj=0 for j,i. An\n",
      "orthonormal system ffigis called an orthonormal basis if there is no other f2H that isorthonormal\n",
      "basis orthogonal to all the ffi;i2Ig (other than the zero function). Although the general theory\n",
      "allows for uncountable bases, in practice5the setIis taken to be countable.\n",
      "Example A.12 (Trigonometric Orthonormal Basis) LetHbe the Hilbert space\n",
      "L2((0;2\u0019);\u0016), where\u0016(dx)=w(x) dxandwis the constant function w(x)=1, 0<x<2\u0019.\n",
      "Alternatively, take X=Randwthe indicator function on (0 ;2\u0019). The trigonometric func-\n",
      "tions\n",
      "g0(x)=1p\n",
      "2\u0019;gk(x)=1p\u0019cos(kx);hk(x)=1p\u0019sin(kx);k=1;2;:::\n",
      "form a countable inﬁnite-dimensional orthonormal basis of H.\n",
      "A Hilbert spaceHwith an orthonormal basis ff1;f2;:::gbehaves very similarly to the\n",
      "familiar Euclidean vector space. In particular, every element (i.e., function) f2H can be\n",
      "written as a unique linear combination of the basis vectors:\n",
      "f=X\n",
      "ihf;fiifi; (A.33)\n",
      "exactly as in Theorem A.3. The right-hand side of (A.33) is called a (generalized) Fourier\n",
      "expansion off. Note that such a Fourier expansion does not require a trigonometric basis;Fourier\n",
      "expansion any orthonormal basis will do.\n",
      "Example A.13 (Example A.12 (cont.)) Consider the indicator function f(x)=1f0<\n",
      "x< \u0019g. As the trigonometric functions fgkgandfhkgform a basis for L2((0;2\u0019);1dx), we\n",
      "can write\n",
      "f(x)=a01p\n",
      "2\u0019+1X\n",
      "k=1ak1p\u0019cos(kx)+1X\n",
      "k=1bk1p\u0019sin(kx); (A.34)\n",
      "where a0=R\u0019\n",
      "01=p\n",
      "2\u0019dx=p\u0019=2,ak=R\u0019\n",
      "0cos(kx)=p\u0019dxandbk=R\u0019\n",
      "0sin(kx)=p\u0019dx,k=\n",
      "1;2;:::. This means that ak=0 for all k,bk=0 for even k, and bk=2=(kp\u0019) for odd k.\n",
      "Consequently,\n",
      "f(x)=1\n",
      "2+2\n",
      "\u00191X\n",
      "k=1sin(kx)\n",
      "k: (A.35)\n",
      "Figure A.7 shows several Fourier approximations obtained by truncating the inﬁnite sum\n",
      "in (A.35).\n",
      "5The function spaces typically encountered in machine learning and data science are usually separable\n",
      "spaces , which allows for the set Ito be considered countable; see, e.g., [106].Appendix A. Linear Algebra and Functional Analysis 389\n",
      "0 0.5 1 1.5 2 2.5 3 3.500.51\n",
      "Figure A.7: Fourier approximations of the unit step function fon the interval (0 ;\u0019), trun-\n",
      "cating the inﬁnite sum in (A.35) to i=2, 4, and 14 terms, giving the dotted blue, dashed\n",
      "red, and solid green curves, respectively.\n",
      "Starting from any countable basis, we can use the Gram–Schmidt procedure to obtain +377\n",
      "an orthonormal basis, as illustrated in the following example.\n",
      "Example A.14 (Legendre Polynomials) Take the function space L2(R;w(x) dx), where\n",
      "w(x)=1f\u00001<x<1g. We wish to construct an orthonormal basis of polynomial functions\n",
      "g0;g1;g2;:::, starting from the collection of monomials: \u00130;\u00131;\u00132;:::, where\u0013k:x7!xk. Us-\n",
      "ing Gram–Schmidt, the ﬁrst normalized zero-degree polynomial is g0=\u00130=k\u00130k=p1=2.\n",
      "To ﬁnd g1(a polynomial of degree 1), project \u00131(the identity function) onto the space\n",
      "spanned by g0. The resulting projection is p1:=hg0;\u00131ig0, written out as\n",
      "p1(x)= Z1\n",
      "\u00001x g0(x) dx!\n",
      "g0(x)=1\n",
      "2Z1\n",
      "\u00001xdx=0:\n",
      "Hence, g1=(\u00131\u0000p1)=k\u00131\u0000p1kis a linear function; that is, of the form g1(x)=ax. The\n",
      "constant ais found by normalization:\n",
      "1=kg1k2=Z1\n",
      "\u00001g2\n",
      "1(x) dx=a2Z1\n",
      "\u00001x2dx=a22\n",
      "3;\n",
      "so that g1(x)=p3=2x. Continuing the Gram–Schmidt procedure, we ﬁnd g2(x)=p5=8(3x2\u00001);g3(x)=p7=8(5x3\u00003x) and, in general,\n",
      "gk(x)=p\n",
      "2k+1\n",
      "2k+1\n",
      "2k!dk\n",
      "dxk(x2\u00001)k;k=0;1;2;::::\n",
      "These are the (normalized) Legendre polynomials . The graphs of g0;g1;g2;andg3are givenLegendre\n",
      "polynomials in Figure A.8.390 A.7. Functional Analysis\n",
      "-1 -0.5 0 0.5 1\n",
      "-11\n",
      "Figure A.8: The ﬁrst 4 normalized Legendre polynomials.\n",
      "As the Legendre polynomials form an orthonormal basis of L2(R;1f\u00001<x<1gdx),\n",
      "they can be used to approximate arbitrary functions in this space. For example, Figure A.9\n",
      "shows an approximation using the ﬁrst 51 Legendre polynomials ( k=0;1;:::; 50) of the\n",
      "Fourier expansion of the indicator function on the interval ( \u00001=2;1=2). These Legendre\n",
      "polynomials form the basis of a 51-dimensional linear subspace onto which the indicator\n",
      "function is orthogonally projected.\n",
      "-1 -0.5 0 0.5 100.20.40.60.81\n",
      "Figure A.9: Approximation of the indicator function on the interval ( \u00001=2;1=2), using the\n",
      "Legendre polynomials g0;g1;:::; g50.\n",
      "The Legendre polynomials were produced in the following way: We started with an\n",
      "unnormalized probability density on R— in this case the probability density of the uniform +426\n",
      "distribution on (\u00001;1). We then constructed a sequence of polynomials by applying the\n",
      "Gram–Schmidt procedure to the monomials 1 ;x;x2;:::.\n",
      "By using exactly the same procedure, but with a di \u000berent probability density, we can\n",
      "produce other such orthogonal polynomials . For example, the density of the standard expo-orthogonal\n",
      "polynomials nential6distribution, w(x)=e\u0000x;x>0, gives the Laguerre polynomials , which are deﬁned\n",
      "Laguerre\n",
      "polynomials\n",
      "6This can be further generalized to the density of a gamma distribution.Appendix A. Linear Algebra and Functional Analysis 391\n",
      "by the recurrence\n",
      "(n+1)gn+1(x)=(2n+1\u0000x)gn(x)\u0000ngn\u00001(x);n=1;2;:::;\n",
      "with g0(x)=1 and g1(x)=1\u0000x, for x>0. The Hermite polynomials Hermite\n",
      "polynomialsare obtained when\n",
      "using instead the density of the standard normal distribution: w(x)=e\u0000x2=2=p\n",
      "2\u0019;x2R.\n",
      "These polynomials satisfy the recursion\n",
      "gn+1(x)=xgn(x)\u0000dgn(x)\n",
      "dx;n=0;1;:::;\n",
      "with g0(x)=1,x2R. Note that the Hermite polynomials as deﬁned above have not been\n",
      "normalized to have norm 1. To normalize, use the fact that kgnk2=n!.\n",
      "We conclude with a number of key results in functional analysis. The ﬁrst one is the\n",
      "celebrated Cauchy–Schwarz Cauchy –\n",
      "Schwarzinequality.\n",
      "Theorem A.15: Cauchy–Schwarz\n",
      "LetHbe a Hilbert space. For every f;g2H it holds that\n",
      "jhf;gij6kfkkgk:\n",
      "Proof: The inequality is trivially true for g=0 (zero function). For g,0, we can write\n",
      "f=\u000bg+h, where h?gand\u000b=hf;gi=kgk2. Consequently,kfk2=j\u000bj2kgk2+khk2>\n",
      "j\u000bj2kgk2. The result follows after rearranging this last inequality. \u0003\n",
      "LetVandWbe two linear vector spaces (for example, Hilbert spaces) on which norms\n",
      "k\u0001kVandk\u0001kWare deﬁned. Suppose A:V!W is a mapping from VtoW. When\n",
      "W=V, such a mapping is often called an operator ; whenW=Rit is called a functional .operator\n",
      "functional Mapping Ais said to be linear ifA(\u000bf+\fg)=\u000bA(f)+\fA(g). In this case we write A f\n",
      "<1such thating instead of A(f). If there exists \n",
      "kfkV;f2V; (A.36)\n",
      "for which (A.36) holds is called thebounded smallest \n",
      "mapping norm ofA; denoted bykAk. A (not necessarily linear) mapping A:V!W is said to be\n",
      "norm continuous atfif for any sequence f1;f2;:::converging to fthe sequence A(f1);A(f2);:::\n",
      "continuous\n",
      "mappingconverges to A(f). That is, if\n",
      "8\">0;9\u000e>0 :8g2V;kf\u0000gkV<\u000e)kA(f)\u0000A(g)kW<\": (A.37)\n",
      "If the above property holds for every f2V, then the mapping Aitself is called continuous .\n",
      "Theorem A.16: Continuity and Boundedness for Linear Mappings\n",
      "For a linear mapping, continuity and boundedness are equivalent.\n",
      "Proof: LetAbe linear and bounded. We may assume that Ais non-zero (otherwise the\n",
      "statement holds trivially), and that therefore 0 <kAk<1. Taking\u000e<\"=kAkin (A.37) now\n",
      "ensures thatkA f\u0000AgkW6kAkkf\u0000gkV<kAk\u000e<\" . This shows that Ais continuous.392 A.8. Fourier Transforms\n",
      "Conversely, suppose Ais continuous. In particular, it is continuous at f=0 (the zero-\n",
      "element ofV). Thus, take f=0 and let\"and\u000ebe as in (A.37). For any g,0, let h=\n",
      "\u000e=(2kgkV)g. AskhkV=\u000e=2<\u000e, it follows from (A.37) that\n",
      "kAhkW=\u000e\n",
      "2kgkVkAgkW<\":\n",
      "Rearranging the last inequality gives kAgkW<2\"=\u000ekgkV, showing that Ais bounded.\u0003\n",
      "Theorem A.17: Riesz Representation Theorem\n",
      "Any bounded linear functional \u001eon a Hilbert space Hcan be represented as \u001e(h)=\n",
      "hh;gi, for some g2H (depending on \u001e).\n",
      "Proof: LetPbe the projection of Honto the nullspace Nof\u001e; that is,N=fg2H :\n",
      "\u001e(g)=0g. If\u001eis not the 0-functional, then there exists a g0,0 with\u001e(g0),0. Let\n",
      "g1=g0\u0000Pg0. Then g1?N and\u001e(g1)=\u001e(g0). Take g2=g1=\u001e(g1). For any h2H ,\n",
      "f:=h\u0000\u001e(h)g2lies inN. As g2?N it holds thathf;g2i=0, which is equivalent to\n",
      "hh;g2i=\u001e(h)kg2k2. By deﬁning g=g2=kg2k2we have found our representation. \u0003\n",
      "A.8 Fourier Transforms\n",
      "We will now brieﬂy introduce the Fourier transform. Before doing so, we will extend the\n",
      "concept of L2space of real-valued functions as follows. +387\n",
      "Deﬁnition A.5: LpSpace\n",
      "LetXbe a subset of Rdwith measure \u0016(dx)=w(x) dxandp2[1;1). Then Lp(X;\u0016)\n",
      "is the linear space of functions from XtoCthat satisfy\n",
      "Z\n",
      "Xjf(x)jpw(x) dx<1: (A.38)\n",
      "When p=2,L2(X;\u0016) is in fact a Hilbert space equipped with inner product\n",
      "hf;gi=Z\n",
      "Xf(x)g(x)w(x) dx: (A.39)\n",
      "We are now in a position to deﬁne the Fourier transform (with respect to the Lebesgue\n",
      "measure). Note that in the following Deﬁnitions A.6 and A.7 we have chosen a particular\n",
      "convention. Equivalent (but not identical) deﬁnitions exist that include scaling constants\n",
      "(2\u0019)dor (2\u0019)\u0000dand where\u00002\u0019tis replaced with 2 \u0019t,t, or\u0000t.Appendix A. Linear Algebra and Functional Analysis 393\n",
      "Deﬁnition A.6: (Multivariate) Fourier Transform\n",
      "TheFourier transform Fourier\n",
      "transformF[f] of a (real- or complex-valued) function f2L1(Rd) is\n",
      "the function efdeﬁned as\n",
      "ef(t) :=Z\n",
      "Rde\u0000i 2\u0019t>xf(x) dx;t2Rd:\n",
      "The Fourier transform efis continuous, uniformly bounded (since f2L1(Rd) im-\n",
      "plies thatjef(t)j6R\n",
      "Rdjf(x)jdx<1), and satisﬁes lim ktk!1ef(t)=0 (a result known as\n",
      "theRiemann–Lebesgue lemma ). However,jefjdoes not necessarily have a ﬁnite integ-\n",
      "ral. A simple example in R1is the Fourier transform of f(x)=1f\u00001=2<x<1=2g. Then\n",
      "ef(t)=sin(\u0019t)=(\u0019t)=sinc(\u0019t), which is not absolutely integrable.\n",
      "Deﬁnition A.7: (Multivariate) Inverse Fourier Transform\n",
      "The inverse Fourier transform inverse Fourier\n",
      "transformF\u00001[ef] of a (real- or complex-valued) function\n",
      "ef2L1(Rd) is the function ˘fdeﬁned as\n",
      "˘f(x) :=Z\n",
      "Rdei 2\u0019t>xef(t) dt;x2Rd:\n",
      "As one would hope, it holds that if fandF[f] are both in L1(Rd), then f=F\u00001[F[f]]\n",
      "almost everywhere.\n",
      "The Fourier transform enjoys many interesting and useful properties, some of which\n",
      "we list below.\n",
      "1.Linearity : For f;g2L1(Rd) and constants a;b2R,\n",
      "F[a f+bg]=aF[f]+bF[g]:\n",
      "2.Space Shifting and Scaling : Let A2Rd\u0002dbe an invertible matrix and b2Rda con-\n",
      "stant vector. Let f2L1(Rd) and deﬁne h(x) :=f(Ax+b). Then\n",
      "F[h](t)=ei 2\u0019(A\u0000>t)>bef(A\u0000>t)=jdet(A)j;\n",
      "where A\u0000>:=(A>)\u00001=(A\u00001)>.\n",
      "3.Frequency Shifting and Scaling : Let A2Rd\u0002dbe an invertible matrix and b2Rda\n",
      "constant vector. Let f2L1(Rd) and deﬁne\n",
      "h(x) :=e\u0000i 2\u0019b>A\u0000>xf(A\u0000>x)=jdet(A)j:\n",
      "ThenF[h](t)=ef(At+b).\n",
      "4.Di\u000berentiation : Let f2L1(Rd)\\C1(Rd) and let fk:=@f=@xkbe the partial derivat-\n",
      "ive of fwith respect to xk. Iffk2L1(Rd) for k=1;:::; d, then\n",
      "F[fk](t)=(i 2\u0019tk)ef(t):394 A.8. Fourier Transforms\n",
      "5.Convolution : Let f;g2L1(Rd) be real or complex valued functions. Their convolu-\n",
      "tion, f\u0003g, is deﬁned as\n",
      "(f\u0003g)(x)=Z\n",
      "Rdf(y)g(x\u0000y) dy;\n",
      "and is also in L1(Rd). Moreover, the Fourier transform satisﬁes\n",
      "F[f\u0003g]=F[f]F[g]:\n",
      "6.Duality : Let fandF[f] both be in L1(Rd). ThenF[F[f]](t)=f(\u0000t).\n",
      "7.Product Formula : Let f;g2L1(Rd) and denote by ef;egtheir respective Fourier trans-\n",
      "forms. Then ef g;feg2L1(Rd), and\n",
      "Z\n",
      "Rdef(z)g(z) dz=Z\n",
      "Rdf(z)eg(z) dz:\n",
      "There are many additional properties which hold if f2L1(Rd)\\L2(Rd). In particular,\n",
      "iff;g2L1(Rd)\\L2(Rd), then ef;eg2L2(Rd) andhef;egi=hf;gi, a result often known as\n",
      "Parseval’s formula . Putting g=fgives the result often referred to as Plancherel’s theorem .\n",
      "The Fourier transform can be extended in several ways, in the ﬁrst instance to functions\n",
      "inL2(Rd) by continuity. A substantial extension of the theory is realized by replacing integ-\n",
      "ration with respect to the Lebesgue measure (i.e.,R\n",
      "Rd\u0001\u0001\u0001dx) with integration with respect\n",
      "to a (ﬁnite Borel) measure \u0016(i.e.,R\n",
      "Rd\u0001\u0001\u0001\u0016(dx)). Moreover, there is a close connection\n",
      "between the Fourier transform and characteristic functions arising in probability theory.\n",
      "Indeed, if Xis a random vector with pdf f, then its characteristic function  satisﬁes +443\n",
      " (t) :=Eeit>X=F[f](\u0000t=(2\u0019)):\n",
      "A.8.1 Discrete Fourier Transform\n",
      "Here, we introduce the (univariate) discrete Fourier transform, which can be viewed as a\n",
      "special case of the Fourier transform introduced in Deﬁnition A.6, where d=1, integration\n",
      "is with respect to the counting measure, and f(x)=0 for x<0 and x>(n\u00001).\n",
      "Deﬁnition A.8: Discrete Fourier Transform\n",
      "The discrete Fourier transform discrete\n",
      "Fourier\n",
      "transform(DFT) of a vector x=[x0;:::; xn\u00001]>2Cnis the\n",
      "vectorex=[ex0;:::;exn\u00001]>whose elements are given by\n",
      "ext=n\u00001X\n",
      "s=0!stxs;t=0;:::; n\u00001; (A.40)\n",
      "where!=exp(\u0000i 2\u0019=n).\n",
      "In other words, exis obtained from xvia the linear transformation\n",
      "ex=Fx;Appendix A. Linear Algebra and Functional Analysis 395\n",
      "where\n",
      "F=26666666666666666666641 1 1 ::: 1\n",
      "1! !2::: !n\u00001\n",
      "1!2!4::: !2(n\u00001)\n",
      ":::::::::::::::\n",
      "1!n\u00001!2(n\u00001)::: !(n\u00001)23777777777777777777775:\n",
      "The matrix Fis a so-called Vandermonde matrix , and is clearly symmetric (i.e., F=F>).\n",
      "Moreover, F=pnis in fact a unitary matrix and hence its inverse is simply its complex\n",
      "conjugate F=pn. Thus, F\u00001=F=nand we have that the inverse discrete Fourier transform inverse discrete\n",
      "Fourier\n",
      "transform(IDFT) is given by\n",
      "xt=1\n",
      "nn\u00001X\n",
      "s=0!\u0000stexs;t=0;:::; n\u00001; (A.41)\n",
      "or in terms of matrices and vectors,\n",
      "x=Fex=n:\n",
      "Observe that the IDFT of a vector yis related to the DFT of its complex conjugate y, since\n",
      "Fy=n=Fy=n:\n",
      "Consequently, an IDFT can be computed via a DFT.\n",
      "There is a close connection between circulant matrices Cand the DFT. To make this\n",
      "connection concrete, let Cbe the circulant matrix corresponding to the vector c2Cnand\n",
      "denote by ftthet-th column of the discrete Fourier matrix F,t=0;1;:::; n\u00001. Then, the\n",
      "s-th element of Cftis\n",
      "n\u00001X\n",
      "k=0c(s\u0000k) mod n!tk=n\u00001X\n",
      "y=0cy!t(s\u0000y)=!ts|{z}\n",
      "s-th element of ftn\u00001X\n",
      "y=0cy!\u0000ty\n",
      "|      {z      }\n",
      "\u0015s:\n",
      "Hence, the eigenvalues of Care\n",
      "\u0015t=c>ft;t=0;1;:::; n\u00001;\n",
      "with corresponding eigenvectors ft. Collecting the eigenvalues into the vector \u0015=\n",
      "[\u00150;:::;\u0015 n\u00001]>=Fc, we therefore have the eigen-decomposition\n",
      "C=Fdiag(\u0015)F=n:\n",
      "Consequently, one can compute the circular convolution of a vector a=[a1;:::; an]>\n",
      "andc=[c0;:::; cn\u00001]>by a series of DFTs as follows. Construct the circulant matrix C\n",
      "corresponding to c. Then, the circular convolution of aandcis given by y=Ca. Proceed\n",
      "in four steps:\n",
      "1. Compute z=Fa=n.\n",
      "2. Compute \u0015=Fc.396 A.8. Fourier Transforms\n",
      "3. Compute p=z\f\u0015=[z1\u00150;:::; zn\u0015n\u00001]>.\n",
      "4. Compute y=Fp.\n",
      "Steps 1 and 2 are (up to constants) in the form of an IDFT, and step 4 is in the form of a\n",
      "DFT. These are computable via the FFT (Section A.8.2) in O(nlnn) time. Step 3 is a dot +396\n",
      "product computable in O(n) time. Thus, the circular convolution can be computed with the\n",
      "aid of the FFT inO(nlnn) time.\n",
      "One can also e \u000eciently compute the product of an n\u0002nToeplitz matrix Tand an n\u00021\n",
      "vector ainO(nlnn) time by embedding Tinto a circulant matrix Cof size 2 n\u00022n. Namely,\n",
      "deﬁne\n",
      "C=\"T B\n",
      "B T#\n",
      ";\n",
      "where\n",
      "B=2666666666666666666666640 tn\u00001\u0001\u0001\u0001 t2 t1\n",
      "t\u0000(n\u00001) 0 tn\u00001 t2\n",
      "::: t\u0000(n\u00001) 0::::::\n",
      "t\u00002::::::tn\u00001\n",
      "t\u00001 t\u00002\u0001\u0001\u0001 t\u0000(n\u00001) 0377777777777777777777775:\n",
      "Then a product of the form y=Tacan be computed in O(nlnn) time, since we may write\n",
      "C\"a\n",
      "0#\n",
      "=\"T B\n",
      "B T#\"a\n",
      "0#\n",
      "=\"Ta\n",
      "Ba#\n",
      ":\n",
      "The left-hand side is a product of a 2 n\u00022ncirculant matrix with vector of length 2 n, and\n",
      "so can be computed in O(nlnn) time via the FFT, as previously discussed.\n",
      "Conceptually, one can also solve equations of the form Cx=bfor a given vector b2Cn\n",
      "and circulant matrix C(corresponding to c2Cn, assuming all its eigenvalues are non-zero)\n",
      "via the following four steps:\n",
      "1. Compute z=Fb=n.\n",
      "2. Compute \u0015=Fc.\n",
      "3. Compute p=z=\u0015=[z1=\u00150;:::; zn=\u0015n\u00001]>.\n",
      "4. Compute x=Fp.\n",
      "Once again, Steps 1 and 2 are (up to constants) in the form of an IDFT, and Step 4 is in\n",
      "the form of a DFT, all of which are computable via the FFT in O(nlnn) time, and Step 3\n",
      "is computable inO(n) time, meaning the solution xcan be computed using the FFT in\n",
      "O(nlnn) time.\n",
      "A.8.2 Fast Fourier Transform\n",
      "Thefast Fourier transform fast Fourier\n",
      "transform(FFT) is a numerical algorithm for the fast evaluation of (A.40)\n",
      "and (A.41). By using a divide-and-conquer strategy, the algorithm reduces the compu-\n",
      "tational complexity from O(n2) (for the naïve evaluation of the linear transformation) to\n",
      "O(nlnn) [60].Appendix A. Linear Algebra and Functional Analysis 397\n",
      "The essence of the algorithm lies in the following observation. Suppose n=r1r2. Then\n",
      "one can express any index tappearing in (A.40) via a pair ( t0;t1), with t=t1r1+t0,\n",
      "where t02f0;1;:::; r1\u00001gandt12f0;1;:::; r2\u00001g. Similarly, one can express any index\n",
      "sappearing in (A.40) via a pair ( s0;s1), with s=s1r2+s0, where s02f0;1;:::; r2\u00001gand\n",
      "s12f0;1;:::; r1\u00001g.\n",
      "Identifying ext\u0011ext1;t0andxs\u0011xs1;s0, we may re-express (A.40) as\n",
      "ext1;t0=r2\u00001X\n",
      "s0=0!s0tr1\u00001X\n",
      "s1=0!s1r2txs1;s0;t0=0;1;:::; r1\u00001;t1=0;1;:::; r2\u00001: (A.42)\n",
      "Observe that !s1r2t=!s1r2t0(because!r1r2=1), so that the inner sum over s1depends only\n",
      "ons0andt0. Deﬁne\n",
      "yt0;s0:=r1\u00001X\n",
      "s1=0!s1r2t0xs1;s0;t0=0;1;:::; r1\u00001;s0=0;1;:::; r2\u00001:\n",
      "Computing each yt0;s0requiresO(n r1) operations. In terms of the fyt0;s0g, (A.42) can be\n",
      "written as\n",
      "ext1;t0=r2\u00001X\n",
      "s0=0!s0tyt0;s0;t1=0;1;:::; r2\u00001;t0=0;1;:::; r1\u00001;\n",
      "requiringO(n r2) operations to compute. Thus, calculating the DFT using this two-step\n",
      "procedure requires O(n(r1+r2)) operations, rather than O(n2).\n",
      "Now supposing n=r1r2\u0001\u0001\u0001rm, repeated application the above divide-and-conquer idea\n",
      "yields an m-step procedure requiring O(n(r1+r2+\u0001\u0001\u0001+rm)) operations. In particular, if\n",
      "rk=rfor all k=1;2;:::; m, we have that n=rmandm=logrn, so that the total number\n",
      "of operations isO(r n m )\u0011O(r nlogr(n)). Typically, the radix r is a small (not necessarily\n",
      "prime) number, for instance r=2.\n",
      "Further Reading\n",
      "A good reference book on matrix computations is Golub and Van Loan [52]. A useful list\n",
      "of many common vector and matrix calculus identities can be found in [95]. Strang’s in-\n",
      "troduction to linear algebra [116] is a classic textbook, and his recent book [117] combines\n",
      "linear algebra with the foundations of deep learning. Fast reliable algorithms for matrices\n",
      "with structure can be found in [64]. Kolmogorov and Fomin’s masterpiece on the theory\n",
      "of functions and functional analysis [67] still provides one of the best introductions to the\n",
      "topic. A popular choice for an advanced course in functional analysis is Rudin [106].398APPENDIXB\n",
      "MULTIVARIATE DIFFERENTIATION AND\n",
      "OPTIMIZATION\n",
      "The purpose of this appendix is to review various aspects of multivariate di \u000beren-\n",
      "tiation and optimization. We assume the reader is familiar with di \u000berentiating a real-\n",
      "valued function.\n",
      "B.1 Multivariate Differentiation\n",
      "For a multivariate function fthat maps a vector x=[x1;:::; xn]>to a real number f(x),\n",
      "thepartial derivative with respect to xi, denoted@f\n",
      "@xi, is the derivative taken with respectpartial\n",
      "derivative toxiwhile all other variables are held constant. We can write all the npartial derivatives\n",
      "neatly using the “scalar /vector” derivative notation:\n",
      "scalar /vector:@f\n",
      "@x:=2666666666664@f\n",
      "@x1:::\n",
      "@f\n",
      "@xn3777777777775: (B.1)\n",
      "This vector of partial derivatives is known as the gradient offatxand is sometimes writtengradientasrf(x).\n",
      "Next, suppose that fis a multivalued (vector-valued) function taking values in Rm,\n",
      "deﬁned by\n",
      "x=26666666666666664x1\n",
      "x2\n",
      ":::\n",
      "xn377777777777777757!26666666666666664f1(x)\n",
      "f2(x)\n",
      ":::\n",
      "fm(x)37777777777777775=:f(x):\n",
      "We can compute each of the partial derivatives @fi=@xjand organize them neatly in a “vec-\n",
      "tor/vector” derivative notation:\n",
      "vector /vector:@f\n",
      "@x:=2666666666666666664@f1\n",
      "@x1@f2\n",
      "@x1\u0001\u0001\u0001@fm\n",
      "@x1@f1\n",
      "@x2@f2\n",
      "@x2\u0001\u0001\u0001@fm\n",
      "@x2::::::\u0001\u0001\u0001:::\n",
      "@f1\n",
      "@xn@f2\n",
      "@xn\u0001\u0001\u0001@fm\n",
      "@xn3777777777777777775: (B.2)\n",
      "399400 B.1. Multivariate Differentiation\n",
      "The transpose of this matrix is known as the matrix of Jacobi matrix of Jacobi offatx(sometimes\n",
      "called the Fréchet derivative offatx); that is,\n",
      "Jf(x) :=\"@f\n",
      "@x#>\n",
      "=2666666666666666664@f1\n",
      "@x1@f1\n",
      "@x2\u0001\u0001\u0001@f1\n",
      "@xn@f2\n",
      "@x1@f2\n",
      "@x2\u0001\u0001\u0001@f2\n",
      "@xn::::::\u0001\u0001\u0001:::\n",
      "@fm\n",
      "@x1@fm\n",
      "@x2\u0001\u0001\u0001@fm\n",
      "@xn3777777777777777775: (B.3)\n",
      "If we deﬁne g(x) :=rf(x) and take the “vector /vector” derivative of gwith respect to\n",
      "x, we obtain the matrix of second-order partial derivatives of f:\n",
      "Hf(x) :=@g\n",
      "@x=266666666666666666664@2f\n",
      "@2x1@2f\n",
      "@x1@x2\u0001\u0001\u0001@2f\n",
      "@x1@xm\n",
      "@2f\n",
      "@x2@x1@2f\n",
      "@2x2\u0001\u0001\u0001@2f\n",
      "@x2@xm::::::\u0001\u0001\u0001:::\n",
      "@2f\n",
      "@xm@x1@2f\n",
      "@xm@x2\u0001\u0001\u0001@2f\n",
      "@2xm377777777777777777775; (B.4)\n",
      "which is known as the Hessian matrix Hessian matrix offatx, also denoted asr2f(x). If these second-\n",
      "order partial derivatives are continuous in a region around x, then@f\n",
      "@xi@xj=@f\n",
      "@xj@xiand, hence,\n",
      "the Hessian matrix Hf(x) issymmetric .\n",
      "Finally, note that we can also deﬁne a “scalar /matrix” derivative of ywith respect to\n",
      "X2Rm\u0002nwith ( i;j)-th entry xi j:\n",
      "@y\n",
      "@X:=2666666666666666664@y\n",
      "@x11@y\n",
      "@x12\u0001\u0001\u0001@y\n",
      "@x1n@y\n",
      "@x21@y\n",
      "@x22\u0001\u0001\u0001@y\n",
      "@x2n::::::\u0001\u0001\u0001:::\n",
      "@y\n",
      "@xm1@y\n",
      "@xm2\u0001\u0001\u0001@y\n",
      "@xmn3777777777777777775\n",
      "and a “matrix /scalar” derivative:\n",
      "@X\n",
      "@y:=2666666666666666664@x11\n",
      "@y@x12\n",
      "@y\u0001\u0001\u0001@x1n\n",
      "@y\n",
      "@x21\n",
      "@y@x22\n",
      "@y\u0001\u0001\u0001@x2n\n",
      "@y::::::\u0001\u0001\u0001:::\n",
      "@xm1\n",
      "@y@xm2\n",
      "@y\u0001\u0001\u0001@xmn\n",
      "@y3777777777777777775:\n",
      "Example B.1 (Scalar /Matrix Derivative) Lety=a>Xb, where X2Rm\u0002n,a2Rm,\n",
      "andb2Rn. Since yis a scalar, we can write y=tr(y)=tr(Xba>), using the cyclic property\n",
      "of the trace (see Theorem A.1). Deﬁning C:=ba>, we have +359\n",
      "y=mX\n",
      "i=1[XC]ii=mX\n",
      "i=1nX\n",
      "j=1xi jcji;\n",
      "so that@y=@xi j=cjior, in matrix form,\n",
      "@y\n",
      "@X=C>=ab>:Appendix B. Multivariate Differentiation and Optimization 401\n",
      "Example B.2 (Scalar /Matrix Derivative via the Woodbury Identity) Lety=tr\u0010\n",
      "X\u00001A\u0011\n",
      ",\n",
      "where X;A2Rn\u0002n. We now prove that\n",
      "@y\n",
      "@X=\u0000X\u0000>A>X\u0000>:\n",
      "To show this, apply the Woodbury matrix identity to an inﬁnitesimal perturbation, X+\"U,\n",
      "ofX, and take\"#0 to obtain the following: +373\n",
      "(X+\"U)\u00001\u0000X\u00001\n",
      "\"=\u0000X\u00001U(I+\"X\u00001U)\u00001X\u00001\u0000!\u0000 X\u00001U X\u00001:\n",
      "Therefore, as \"#0\n",
      "tr\u0010\n",
      "(X+\"U)\u00001A\u0011\n",
      "\u0000tr\u0010\n",
      "X\u00001A\u0011\n",
      "\"\u0000!\u0000 tr\u0010\n",
      "X\u00001U X\u00001A\u0011\n",
      "=\u0000tr\u0010\n",
      "U X\u00001AX\u00001\u0011\n",
      ":\n",
      "Now, suppose that Uis an all zero matrix with a one in the ( i;j)-th position. We can write,\n",
      "@y\n",
      "@xi j=lim\n",
      "\"#0tr\u0010\n",
      "(X+\"U)\u00001A\u0011\n",
      "\u0000tr\u0010\n",
      "X\u00001A\u0011\n",
      "\"=\u0000tr\u0010\n",
      "UX\u00001AX\u00001\u0011\n",
      "=\u0000h\n",
      "X\u00001AX\u00001i\n",
      "ji:\n",
      "Therefore,@y\n",
      "@X=\u0000\u0010\n",
      "X\u00001AX\u00001\u0011>.\n",
      "The following two examples specify multivariate derivatives for the important special\n",
      "cases of linear and quadratic functions.\n",
      "Example B.3 (Gradient of a Linear Function) Letf(x)=Axfor some m\u0002nconstant\n",
      "matrix A. Then, its vector /vector derivative (B.2) is the matrix\n",
      "@f\n",
      "@x=A>: (B.5)\n",
      "To see this, let ai jdenote the ( i;j)-th element of A, so that\n",
      "f(x)=Ax=26666666664Pn\n",
      "k=1a1kxk\n",
      ":::Pn\n",
      "k=1amkxk37777777775:\n",
      "To ﬁnd the ( j;i)-th element of@f\n",
      "@x, we di \u000berentiate the i-th element of fwith respect to xj:\n",
      "@fi\n",
      "@xj=@\n",
      "@xjnX\n",
      "k=1aikxk=ai j:\n",
      "In other words, the ( i;j)-th element of@f\n",
      "@xisaji, the ( i;j)-th element of A>.\n",
      "Example B.4 (Gradient and Hessian of a Quadratic Function) Letf(x)=x>Axfor\n",
      "some n\u0002nconstant matrix A. Then,\n",
      "rf(x)=(A+A>)x: (B.6)402 B.1. Multivariate Differentiation\n",
      "It follows immediately that if Aissymmetric , that is, A=A>, thenr(x>Ax)=2Axand\n",
      "r2(x>Ax)=2A.\n",
      "To prove (B.6), ﬁrst observe that f(x)=x>Ax=Pn\n",
      "i=1Pn\n",
      "j=1ai jxixj, which is a quadratic\n",
      "form in x, is real-valued, with\n",
      "@f\n",
      "@xk=@\n",
      "@xknX\n",
      "i=1nX\n",
      "j=1ai jxixj=nX\n",
      "j=1ak jxj+nX\n",
      "i=1aikxi:\n",
      "The ﬁrst term on the right-hand side is equal to the k-th element of Ax, whereas the second\n",
      "term equals the k-th element of x>A;or equivalently the k-th element of A>x.\n",
      "B.1.1 Taylor Expansion\n",
      "The matrix of Jacobi and the Hessian matrix feature prominently in multidimensional\n",
      "Taylor expansions.\n",
      "Theorem B.1: Multidimensional Taylor Expansions\n",
      "LetXbe an open subset of Rnand let a2X. Iff:X!Ris a continuously twice\n",
      "di\u000berentiable function with Jacobian matrix Jf(x) and Hessian matrix Hf(x), then\n",
      "for every x2Xwe have the following ﬁrst- and second-order Taylor expansions:\n",
      "f(x)=f(a)+Jf(a) (x\u0000a)+O(kx\u0000ak2) (B.7)\n",
      "and\n",
      "f(x)=f(a)+Jf(a) (x\u0000a)+1\n",
      "2(x\u0000a)>Hf(a) (x\u0000a)+O(kx\u0000ak3) (B.8)\n",
      "askx\u0000ak! 0. By dropping the Oremainder terms, one obtains the corresponding\n",
      "Taylor approximations.\n",
      "The result is essentially saying that a smooth enough function behaves locally (in the\n",
      "neighborhood of a point x) like a linear and quadratic function. Thus, the gradient or Hes-\n",
      "sian of an approximating linear or quadratic function is a basic building block of many\n",
      "approximation and optimization algorithms.\n",
      "Remark B.1 (Version Without Remainder Terms) An alternative version of Taylor’s\n",
      "theorem states that there exists an a0that lies on the line segment between xandasuch\n",
      "that (B.7) and (B.8) hold without remainder terms, with Jf(a) in (B.7) replaced by Jf(a0)\n",
      "andHf(a) in (B.8) replaced by Hf(a0).\n",
      "B.1.2 Chain Rule\n",
      "Consider the functions f:Rk!Rmandg:Rm!Rn. The function x7!g(f(x)) is called\n",
      "thecomposition composition ofgandf, written as g\u000ef, and is a function from RktoRn. Suppose\n",
      "y=f(x) and z=g(y), as in Figure B.1. Let Jf(x) and Jg(y) be the (Fréchet) derivatives\n",
      "off(atx) and g(aty), respectively. We may think of Jf(x) as the matrix that describesAppendix B. Multivariate Differentiation and Optimization 403\n",
      "how, in a neighborhood of x, the function fcan be approximated by a linear function:\n",
      "f(x+h)\u0019f(x)+Jf(x)h, and similarly for Jg(y). The well-known chain rule chain rule of calculus\n",
      "simply states that the derivative of the composition g\u000efis the matrix product of the\n",
      "derivatives of gandf; that is,\n",
      "Jg\u000ef(x)=Jg(y)Jf(x):\n",
      "g◦f\n",
      "x yz\n",
      "fg\n",
      "RnRmRk\n",
      "Figure B.1: Function composition. The blue arrows symbolize the linear mappings.\n",
      "In terms of our vector /vector derivative notation, we have\n",
      "\"@z\n",
      "@x#>\n",
      "=\"@z\n",
      "@y#>\"@y\n",
      "@x#>\n",
      "or, more simply,\n",
      "@z\n",
      "@x=@y\n",
      "@x@z\n",
      "@y: (B.9)\n",
      "In a similar way we can establish a scalar /matrix chain rule. In particular, suppose Xis\n",
      "ann\u0002pmatrix, which is mapped to y:=X\u000bfor a ﬁxed p-dimensional vector \u000b. In turn, y\n",
      "is mapped to a scalar z:=g(y) for some function g. Denote the columns of Xbyx1;:::; xp.\n",
      "Then,\n",
      "y=X\u000b=pX\n",
      "j=1\u000bjxj;\n",
      "and, therefore, @y=@xj=\u000bjIn. It follows by the chain rule (B.9) that\n",
      "@z\n",
      "@xi=@y\n",
      "@xi@z\n",
      "@y=\u000biIn@z\n",
      "@y=\u000bi@z\n",
      "@y:\n",
      "Therefore,\n",
      "@z\n",
      "@X=h@z\n",
      "@x1;:::;@z\n",
      "@xpi\n",
      "=h\n",
      "\u000b1@z\n",
      "@y;:::;\u000b p@z\n",
      "@yi\n",
      "=@z\n",
      "@y\u000b>: (B.10)\n",
      "Example B.5 (Derivative of the Log-Determinant) Suppose we are given a positive\n",
      "deﬁnite matrix A2Rp\u0002pand wish to compute the scalar /matrix derivative@lnjAj\n",
      "@A. The result\n",
      "is@lnjAj\n",
      "@A=A\u00001:\n",
      "To see this, we can reason as follows. By Theorem A.8, we can write A=Q D Q>, where +368404 B.2. Optimization Theory\n",
      "Qis an orthogonal matrix and D=diag(\u00151;:::;\u0015 p) is the diagonal matrix of eigenvalues of\n",
      "A. The eigenvalues are strictly positive, since Ais positive deﬁnite. Denoting the columns\n",
      "ofQby (qi), we have\n",
      "\u0015i=q>\n",
      "iAqi=tr\u0000qiAq>\n",
      "i\u0001;i=1;:::; p: (B.11)\n",
      "From the properties of determinants, we have y:=lnjAj=lnjQ D Q>j=ln(jQjjDjjQ>j)=\n",
      "lnjDj=Pp\n",
      "i=1ln\u0015i. We can thus write\n",
      "@lnjAj\n",
      "@A=pX\n",
      "i=1@ln\u0015i\n",
      "@A=pX\n",
      "i=1@\u0015i\n",
      "@A@ln\u0015i\n",
      "@\u0015i=pX\n",
      "i=1@\u0015i\n",
      "@A1\n",
      "\u0015i;\n",
      "where the second equation follows from the chain rule applied to the function composition\n",
      "A7!\u0015i7!y. From (B.11) and Example B.1 we have @\u0015i=@A=qiq>\n",
      "i. It follows that\n",
      "@y\n",
      "@A=pX\n",
      "i=1qiq>\n",
      "i1\n",
      "\u0015i=Q D\u00001Q>=A\u00001:\n",
      "B.2 Optimization Theory\n",
      "Optimization is concerned with ﬁnding minimal or maximal solutions of a real-valued\n",
      "objective function objective\n",
      "functionfin some setX:\n",
      "min\n",
      "x2Xf(x) or max\n",
      "x2Xf(x): (B.12)\n",
      "Since any maximization problem can easily be converted into a minimization problem via\n",
      "the equivalence max xf(x)\u0011\u0000min x\u0000f(x), we focus only on minimization problems. We\n",
      "use the following terminology. A local minimizer local minimizer off(x) is an element x\u00032Xsuch that\n",
      "f(x\u0003)6f(x) for all xin some neighborhood of x\u0003. Iff(x\u0003)6f(x) for all x2X, then x\u0003is\n",
      "called a global minimizer global\n",
      "minimizerorglobal solution . The set of global minimizers is denoted by\n",
      "argmin\n",
      "x2Xf(x):\n",
      "The function value f(x\u0003) corresponding to a local /global minimizer x\u0003is referred to as the\n",
      "local/global minimum local/global\n",
      "minimumoff(x).\n",
      "Optimization problems may be classiﬁed by the set Xand the objective function f.\n",
      "IfXis countable, the optimization problem is called discrete orcombinatorial . If instead\n",
      "Xis a nondenumerable set such as Rnand ftakes values in a nondenumerable set, then\n",
      "the problem is said to be continuous . Optimization problems that are neither discrete nor\n",
      "continuous are said to be mixed .\n",
      "The search setXis often deﬁned by means of constraints . A standard setting for con-\n",
      "strained optimization (minimization) is the following:\n",
      "min\n",
      "x2Yf(x)\n",
      "subject to: hi(x)=0;i=1;:::; m;\n",
      "gi(x)60;i=1;:::; k:(B.13)Appendix B. Multivariate Differentiation and Optimization 405\n",
      "Here, fis the objective function, and fgigandfhigare given functions so that hi(x)=0\n",
      "andgi(x)60 represent the equality andinequality constraints, respectively. The region\n",
      "X\u0012Y where the objective function is deﬁned and where all the constraints are satisﬁed\n",
      "is called the feasible region feasible region . An optimization problem without constraints is said to be an\n",
      "unconstrained problem.\n",
      "For an unconstrained continuous optimization problem, the search space Xis often\n",
      "taken to be (a subset of) Rn, and fis assumed to be a Ckfunction for su \u000eciently high\n",
      "k(typically k=2 or 3 su \u000eces); that is, its k-th order derivative is continuous. For a C1\n",
      "function the standard approach to minimizing f(x) is to solve the equation\n",
      "rf(x)=0; (B.14)\n",
      "whererf(x) is the gradient offatx. The solutions x\u0003to (B.14) are called station- +399\n",
      "ary points stationary\n",
      "points. Stationary points can be local /global minimizers, local /global maximizers, or\n",
      "saddle points (which are neither). If, in addition, the function is C2, the conditionsaddle points\n",
      "y>(r2f(x\u0003))y>0 for all y,0 (B.15)\n",
      "ensures that the stationary point x\u0003is a local minimizer of f. The condition (B.15) states\n",
      "that the Hessian matrix of fatx\u0003ispositive deﬁnite . Recall that we write H\u001f0 to indicate +400\n",
      "that a matrix His positive deﬁnite.\n",
      "In Figure B.2 we have a multiextremal objective function on X=R. There are four\n",
      "stationary points: two are local minimizers, one is a local maximizer, and one is neither a\n",
      "minimizer nor a maximizer, but a saddle point.\n",
      "xLocal minimumLocal maximum\n",
      "Global minimumSaddle pointf(x)\n",
      "Figure B.2: A multiextremal objective function in one dimension.\n",
      "B.2.1 Convexity and Optimization\n",
      "An important class of optimization problems is related to the notion of convexity . A setX\n",
      "is said to be convex if for all x1;x22Xit holds that \u000bx1+(1\u0000\u000b)x22Xfor all 06\u000b61.\n",
      "In addition, the objective function fis aconvex function convex\n",
      "functionprovided that for each xin the\n",
      "interior ofXthere exists a vector vsuch that\n",
      "f(y)>f(x)+(y\u0000x)>v;y2X: (B.16)406 B.2. Optimization Theory\n",
      "The vector vin (B.16) may not be unique and is referred to as a subgradient off.subgradientOne of the crucial properties of a convex function fis that Jensen’s inequality holds\n",
      "(see Exercise 14 in Chapter 2): +62\n",
      "Ef(X)>f(EX);\n",
      "for any random vector X.\n",
      "Example B.6 (Convexity and Directional Derivative) Thedirectional derivative directional\n",
      "derivativeof a\n",
      "multivariate function fatxin the direction dis deﬁned as the right derivative of g(t) :=\n",
      "f(x+td) att=0:\n",
      "lim\n",
      "t#0f(x+td)\u0000f(x)\n",
      "t=lim\n",
      "t\"1t(f(x+d=t)\u0000f(x)):\n",
      "This right derivative may not always exist. However, if fis a convex function, then the\n",
      "directional derivative of fatxin the interior of its domain always exists (in any direction\n",
      "d).\n",
      "To see this, let t1>t2>0. By Jensen’s inequality we have for any xandyin the interior\n",
      "of the domain:\n",
      "t2\n",
      "t1f(y)+ \n",
      "1\u0000t2\n",
      "t1!\n",
      "f(x)>f t2\n",
      "t1y+ \n",
      "1\u0000t2\n",
      "t1!\n",
      "x!\n",
      ":\n",
      "Making the substitution y=x+t1dand rearranging the last equation yields:\n",
      "f(x+t1d)\u0000f(x)\n",
      "t1>f(x+t2d)\u0000f(x)\n",
      "t2:\n",
      "In other words, the function t7!(f(x+td)\u0000f(x))=tis increasing for t>0 and therefore\n",
      "the directional derivative satisﬁes:\n",
      "lim\n",
      "t#0f(x+td)\u0000f(x)\n",
      "t=inf\n",
      "t>0f(x+td)\u0000f(x)\n",
      "t:\n",
      "Hence, to show existence it is enough to show that ( f(x+td)\u0000f(x))=tis bounded from\n",
      "below.\n",
      "Since xlies in the interior of the domain of f, we can choose tsmall enough so that\n",
      "x+tdalso lies in the interior. Therefore, the convexity of fimplies that there exists a\n",
      "subgradient vector vsuch that f(x+td)>f(x)+v>(td). In other words,\n",
      "f(x+td)\u0000f(x)\n",
      "t>v>d\n",
      "provides a lower bound for all t>0, and the directional derivative of fat an interior x\n",
      "always exists (in any direction).\n",
      "A function fsatisfying (B.16) with strict inequality is said to be strictly convex . It is\n",
      "said to be a (strictly) concave function concave\n",
      "functionif\u0000fis (strictly) convex. Assuming that Xis an\n",
      "open set, convexity for f2C1is equivalent to\n",
      "f(y)>f(x)+(y\u0000x)>rf(x) for all x;y2X:\n",
      "Moreover, for f2C2strict convexity is equivalent to the Hessian matrix being positive\n",
      "deﬁnite for all x2X, and convexity is equivalent to the Hessian matrix being positive\n",
      "semideﬁnite for all x; that is, y>\u0010\n",
      "r2f(x)\u0011\n",
      "y>0 for all yandx. Recall that we write H\u00170\n",
      "to indicate that a matrix His positive semideﬁnite. +369Appendix B. Multivariate Differentiation and Optimization 407\n",
      "Example B.7 (Convexity and Di \u000berentiability) Iffis a continuously di \u000berentiable\n",
      "multivariate function, then fis convex if and only if the univariate function\n",
      "g(t) :=f(x+td);t2[0;1]\n",
      "is a convex function for any xandx+din the interior of the domain of f. This property\n",
      "provides an alternative deﬁnition for convexity of a multivariate and di \u000berentiable function.\n",
      "To see why it is true, ﬁrst assume that fis convex and t1;t22[0;1]. Then, using the\n",
      "subgradient deﬁnition of convexity in (B.16), we have f(a)>f(b)+(a\u0000b)>vfor some\n",
      "subgradient v. Substituting with a=x+t1dandb=x+t2d, we obtain\n",
      "g(t1)>g(t2)+(t1\u0000t2)v>d\n",
      "for any two points t1;t22[0;1]. Therefore, gis convex, because we have identiﬁed the\n",
      "existence of a subgradient v>dfor each t2.\n",
      "Conversely, assume that gis convex for t2[0;1]. Since fis di\u000berentiable, then so is g.\n",
      "Then, the convexity of gimplies that there is a subgradient vat 0 such that: g(t)>g(0)+t v\n",
      "for all t2[0;1]. Rearranging,\n",
      "v>g(t)\u0000g(0)\n",
      "t;\n",
      "and taking the right limit as t#0 we obtain v>g0(0)=d>rf(x):Therefore,\n",
      "g(t)>g(0)+t v>g(0)+td>rf(x)\n",
      "and substituting t=1 yields:\n",
      "f(x+d)>f(x)+d>rf(x);\n",
      "so that there exists a subgradient vector, namely rf(x), for each x. Hence, fis convex by\n",
      "the deﬁnition in (B.16).\n",
      "An optimization program of the form (B.13) is said to be a convex programming prob-\n",
      "lemif:convex\n",
      "programming\n",
      "problem1. The objective fis aconvex function .\n",
      "2. The inequality constraint functions fgigare convex.\n",
      "3. The equality constraint functions fhigarea\u000ene, that is, of the form a>\n",
      "ix\u0000bi. This is\n",
      "equivalent to both hiand\u0000hibeing convex for all i.\n",
      "Table B.1 summarizes some commonly encountered problems, all of which are convex,\n",
      "with the exception of the quadratic programs with A\u000f0.408 B.2. Optimization Theory\n",
      "Table B.1: Some common classes of optimization problems.\n",
      "Name f(x) Constraints\n",
      "Linear Program (LP) c>x Ax=bandx>0\n",
      "Inequality Form LP c>x Ax6b\n",
      "Quadratic Program (QP)1\n",
      "2x>Ax+b>xDx6d,Ex=e\n",
      "Convex QP1\n",
      "2x>Ax+b>xDx6d,Ex=e(A\u00170)\n",
      "Convex Program f(x) convexfgi(x)gconvex,fhi(x)gof the form a>\n",
      "ix\u0000bi\n",
      "Recognizing convex optimization problems or those that can be transformed to convex\n",
      "optimization problems can be challenging. However, once formulated as convex optimiz-\n",
      "ation problems, these can be e \u000eciently solved using subgradient [112], bundle [57], and\n",
      "cutting-plane methods [59].\n",
      "B.2.2 Lagrangian Method\n",
      "The main components of the Lagrangian method are the Lagrange multipliers and the\n",
      "Lagrange function. The method was developed by Lagrange in 1797 for the optimization\n",
      "problem (B.13) with only equality constraints. In 1951 Kuhn and Tucker extended Lag-\n",
      "range’s method to inequality constraints. Given an optimization problem (B.13) containing\n",
      "only equality constraints hi(x)=0;i=1;:::; m, the Lagrange function Lagrange\n",
      "functionis deﬁned as\n",
      "L(x;\f)=f(x)+mX\n",
      "i=1\fihi(x);\n",
      "where the coe \u000ecientsf\figare called the Lagrange multipliers Lagrange\n",
      "multipliers. A necessary condition for a\n",
      "point x\u0003to be a local minimizer of f(x) subject to the equality constraints hi(x)=0;i=\n",
      "1;:::; m, is\n",
      "rxL(x\u0003;\f\u0003)=0;\n",
      "r\fL(x\u0003;\f\u0003)=0;\n",
      "for some value \f\u0003. The above conditions are also su \u000ecient ifL(x;\f\u0003) is a convex function\n",
      "ofx.\n",
      "Given the original optimization problem (B.13), containing both the equality and in-\n",
      "equality constraints, the generalized Lagrange function , orLagrangian Lagrangian , is deﬁned as\n",
      "L(x;\u000b;\f)=f(x)+kX\n",
      "i=1\u000bigi(x)+mX\n",
      "i=1\fihi(x):Appendix B. Multivariate Differentiation and Optimization 409\n",
      "Theorem B.2: Karush–Kuhn–Tucker (KKT) Conditions\n",
      "A necessary condition for a point x\u0003to be a local minimizer of f(x) in the optimiz-\n",
      "ation problem (B.13) is the existence of an \u000b\u0003and\f\u0003such that\n",
      "rxL(x\u0003;\u000b\u0003;\f\u0003)=0;\n",
      "r\fL(x\u0003;\u000b\u0003;\f\u0003)=0;\n",
      "gi(x\u0003)60;i=1;:::; k;\n",
      "\u000b\u0003\n",
      "i>0;i=1;:::; k;\n",
      "\u000b\u0003\n",
      "igi(x\u0003)=0;i=1;:::; k:\n",
      "Forconvex programs we have the following important results [18, 43]:\n",
      "1. Every local solution x\u0003to a convex programming problem is a global solution and\n",
      "the set of global solutions is convex. If, in addition, the objective function is strictly\n",
      "convex, then any global solution is unique.\n",
      "2. For a strictly convex programming problem with C1objective and constraint func-\n",
      "tions, the KKT conditions are necessary and su \u000ecient for a unique global solution.\n",
      "B.2.3 Duality\n",
      "The aim of duality is to provide an alternative formulation of an optimization problem\n",
      "which is often more computationally e \u000ecient or has some theoretical signiﬁcance (see [43,\n",
      "Page 219]). The original problem (B.13) is referred to as the primal primal problem whereas the\n",
      "reformulated problem, based on Lagrange multipliers, is called the dual dual problem. Duality\n",
      "theory is most relevant to convex optimization problems. It is well known that if the primal\n",
      "optimization problem is (strictly) convex then the dual problem is (strictly) concave and\n",
      "has a (unique) solution from which the (unique) optimal primal solution can be deduced.\n",
      "TheLagrange dual program Lagrange dual\n",
      "program(also called the Wolfe dual ) of the primal program (B.13),\n",
      "is:\n",
      "max\n",
      "\u000b;\fL\u0003(\u000b;\f)\n",
      "subject to: \u000b>0;\n",
      "whereL\u0003is the Lagrange dual function :\n",
      "L\u0003(\u000b;\f)=inf\n",
      "x2XL(x;\u000b;\f); (B.17)\n",
      "giving the greatest lower bound (inﬁmum) of L(x;\u000b;\f) over all possible x2X.\n",
      "It is not di \u000ecult to see that if f\u0003is the minimal value of the primal problem, then\n",
      "L\u0003(\u000b;\f)6f\u0003for any\u000b>0and any\f. This property is called weak duality . The Lag-\n",
      "rangian dual program thus determines the best lower bound on f\u0003. Ifd\u0003is the optimal\n",
      "value for the dual problem then d\u00036f\u0003. The di \u000berence f\u0003\u0000d\u0003is called the duality gap .\n",
      "The duality gap is extremely useful for providing lower bounds for the solutions of\n",
      "primal problems that may be impossible to solve directly. It is important to note that for410 B.3. Numerical Root-Finding and Minimization\n",
      "linearly constrained problems, if the primal is infeasible (does not have a solution satisfying\n",
      "the constraints), then the dual is either infeasible or unbounded. Conversely, if the dual\n",
      "is infeasible then the primal has no solution. Of crucial importance is the strong duality strong duality\n",
      "theorem, which states that for convex programs (B.13) with linear constrained functions hi\n",
      "andgithe duality gap is zero, and any x\u0003and (\u000b\u0003;\f\u0003) satisfying the KKT conditions are\n",
      "(global) solutions to the primal and dual programs, respectively. In particular, this holds for\n",
      "linear and convex quadratic programs (note that not all quadratic programs are convex).\n",
      "For a convex primal program with C1objective and constraint functions, the Lagrangian\n",
      "dual function (B.17) can be obtained by simply setting the gradient (with respect to x) of\n",
      "the LagrangianL(x;\u000b;\f) to zero. One can further simplify the dual program by substitut-\n",
      "ing into the Lagrangian the relations between the variables thus obtained.\n",
      "Further, for a convex primal problem, if there is a strictly feasible pointex(that is, a\n",
      "feasible point satisfying all of the inequality constraints with strict inequality), then the\n",
      "duality gap is zero, and strong duality holds. This is known as Slater’s condition [18, Page\n",
      "226].\n",
      "The Lagrange dual problem is an important example of a saddle-point problem ormin-\n",
      "imax problem. In such problems the aim is to locate a point ( x\u0003;y\u0003)2X\u0002Y that satisﬁes\n",
      "sup\n",
      "y2Yinf\n",
      "x2Xf(x;y)=inf\n",
      "x2Xf(x;y\u0003)=f(x\u0003;y\u0003)=sup\n",
      "y2Yf(x\u0003;y)=inf\n",
      "x2Xsup\n",
      "y2Yf(x;y):\n",
      "The equation\n",
      "sup\n",
      "y2Yinf\n",
      "x2Xf(x;y)=inf\n",
      "x2Xsup\n",
      "y2Yf(x;y)\n",
      "is known as the minimax minimax equality. Other problems that fall into this framework are zero-\n",
      "sum games in game theory; see also [24] for a number of combinatorial optimization prob-\n",
      "lems that can be viewed as minimax problems.\n",
      "B.3 Numerical Root-Finding and Minimization\n",
      "In order to minimize a C1function f:Rn!Rone may solve\n",
      "rf(x)=0;\n",
      "which gives a stationary point of f. As a consequence, any technique for root-ﬁnding can\n",
      "be transformed into an unconstrained optimization method by attempting to locate roots\n",
      "of the gradient. However, as noted in Section B.2, not all stationary points are minima,\n",
      "and so additional information (such as is contained in the Hessian, if fisC2) needs to be\n",
      "considered in order to establish the type of stationary point.\n",
      "Alternatively, a root of a continuous function g:Rn!Rnmay be found by minimizing\n",
      "the norm of g(x) over all x; that is, by solving min xf(x), with f(x) :=kg(x)kp, where for\n",
      "p>1 the p-norm p-norm ofy=[y1;:::; yn]>is deﬁned as\n",
      "kykp:=\u0012nX\n",
      "i=1jyijp\u00131=p\n",
      ":\n",
      "Hence, any (un)constrained optimization method can be transformed into a technique for\n",
      "locating the roots of a function.Appendix B. Multivariate Differentiation and Optimization 411\n",
      "Starting with an initial guess x0, most minimization and root-ﬁnding algorithms create\n",
      "a sequence x0;x1;:::using the iterative updating rule:\n",
      "xt+1=xt+\u000btdt;t=0;1;2;:::; (B.18)\n",
      "where\u000bt>0 is a (typically small) step size, called the learning rate learning rate , and the vector dt\n",
      "is the search direction at step t. The iteration (B.18) continues until the sequence fxtgis\n",
      "deemed to have converged to a solution, or a computational budget has been exhausted.\n",
      "The performance of all such iterative methods depends crucially on the quality of the initial\n",
      "guess x0.\n",
      "There are two broad categories of iterative optimization algorithms of the form (B.18):\n",
      "Those of line search line search type, where at iteration twe ﬁrst compute a direction dtand\n",
      "then determine a reasonable step size \u000btalong this direction. For example, in the\n",
      "case of minimization, \u000bt>0 may be chosen to approximately minimize f(xt+\u000bdt)\n",
      "for ﬁxed xtanddt.\n",
      "Those of trust region trust region type, where at each iteration twe ﬁrst determine a suitable step\n",
      "size\u000btand then compute an approximately optimal direction dt.\n",
      "In the following sections, we review several widely-used root-ﬁnding and optimization\n",
      "algorithms of the line search type.\n",
      "B.3.1 Newton-Like Methods\n",
      "Suppose we wish to ﬁnd roots of a function f:Rn!Rn. Iffis inC1, we can approximate\n",
      "faround a point xtas\n",
      "f(x)\u0019f(xt)+Jf(xt)(x\u0000xt);\n",
      "where Jfis the matrix of Jacobi — the matrix of partial derivatives of f; see (B.3). When +400\n",
      "Jf(xt) is invertible, this linear approximation has root xt\u0000J\u00001\n",
      "f(xt)f(xt):This gives the\n",
      "iterative updating formula (B.18) for ﬁnding roots of fwith direction dt=\u0000J\u00001\n",
      "f(xt)f(xt)\n",
      "and learning rate \u000bt=1. This is known as Newton’s method Newton ’s\n",
      "method(or the Newton–Raphson\n",
      "method ) for root-ﬁnding.\n",
      "Instead of a unit learning rate, sometimes it is more e \u000bective to use an \u000btthat satisﬁes\n",
      "theArmijo inexact line search Armijo inexact\n",
      "line searchcondition:\n",
      "kf(xt+\u000btdt)k<(1\u0000\"1\u000bt)kf(xt)k;\n",
      "where\"1is a small heuristically chosen constant, say \"1=10\u00004. For C1functions, such an\n",
      "\u000btalways exists by continuity and can be computed as in the following algorithm.412 B.3. Numerical Root-Finding and Minimization\n",
      "Algorithm B.3.1: Newton–Raphson for Finding Roots of f(x)=0\n",
      "input: An initial guess xand stopping error \">0.\n",
      "output: The approximate root of f(x)=0.\n",
      "1whilekf(x)k>\"and budget is not exhausted do\n",
      "2 Solve the linear system Jf(x)d=\u0000f(x).\n",
      "3\u000b 1\n",
      "4 whilekf(x+\u000bd)k>(1\u000010\u00004\u000b)kf(x)kdo\n",
      "5\u000b \u000b=2\n",
      "6 x x+\u000bd\n",
      "7return x\n",
      "We can adapt a root-ﬁnding Newton-like method in order to minimize a di \u000berentiable\n",
      "function f:Rn!R. We simply try to locate a zero of the gradient of f. When fis a\n",
      "C2function, the function rf:Rn!Rnis continuous, and so the root of rfleads to the\n",
      "search direction\n",
      "dt=\u0000H\u00001\n",
      "trf(xt); (B.19)\n",
      "where Htis the Hessian matrix at xt(the matrix of Jacobi of the gradient is the Hessian).\n",
      "When the learning rate \u000btis equal to 1, the update xt\u0000H\u00001\n",
      "trf(xt) can alternatively be\n",
      "derived by assuming that f(x) is approximately quadratic and convex in the neighborhood\n",
      "ofxt, that is,\n",
      "f(x)\u0019f(xt)+(x\u0000xt)>rf(xt)+1\n",
      "2(x\u0000xt)>Ht(x\u0000xt); (B.20)\n",
      "and then minimizing the right-hand side of (B.20) with respect to x.\n",
      "The following algorithm uses an Armijo inexact line search for minimization and\n",
      "guards against the possibility that the Hessian may not be positive deﬁnite (that is, its\n",
      "Cholesky decomposition does not exist). +375\n",
      "Algorithm B.3.2: Newton–Raphson for Minimizing f(x)\n",
      "input: An initial guess x; stopping error \">0; line search parameter \u00182(0;1).\n",
      "output: An approximate minimizer of f(x).\n",
      "1L In(the identity matrix)\n",
      "2whilekrf(x)k>\"and budget is not exhausted do\n",
      "3 Compute the Hessian Hatx.\n",
      "4 if H\u001f0 then // Cholesky is successful\n",
      "5 Update Lto be the Cholesky factor satisfying LL>=H.\n",
      "6 else\n",
      "7 Do not update the lower triangular L.\n",
      "8 d \u0000 L\u00001rf(x) (computed by forward substitution)\n",
      "9 d L\u0000>d(computed by backward substitution)\n",
      "10\u000b 1\n",
      "11 while f(x+\u000bd)>f(x)+\u000b10\u00004rf(x)>ddo\n",
      "12\u000b \u000b\u0002\u0018\n",
      "13 x x+\u000bd\n",
      "14return xAppendix B. Multivariate Differentiation and Optimization 413\n",
      "A downside with all Newton-like methods is that at each step they require the calcu-\n",
      "lation and inversion of an n\u0002nHessian matrix, which has computing time of O(n3), and\n",
      "is thus infeasible for large n. One way to avoid this cost is to use quasi-Newton methods,\n",
      "described next.\n",
      "B.3.2 Quasi-Newton Methods\n",
      "The idea behind quasi-Newton methods is to replace the inverse Hessian in (B.19) at iter-\n",
      "ation tby an n\u0002nmatrix Csatisfying the secant condition secant\n",
      "condition:\n",
      "C1=\u000e; (B.21)\n",
      "where\u000e xt\u0000xt\u00001and1 r f(xt)\u0000rf(xt\u00001) are vectors stored in memory at each iter-\n",
      "ation t. The secant condition is satisﬁed, for example, by the Broyden’s family of matrices:\n",
      "A+1\n",
      "u>1(\u000e\u0000A1)u>\n",
      "for some u,0andA. Since there is an inﬁnite number of matrices that satisfy the condi-\n",
      "tion (B.21), we need a way to determine a unique Cat each iteration tsuch that computing\n",
      "and storing Cfrom one step to the next is fast and avoids any costly matrix inversion. The\n",
      "following examples illustrate how, starting with an initial guess C=Iatt=0, such a\n",
      "matrix Ccan be e \u000eciently updated from one iteration to the next.\n",
      "Example B.8 (Low-Rank Hessian Update) The quadratic model (B.20) can be\n",
      "strengthened by further assuming that exp( \u0000f(x)) is proportional to a probability density\n",
      "that can be approximated in the neighborhood of xtby the pdf of the N(xt+1;H\u00001\n",
      "t) dis-\n",
      "tribution. This normal approximation allows us to measure the discrepancy between two\n",
      "pairs ( x1;H0) and ( x2;H1) using the Kullback–Leibler divergence between the pdfs of the + 42\n",
      "N(x1;H\u00001\n",
      "0) andN(x2;H\u00001\n",
      "1) distributions (see Exercise 4 on page 352):\n",
      "D(x1;H\u00001\n",
      "0jx2;H\u00001\n",
      "1) :=1\n",
      "2\u0010\n",
      "tr(H1H\u00001\n",
      "0)\u0000lnjH1H\u00001\n",
      "0j+(x2\u0000x1)>H1(x2\u0000x1)\u0000n\u0011\n",
      ":(B.22)\n",
      "Suppose that the latest approximation to the inverse Hessian is Cand we wish to com-\n",
      "pute an updated approximation for step t. One approach is to ﬁnd the symmetric matrix\n",
      "that minimizes its Kullback–Leibler discrepancy from C, as deﬁned above, subject to the\n",
      "constraint (B.21). In other words,\n",
      "min\n",
      "AD(0;Cj0;A)\n",
      "subject to: A1=\u000e;A=A>:\n",
      "The solution to this constrained optimization (see Exercise 10 on page 354) yields the\n",
      "Broyden–Fletcher–Goldfarb–Shanno or BFGS formula bfgs formula for updating the matrix Cfrom one\n",
      "iteration to the next:\n",
      "CBFGS=C+1>\u000e+1>C1\n",
      "(1>\u000e)2\u000e\u000e>\u00001\n",
      "1>\u000e\u0000\u000e1>C+(\u000e1>C)>\u0001\n",
      "|                                                     {z                                                     }\n",
      "BFGS update: (B.23)414 B.3. Numerical Root-Finding and Minimization\n",
      "In a practical implementation, we keep a single copy of Cin memory and apply the BFGS\n",
      "update to it at every iteration. Note that if the current Cis symmetric, then so is the updated\n",
      "matrix. Moreover, the BFGS update is a matrix of rank two.\n",
      "Since the Kullback–Leibler divergence is not symmetric, it is possible to ﬂip the roles\n",
      "ofH0andH1in (B.22) and instead solve\n",
      "min\n",
      "AD(0;Aj0;C)\n",
      "subject to: A1=\u000e;A=A>:\n",
      "The solution (see Exercise 10 on page 354) gives the Davidon–Fletcher–Powell or DFP\n",
      "formula dfp formula for updating the matrix Cfrom one iteration to the next:\n",
      "CDFP=C+\u000e\u000e>\n",
      "1>\u000e\u0000C11>C\n",
      "1>C1|             {z             }\n",
      "DFP update: (B.24)\n",
      "Note that if the curvature condition 1>\u000e>0 holds and the current Cis symmetric positive\n",
      "deﬁnite, then so is its update.\n",
      "Example B.9 (Diagonal Hessian Update) The original BFGS formula requires O(n2)\n",
      "storage and computation, which may be unmanageable for large n. One way to circumvent\n",
      "the prohibitive quadratic cost is to only store and update a diagonal Hessian matrix from\n",
      "one iteration to the next. If Cis diagonal, then we may not be able to satisfy the secant\n",
      "condition (B.21) and maintain positive deﬁniteness. Instead the secant condition (B.21)\n",
      "can be relaxed to the set of inequalities 1>C\u00001\u000e, which are related to the deﬁnition of a\n",
      "subgradient for convex functions. We can then ﬁnd a unique diagonal matrix by minimizing +405\n",
      "D(xt;Cjxt+1;A) with respect to Aand subject to the constraints that A1>\u000eandAis\n",
      "diagonal. The solution (Exercise 15 on page 354) yields the updating formula for a diagonal\n",
      "element ciofC:\n",
      "ci 8>>>>><>>>>>:2ci\n",
      "1+q\n",
      "1+4ciu2\n",
      "i;if2ci\n",
      "1+q\n",
      "1+4ciu2\n",
      "i>\u000ei=1i\n",
      "\u000ei=1i; otherwise;(B.25)\n",
      "where u:=rf(xt) and we assume a unit learning rate: xt+1=xt\u0000Au.\n",
      "Example B.10 (Scalar Hessian Update) If the identity matrix is used in place of the\n",
      "Hessian in (B.19), one obtains steepest descent steepest\n",
      "descentorgradient descent methods, in which the\n",
      "iteration (B.18) reduces to xt+1=xt\u0000\u000btrf(xt).\n",
      "The rationale for the name steepest descent is as follows. If we start from any point\n",
      "xand make an inﬁnitesimal move in some direction, then the function value is reduced\n",
      "by the largest magnitude in the (unit norm) direction: u\u0003:=\u0000rf(x)=krf(x)k. This is seen\n",
      "from the following inequality for all unit vectors u(that is,kuk=1):\n",
      "d\n",
      "dtf(x+tu\u0003)\f\f\f\ft=06d\n",
      "dtf(x+tu)\f\f\f\ft=0:\n",
      "Observe that equality is achieved if and only if u=u\u0003. This inequality is an easy con-\n",
      "sequence of the Cauchy–Schwarz inequality: +391Appendix B. Multivariate Differentiation and Optimization 415\n",
      "\u0000rf>u6jrf>uj6|{z}\n",
      "Cauchy–Schwartzkukkrfk=krfk=\u0000rf>u\u0003:\n",
      "The steepest descent iteration, xt+1=xt\u0000\u000btrf(xt), still requires a suitable choice of the\n",
      "learning rate \u000bt. An alternative way to think about the iteration is to assume that the learning\n",
      "rate is always unity, and that at each iteration we use an inverse Hessian matrix of the form\n",
      "\u000btIfor some positive constant \u000bt. Satisfying the secant condition (B.21) with a matrix of the\n",
      "form C=\u000bIis not possible. However, it is possible to choose \u000bso that the secant condition\n",
      "(B.21) is satisﬁed in the direction of 1(or alternatively \u000e). This gives the Barzilai–Borwein\n",
      "formulas Barzilai –\n",
      "Borwein\n",
      "formulasfor the learning rate at iteration t:\n",
      "\u000bt=1>\u000e\n",
      "k1k2 \n",
      "or alternatively \u000bt=k\u000ek2\n",
      "\u000e>1!\n",
      ": (B.26)\n",
      "B.3.3 Normal Approximation Method\n",
      "Let'H\u00001\n",
      "t(x\u0000xt+1) denote the pdf of the N(xt+1;H\u00001\n",
      "t) distribution. As we already saw in Ex-\n",
      "ample B.8, the quadratic approximation (B.20) of fin the neighborhood of xtis equivalent\n",
      "(up to a constant) to the minus of the logarithm of the pdf 'H\u00001\n",
      "t(x\u0000xt+1). In other words,\n",
      "we use'H\u00001\n",
      "t(x\u0000xt+1) as a simple model for the density\n",
      "exp(\u0000f(x)).Z\n",
      "exp(\u0000f(y)) dy:\n",
      "One consequence of the normal approximation is that for xin the neighborhood of xt+1,\n",
      "we can write:\n",
      "\u0000rf(x)\u0019@\n",
      "@xln'H\u00001\n",
      "t(x\u0000xt+1)=\u0000Ht(x\u0000xt+1):\n",
      "In other words, using the fact that H>\n",
      "t=Ht,\n",
      "rf(x)[rf(x)]>\u0019Ht(x\u0000xt+1)(x\u0000xt+1)>Ht;\n",
      "and taking expectations on both sides with respect to X\u0018N(xt+1;H\u00001\n",
      "t) gives:\n",
      "Erf(X) [rf(X)]>\u0019Ht:\n",
      "This suggests that, given the gradient vectors computed in the past h(where hstands for\n",
      "history) of Newton iterations:\n",
      "ui:=rf(xi);i=t\u0000(h\u00001);:::; t;\n",
      "the Hessian matrix Htcan be approximated via the average\n",
      "1\n",
      "htX\n",
      "i=t\u0000h+1uiu>\n",
      "i:\n",
      "A shortcoming of this approximation is that, unless his large enough, the Hessian approx-\n",
      "imationPt\n",
      "i=t\u0000h+1uiu>\n",
      "imay not be full rank and hence not invertible. To ensure that the416 B.3. Numerical Root-Finding and Minimization\n",
      "Hessian approximation is invertible, we add a suitable diagonal matrix A0to obtain the\n",
      "regularized version of the approximation: +217\n",
      "Ht\u0019A0+1\n",
      "htX\n",
      "i=t\u0000h+1uiu>\n",
      "i:\n",
      "With this full-rank approximation of the Hessian, the Newton search direction in (B.19)\n",
      "becomes:\n",
      "dt=\u00000BBBBB@A0+1\n",
      "htX\n",
      "i=t\u0000h+1uiu>\n",
      "i1CCCCCA\u00001\n",
      "ut: (B.27)\n",
      "Thus, dtcan be computed in O(h2n) time via the Sherman–Morrison Algorithm A.6.1. +375\n",
      "Further to this, the search direction (B.27) can be e \u000eciently updated to the next one:\n",
      "dt+1=\u00000BBBBB@A0+1\n",
      "ht+1X\n",
      "i=t\u0000h+2uiu>\n",
      "i1CCCCCA\u00001\n",
      "ut+1\n",
      "inO(h n) time, thus avoiding the usual O(h2n) cost (see Exercise 6 on page 353).\n",
      "B.3.4 Nonlinear Least Squares\n",
      "Consider the squared-error training loss in nonlinear regression: +188\n",
      "`\u001c(g(\u0001j\f))=1\n",
      "nnX\n",
      "i=1(g(xij\f)\u0000yi)2;\n",
      "where g(\u0001j\f) is a nonlinear prediction function that depends on the parameter \f(for ex-\n",
      "ample, (5.29) shows the nonlinear logistic prediction function). The training loss can be\n",
      "written as1\n",
      "nkg(\u001cj\f)\u0000yk2, where g(\u001cj\f) :=[g(x1j\f);:::; g(xnj\f)]>is the vector of out-\n",
      "puts.\n",
      "We wish to minimize the training loss in terms of \f. In the Newton-like methods in\n",
      "Section B.3.1, one derives an iterative minimization algorithm that is inspired by a Taylor\n",
      "expansion of `\u001c(g(\u0001j\f)). Instead, given a current guess \ft, we can consider the Taylor ex-\n",
      "pansion of the nonlinear prediction function g:\n",
      "g(\u001cj\f)\u0019g(\u001cj\ft)+Gt(\f\u0000\ft);\n",
      "where Gt:=Jg(\ft) is the matrix of Jacobi of g(\u001cj\f) at\ft. Denoting the residual et:= +400\n",
      "g(\u001cj\ft)\u0000yand replacing g(\u001cj\f) with its Taylor approximation in `\u001c(g(\u0001j\f)), we obtain\n",
      "the approximation to the training loss in the neighborhood of \ft:\n",
      "`\u001c(g(\u0001j\f))\u00191\n",
      "2:(\f\u0000\ft)+et\n",
      "The minimization of the right-hand side is a linear least-squares problem and therefore\n",
      "dt:=\f\u0000\ftsatisﬁes the normal equations: G>\n",
      "tGtdt=G>\n",
      "t(\u0000et). Assuming that G>\n",
      "tGtis +28\n",
      "invertible, the normal equations yield the Gauss–Newton Gauss–Newton search direction:\n",
      "dt=\u0000(G>\n",
      "tGt)\u00001G>\n",
      "tet:Appendix B. Multivariate Differentiation and Optimization 417\n",
      "Unlike the search direction (B.19) for Newton-like algorithms, the search direction of a\n",
      "Gauss–Newton algorithm does not require the computation of a Hessian matrix.\n",
      "Observe that in the Gauss–Newton approach we determine dtby viewing the search\n",
      "direction as coe \u000ecients in a linear regression with feature matrix Gtand response\u0000et. This\n",
      "suggests that instead of using a linear regression, we can compute dtvia a ridge regression\n",
      ": +217 suitable choice for the regularization parameter \n",
      "dt=\u0000(G>\n",
      "Ip)\u00001G>\n",
      "tet:\n",
      "If we replace nIpwith the diagonal matrix diag( G>\n",
      "tGt), we then obtain the Levenberg–\n",
      "Marquardt Levenberg –\n",
      "Marquardtsearch direction:\n",
      "dt=\u0000(G>\n",
      "diag( G>\n",
      "tGt))\u00001G>\n",
      "tet: (B.28)\n",
      "has the following e \u000bect on the least-squaresr \n",
      "solution: When it is zero, then the solution dtcoincides with the search direction of the\n",
      "tends to inﬁnity, then kdtktends to zero. Thus, \n",
      "controls both the magnitude and direction of vector dt. A simple version of the Levenberg–\n",
      "Marquardt algorithm is the following.\n",
      "Algorithm B.3.3: Levenberg–Marquardt for Minimizing1\n",
      "nkg(\u001cj\f)\u0000yk2\n",
      "input: An initial guess \f0; stopping error \">0; training set \u001c.\n",
      "output: An approximate minimizer of1\n",
      "nkg(\u001cj\f)\u0000yk2.\n",
      " 0:01 (or another default value)\n",
      "2while stopping condition is not met do\n",
      "3 Compute the search direction dtvia (B.28).\n",
      "4 et+1 g(\u001cj\ft+dt)\u0000y\n",
      "5 ifket+1k<ketkthen\n",
      "=10, et+1 et,\ft+1 \ft+dt\n",
      "7 else\n",
      "\u000210\n",
      "9 t t+1\n",
      "10return\ft\n",
      "B.4 Constrained Minimization via Penalty Functions\n",
      "A constrained optimization problem of the form (B.13) can sometimes be reformulated as a\n",
      "simpler unconstrained problem — for example, the unconstrained set Ycan be transformed\n",
      "to the feasible region Xof the constrained problem via a function \u001e:Rn!Rnsuch that\n",
      "X=\u001e(Y). Then, (B.13) is equivalent to the minimization problem\n",
      "min\n",
      "y2Yf(\u001e(y));\n",
      "in the sense that a solution x\u0003of the original problem is obtained from a transformed\n",
      "solution y\u0003viax\u0003=\u001e(y\u0003). Table B.2 lists some examples of possible transformations.418 B.4. Constrained Minimization via Penalty Functions\n",
      "Table B.2: Some transformations to eliminate constraints.\n",
      "Constrained Unconstrained\n",
      "x>0 exp( y)\n",
      "x>0 y2\n",
      "a6x6b a +(b\u0000a) sin2(y)\n",
      "Unfortunately, an unconstrained minimization method used in combination with these\n",
      "transformations is rarely e \u000bective. Instead, it is more common to use penalty functions.\n",
      "The overarching idea of penalty functions penalty\n",
      "functionsis to transform a constrained problem into\n",
      "an unconstrained problem by adding weighted constraint-violation terms to the original\n",
      "objective function, with the premise that the new problem has a solution that is identical or\n",
      "close to the original one.\n",
      "For example, if there are only equality constraints, then\n",
      "ef(x) :=f(x)+mX\n",
      "i=1aijhi(x)jp\n",
      "for some constants a1;:::; am>0 and integer p2f1;2g, gives an exact penalty function ,\n",
      "in the sense that the minimizer of the penalized function efis equal to the minimizer of f\n",
      "subject to the mequality constraints h1;:::; hm. With the addition of inequality constraints,\n",
      "one could use\n",
      "ef(x)=f(x)+mX\n",
      "i=1aijhi(x)jp+kX\n",
      "j=1bjmaxfgj(x);0g\n",
      "for some constants a1;:::; am;b1;:::; bk>0.\n",
      "Example B.11 (Alternating Direction Method of Multipliers) The Lagrange method\n",
      "is designed to handle convex minimization subject to equality constraints. Nevertheless, +408\n",
      "some practical algorithms may still use the penalty function approach in combination with\n",
      "the Lagrangian method. An example is the alternating direction method of multipliers alternating\n",
      "direction\n",
      "method of\n",
      "multipliers(ADMM) [17]. The ADMM solves problems of the form:\n",
      "min\n",
      "x2Rn;z2Rmf(x)+g(z)\n",
      "subject to: Ax+Bz=c;(B.29)\n",
      "where A2Rp\u0002n,B2Rp\u0002m, and c2Rp, and f:Rn!Randg:Rm!Rare convex func-\n",
      "tions. The approach is to form an augmented Lagrangian\n",
      "L%(x;z;\f) :=f(x)+g(z)+\f>(Ax+Bz\u0000c)+%\n",
      "2kAx+Bz\u0000ck2;\n",
      "where%>0 is a penalty parameter, and \f2Rpare dual variables. The ADMM then iterates\n",
      "through updates of the following form:\n",
      "x(t+1)=argmin\n",
      "x2RnL%(x;z(t);\f(t))\n",
      "z(t+1)=argmin\n",
      "z2RmL%(x(t+1);z;\f(t))\n",
      "\f(t+1)=\f(t)+%\u0010\n",
      "Ax(t+1)+Bz(t+1)\u0000c\u0011\n",
      ":Appendix B. Multivariate Differentiation and Optimization 419\n",
      "Suppose that (B.13) has inequality constraints only. Barrier functions Barrier\n",
      "functionsare an important ex-\n",
      "ample of penalty functions that can handle inequality constraints. The prototypical example\n",
      "is alogarithmic barrier function which gives the unconstrained optimization:\n",
      "ef(x)=f(x)\u0000\u0017kX\n",
      "j=1ln(\u0000gj(x)); \u0017> 0;\n",
      "such that the minimizer of eftends to the minimizer of fas\u0017!0. Direct minimization\n",
      "ofefvia an unconstrained minimization algorithm is frequently too di \u000ecult. Instead, it\n",
      "is common to combine the logarithmic barrier function with the Lagrangian method as\n",
      "follows.\n",
      "The idea is to introduce knonnegative auxiliary or slack variables slack variables s1;:::; skthat satisfy\n",
      "the equalities gj(x)+sj=0 for all j. These equalities ensure that the inequality constraints\n",
      "are maintained: gj(x)=\u0000sj60 for all j. Then, instead of the unconstrained optimization\n",
      "ofef, we consider the unconstrained optimization of the Lagrangian:\n",
      "L(x;s;\f)=f(x)\u0000\u0017kX\n",
      "j=1lnsj+kX\n",
      "j=1\fj(gj(x)+sj); (B.30)\n",
      "where\u0017>0 and\fare the Lagrange multipliers for the equalities gj(x)+sj=0;j=1;:::; k.\n",
      "Observe how the logarithmic barrier function keeps the slack variables positive. In\n",
      "addition, while the optimization of efis over ndimensions (recall that x2Rn), the optimiz-\n",
      "ation of the Lagrangian function Lis over n+2kdimensions. Despite this enlargement of\n",
      "the search space with the variables sand\f, the optimization of the Lagrangian Lis easier\n",
      "in practice than the direct optimization of ef.\n",
      "Example B.12 (Interior-Point Method for Nonnegativity) One of the simplest and\n",
      "most common constrained optimization problems can be formulated as the minimization\n",
      "off(x) subject to nonnegative x, that is: min x>0f(x). In this case, the Lagrangian with\n",
      "logarithmic barrier (B.30) is:\n",
      "L(x;s;\f)=f(x)\u0000\u0017X\n",
      "klnsk+\f>(s\u0000x):\n",
      "The KKT conditions in Theorem B.2 are a necessary condition for a minimizer, and yield\n",
      "the nonlinear system for [ x>;s>;\f>]>2R3n:\n",
      "2666666664rf(x)\u0000\f\n",
      "\u0000\u0017=s+\f\n",
      "s\u0000x3777777775=0;\n",
      "where\u0017=sis a shorthand notation for a column vector with components f\u0017=sjg. To solve this\n",
      "system, we can use Newton’s method for root ﬁnding (see, for example, Algorithm B.3.1),\n",
      "which requires a formula for the matrix of Jacobi of L. Here, this (3 n)\u0002(3n) matrix is:\n",
      "JL(x;s;\f)=2666666664H O\u0000I\n",
      "O D I\n",
      "\u0000I I O3777777775=\"H B\n",
      "B>E#\n",
      ";420 B.4. Constrained Minimization via Penalty Functions\n",
      "where His the n\u0002nHessian of fatx;D:=diag (\u0017=(s\fs))is an n\u0002ndiagonal matrix;\n",
      "B:=[O;\u0000I] is an n\u0002(2n) matrix, and1\n",
      "E:=\"D I\n",
      "I O#\n",
      "=\"O I\n",
      "I\u0000D#\u00001\n",
      ":\n",
      "Further, we deﬁne\n",
      "H\u0017:=(H\u0000BE\u00001B>)\u00001=(H+D)\u00001:\n",
      "Using this notation and applying the matrix blockwise inversion formula (A.14), we obtain +373\n",
      "the inverse of the matrix of Jacobi:\n",
      "\"H B\n",
      "B>E#\u00001\n",
      "=\"H\u0017\u0000H\u0017BE\u00001\n",
      "\u0000E\u00001B>H\u0017E\u00001+E\u00001B>H\u0017BE\u00001#\n",
      "=2666666664H\u0017 H\u0017\u0000H\u0017D\n",
      "H\u0017 H\u0017 I\u0000H\u0017D\n",
      "\u0000DH\u0017I\u0000DH\u0017DH\u0017D\u0000D3777777775:\n",
      "Therefore, the search direction in Newton’s root-ﬁnding method is given by:\n",
      "\u0000J\u00001\n",
      "L2666666664rf(x)\u0000\f\n",
      "\u0000\u0017=s+\f\n",
      "s\u0000x3777777775=2666666664dx\n",
      "dx+x\u0000s\n",
      "\u0017=s\u0000\f\u0000D(dx+x\u0000s)3777777775;\n",
      "where\n",
      "dx:=\u0000(H+D)\u00001h\n",
      "rf(x)\u00002\u0017=s+Dxi\n",
      ";\n",
      "and we have assumed that H+Dis a positive-deﬁnite matrix. If at any step of the iteration\n",
      "the matrix H+Dfails to be positive-deﬁnite, then Newton’s root-ﬁnding algorithm may\n",
      "fail to converge. Thus, any practical implementation will have to include a fail-safe feature\n",
      "to guard against this possibility.\n",
      "In summary, for a given penalty parameter \u0017 >0, we can locate the approximate non-\n",
      "negative minimizer of fusing, for example, the version of the Newton–Raphson root-\n",
      "ﬁnding method given in Algorithm B.4.1.\n",
      "In practice, one needs to choose a su \u000eciently small value for \u0017, so that the output x\u0017\n",
      "of Algorithm B.4.1 is a good approximation to x\u0003=argminx>0f(x). Alternatively, one\n",
      "can create a decreasing sequence of penalty parameters \u00171> \u0017 2>\u0001\u0001\u0001and compute the\n",
      "corresponding solutions x\u00171;x\u00172;:::of the penalized problems. In the so-called interior-\n",
      "point method , a given x\u0017iis used as an initial guess for computing x\u0017i+1and so on until theinterior -point\n",
      "method approximation to the minimizer x\u0003=argminx>0f(x) is deemed accurate.\n",
      "1Here Ois an n\u0002nmatrix of zeros and Iis the n\u0002nidentity matrix.Appendix B. Multivariate Differentiation and Optimization 421\n",
      "Algorithm B.4.1: Approximating x\u0003=argminx>0f(x) with Logarithmic Barrier\n",
      "input: An initial guess xand stopping error \">0.\n",
      "output: The approximate nonnegative minimizer x\u0017off.\n",
      "1s x,\f \u0017=s, dx \f\n",
      "2whilekdxk>\"and budget is not exhausted do\n",
      "3 Compute the gradient uand the Hessian Hoffatx.\n",
      "4 s1 \u0017=s,s2 s1=s,w 2s1\u0000u\u0000s2\fx\n",
      "5 if(H+diag( s2))\u001f0 then // if Cholesky successful\n",
      "6 Compute the Cholesky factor Lsatisfying LL>=H+diag( s2).\n",
      "7 dx L\u00001w(computed by forward substitution)\n",
      "8 dx L\u0000>dx(computed by backward substitution)\n",
      "9 else\n",
      "10 dx w=s2 // if Cholesky fails, do steepest descent\n",
      "11 ds dx+x\u0000s, d\f s1\u0000\f\u0000s2\fds,\u000b 1\n",
      "12 while min jfsj+\u000bdsjg<0do\n",
      "13\u000b \u000b=2 // ensure nonnegative slack variables\n",
      "14 x x+\u000bdx,s s+\u000bds,\f \f+\u000bd\f\n",
      "15return x\u0017 x\n",
      "Further Reading\n",
      "For an excellent introduction to convex optimization and Lagrangian duality see [18]. A\n",
      "classical text on optimization algorithms and, in particular, on quasi-Newton methods is\n",
      "[43]. For more details on the alternating direction method of multipliers see [17].422APPENDIXC\n",
      "PROBABILITY AND STATISTICS\n",
      "The purpose of this chapter is to establish the baseline probability and statistics\n",
      "background for this book. We review basic concepts such as the sum and product rules\n",
      "of probability, random variables and their probability distributions, expectations, in-\n",
      "dependence, conditional probability, transformation rules, limit theorems, and Markov\n",
      "chains. The properties of the multivariate normal distribution are discussed in more de-\n",
      "tail. The main ideas from statistics are also reviewed, including estimation techniques\n",
      "(such as maximum likelihood estimation), conﬁdence intervals, and hypothesis testing.\n",
      "C.1 Random Experiments and Probability Spaces\n",
      "The basic notion in probability theory is that of a random experiment random\n",
      "experiment: an experiment\n",
      "whose outcome cannot be determined in advance. Mathematically, a random experiment is\n",
      "modeled via a triplet ( \n",
      ";H;P), where:\n",
      "\n",
      "is the set of all possible outcomes of the experiment, called the sample space sample space .\n",
      "His the collection of all subsets of \n",
      "to which a probability can be assigned; such\n",
      "subsets are called events events .\n",
      "Pis aprobability measure probability\n",
      "measure, which assigns to each event Aa numberP[A] between 0\n",
      "and 1, indicating the likelihood that the outcome of the random experiment lies in A.\n",
      "Any probability measure Pmust satisfy the following Kolmogorov axioms Kolmogorov\n",
      "axioms:\n",
      "1.P[A]>0 for every event A.\n",
      "2.P[\n",
      "]=1.\n",
      "3. For any sequence A1;A2;:::of events,\n",
      "Ph[\n",
      "iAii\n",
      "6X\n",
      "iP[Ai]; (C.1)\n",
      "with strict equality whenever the events are disjoint (that is, non-overlapping).\n",
      "423424 C.2. Random Variables and Probability Distributions\n",
      "When (C.1) holds as an equality, it is often referred to as the sum rule sum rule of probability. It\n",
      "simply states that if an event can happen in a number of di \u000berent but not simultaneous\n",
      "ways, the probability of that event is the sum of the probabilities of the comprising events.\n",
      "If the events are allowed to overlap, then the inequality (C.1) is called the union bound union bound .\n",
      "In many applications the sample space is countable ; that is, \n",
      " =fa1;a2;:::g. In this\n",
      "case the easiest way to specify a probability measure Pis to ﬁrst assign a number pito\n",
      "each elementary event elementary\n",
      "eventfaig, withP\n",
      "ipi=1, and then to deﬁne\n",
      "P[A]=X\n",
      "i:ai2Apifor all A\u0012\n",
      ":\n",
      "Here the collection of events Hcan be taken to be equal to the collection of allsubsets\n",
      "of\n",
      ". The triple ( \n",
      ";H;P) is called a discrete probability space discrete\n",
      "probability\n",
      "space. This idea is graphically\n",
      "represented in Figure C.1. Each element ai, represented by a dot, is assigned a weight (that\n",
      "is, probability) pi, indicated by the size of the dot. The probability of the event Ais simply\n",
      "the sum of the weights of all the outcomes in A.\n",
      "Ω\n",
      "A\n",
      "Figure C.1: A discrete probability space.\n",
      "Remark C.1 (Equilikely Principle) A special case of a discrete probability space oc-\n",
      "curs when a random experiment has ﬁnitely many outcomes that are all equally likely . In\n",
      "this case the probability measure is given by\n",
      "P[A]=jAj\n",
      "j\n",
      "j; (C.2)\n",
      "wherejAjdenotes the number of outcomes in Aandj\n",
      "jis the total number of outcomes.\n",
      "Thus, the calculation of probabilities reduces to counting numbers of outcomes in events.\n",
      "This is called the equilikely principle equilikely\n",
      "principle.\n",
      "C.2 Random Variables and Probability Distributions\n",
      "It is often convenient to describe a random experiment via “random variables”, repres-\n",
      "enting numerical measurements of the experiment. Random variables are usually denoted\n",
      "by capital letters from the last part of the alphabet. From a mathematical point of view, a\n",
      "random variable random\n",
      "variableXis a function from \n",
      "toRsuch that sets of the form fa<X6bg:=\n",
      "f!2\n",
      ":a<X(!)6bgare events (and so can be assigned a probability).Appendix C. Probability and Statistics 425\n",
      "All probabilities involving a random variable Xcan be computed, in principle, from its\n",
      "cumulative distribution function cumulative\n",
      "distribution\n",
      "function(cdf), deﬁned by\n",
      "F(x)=P[X6x];x2R:\n",
      "For example P[a<X6b]=P[X6b]\u0000P[X6a]=F(b)\u0000F(a). Figure C.2 shows a\n",
      "generic cdf. Note that any cdf is right-continuous, increasing, and lies between 0 and 1.\n",
      "01\n",
      "Figure C.2: A cumulative distribution function (cdf).\n",
      "A cdf Fdis called discrete discrete cdf if there exist numbers x1;x2;:::and probabilities 0 <f(xi)6\n",
      "1 summing up to 1, such that for all x\n",
      "Fd(x)=X\n",
      "xi6xf(xi): (C.3)\n",
      "Such a cdf is piecewise constant and has jumps of sizes f(x1);f(x2);:::at points x1;x2;:::,\n",
      "respectively. The function f(x) is called a probability mass function ordiscrete probability\n",
      "density function (pdf). discrete pdf It is often easier to use the pdf rather than the cdf, since probabilities\n",
      "can simply be calculated from it via summation:\n",
      "P[X2B]=X\n",
      "x2Bf(x);\n",
      "as illustrated in Figure C.3.\n",
      "Figure C.3: Discrete probability density function (pdf). The darker area corresponds to the\n",
      "probabilityP[X2B].426 C.2. Random Variables and Probability Distributions\n",
      "A cdf Fcis called continuous1, continuous cdf if there exists a positive function fsuch that for all x\n",
      "Fc(x)=Zx\n",
      "\u00001f(u) du: (C.4)\n",
      "Note that such an Fcis di\u000berentiable (and hence continuous) with derivative f. The func-\n",
      "tion fis called the probability density function (continuous pdf) . pdf By the fundamental the-\n",
      "orem of integration, we have\n",
      "P[a<X6b]=F(b)\u0000F(a)=Zb\n",
      "af(x) dx:\n",
      "Thus, calculating probabilities reduces to integration, as illustrated in Figure C.4.\n",
      "Figure C.4: Continuous probability density function (pdf). The shaded area corresponds to\n",
      "the probability P[X2B], with Bbeing here the interval ( a;b].\n",
      "Remark C.2 (Probability Density and Probability Mass) It is important to note that\n",
      "we deliberately use the same name, “pdf”, and symbol, f, in both the discrete and the\n",
      "continuous case, rather than distinguish between a probability mass function (pmf) and\n",
      "probability density function (pdf). From a theoretical point of view the pdf plays exactly\n",
      "the same role in the discrete and continuous cases. We use the notation X\u0018Dist,X\u0018f,\n",
      "andX\u0018Fto indicate that Xhas distribution Dist, pdf f, and cdf F.\n",
      "Tables C.1 and C.2 list a number of important continuous and discrete distributions.\n",
      "Note that in Table C.1, \u0000is the gamma function: \u0000(\u000b)=R1\n",
      "0e\u0000xx\u000b\u00001dx; \u000b> 0.\n",
      "1In advanced probability, we would say “ absolutely continuous with respect to the Lebesgue measure ”.Appendix C. Probability and Statistics 427\n",
      "Table C.1: Commonly used continuous distributions.\n",
      "Name Notation f(x) x2 Parameters\n",
      "Uniform U[\u000b;\f]1\n",
      "\f\u0000\u000b[\u000b;\f]\u000b<\f\n",
      "Normal N(\u0016;\u001b2)1\n",
      "\u001bp\n",
      "2\u0019e\u00001\n",
      "2(x\u0000\u0016\n",
      "\u001b)2\n",
      "R\u001b>0; \u00162R\n",
      "Gamma Gamma (\u000b;\u0015)\u0015\u000bx\u000b\u00001e\u0000\u0015x\n",
      "\u0000(\u000b)R+\u000b;\u0015> 0\n",
      "Inverse Gamma InvGamma (\u000b;\u0015)\u0015\u000bx\u0000\u000b\u00001e\u0000\u0015x\u00001\n",
      "\u0000(\u000b)R+\u000b;\u0015> 0\n",
      "Exponential Exp(\u0015) \u0015e\u0000\u0015xR+\u0015>0\n",
      "Beta Beta (\u000b;\f)\u0000(\u000b+\f)\n",
      "\u0000(\u000b)\u0000(\f)x\u000b\u00001(1\u0000x)\f\u00001[0;1]\u000b;\f> 0\n",
      "Weibull Weib (\u000b;\u0015) \u000b\u0015(\u0015x)\u000b\u00001e\u0000(\u0015x)\u000bR+\u000b;\u0015> 0\n",
      "Pareto Pareto (\u000b;\u0015) \u000b\u0015(1+\u0015x)\u0000(\u000b+1)R+\u000b;\u0015> 0\n",
      "Student t\u0017\u0000(\u0017+1\n",
      "2)\n",
      "p\u0017\u0019\u0000(\u0017\n",
      "2) \n",
      "1+x2\n",
      "\u0017!\u0000(\u0017+1)=2\n",
      "R\u0017>0\n",
      "F F(m;n)\u0000(m+n\n",
      "2) (m=n)m=2x(m\u00002)=2\n",
      "\u0000(m\n",
      "2)\u0000(n\n",
      "2) [1+(m=n)x](m+n)=2R+ m;n2N+\n",
      "TheGamma (n=2;1=2) distribution is called the chi-squared distribution \u001f2\n",
      "ndistribution with ndegrees\n",
      "of freedom, denoted \u001f2\n",
      "n. The t1distribution is also called the Cauchy distribution.\n",
      "Table C.2: Commonly used discrete distributions.\n",
      "Name Notation f(x) x2 Parameters\n",
      "Bernoulli Ber(p) px(1\u0000p)1\u0000xf0;1g 06p61\n",
      "Binomial Bin(n;p) n\n",
      "x!\n",
      "px(1\u0000p)n\u0000xf0;1;:::; ng06p61;\n",
      "n2N\n",
      "Discrete\n",
      "uniformUf1;:::; ng1\n",
      "nf1;:::; ng n2f1;2;:::g\n",
      "Geometric Geom (p) p(1\u0000p)x\u00001f1;2;:::g 06p61\n",
      "Poisson Poi(\u0015) e\u0000\u0015\u0015x\n",
      "x!N \u0015>0428 C.3. Expectation\n",
      "C.3 Expectation\n",
      "It is often useful to consider di \u000berent kinds of numerical characteristics of a random vari-\n",
      "able. One such quantity is the expectation, which measures the “average” value of the\n",
      "distribution.\n",
      "Theexpectation expectation (or expected value or mean) of a random variable Xwith pdf f, denoted\n",
      "byEXor2E[X] (and sometimes \u0016), is deﬁned by\n",
      "EX=8>><>>:P\n",
      "xx f(x) discrete case,R1\n",
      "\u00001x f(x) dxcontinuous case :\n",
      "IfXis a random variable, then a function of X, such as X2or sin( X), is again a random\n",
      "variable. Moreover, the expected value of a function of Xis simply a weighted average of\n",
      "the possible values that this function can take. That is, for any real function h\n",
      "Eh(X)=8>><>>:P\n",
      "xh(x)f(x) discrete case ;R1\n",
      "\u00001h(x)f(x) dxcontinuous case ;\n",
      "provided that the sum or integral are well-deﬁned.\n",
      "Thevariance variance of a random variable X, denoted by VarX(and sometimes \u001b2), is deﬁned\n",
      "by\n",
      "VarX=E(X\u0000E[X])2=EX2\u0000(EX)2:\n",
      "The square root of the variance is called the standard deviation standard\n",
      "deviation. Table C.3 lists the expect-\n",
      "ations and variances for some well-known distributions. Both variance and standard devi-\n",
      "ation measure the spread or dispersion of the distribution. Note, however, that the standard\n",
      "deviation measures the dispersion in the same units as the random variable, unlike the\n",
      "variance, which uses squared units.\n",
      "Table C.3: Expectations and variances for some well-known distributions.\n",
      "Dist. EXVarX\n",
      "Bin(n;p) np np (1\u0000p)\n",
      "Geom (p)1\n",
      "p1\u0000p\n",
      "p2\n",
      "Poi(\u0015)\u0015 \u0015\n",
      "U[\u000b;\f]\u000b+\f\n",
      "2(\f\u0000\u000b)2\n",
      "12\n",
      "Exp(\u0015)1\n",
      "\u00151\n",
      "\u00152\n",
      "t\u0017 0 (\u0017>1)\u0017\n",
      "\u0017\u00002(\u0017>2)Dist. EX VarX\n",
      "Gamma (\u000b;\u0015)\u000b\n",
      "\u0015\u000b\n",
      "\u00152\n",
      "N(\u0016;\u001b2) \u0016 \u001b2\n",
      "Beta (\u000b;\f)\u000b\n",
      "\u000b+\f\u000b\f\n",
      "(\u000b+\f)2(1+\u000b+\f)\n",
      "Weib (\u000b;\u0015)\u0000(1=\u000b)\n",
      "\u000b\u00152\u0000(2=\u000b)\n",
      "\u000b\u0000\u0010\u0000(1=\u000b)\n",
      "\u000b\u0015\u00112\n",
      "F(m;n)n\n",
      "n\u00002(n>2)2n2(m+n\u00002)\n",
      "m(n\u00002)2(n\u00004)(n>4)\n",
      "2We only use brackets in an expectation if it is unclear with respect to which random variable the ex-\n",
      "pectation is taken.Appendix C. Probability and Statistics 429\n",
      "It is sometimes useful to consider the moment generating function moment\n",
      "generating\n",
      "functionof a random variable\n",
      "X. This is the function Mdeﬁned by\n",
      "M(s)=EesX;s2R: (C.5)\n",
      "The moment generating functions of two random variables coincide if and only if the ran-\n",
      "dom variables have the same distribution; see also Theorem C.12.\n",
      "Example C.1 (Moment Generation Function of the Gamma (\u000b;\u0015)Distribution) Let\n",
      "X\u0018Gamma (\u000b;\u0015). For s<\u0015, the moment generating function of Xatsis given by\n",
      "M(s)=EesX=Z1\n",
      "0esxe\u0000\u0015x\u0015\u000bx\u000b\u00001\n",
      "\u0000(\u000b)dx\n",
      "=\u0012\u0015\n",
      "\u0015\u0000s\u0013\u000bZ1\n",
      "0e\u0000(\u0015\u0000s)x(\u0015\u0000s)\u000bx\u000b\u00001\n",
      "\u0000(\u000b)|                    {z                    }\n",
      "pdf of Gamma (\u000b;\u0015\u0000s)dx=\u0012\u0015\n",
      "\u0015\u0000s\u0013\u000b\n",
      ":\n",
      "Fors>\u0015,M(s)=1. Interestingly, the moment generating function has a much simpler\n",
      "formula than the pdf.\n",
      "C.4 Joint Distributions\n",
      "Distributions for random vectors and stochastic processes can be speciﬁed in much the\n",
      "same way as for random variables. In particular, the distribution of a random vector X=\n",
      "[X1;:::; Xn]>is completely determined by specifying the joint cdf joint cdf F, deﬁned by\n",
      "F(x1;:::; xn)=P[X16x1;:::; Xn6xn];xi2R;i=1;:::; n:\n",
      "Similarly, the distribution of a stochastic process stochastic\n",
      "process, that is, a collection of random vari-\n",
      "ablesfXt;t2Tg, for some index set T, is completely determined by its ﬁnite-dimensional\n",
      "distributions; speciﬁcally, the distributions of the random vectors [ Xt1;:::; Xtn]>for every\n",
      "choice of nandt1;:::; tn.\n",
      "By analogy to the one-dimensional case, a random vector X=[X1;:::; Xn]>taking\n",
      "values inRnis said to have a pdf fif, in the continuous case,\n",
      "P[X2B]=Z\n",
      "Bf(x) dx; (C.6)\n",
      "for all n-dimensional rectangles B. Replace the integral with a sum for the discrete case.\n",
      "The pdf is also called the joint pdf joint pdf ofX1;:::; Xn. The pdfs of the individual components —\n",
      "called marginal pdfs — marginal pdf can be recovered from the joint pdf by “integrating out the other\n",
      "variables”. For example, for a continuous random vector [ X;Y]>with pdf f, the pdf fXof\n",
      "Xis given by\n",
      "fX(x)=Z\n",
      "f(x;y) dy:430 C.5. Conditioning and Independence\n",
      "C.5 Conditioning and Independence\n",
      "Conditional probabilities and conditional distributions are used to model additional inform-\n",
      "ation on a random experiment. Independence is used to model lack of such information.\n",
      "C.5.1 Conditional Probability\n",
      "Suppose some event B\u0012\n",
      "occurs. Given this fact, event Awill occur if and only if\n",
      "A\\Boccurs, and the relative chance of Aoccurring is therefore P[A\\B]=P[B], provided\n",
      "P[B]>0. This leads to the deﬁnition of the conditional probability conditional\n",
      "probabilityofAgiven B:\n",
      "P[AjB]=P[A\\B]\n",
      "P[B];ifP[B]>0: (C.7)\n",
      "The above deﬁnition breaks down if P[B]=0. Such conditional probabilities must be\n",
      "treated with more care [11].\n",
      "Three important consequences of the deﬁnition of conditional probability are:\n",
      "1.Product rule Product rule : For any sequence of events A1;A2;:::; An,\n",
      "P[A1\u0001\u0001\u0001An]=P[A1]P[A2jA1]P[A3jA1A2]\u0001\u0001\u0001P[AnjA1\u0001\u0001\u0001An\u00001]; (C.8)\n",
      "using the abbreviation A1A2\u0001\u0001\u0001Ak:=A1\\A2\\\u0001\u0001\u0001\\ Ak.\n",
      "2.Law of total probability Law of total\n",
      "probability: IffBigforms a partition of\n",
      "(that is, Bi\\Bj=;;i,jand\n",
      "[iBi= \n",
      "), then for any event A\n",
      "P[A]=X\n",
      "iP[AjBi]P[Bi]: (C.9)\n",
      "3.Bayes’ rule Bayes’rule : LetfBigform a partition of \n",
      ". Then, for any event AwithP[A]>0,\n",
      "P[BjjA]=P[AjBj]P[Bj]P\n",
      "iP[AjBi]P[Bi]: (C.10)\n",
      "C.5.2 Independence\n",
      "Two events AandBare said to be independent independent\n",
      "eventsif the knowledge that Bhas occurred does\n",
      "not change the probability that Aoccurs. That is, A,Bindependent,P[AjB]=P[A].\n",
      "SinceP[AjB]P[B]=P[A\\B], an alternative deﬁnition of independence is\n",
      "A,Bindependent,P[A\\B]=P[A]P[B]:\n",
      "This deﬁnition covers the case where P[B]=0 and can be extended to arbitrarily many\n",
      "events: events A1;A2;:::are said to be (mutually) independent if for any kand any choice\n",
      "of distinct indices i1;:::; ik,\n",
      "P[Ai1\\Ai2\\\u0001\u0001\u0001\\ Aik]=P[Ai1]P[Ai2]\u0001\u0001\u0001P[Aik]:Appendix C. Probability and Statistics 431\n",
      "The concept of independence can also be formulated for random variables. Random\n",
      "variables X1;X2;:::are said to be independent independent\n",
      "random\n",
      "variablesif the eventsfXi16xi1g;:::;fXin6xingare\n",
      "independent for all ﬁnite choices of ndistinct indices i1;:::; inand values xi1;:::; xin.\n",
      "An important characterization of independent random variables is the following (for a\n",
      "proof, see [101], for example).\n",
      "Theorem C.1: Independence Characterization\n",
      "Random variables X1;:::; Xnwith marginal pdfs fX1;:::; fXnand joint pdf fare in-\n",
      "dependent if and only if\n",
      "f(x1;:::; xn)=fX1(x1)\u0001\u0001\u0001fXn(xn) for all x1;:::; xn: (C.11)\n",
      "Many probabilistic models involve random variables X1;X2;:::that are independent\n",
      "and identically distributed , abbreviated as iid iid . We use this abbreviation throughout this\n",
      "book.\n",
      "C.5.3 Expectation and Covariance\n",
      "Similar to the univariate case, the expected value of a real-valued function hof a random\n",
      "vector X\u0018fis a weighted average of all values that h(X) can take. Speciﬁcally, in the\n",
      "continuous case, Eh(X)=R\n",
      "h(x)f(x) dx. In the discrete case replace this multidimensional\n",
      "integral with a sum. Using this result, it is not di \u000ecult to show that for any collection of\n",
      "dependent or independent random variables X1;:::; Xn,\n",
      "E[a+b1X1+b2X2+\u0001\u0001\u0001+bnXn]=a+b1EX1+\u0001\u0001\u0001+bnEXn (C.12)\n",
      "for all constants a,b1;:::; bn. Moreover, for independent random variables,\n",
      "E[X1X2\u0001\u0001\u0001Xn]=EX1EX2\u0001\u0001\u0001EXn: (C.13)\n",
      "We leave the proofs as an exercise.\n",
      "Thecovariance covariance of two random variables XandYwith expectations \u0016Xand\u0016Y, respect-\n",
      "ively, is deﬁned as\n",
      "Cov(X;Y)=E[(X\u0000\u0016X)(Y\u0000\u0016Y)]:\n",
      "This is a measure of the amount of linear dependency between the variables. Let \u001b2\n",
      "X=\n",
      "VarXand\u001b2\n",
      "Y=VarY. A scaled version of the covariance is given by the correlation\n",
      "coe\u000ecient correlation\n",
      "coefficient,\n",
      "%(X;Y)=Cov(X;Y)\n",
      "\u001bX\u001bY:\n",
      "The following properties follow directly from the deﬁnitions of variance and covariance.\n",
      "1.VarX=EX2\u0000\u00162\n",
      "X:\n",
      "2.Var[aX+b]=a2\u001b2\n",
      "X:\n",
      "3.Cov(X;Y)=E[XY]\u0000\u0016X\u0016Y.\n",
      "4.Cov(X;Y)=Cov(Y;X).432 C.5. Conditioning and Independence\n",
      "5.\u0000\u001bX\u001bY6Cov(X;Y)6\u001bX\u001bY.\n",
      "6.Cov(aX+bY;Z)=aCov(X;Z)+bCov(Y;Z).\n",
      "7.Cov(X;X)=\u001b2\n",
      "X.\n",
      "8.Var[X+Y]=\u001b2\n",
      "X+\u001b2\n",
      "Y+2Cov(X;Y).\n",
      "9. If XandYare independent, then Cov(X;Y)=0.\n",
      "As a consequence of Properties 2 and 8 we have that for any sequence of independent\n",
      "random variables X1, . . . , Xnwith variances \u001b2\n",
      "1;:::;\u001b2\n",
      "n,\n",
      "Var[a1X1+a2X2+\u0001\u0001\u0001+anXn]=a2\n",
      "1\u001b2\n",
      "1+a2\n",
      "2\u001b2\n",
      "2+\u0001\u0001\u0001+a2\n",
      "n\u001b2\n",
      "n; (C.14)\n",
      "for any choice of constants a1;:::; an.\n",
      "For random column vectors, such as X=[X1;:::; Xn]>, it is convenient to write the\n",
      "expectations and covariances in vector and matrix notation. For a random vector Xwe\n",
      "deﬁne its expectation vector expectation\n",
      "vectoras the vector of expectations\n",
      "\u0016=[\u00161;:::;\u0016 n]>=[EX1;:::;EXn]>:\n",
      "Similarly, if the expectation of a matrix is the matrix of expectations, then given two ran-\n",
      "dom vectors X2RnandY2Rm, the n\u0002mmatrix\n",
      "Cov(X;Y)=E[(X\u0000EX)(Y\u0000EY)>] (C.15)\n",
      "has ( i;j)-th element Cov(Xi;Yj)=E[(Xi\u0000EXi)(Yj\u0000EYj)]:A consequence of this deﬁnition\n",
      "is that\n",
      "Cov(AX;BY)=ACov(X;Y)B>;\n",
      "where AandBare two matrices with nandmcolumns, respectively.\n",
      "Thecovariance matrix covariance\n",
      "matrixof the vector Xis deﬁned as the n\u0002nmatrixCov(X;X). The co-\n",
      "variance matrix is also denoted as Var(X)=Cov(X;X), in analogy with the scalar identity\n",
      "Var(X)=Cov(X;X).\n",
      "A useful application of the cyclic property of the trace of a matrix (see Theorem A.1)\n",
      "is the following. +359\n",
      "Theorem C.2: Expectation of a Quadratic Form\n",
      "LetAbe an n\u0002nmatrix and Xann-dimensional random vector with expectation\n",
      "vector\u0016and covariance matrix \u0006. The random variable Y:=X>AXhas expectation\n",
      "tr(A\u0006)+\u0016>A\u0016.\n",
      "Proof: Since Yis a scalar, it is equal to its trace. Now, using the cyclic property: EY=\n",
      "Etr(Y)=Etr(X>AX)=Etr(AXX>)=tr(AE[XX>])=tr(A(\u0006+\u0016\u0016>))=tr(A\u0006)+\n",
      "tr(A\u0016\u0016>)=tr(A\u0006)+\u0016>A\u0016. \u0003Appendix C. Probability and Statistics 433\n",
      "C.5.4 Conditional Density and Conditional Expectation\n",
      "Suppose XandYare both discrete or both continuous, with joint pdf f, and suppose fX(x)>\n",
      "0. Then, the conditional pdf conditional pdf ofYgiven X=xis given by\n",
      "fYjX(yjx)=f(x;y)\n",
      "fX(x)for all y: (C.16)\n",
      "In the discrete case, the formula is a direct translation of (C.7), with fYjX(yjx)=P[Y=\n",
      "yjX=x]. In the continuous case, a similar interpretation in terms of densities can be used;\n",
      "see, for example, [101, Page 221]. The corresponding distribution is called the conditional\n",
      "distribution ofYgiven X=x. Note that (C.16) implies thatconditional\n",
      "distribution\n",
      "f(x;y)=fX(x)fYjX(yjx):\n",
      "This is useful when the marginal and conditional pdfs are given, rather than the joint one.\n",
      "More generally, for the n-dimensional case we have\n",
      "f(x1;:::; xn)=fX1(x1)fX2jX1(x2jx1)\u0001\u0001\u0001fXnjX1;:::;Xn\u00001(xnjx1;:::; xn\u00001); (C.17)\n",
      "which is in essence a rephrasing of the product rule (C.8) in terms of probability densities. +430\n",
      "As a conditional pdf has all the properties of an ordinary pdf, we may deﬁne expecta-\n",
      "tions with respect to it. The conditional expectation of a random variable Ygiven X=xisconditional\n",
      "expectation deﬁned as\n",
      "E[YjX=x]=8>><>>:P\n",
      "yy fYjX(yjx) discrete case,R\n",
      "y fYjX(yjx) dycontinuous case.(C.18)\n",
      "Note thatE[YjX=x] is a function of x. The corresponding random variable is written\n",
      "asE[YjX]. A similar formalism can be used when conditioning on a sequence of random\n",
      "variables X1;:::; Xn. The conditional expectation has similar properties to the ordinary\n",
      "expectation. Other useful properties (see, for example, [127]) are:\n",
      "1.Tower property : IfEYexists, then\n",
      "EE[YjX]=EY: (C.19)\n",
      "2.Taking out what is known : IfEYexists, then\n",
      "E[XYjX]=XE[YjX]:\n",
      "C.6 Functions of Random Variables\n",
      "Letx=[x1;:::; xn]>be a column vector in RnandAanm\u0002nmatrix. The mapping x7!z,\n",
      "with z=Ax, is a linear transformation, as discussed in Section A.1. Now consider a +357\n",
      "random vector X=[X1;:::; Xn]>and let Z:=AX. Then Zis a random vector in Rm. The\n",
      "following theorem details how the distribution of Zis related to that of X.434 C.6. Functions of Random Variables\n",
      "Theorem C.3: Linear Transformation\n",
      "IfXhas an expectation vector \u0016Xand covariance matrix \u0006X, then the expectation\n",
      "vector of Zis\n",
      "\u0016Z=A\u0016X (C.20)\n",
      "and the covariance matrix of Zis\n",
      "\u0006Z=A\u0006XA>: (C.21)\n",
      "If, in addition, Ais an invertible n\u0002nmatrix and Xis a continuous random vector\n",
      "with pdf fX, then the pdf of the continuous random vector Z=AXis given by\n",
      "fZ(z)=fX(A\u00001z)\n",
      "jdet(A)j;z2Rn; (C.22)\n",
      "wherejdet(A)jdenotes the absolute value of the determinant of A.\n",
      "Proof: We have\u0016Z=EZ=E[AX]=AEX=A\u0016Xand\n",
      "\u0006Z=E[(Z\u0000\u0016Z)(Z\u0000\u0016Z)>]=E[A(X\u0000\u0016X)(A(X\u0000\u0016X))>]\n",
      "=AE[(X\u0000\u0016X)(X\u0000\u0016X)>]A>\n",
      "=A\u0006XA>:\n",
      "ForAinvertible and Xcontinuous (as opposed to discrete), let z=Axandx=A\u00001z.\n",
      "Consider the n-dimensional cube C=[z1;z1+h]\u0002\u0001\u0001\u0001\u0002 [zn;zn+h]. Then,\n",
      "P[Z2C]\u0019hnfZ(z);\n",
      "by deﬁnition of the joint density of Z. Let Dbe the image of Cunder A\u00001— that is, all\n",
      "points xsuch that Ax2C. Recall from Section A.1 that any matrix Blinearly transforms an +357\n",
      "n-dimensional rectangle with volume Vinto an n-dimensional parallelepiped with volume\n",
      "Vjdet(B)j. Thus, in addition to the above expression for P[Z2C], we also have\n",
      "P[Z2C]=P[X2D]\u0019hnjdet(A\u00001)jfX(x)=hnjdet(A)j\u00001fX(x):\n",
      "Equating these two expressions for P[Z2C], dividing both sides by hn, and letting hgo to\n",
      "0, we obtain (C.22). \u0003\n",
      "For a generalization of the linear transformation rule (C.22), consider an arbitrary map-\n",
      "ping x7!g(x), written out:\n",
      "26666666666666664x1\n",
      "x2\n",
      ":::\n",
      "xn377777777777777757!26666666666666664g1(x)\n",
      "g2(x)\n",
      ":::\n",
      "gn(x)37777777777777775:Appendix C. Probability and Statistics 435\n",
      "Theorem C.4: Transformation Rule\n",
      "LetXbe an n-dimensional vector of continuous random variables with pdf fX. Let\n",
      "Z=g(X), where gis an invertible mapping with inverse g\u00001andmatrix of Jacobi\n",
      "Jg; that is, the matrix of partial derivatives of g. Then, at z=g(x) the random vector\n",
      "Zhas pdf\n",
      "fZ(z)=fX(x)\n",
      "jdet(Jg(x))j=fX(g\u00001(z))jdet(Jg\u00001(z))j;z2Rn: (C.23)\n",
      "Proof: For a ﬁxed x, let z=g(x); and thus x=g\u00001(z). In the neighborhood of x, the\n",
      "function gbehaves like a linear function, in the sense that g(x+\u000e)\u0019g(x)+Jg(x)\u000efor\n",
      "small vectors \u000e; see also Section B.1. Consequently, an inﬁnitesimally small n-dimensional +399\n",
      "rectangle at xwith volume Vis transformed into an inﬁnitesimally small n-dimensional\n",
      "parallelepiped at zwith volume Vjdet(Jg(x))j. Now, as in the proof of the linear case, let\n",
      "Cbe a small cube around z=g(x) with volume hn. Let Dbe the image of Cunder g\u00001.\n",
      "Then,\n",
      "hnfZ(z)\u0019P[Z2C]\u0019hnjdet(Jg\u00001(z))jfX(x);\n",
      "and sincejdet(Jg\u00001(z))j=1=jdet(Jg(x))j, (C.23) follows as hgoes to 0. \u0003\n",
      "Typically, in coordinate transformations it is g\u00001that is given — that is, an expres-\n",
      "sion for xas a function of z.\n",
      "Example C.2 (Polar Transform) Suppose X;Yare independent and have standard nor-\n",
      "mal distribution. The joint pdf is\n",
      "fX;Y(x;y)=1\n",
      "2\u0019e\u00001\n",
      "2(x2+y2);(x;y)2R2:\n",
      "In polar coordinates we have\n",
      "X=Rcos\u0002 and Y=Rsin\u0002; (C.24)\n",
      "where R>0 is the radius and \u00022[0;2\u0019) the angle of the point ( X;Y). What is the joint pdf\n",
      "ofRand\u0002? By the radial symmetry of the bivariate normal distribution, we would expect\n",
      "\u0002to be uniform on (0 ;2\u0019). But what is the pdf of R? To work out the joint pdf, consider\n",
      "the inverse transformation g\u00001, deﬁned by\n",
      "\"r\n",
      "\u0012#\n",
      "g\u00001\n",
      "7\u0000!\"rcos\u0012\n",
      "rsin\u0012#\n",
      "=\"x\n",
      "y#\n",
      ":\n",
      "The corresponding matrix of Jacobi is\n",
      "Jg\u00001(r;\u0012)=\"cos\u0012\u0000rsin\u0012\n",
      "sin\u0012 rcos\u0012#\n",
      ";436 C.7. Multivariate Normal Distribution\n",
      "which has determinant r. Since x2+y2=r2(cos2\u0012+sin2\u0012)=r2, it follows by the trans-\n",
      "formation rule (C.23) that the joint pdf of Rand\u0002is given by\n",
      "fR;\u0002(r;\u0012)=fX;Y(x;y)r=1\n",
      "2\u0019e\u00001\n",
      "2r2r; \u00122(0;2\u0019);r>0:\n",
      "By integrating out \u0012andr, respectively, we ﬁnd fR(r)=re\u0000r2=2andf\u0002(\u0012)=1=(2\u0019). Since\n",
      "fR;\u0002is the product of fRandf\u0002, the random variables Rand\u0002are independent.\n",
      "C.7 Multivariate Normal Distribution\n",
      "The normal (or Gaussian) distribution — especially its multidimensional version — plays\n",
      "a central role in data science and machine learning. Recall from Table C.1 that a random\n",
      "variable Xis said to have a normal normal\n",
      "distributiondistribution with parameters \u0016and\u001b2if its pdf is given\n",
      "by\n",
      "f(x)=1\n",
      "\u001bp\n",
      "2\u0019e\u00001\n",
      "2(x\u0000\u0016\n",
      "\u001b)2\n",
      ";x2R: (C.25)\n",
      "We write X\u0018N(\u0016;\u001b2). The parameters \u0016and\u001b2are the expectation and variance of the\n",
      "distribution, respectively. If \u0016=0 and\u001b=1 then\n",
      "f(x)=1p\n",
      "2\u0019e\u0000x2=2;\n",
      "and the distribution is known as the standard normal standard\n",
      "normaldistribution. The cdf of the standard\n",
      "normal distribution is often denoted byand its pdf by '. In Figure C.5 the pdf of the\n",
      "N(\u0016;\u001b2) distribution for various \u0016and\u001b2is plotted.\n",
      "-4 -2 0 2 4 600.20.40.60.8\n",
      "N(0,1)N(0,1/4)\n",
      "N(2,1)\n",
      "Figure C.5: The pdf of the N(\u0016;\u001b2) distribution for various \u0016and\u001b2.\n",
      "We next consider some important properties of the normal distribution.\n",
      "Theorem C.5: Standardization\n",
      "LetX\u0018N(\u0016;\u001b2) and deﬁne Z=(X\u0000\u0016)=\u001b. Then Zhas a standard normal distribu-\n",
      "tion.Appendix C. Probability and Statistics 437\n",
      "Proof: The cdf of Zis given by\n",
      "P[Z6z]=P[(X\u0000\u0016)=\u001b6z]=P[X6\u0016+\u001bz]\n",
      "=Z\u0016+\u001bz\n",
      "\u000011\n",
      "\u001bp\n",
      "2\u0019e\u00001\n",
      "2(x\u0000\u0016\n",
      "\u001b)2\n",
      "dx=Zz\n",
      "\u000011p\n",
      "2\u0019e\u0000y2=2dy=(z);\n",
      "where we make a change of variable y=(x\u0000\u0016)=\u001bin the fourth equation. Hence, Z\u0018\n",
      "N(0;1). \u0003\n",
      "The rescaling procedure in Theorem C.5 is called standardization standardization . It follows from The-\n",
      "orem C.5 that any X\u0018N(\u0016;\u001b2) can be written as\n",
      "X=\u0016+\u001bZ;where Z\u0018N(0;1):\n",
      "In other words, any normal random variable can be viewed as an a\u000ene transformation affine\n",
      "transformation—\n",
      "that is, a linear transformation plus a constant — of a standard normal random variable.\n",
      "We now generalize this to ndimensions. Let Z1;:::; Znbe independent and standard\n",
      "normal random variables. The joint pdf of Z=[Z1;:::; Zn]>is given by\n",
      "fZ(z)=nY\n",
      "i=11p\n",
      "2\u0019e\u00001\n",
      "2z2\n",
      "i=(2\u0019)\u0000n\n",
      "2e\u00001\n",
      "2z>z;z2Rn: (C.26)\n",
      "We write Z\u0018N(0;I), where Iis the identity matrix. Consider the a \u000ene transformation\n",
      "X=\u0016+BZ (C.27)\n",
      "for some m\u0002nmatrix Bandm-dimensional vector \u0016. Note that, by (C.20) and (C.21), X +434\n",
      "has expectation vector \u0016and covariance matrix \u0006=BB>:We say that Xhas a multivariate\n",
      "normal multivariate\n",
      "normalormultivariate Gaussian distribution with mean vector \u0016and covariance matrix \u0006.\n",
      "We write X\u0018N(\u0016;\u0006).\n",
      "The following theorem states that any a \u000ene combination of independent multivariate\n",
      "normal random variables is again multivariate normal.\n",
      "Theorem C.6: A \u000ene Transformation of Normal Random Vectors\n",
      "LetX1;X2;:::; Xrbe independent mi-dimensional normal random vectors, with\n",
      "Xi\u0018N(\u0016i;\u0006i),i=1;:::; r. Then, for any n\u00021 vector aandn\u0002mimatrices\n",
      "B1;:::; Br,\n",
      "a+rX\n",
      "i=1BiXi\u0018N\u0012\n",
      "a+rX\n",
      "i=1Bi\u0016i;rX\n",
      "i=1Bi\u0006iB>\n",
      "i\u0013\n",
      ": (C.28)\n",
      "Proof: Denote the n-dimensional random vector in the left-hand side of (C.28) by Y. By\n",
      "deﬁnition, each Xican be written as \u0016i+AiZi, where thefZigare independent (because the\n",
      "fXigare independent), so that\n",
      "Y=a+rX\n",
      "i=1Bi(\u0016i+AiZi)=a+rX\n",
      "i=1Bi\u0016i+rX\n",
      "i=1BiAiZi;438 C.7. Multivariate Normal Distribution\n",
      "which is an a \u000ene combination of independent standard normal random vectors. Hence, Y\n",
      "is multivariate normal. Its expectation vector and covariance matrix can be found easily\n",
      "from Theorem C.3. \u0003 +434\n",
      "The next theorem shows that the distribution of a subvector of a multivariate normal\n",
      "random vector is again normal.\n",
      "Theorem C.7: Marginal Distributions of Normal Random Vectors\n",
      "LetX\u0018N(\u0016;\u0006) be an n-dimensional normal random vector. Decompose X,\u0016, and\n",
      "\u0006as\n",
      "X=\"Xp\n",
      "Xq#\n",
      ";\u0016=\"\u0016p\n",
      "\u0016q#\n",
      ";\u0006=\"\u0006p\u0006r\n",
      "\u0006>\n",
      "r\u0006q#\n",
      "; (C.29)\n",
      "where \u0006pis the upper left p\u0002pcorner of \u0006and\u0006qis the lower right q\u0002qcorner of\n",
      "\u0006. Then, Xp\u0018N(\u0016p;\u0006p).\n",
      "Proof: We give a proof assuming that \u0006is positive deﬁnite. Let BB>be the (lower)\n",
      "Cholesky decomposition of \u0006. We can write +375\n",
      "\"Xp\n",
      "Xq#\n",
      "=\"\u0016p\n",
      "\u0016q#\n",
      "+\"BpO\n",
      "CrCq#\n",
      "|     {z     }\n",
      "B\"Zp\n",
      "Zq#\n",
      "; (C.30)\n",
      "where ZpandZqare independent p- and q-dimensional standard normal random vectors.\n",
      "In particular, Xp=\u0016p+BpZp, which means that Xp\u0018N(\u0016p;\u0006p), since BpB>\n",
      "p=\u0006p.\u0003\n",
      "By relabeling the elements of Xwe see that Theorem C.7 implies that anysubvector of\n",
      "Xhas a multivariate normal distribution. For example, Xq\u0018N(\u0016q;\u0006q).\n",
      "The following theorem shows that not only the marginal distributions of a normal ran-\n",
      "dom vector are normal, but also its conditional distributions .\n",
      "Theorem C.8: Conditional Distributions of Normal Random Vectors\n",
      "LetX\u0018N(\u0016;\u0006) be an n-dimensional normal random vector with det( \u0006)>0. IfX\n",
      "is decomposed as in (C.29), then\n",
      "\u0010\n",
      "XqjXp=xp\u0011\n",
      "\u0018N(\u0016q+\u0006>\n",
      "r\u0006\u00001\n",
      "p(xp\u0000\u0016p);\u0006q\u0000\u0006>\n",
      "r\u0006\u00001\n",
      "p\u0006r): (C.31)\n",
      "As a consequence, XpandXqareindependent if and only if they are uncorrelated ;\n",
      "that is, if \u0006r=O(zero matrix).\n",
      "Proof: From (C.30) we see that Xp=\u0016p+BpZpandXq=\u0016q+CrZp+CqZq. Consequently,\n",
      "(XqjXp=xp)=\u0016q+CrB\u00001\n",
      "p(xp\u0000\u0016p)+CqZq;\n",
      "where Zqis aq-dimensional multivariate standard normal random vector. It follows that\n",
      "Xqconditional on Xp=xphas aN(\u0016q+CrB\u00001\n",
      "p(xp\u0000\u0016p);CqC>\n",
      "q) distribution. The proof ofAppendix C. Probability and Statistics 439\n",
      "(C.31) is completed by observing that \u0006>\n",
      "r\u0006\u00001\n",
      "p=CrB>\n",
      "p(B>\n",
      "p)\u00001B\u00001\n",
      "p=CrB\u00001\n",
      "p, and\n",
      "\u0006q\u0000\u0006>\n",
      "r\u0006\u00001\n",
      "p\u0006r=CrC>\n",
      "r+CqC>\n",
      "q\u0000CrB\u00001\n",
      "p\u0006r|{z}\n",
      "BpC>r=CqC>\n",
      "q:\n",
      "IfXpandXqare independent, then they are obviously uncorrelated, as \u0006r=E[(Xp\u0000\n",
      "\u0016p)(Xq\u0000\u0016q)>]=E(Xp\u0000\u0016p)E(Xq\u0000\u0016q)>=O. Conversely, if \u0006r=O, then by (C.31) the\n",
      "conditional distribution of Xqgiven Xpis the same as the unconditional distribution of Xq;\n",
      "that is, N(\u0016q;\u0006q). In other words, Xqis independent of Xp. \u0003\n",
      "The next few results are about the relationships between the normal, chi-squared,\n",
      "\u001f2distribution Student, and Fdistributions, deﬁned in Table C.1. Recall that the chi-squared family of\n",
      "distributions, denoted by \u001f2\n",
      "n, are simply Gamma (n=2;1=2) distributions, where the para-\n",
      "meter n2f1;2;3;:::gis called the degrees of freedom .\n",
      "Theorem C.9: Relationship Between Normal and \u001f2Distributions\n",
      "IfX\u0018N(\u0016;\u0006) is an n-dimensional normal random vector with det( \u0006)>0, then\n",
      "(X\u0000\u0016)>\u0006\u00001(X\u0000\u0016)\u0018\u001f2\n",
      "n: (C.32)\n",
      "Proof: LetBB>be the Cholesky decomposition of \u0006, where Bis invertible. Since Xcan\n",
      "be written as \u0016+BZ, where Z=[Z1;:::; Zn]>is a vector of independent standard normal\n",
      "random variables, we have\n",
      "(X\u0000\u0016)>\u0006\u00001(X\u0000\u0016)=(X\u0000\u0016)>(BB>)\u00001(X\u0000\u0016)=Z>Z=nX\n",
      "i=1Z2\n",
      "i:\n",
      "Using the independence of Z1;:::; Zn, the moment generating function of Y=Pn\n",
      "i=1Z2\n",
      "iis +429\n",
      "given by\n",
      "EesY=Ees(Z2\n",
      "1+\u0001\u0001\u0001+Z2\n",
      "n)=E[esZ2\n",
      "1\u0001\u0001\u0001esZ2\n",
      "n]=\u0010\n",
      "EesZ2\u0011n;\n",
      "where Z\u0018N(0;1). The moment generating function of Z2is\n",
      "EesZ2=Z1\n",
      "\u00001esz21p\n",
      "2\u0019e\u0000z2=2dz=1p\n",
      "2\u0019Z1\n",
      "\u00001e\u00001\n",
      "2(1\u00002s)z2dz=1p\n",
      "1\u00002s;\n",
      "so thatEesY=\u0010\n",
      "1\n",
      "2=(1\n",
      "2\u0000s)\u0011n\n",
      "2,s<1\n",
      "2, which is the moment generating function of the\n",
      "Gamma (n=2;1=2) distribution; that is, the \u001f2\n",
      "ndistribution — see Example C.1. The res- +429\n",
      "ult now follows from the uniqueness of the moment generating function. \u0003\n",
      "A consequence of Theorem C.9 is that if X=[X1;:::; Xn]>isn-dimensional standard\n",
      "normal, then the squared length kXk2=X2\n",
      "1+\u0001\u0001\u0001+X2\n",
      "nhas a\u001f2\n",
      "ndistribution. If instead Xi\u0018\n",
      "N(\u0016i;1),i=1;:::, thenkXk2is said to have a noncentral\u001f2\n",
      "ndistribution noncentral \u001f2\n",
      "ndistribution. This distribution\n",
      "depends on thef\u0016igonly through the norm k\u0016k. We writekXk2\u0018\u001f2\n",
      "n(\u0012), where\u0012=k\u0016kis\n",
      "thenoncentrality parameter noncentrality\n",
      "parameter.\n",
      "Such distributions frequently occur when considering projections of multivariate nor-\n",
      "mal random variables, as summarized in the following theorem.440 C.7. Multivariate Normal Distribution\n",
      "Theorem C.10: Relationship Between Normal and Noncentral \u001f2Distributions\n",
      "LetX\u0018N(\u0016;In) be an n-dimensional normal random vector and let Vk\u001aV mbe\n",
      "linear subspaces of dimensions kandm, respectively, with k<m6n. Let Xkand\n",
      "Xmbe orthogonal projections of XontoVkandVm, and let\u0016kand\u0016mbe the cor-\n",
      "responding projections of \u0016. Then, the following holds.\n",
      "1. The random vectors Xk,Xm\u0000Xk, and X\u0000Xmare independent.\n",
      "2.kXkk2\u0018\u001f2\n",
      "k(k\u0016kk),kXm\u0000Xkk2\u0018\u001f2\n",
      "m\u0000k(k\u0016m\u0000\u0016kk), andkX\u0000Xmk2\u0018\u001f2\n",
      "n\u0000m(k\u0016\u0000\n",
      "\u0016mk).\n",
      "Proof: Letv1;:::; vnbe an orthonormal basis of Rnsuch that v1;:::; vkspansVkand\n",
      "v1;:::; vmspansVm. By (A.8) we can write the orthogonal projection matrices onto Vj, +364\n",
      "asPj=Pj\n",
      "i=1viv>\n",
      "i,j=k;m;n, whereVnis deﬁned as Rn. Note that Pnis simply the iden-\n",
      "tity matrix. Let V:=[v1;:::; vn] and deﬁne Z:=[Z1;:::; Zn]>=V>X. Recall from Sec-\n",
      "tion A.2 that any orthogonal transformation such as z=V>xislength preserving ; that is, +363\n",
      "kzk=kxk.\n",
      "To prove the ﬁrst statement of the theorem, note that V>Xj=V>PjX=[Z1;:::; Zj;\n",
      "0;:::; 0]>,j=k;m. It follows that V>(Xm\u0000Xk)=[0;:::; 0;Zk+1;:::; Zm;0;:::; 0]>and\n",
      "V>(X\u0000Xm)=[0;:::; 0;Zm+1;:::; Zn]>. Moreover, being a linear transformation of a nor-\n",
      "mal random vector, Zis also normal, with covariance matrix V>V=In. In particular, the\n",
      "fZigareindependent . This shows that Xk,Xm\u0000XkandX\u0000Xmare independent as well.\n",
      "Next, observe that kXkk=kV>Xkk=kZkk, where Zk:=[Z1;:::; Zk]>. The latter vector\n",
      "has independent components with variances 1, and its squared norm has therefore (by\n",
      "deﬁnition) a \u001f2\n",
      "k(\u0012) distribution. The noncentrality parameter is \u0012=kEZkk=kEXkk=k\u0016kk,\n",
      "again by the length-preserving property of orthogonal transformations. This shows that\n",
      "kXkk2\u0018\u001f2\n",
      "k(k\u0016kk). The distributions of kXm\u0000Xkk2andkX\u0000Xmk2follow by analogy. \u0003\n",
      "Theorem C.10 is frequently used in the statistical analysis of normal linear models ; see\n",
      "Section 5.4. In typical situations \u0016lies in the subspace Vmor evenVk— in which case +182\n",
      "kXm\u0000Xkk2\u0018\u001f2\n",
      "m\u0000kandkX\u0000Xmk2\u0018\u001f2\n",
      "n\u0000m, independently. The (scaled) quotient then turns\n",
      "out to have an Fdistribution — a consequence of the following theorem.\n",
      "Theorem C.11: Relationship Between \u001f2andFDistributions\n",
      "LetU\u0018\u001f2\n",
      "mandV\u0018\u001f2\n",
      "nbe independent. Then,\n",
      "U=m\n",
      "V=n\u0018F(m;n):\n",
      "Proof: For notational simplicity, let c=m=2 and d=n=2. The pdf of W=U=Vis\n",
      "given by fW(w)=R1\n",
      "0fU(wv)v fV(v) dv. Substituting the pdfs of the corresponding GammaAppendix C. Probability and Statistics 441\n",
      "distributions, we have\n",
      "fW(w)=Z1\n",
      "0(wv)c\u00001e\u0000wv=2\n",
      "\u0000(c) 2cvvd\u00001e\u0000v=2\n",
      "\u0000(d) 2ddv=wc\u00001\n",
      "\u0000(c)\u0000(d) 2c+dZ1\n",
      "0vc+d\u00001e\u0000(1+w)v=2dv\n",
      "=\u0000(c+d)\n",
      "\u0000(c)\u0000(d)wc\u00001\n",
      "(1+w)c+d;\n",
      "where the last equality follows from the fact that the integrand is equal to \u0000(\u000b)\u0015\u0000\u000btimes\n",
      "the density of the Gamma (\u000b;\u0015) distribution with \u000b=c+dand\u0015=(1+w)=2. The density\n",
      "ofZ=n\n",
      "mU\n",
      "Vis given by\n",
      "fZ(z)=fW(z m=n)m=n:\n",
      "The proof is completed by comparing the resulting expression with the pdf of the Fdistri-\n",
      "bution given in Table C.1. \u0003 +427\n",
      "Corollary C.1 (Relationship Between Normal, \u001f2, and tDistributions) LetZ\u0018N(0;1)\n",
      "andV\u0018\u001f2\n",
      "nbe independent. Then,\n",
      "ZpV=n\u0018tn:\n",
      "Proof: LetT=Z=pV=n. Because Z2\u0018\u001f2\n",
      "1, we have by Theorem C.11 that T2\u0018F(1;n).\n",
      "The result follows now from the symmetry around 0 of the pdf of Tand the fact that the\n",
      "square of a tnrandom variable has an F(1;n) distribution. \u0003\n",
      "C.8 Convergence of Random Variables\n",
      "Recall that a random variable Xis a function from \n",
      "toR. If we have a sequence of random\n",
      "variables X1;X2;:::(for instance, Xn(!)=X(!)+1\n",
      "nfor each!2\n",
      "), then one can consider\n",
      "the pointwise convergence:\n",
      "lim\n",
      "n!1Xn(!)=X(!);for all!2\n",
      ";\n",
      "in which case we say that X1;X2;:::converges surely sure\n",
      "convergencetoX. A more interesting type of\n",
      "convergence uses the probability measure Passociated with X.\n",
      "Deﬁnition C.1: Convergence in Probability\n",
      "The sequence of random variables X1;X2;::: converges in probability to a random\n",
      "variable Xif, for all\">0,\n",
      "lim\n",
      "n!1P[jXn\u0000Xj>\"]=0:\n",
      "We denote the convergence in probability convergence in\n",
      "probabilityasXnP\u0000!X:442 C.8. Convergence of Random Variables\n",
      "Convergence in probability refers only to the distribution of Xn. Instead, if the sequence\n",
      "X1;X2;:::is deﬁned on a common probability space, then we can consider the following\n",
      "mode of convergence that uses the joint distribution of the sequence of random variables.\n",
      "Deﬁnition C.2: Almost Sure Convergence\n",
      "The sequence of random variables X1;X2;::: converges almost surely to a random\n",
      "variable Xif for every \">0\n",
      "lim\n",
      "n!1P\"\n",
      "sup\n",
      "k>njXk\u0000Xj>\"#\n",
      "=0:\n",
      "We denote the almost sure convergence almost sure\n",
      "convergenceasXna:s:\u0000!X.\n",
      "Note that in accordance with these deﬁnitions Xna:s:\u0000!0 is equivalent to supk>njXkjP\u0000!0.\n",
      "Example C.3 (Convergence in Probability Versus Almost Sure Convergence) Since\n",
      "the eventfjXn\u0000Xj>\"gis contained infsupk>njXk\u0000Xj>\"g, we can conclude that almost\n",
      "sure convergence implies convergence in probability. However, the converse is not true in\n",
      "general. For instance, consider the iid sequence X1;X2;:::with marginal distribution\n",
      "P[Xn=1]=1\u0000P[Xn=0]=1=n:\n",
      "Clearly, XnP\u0000!0. However, for \"<1 and any n=1;2;:::we have,\n",
      "P\"\n",
      "sup\n",
      "k>njXkj6\"#\n",
      "=P[Xn6\";Xn+16\";::: ]\n",
      "=P[Xn6\"]\u0002P[Xn+16\"]\u0002\u0001\u0001\u0001 (using independence)\n",
      "=lim\n",
      "m!1mY\n",
      "k=nP[Xk6\"]=lim\n",
      "m!1mY\n",
      "k=n \n",
      "1\u00001\n",
      "k!\n",
      "=lim\n",
      "m!1n\u00001\n",
      "n\u0002n\n",
      "n+1\u0002\u0001\u0001\u0001\u0002m\u00001\n",
      "m=0:\n",
      "It follows that P[supk>njXk\u00000j>\"]=1 for any 0<\"< 1 and all n>1. In other words, it\n",
      "isnottrue that Xna:s:\u0000!0.\n",
      "Another important type of convergence is useful when we are interested in estimating\n",
      "expectations or multidimensional integrals via Monte Carlo methodology. +67\n",
      "Deﬁnition C.3: Convergence in Distribution\n",
      "The sequence of random variables X1;X2;::: is said to converge in distribution to a\n",
      "random variable Xwith distribution function FX(x)=P[X6x] provided that:\n",
      "lim\n",
      "n!1P[Xn6x]=FX(x) for all xsuch that lim\n",
      "a!xFX(a)=FX(x): (C.33)\n",
      "We denote the convergence in distribution convergence in\n",
      "distributionas either Xnd\u0000!X, orXnd\u0000!FX.Appendix C. Probability and Statistics 443\n",
      "The generalization to random vectors replaces (C.33) with\n",
      "lim\n",
      "n!1P[Xn2A]=P[X2A] for all A\u001aRnsuch thatP[X2@A]=0; (C.34)\n",
      "where@Adenotes the boundary of the set A.\n",
      "A useful tool for demonstrating convergence in distribution is the characteristic func-\n",
      "tion Xof a random vector X, deﬁned as the expectation:characteristic\n",
      "function\n",
      "+225  X(t) :=Eeit>X;t2Rn: (C.35)\n",
      "The moment generating function in (C.5) is a special case of the characteristic function\n",
      "evaluated at t=\u0000is. Note that while the moment generating function of a random variable\n",
      "may not exist, its characteristic function always exists. The characteristic function of a\n",
      "random vector X\u0018fis closely related to the Fourier transform of its pdf f. +392\n",
      "Example C.4 (Characteristic Function of a Multivariate Gaussian Random Vector)\n",
      "The density of the multivariate standard normal distribution is given in (C.26) and thus the\n",
      "characteristic function of Z\u0018N(0;In) is\n",
      " Z(t)=Eeit>Z=(2\u0019)\u0000n=2Z\n",
      "Rneit>z\u00001\n",
      "2kzk2dz\n",
      "=e\u0000ktk2=2(2\u0019)\u0000n=2Z\n",
      "Rne\u00001\n",
      "2kz\u0000it>k2dz=e\u0000ktk2=2;t2Rn:\n",
      "Hence, the characteristic function of the random vector X=\u0016+BZin (C.27) with mul-\n",
      "tivariate normal distribution N(\u0016;\u0006) is given by +437\n",
      " X(t)=Eeit>X=Eeit>(\u0016+BZ)\n",
      "=eit>\u0016Eei(B>t)>Z=eit>\u0016 Z(B>t)\n",
      "=eit>\u0016\u0000kB>tk2=2=eit>\u0016\u0000t>\u0006t=2:\n",
      "The importance of the characteristic function is mainly derived from the following\n",
      "result, for which a proof can be found, for example, in [11].\n",
      "Theorem C.12: Characteristic Function\n",
      "Suppose that  X1(t); X2(t);:::are the characteristic functions of the sequence of\n",
      "random vectors X1;X2;:::and X(t) is the characteristic function of X. Then, the\n",
      "following three statements are equivalent:\n",
      "1. lim n!1 Xn(t)= X(t) for all t2Rn.\n",
      "2.Xnd\u0000!X.\n",
      "3. lim n!1Eh(Xn)=Eh(X) for all bounded continuous functions h:Rd7!R.444 C.8. Convergence of Random Variables\n",
      "Example C.5 (Convergence in Distribution) Deﬁne the random variables Y1;Y2;:::\n",
      "as\n",
      "Yn:=nX\n",
      "k=1Xk 1\n",
      "2!k\n",
      "; n=1;2;:::;\n",
      "where X1;X2;:::iid\u0018Ber(1=2). We now show that Ynd\u0000!U(0;1). First, note that\n",
      "Eexp(i tYn)=nY\n",
      "k=1Eexp(i tXk=2k)=2\u0000nnY\n",
      "k=1(1+exp(i t=2k)):\n",
      "Second, from the collapsing product, (1 \u0000exp(i t=2n))Qn\n",
      "k=1(1+exp(i t=2k))=1\u0000exp(i t),\n",
      "we have\n",
      "Eexp(i tYn)=(1\u0000exp(i t))1=2n\n",
      "1\u0000exp(i t=2n):\n",
      "It follows that lim n!1Eexp(i tYn)=(exp(i t)\u00001)=(it);which we recognize as the charac-\n",
      "teristic function of the U(0;1) distribution. +443\n",
      "Yet another mode of convergence is the following.\n",
      "Deﬁnition C.4: Convergence in Lp-norm\n",
      "The sequence of random variables X1;X2;::: converges in Lp-norm to a random\n",
      "variable Xif\n",
      "lim\n",
      "n!1EjXn\u0000Xjp=0;p>1:\n",
      "We denote the convergence in Lp-norm convergence in\n",
      "Lp-normasXnLp\n",
      "\u0000!X.\n",
      "The case for p=2 corresponds to convergence in mean squared error. The following\n",
      "example illustrates that convergence in Lp-norm is qualitatively di \u000berent from convergence\n",
      "in distribution.\n",
      "Example C.6 (Comparison of Modes of Convergence) Deﬁne Xn:=1\u0000X, where X\n",
      "has a uniform distribution on the interval (0,1). Clearly, Xnd\u0000!U(0;1). However, EjXn\u0000\n",
      "Xj\u0000!Ej1\u00002Xj=1=2 and so the sequence does not converge in L1-norm. In addition,\n",
      "P[jXn\u0000Xj>\"]\u0000!1\u0000\",0 and so Xndoes not converge in probability as well.\n",
      "Thus, in general Xnd\u0000!Ximplies neither XnP\u0000!X, nor XnL1\n",
      "\u0000!X.\n",
      "We mention, however, that if Xnd\u0000!cfor some constant c, then XnP\u0000!cas well. To\n",
      "see this, note that Xnd\u0000!cstands for\n",
      "lim\n",
      "n!1P[Xn6x]=8>><>>:1;x>c\n",
      "0;x<c:\n",
      "In other words, we can write:\n",
      "P[jXn\u0000cj>\"]61\u0000P[Xn6c+\"]+P[Xn6c\u0000\"]\u0000!1\u00001+0=0;n!1;\n",
      "which shows that XnP\u0000!cby deﬁnition.Appendix C. Probability and Statistics 445\n",
      "Deﬁnition C.5: Complete Convergence\n",
      "The sequence of random variables X1;X2;::: is said to converge completely toXif\n",
      "for all\">0 X\n",
      "nP[jXn\u0000Xj>\"]<1:\n",
      "We denote the complete convergence complete\n",
      "convergenceasXncpl:\u0000!X.\n",
      "Example C.7 (Complete and Almost Sure Convergence) We show that complete\n",
      "convergence implies almost sure convergence. We can bound the criterion for almost sure\n",
      "convergence as follows:\n",
      "P[sup\n",
      "k>njXk\u0000Xj>\"]=P[[k>nfjXk\u0000Xj>\"g]\n",
      "6X\n",
      "k>nP[jXk\u0000Xj>\"] by union bound in (C.1)\n",
      "61X\n",
      "k=1P[jXk\u0000Xj>\"]\n",
      "|                  {z                  }\n",
      "=c<1from Xncpl.\u0000!X\u0000n\u00001X\n",
      "k=1P[jXk\u0000Xj>\"]\n",
      "6c\u0000n\u00001X\n",
      "k=1P[jXk\u0000Xj>\"]\u0000!c\u0000c=0; n!1:\n",
      "Hence, by deﬁnition Xna:s:\u0000!X.\n",
      "The next theorem shows how the di \u000berent types of convergence are related to each\n",
      "other. For example, in the diagram below, the notationp>q)means that Lp-norm convergence\n",
      "implies Lq-norm convergence under the assumption that p>q>1.\n",
      "Theorem C.13: Modes of Convergence\n",
      "The most general relationships among the various modes of convergence for numer-\n",
      "ical random variables are shown on the following hierarchical diagram:\n",
      "Xncpl.\u0000!X)Xna:s:\u0000!X\n",
      "+\n",
      "XnP\u0000!X)Xnd\u0000!X\n",
      "*\n",
      "XnLp\n",
      "\u0000!Xp>q)XnLq\n",
      "\u0000!X:446 C.8. Convergence of Random Variables\n",
      "Proof: 1.First, we show that XnP\u0000!X)Xnd\u0000!Xusing the inequality P[A\\B]6P[A]\n",
      "for any event B. To this end, consider the distribution function FXofX:\n",
      "FXn(x)=P[Xn6x]=P[Xn6x;jXn\u0000Xj>\"]+P[Xn6x;jXn\u0000Xj6\"]\n",
      "6P[jXn\u0000Xj>\"]+P[Xn6x;X6Xn+\"]\n",
      "6P[jXn\u0000Xj>\"]+P[X6x+\"]:\n",
      "Now, in the arguments above we can switch the roles of XnandX(there is a symmetry) to\n",
      "deduce the analogous result: FX(x)6P[jX\u0000Xnj>\"]+P[Xn6x+\"]. Therefore, making\n",
      "the switch x!x\u0000\"gives FX(x\u0000\")6P[jX\u0000Xnj> \"]+FXn(x). Putting it all together\n",
      "gives:\n",
      "FX(x\u0000\")\u0000P[jX\u0000Xnj>\"]6FXn(x)6P[jXn\u0000Xj>\"]+FX(x+\"):\n",
      "Taking n!1 on both sides yields for any \">0:\n",
      "FX(x\u0000\")6lim\n",
      "n!1FXn(x)6FX(x+\"):\n",
      "Since FXis continuous at xby assumption we can take \"#0 to conclude that\n",
      "lim n!1FXn(x)=FX(x).\n",
      "2.Second, we show that XnLp\n",
      "\u0000! X)XnLq\n",
      "\u0000! Xforp>q>1. Since the function\n",
      "f(x)=xq=pis concave for q=p61, Jensen’s inequality yields: +62\n",
      "(EjXjp)q=p=f(EjXjp)>Ef(jXjp)=EjXjq:\n",
      "In other words, ( EjXn\u0000Xjq)1=q6(EjXn\u0000Xjp)1=p\u0000!0, proving the statement of the theorem.\n",
      "3.Third, we show that XnL1\n",
      "\u0000!X)XnP\u0000!X. First note that for any random variable\n",
      "Y, we can write: EjYj>E[jYj1fjYj>\"g]>E[j\"j1fjYj>\"g]=\"P[jYj> \"]:Therefore, we obtain\n",
      "Chebyshev’s inequality Chebyshev ’s\n",
      "inequality:\n",
      "P[jYj>\"]6EjYj\n",
      "\": (C.36)\n",
      "Using Chebyshev’s inequality and XnL1\n",
      "\u0000!X, we can write\n",
      "P[jXn\u0000Xj>\"]6EjXn\u0000Xj\n",
      "\"\u0000!0;n!1:\n",
      "Hence, by deﬁnition XnP\u0000!X.\n",
      "4.Finally, Xncpl.\u0000!X)Xna:s:\u0000!X)XnP\u0000!Xis proved in Examples C.7 and C.3. \u0003\n",
      "Finally, we will make use of the following theorem.\n",
      "Theorem C.14: Slutsky\n",
      "Letg(x;y) be a continuous scalar function of vectors xandy. Suppose that Xnd\u0000!X\n",
      "andYnP\u0000!cfor some ﬁnite constant c. Then,\n",
      "g(Xn;Yn)d\u0000!g(X;c):Appendix C. Probability and Statistics 447\n",
      "Proof: We prove the theorem for scalar XandY. The proof for random vectors is analog-\n",
      "ous. First, we show that Zn:=\"Xn\n",
      "Yn#\n",
      "d\u0000!\"X\n",
      "c#\n",
      "=:Zusing, for example, Theorem C.12. In +443\n",
      "other words, we wish to show that the characteristic function of the joint distribution of Xn\n",
      "andYnconverges pointwise as n!1 :\n",
      " Xn;Yn(t)=Eei(t1Xn+t2Yn)\u0000!eit2cEeit1X= X;c(t);8t2R2:\n",
      "To show the limit above, consider\n",
      "j Xn;Yn(t)\u0000 X;c(t)j6j Xn;c(t)\u0000 X;c(t)j+j Xn;Yn(t)\u0000 Xn;c(t)j\n",
      "=jeit2cE(eit1Xn\u0000eit1X)j+jEei(t1Xn+t2c)(eit2(Yn\u0000c)\u00001)j\n",
      "6jeit2cj\u0002jE(eit1Xn\u0000eit1X)j+Ejei(t1Xn+t2c)j\u0002jeit2(Yn\u0000c)\u00001j\n",
      "6j Xn(t1)\u0000 X(t1)j+Ejeit2(Yn\u0000c)\u00001j:\n",
      "Since Xnd\u0000!X, Theorem C.12 implies that  Xn(t1)\u0000! X(t1), and the ﬁrst term j Xn(t1)\u0000\n",
      " X(t1)jgoes to zero. For the second term we use the fact that\n",
      "jeix\u00001j=\f\f\fRx\n",
      "0i ei\u0012d\u0012\f\f\f6\f\f\fRx\n",
      "0ji ei\u0012jd\u0012\f\f\f=jxj;x2R\n",
      "to obtain the bound:\n",
      "Ejeit2(Yn\u0000c)\u00001j=Ejeit2(Yn\u0000c)\u00001j1fjYn\u0000cj>\"g+Ejeit2(Yn\u0000c)\u00001j1fjYn\u0000cj6\"g\n",
      "62E1fjYn\u0000cj>\"g+Ejt2(Yn\u0000c)j1fjYn\u0000cj6\"g\n",
      "62P[jYn\u0000cj>\"]+jt2j\"\u0000!j t2j\"; n!1:\n",
      "Since\"is arbitrary, we can let \"#0 to conclude that lim n!1j Xn;Yn(t)\u0000 X;c(t)j=0. In other\n",
      "words, Znd\u0000!Z, and by the continuity of g, we have g(Zn)d\u0000!g(Z) org(Xn;Yn)d\u0000!\n",
      "g(X;c). \u0003\n",
      "Example C.8 (Necessity of Slutsky’s Condition) The condition that Ynconverges in\n",
      "probability to a constant cannot be relaxed. For example, suppose that g(x;y)=x+y,\n",
      "Xnd\u0000!X\u0018N(0;1) and Ynd\u0000!Y\u0018N(0;1). Then, our intuition tempts us to incorrectly\n",
      "conclude that Xn+Ynd\u0000!N(0;2). This intuition is false, because we can have Yn=\u0000Xn\n",
      "for all nso that Xn+Yn=0, while both XandYhave the same marginal distribution (in\n",
      "this case standard normal).\n",
      "C.9 Law of Large Numbers and Central Limit Theorem\n",
      "Two main results in probability are the law of large numbers andthe central limit theorem .\n",
      "Both are limit theorems involving sums of independent random variables. In particular,\n",
      "consider a sequence X1;X2;:::of iid random variables with ﬁnite expectation \u0016and ﬁnite\n",
      "variance\u001b2. For each ndeﬁne Xn:=(X1+\u0001\u0001\u0001+Xn)=n. What can we say about the (random)\n",
      "sequence of averages X1;X2;X3;:::? By (C.12) and (C.14) we have EXn=\u0016andVarXn= +431\n",
      "\u001b2=n:Hence, as nincreases, the variance of the (random) average Xngoes to 0. This means448 C.9. Law of Large Numbers and Central Limit Theorem\n",
      "that by Deﬁnition C.8, the average Xnconverges to \u0016inL2-norm as n!1 , that is, XnL2\n",
      "\u0000!\n",
      "\u0016.\n",
      "In fact, to obtain convergence in probability the variance need not be ﬁnite — it is\n",
      "su\u000ecient to assume that \u0016=EX<1.\n",
      "Theorem C.15: Weak Law of Large Numbers\n",
      "law of large\n",
      "numbersIfX1;:::; Xnare iid with ﬁnite expectation \u0016, then for all \">0\n",
      "lim\n",
      "n!1Ph\n",
      "jXn\u0000\u0016j>\"i\n",
      "=0:\n",
      "In other words, XnP\u0000!\u0016.\n",
      "The theorem has a natural generalization for random vectors. Namely, if \u0016=EX<1,\n",
      "thenPh\n",
      "kXn\u0000\u0016k>\"i\n",
      "!0, wherek\u0001kis the Euclidean norm. We give a proof in the scalar +357\n",
      "case.\n",
      "Proof: LetZk:=Xk\u0000\u0016for all k, so thatEZ=0. We thus need to show that ZnP\u0000!0.\n",
      "We use the properties of the characteristic function of Zdenoted as  Z. Due to the iid +443\n",
      "assumption, we have\n",
      " Zn(t)=EeitZn=EnY\n",
      "i=1eitZi=n=nY\n",
      "i=1EeiZit=n=nY\n",
      "i=1 Z(t=n)=[ Z(t=n)]n: (C.37)\n",
      "An application of Taylor’s Theorem B.1 in the neighborhood of t=0 yields\n",
      " Z(t=n)= Z(0)+o(t=n):\n",
      "Since Z(0)=1, we have:\n",
      " Zn(t)=[ Z(t=n)]n=[1+o(1=n)]n\u0000!1;n!1:\n",
      "The characteristic function of a random variable that always equals zero is 1. Therefore,\n",
      "Theorem C.12 implies that Znd\u0000!0. However, according to Example C.6, convergence in\n",
      "distribution to a constant implies convergence in probability. Hence, ZnP\u0000!0.\u0003\n",
      "There is also a stronger version of this theorem, as follows.\n",
      "Theorem C.16: Strong Law of Large Numbers\n",
      "strong law of\n",
      "large numbersIfX1;:::; Xnare iid with expectation \u0016andEX2<1, then for all \">0\n",
      "lim\n",
      "n!1P\"\n",
      "sup\n",
      "k>njXk\u0000\u0016j>\"#\n",
      "=0:\n",
      "In other words, Xna:s:\u0000!\u0016.Appendix C. Probability and Statistics 449\n",
      "Proof: First, note that any random variable Xcan be written as the di \u000berence of two non-\n",
      "negative random variables: X=X+\u0000X\u0000, where X+:=maxfX;0gandX\u0000:=\u0000minfX;0g.\n",
      "Thus, without loss of generality, we assume that the random variables in the theorem above\n",
      "are nonnegative.\n",
      "Second, from the sequence fX1;X2;X3;:::gwe can pick up the subsequence fX1;X4;X9;\n",
      "X16;:::g=:fXj2g. Then, from Chebyshev’s inequality (C.36) and the iid condition, we have\n",
      "1X\n",
      "j=1Ph\f\f\fXj2\u0000\u0016\f\f\f>\"i\n",
      "6VarX\n",
      "\"21X\n",
      "j=11\n",
      "j2<1:\n",
      "Therefore, by deﬁnition Xn2cpl:\u0000!\u0016and from Theorem C.13 we conclude that Xn2a:s:\u0000!\u0016.\n",
      "Third, for any arbitrary n, we can ﬁnd a k, say k=bpnc, so that k26n6(k+1)2:For\n",
      "such a kand nonnegative X1;X2;:::, it holds that\n",
      "k2\n",
      "(k+1)2Xk26Xn6X(k+1)2(k+1)2\n",
      "k2:\n",
      "Since Xk2andX(k+1)2converge almost surely to \u0016ask(and hence n) goes to inﬁnity, we\n",
      "conclude that Xna:s:\u0000!\u0016. \u0003\n",
      "Note that the condition EX2<1in Theorem C.16 can be weakened to EjXj<1and\n",
      "the iid condition on the variables X1;:::; Xncan be relaxed to mere pairwise independence.\n",
      "The corresponding proof, however, is signiﬁcantly more di \u000ecult.\n",
      "TheCentral Limit Theorem Central Limit\n",
      "Theoremdescribes the approximate distribution of Xn, and it applies\n",
      "to both continuous and discrete random variables. Loosely, it states that\n",
      "the average of a large number of iid random variables\n",
      "approximately has a normal distribution.\n",
      "Speciﬁcally, the random variable Xnhas a distribution that is approximately normal, with\n",
      "expectation \u0016and variance \u001b2=n.\n",
      "Theorem C.17: Central Limit Theorem\n",
      "IfX1;:::; Xnare iid with ﬁnite expectation \u0016and ﬁnite variance \u001b2, then for all\n",
      "x2R,\n",
      "lim\n",
      "n!1P266664Xn\u0000\u0016\n",
      "\u001b=pn6x377775=(x);\n",
      "whereis the cdf of the standard normal distribution.\n",
      "Proof: LetZk:=(Xk\u0000\u0016)=\u001bfor all k, so thatEZ=0 andEZ2=1. We thus need to show\n",
      "thatpnZnd\u0000!N(0;1). We again use the properties of the characteristic function. Let  Z +443\n",
      "be the characteristic function of an iid copy of Z, then due to the iid assumption a similar\n",
      "calculation to the one in (C.37) yields:\n",
      " pnZn(t)=EeitpnZn=[ Z(t=pn)]n:450 C.9. Law of Large Numbers and Central Limit Theorem\n",
      "An application of Taylor’s Theorem B.1 in the neighborhood of t=0 yields\n",
      " Z(t=pn)=1+tpn 0\n",
      "Z(0)+t2\n",
      "2n 00\n",
      "Z(0)+o(t2=n):\n",
      "Since 0\n",
      "Z(0)=Ed\n",
      "dteitZ\f\f\ft=0=iEZ=0 and 00\n",
      "Z(0)=i2EZ2=\u00001, we have:\n",
      " pnZn(t)=h\n",
      " Z(t=pn)in=\"\n",
      "1\u0000t2\n",
      "2n+o(1=n)#n\n",
      "\u0000!e\u0000t2=2;n!1:\n",
      "From Example C.4, we recognize e\u0000t2=2as the characteristic function of the standard normal\n",
      "distribution. Thus, from Theorem C.12 we conclude thatpnZnd\u0000!N(0;1).\u0003\n",
      "Figure C.6 shows the central limit theorem in action. The left part shows the pdfs of\n",
      "X1;2X2;:::; 4X4for the case where the fXighave a U[0;1] distribution. The right part\n",
      "shows the same for the Exp(1) distribution. In both cases, we clearly see convergence to a\n",
      "bell-shaped curve, characteristic of the normal distribution.\n",
      "0 1 2 3 400.20.40.60.81\n",
      "0 2 4 6 800.20.40.60.81\n",
      "Figure C.6: Illustration of the central limit theorem for (left) the uniform distribution and\n",
      "(right) the exponential distribution.\n",
      "The multivariate version of the central limit theorem is the basis for many asymptotic\n",
      "(in the size of the training set) results in machine learning and data science.\n",
      "Theorem C.18: Multivariate Central Limit Theorem\n",
      "LetX1;:::; Xnbe iid random vectors with expectation vector \u0016and ﬁnite covariance\n",
      "matrix \u0006. Deﬁne Xn:=(X1+\u0001\u0001\u0001+Xn)=n. Then,\n",
      "pn(Xn\u0000\u0016)d\u0000!N(0;\u0006) as n!1:\n",
      "One application is as follows. Suppose that a parameter of interest, \u0012\u0003, is the unique\n",
      "solution of the system of equations E (Xj\u0012\u0003)=0, where is a vector-valued (or multi-\n",
      "valued) function and the distribution of Xdoes not depend on \u0012. An M-estimator M-estimator of\u0012\u0003,Appendix C. Probability and Statistics 451\n",
      "denoted b\u0012n, is the solution to the system of equations that results from approximating the\n",
      "expectation with respect to Xusing an average of niid copies of X:\n",
      " n(\u0012) :=1\n",
      "nnX\n",
      "i=1 (Xij\u0012):\n",
      "Thus, n(b\u0012n)=0.\n",
      "Theorem C.19: M-estimator\n",
      "The M-estimator is asymptotically normal as n!1 :\n",
      "pn(b\u0012n\u0000\u0012\u0003)d\u0000!N(0;A\u00001BA\u0000>); (C.38)\n",
      "where A:=\u0000E@ \n",
      "@\u0012(Xj\u0012\u0003) +400 and B:=E\u0002 (Xj\u0012\u0003) (Xj\u0012\u0003)>\u0003is the covariance matrix\n",
      "of (Xj\u0012\u0003).\n",
      "Proof: We give a proof under the simplifying assumption3thatb\u0012nis a unique root, that is,\n",
      "for any\u0012and\", there exists a \u000e>0 such thatkb\u0012n\u0000\u0012k>\"implies thatk n(\u0012)k>\u000e.\n",
      "First, we argue that b\u0012nP\u0000!\u0012\u0003; that is,P[kb\u0012n\u0000\u0012\u0003k> \"]!0. From the multivariate\n",
      "extension of Theorem C.15, we have that\n",
      " n(\u0012\u0003)P\u0000!E n(\u0012\u0003)=E (Xj\u0012\u0003)=0:\n",
      "Therefore, using the uniqueness of b\u0012n, we can show that b\u0012nP\u0000!\u0012\u0003via the bound:\n",
      "Ph\n",
      "kb\u0012n\u0000\u0012\u0003k>\"i\n",
      "6Ph\n",
      "k n(\u0012\u0003)k>\u000ei\n",
      "=Ph\n",
      "k n(\u0012\u0003)\u0000E n(\u0012\u0003)k>\u000ei\n",
      "!0;n!1:\n",
      "Second, we take a Taylor expansion of each component of the vector  n(b\u0012n) around\u0012\u0003to\n",
      "obtain:\n",
      " n(b\u0012n)= n(\u0012\u0003)+Jn(\u00120)(b\u0012n\u0000\u0012\u0003);\n",
      "where Jn(\u0012) is the Jacobian of  nat\u0012, and\u00120lies on the line segment joining b\u0012nand\u0012\u0003.\n",
      "Rearrange the last equation and multiply both sides bypnA\u00001to obtain:\n",
      "\u0000A\u00001Jn(\u00120)pn(b\u0012n\u0000\u0012\u0003)=A\u00001pn n(\u0012\u0003):\n",
      "By the central limit theorem,pn (\u0012\u0003) converges in distribution to N(0;B). Therefore,\n",
      "\u0000A\u00001Jn(\u00120)pn(b\u0012n\u0000\u0012\u0003)d\u0000!N(0;A\u00001BA\u0000>):\n",
      "Theorem C.15 (the weak law of large numbers) applied to the iid random matrices\n",
      "f@\n",
      "@\u0012 (Xij\u0012)gshows that\n",
      "Jn(\u0012)P\u0000!E@\n",
      "@\u0012 (Xj\u0012):\n",
      "Moreover, since b\u0012nP\u0000!\u0012\u0003andJnis continuous in \u0012, we have that Jn(\u00120)P\u0000!\u0000 A. Therefore,\n",
      "by Slutsky’s theorem, \u0000A\u00001Jn(\u00120)pn(b\u0012n\u0000\u0012\u0003)\u0000pn(b\u0012n\u0000\u0012\u0003)P\u0000!0. \u0003 +446\n",
      "3The result holds under far less stringent assumptions.452 C.9. Law of Large Numbers and Central Limit Theorem\n",
      "Finally, we mention Laplace’s approximation Laplace ’s\n",
      "approximation, which shows how integrals or expecta-\n",
      "tions behave under the normal distribution with a vanishingly small variance.\n",
      "Theorem C.20: Laplace’s Approximation\n",
      "Suppose that \u0012n!\u0012\u0003, where\u0012\u0003lies in the interior of the open set \u0002\u0012Rpand that\n",
      "\u0006nis ap\u0002pcovariance matrix such that \u0006n!\u0006\u0003. Let g:\u00027!Rbe a continuous\n",
      "function with g(\u0012\u0003),0. Then, as n!1 ,\n",
      "np=2Z\n",
      "\u0002g(\u0012) e\u0000n\n",
      "2(\u0012\u0000\u0012n)>\u0006\u00001\n",
      "n(\u0012\u0000\u0012n)d\u0012!g(\u0012\u0003)p\n",
      "j2\u0019\u0006\u0003j: (C.39)\n",
      "Proof: (Sketch for a bounded domain \u0002.) The left-hand side of (C.39) can be written as\n",
      "the expectation with respect to the N(\u0012n;\u0006n=n) distribution:\n",
      "p\n",
      "j2\u0019\u0006njZ\n",
      "\u0002g(\u0012)exp\u0010\n",
      "\u0000n\n",
      "2(\u0012\u0000\u0012n)>\u0006\u00001\n",
      "n(\u0012\u0000\u0012n)\u0011\n",
      "j2\u0019\u0006n=nj1=2d\u0012=p\n",
      "j2\u0019\u0006njE[g(Xn)1fXn2\u0002g];\n",
      "where Xn\u0018N(\u0012n;\u0006n=n). Let Z\u0018N(0;I). Then,\u0012n+\u00061=2\n",
      "nZ=pnhas the same distribution\n",
      "asXnand\u0010\n",
      "\u0012n+\u00061=2\n",
      "nZ=pn\u0011\n",
      "!\u0012\u0003asn!1 . By continuity of g(\u0012)1f\u00122\u0002gin the interior\n",
      "of\u0002, asn!1 :4\n",
      "E[g(Xn)1fXn2\u0002g]=E\u0014\n",
      "g\u0012\n",
      "\u0012n+\u00061=2\n",
      "nZpn\u0013\n",
      "1\u001a\u0012\n",
      "\u0012n+\u00061=2\n",
      "nZpn\u0013\n",
      "2\u0002\u001b\u0015\n",
      "\u0000!g(\u0012\u0003)1f\u0012\u00032\u0002g:\n",
      "Since\u0012\u0003lies in the interior of \u0002, we have 1f\u0012\u00032\u0002g=1, completing the proof. \u0003\n",
      "As an application of Theorem C.20 we can show the following.\n",
      "Theorem C.21: Approximation of Integrals\n",
      "Suppose that r:\u00127!Ris twice continuously di \u000berentiable with a unique global\n",
      "minimum at \u0012\u0003andg:\u00127!Ris continuous with g(\u0012\u0003)>0. Then, as n!1 ,\n",
      "lnZ\n",
      "Rpg(\u0012) e\u0000n r(\u0012)d\u0012'\u0000n r(\u0012\u0003)\u0000p\n",
      "2lnn: (C.40)\n",
      "More generally, if rnhas a unique global minimum \u0012nandrn!r)\u0012n!\u0012\u0003, then\n",
      "lnZ\n",
      "Rpg(\u0012) e\u0000n rn(\u0012)d\u0012'\u0000n r(\u0012\u0003)\u0000p\n",
      "2lnn:\n",
      "Proof: We only sketch the proof of (C.40). Let H(\u0012) be the Hessian matrix of rat\u0012. By +402\n",
      "Taylor’s theorem we can write\n",
      "r(\u0012)\u0000r(\u0012\u0003)=(\u0012\u0000\u0012\u0003)>@r(\u0012\u0003)\n",
      "@\u0012| {z }\n",
      "=0+1\n",
      "2(\u0012\u0000\u0012\u0003)>H(\u0012)(\u0012\u0000\u0012\u0003);\n",
      "4We can exchange the limit and expectation, as g(\u0012)1f\u00122\u0002g6max\u00122\u0002g(\u0012) andR\n",
      "\u0002max\u00122\u0002g(\u0012) d\u0012=\n",
      "j\u0002jmax\u00122\u0002g(\u0012)<1.Appendix C. Probability and Statistics 453\n",
      "where\u0012is a point that lies on the line segment joining \u0012\u0003and\u0012. Since\u0012\u0003is a unique\n",
      "global minimum, there must be a small enough neighborhood of \u0012\u0003, say \u0002, such that ris a\n",
      "strictly (also known as strongly) convex function on \u0002. In other words, H(\u0012) is a positive +405\n",
      "deﬁnite matrix for all \u00122\u0002and there exists a smallest positive eigenvalue \u00151>0 such\n",
      "thatx>H(\u0012)x>\u00151kxk2for all x. In addition, since the maximum eigenvalue of H(\u0012) is a\n",
      "continuous function of \u00122\u0002and\u0002is bounded, there must exist a constant \u00152>\u0015 1such\n",
      "thatx>H(\u0012)x6\u00152kxk2for all x. In other words, denoting r\u0003:=r(\u0012\u0003), we have the bounds:\n",
      "\u0000\u00152\n",
      "2k\u0012\u0000\u0012\u0003k26\u0000(r(\u0012)\u0000r\u0003)6\u0000\u00151\n",
      "2k\u0012\u0000\u0012\u0003k2;\u00122\u0002:\n",
      "Therefore,\n",
      "e\u0000n r\u0003Z\n",
      "\u0002g(\u0012) e\u0000n\u00152\n",
      "2k\u0012\u0000\u0012\u0003k2d\u00126Z\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u00126e\u0000n r\u0003Z\n",
      "\u0002g(\u0012) e\u0000n\u00151\n",
      "2k\u0012\u0000\u0012\u0003k2d\u0012:\n",
      "An application of Theorem C.20 yieldsR\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u0012=O(e\u0000n r\u0003=np=2) and, more import-\n",
      "antly,\n",
      "lnZ\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u0012'\u0000n r\u0003\u0000p\n",
      "2lnn:\n",
      "Thus, the proof will be complete once we show thatR\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u0012;with\u0002:=Rpn\u0002, is\n",
      "asymptotically negligible compared toR\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u0012. Since\u0012\u0003is a global minimum that\n",
      "lies outside any neighborhood of \u0002, there must exists a constant c>0 such that r(\u0012)\u0000r\u0003>c\n",
      "for all\u00122\u0002. Therefore,\n",
      "Z\n",
      "\u0002g(\u0012) e\u0000n r(\u0012)d\u0012=e\u0000(n\u00001)r\u0003Z\n",
      "\u0002g(\u0012) e\u0000r(\u0012)e\u0000(n\u00001)(r(\u0012)\u0000r\u0003)d\u0012\n",
      "6e\u0000(n\u00001)r\u0003Z\n",
      "\u0002g(\u0012) e\u0000r(\u0012)e\u0000(n\u00001)cd\u0012\n",
      "6e\u0000(n\u00001)(r\u0003+c)Z\n",
      "Rpg(\u0012) e\u0000r(\u0012)d\u0012=O(e\u0000n(r\u0003+c)):\n",
      "The last expression is of order o(e\u0000n r\u0003=np=2), concluding the proof. \u0003\n",
      "C.10 Markov Chains\n",
      "Deﬁnition C.6: Markov Chain\n",
      "AMarkov chain Markov chain is a collectionfXt;t=0;1;2;:::gof random variables (or ran-\n",
      "dom vectors) whose futures are conditionally independent of their pasts given their\n",
      "present values. That is,\n",
      "P[Xt+12AjXs;s6t]=P[Xt+12AjXt] for all t: (C.41)\n",
      "In other words, the conditional distribution of the future variable Xt+1, given the entire\n",
      "pastfXs;s6tg, is the same as the conditional distribution of Xt+1given only the present Xt.\n",
      "Property (C.41) is called the Markov property Markov\n",
      "property.454 C.10. Markov Chains\n",
      "The index tinXtis usually seen as a “time” or “step” parameter. The index set\n",
      "f0;1;2;:::gin the deﬁnition above was chosen out of convenience. It can be replaced by any\n",
      "countable index set. We restrict ourselves to time-homogeneous time-\n",
      "homogeneousMarkov chains — Markov\n",
      "chains for which the conditional pdfs fXt+1jXt(yjx) do not depend on t; we abbreviate these\n",
      "asq(yjx). Thefq(yjx)gare called the (one-step) transition densities transition\n",
      "densityof the Markov chain.\n",
      "Note that the random variables or vectors fXtgmay be discrete (e.g., taking values in some\n",
      "setf1;:::; rg) orcontinuous (e.g., taking values in an interval [0 ;1] orRd). In particular, in\n",
      "thediscrete case, each q(yjx) is a probability: q(yjx)=P[Xt+1=yjXt=x].\n",
      "The distribution of X0is called the initial distribution initial\n",
      "distributionof the Markov chain. The one-\n",
      "step transition densities and the initial distribution completely specify the distribution of\n",
      "the random vector [ X0;X1;:::; Xt]>. Namely, we have by the product rule (C.17) and the +433\n",
      "Markov property that the joint pdf is given by\n",
      "fX0;:::;Xt(x0;:::; xt)=fX0(x0)fX1jX0(x1jx0)\u0001\u0001\u0001fXtjXt\u00001;:::;X0(xtjxt\u00001;:::; x0)\n",
      "=fX0(x0)fX1jX0(x1jx0)\u0001\u0001\u0001fXtjXt\u00001(xtjxt\u00001)\n",
      "=fX0(x0)q(x1jx0)q(x2jx1)\u0001\u0001\u0001q(xtjxt\u00001):\n",
      "A Markov chain is said to be ergodic ergodic if the probability distribution of Xtconverges to\n",
      "a ﬁxed distribution as t!1 . Ergodicity is a property of many Markov chains. Intuitively,\n",
      "the probability of encountering the Markov chain in a state xat a time tfar into the future\n",
      "should not depend on the t, provided that the Markov chain can reach every state from any\n",
      "other state — such Markov chains are said to be irreducible — and does not “escape” to\n",
      "inﬁnity. Thus, for an ergodic Markov chain the pdf fXt(x) converges to a ﬁxed limiting pdf limiting pdf\n",
      "f(x) ast!1 , irrespective of the starting state. For the discrete case, f(x) corresponds to\n",
      "the long-run fraction of times that the Markov process visits x.\n",
      "Under mild conditions (such as irreducibility) the limiting pdf f(x) can be found by\n",
      "solving the global balance equations global balance\n",
      "equations:\n",
      "f(x)=8>><>>:P\n",
      "yf(y)q(xjy) (discrete case) ;R\n",
      "f(y)q(xjy) dy (continuous case) :(C.42)\n",
      "For the discrete case the rationale behind this is as follows. Since f(x) is the long-run\n",
      "proportion of time that the Markov chain spends in x, the proportion of transitions out of\n",
      "xisf(x). This should be balanced with the proportion of transitions intostate x, which isP\n",
      "yf(y)q(xjy).\n",
      "One is often interested in a stronger type of balance equations. Imagine that we have\n",
      "taken a video of the evolution of the Markov chain, which we may run in forward and\n",
      "reverse time. If we cannot determine whether the video is running forward or backward\n",
      "(we cannot determine any systematic “looping”, which would indicate in which direction\n",
      "time is ﬂowing), the chain is said to be time-reversible or simply reversible reversible .\n",
      "Although not every Markov chain is reversible, each ergodic Markov chain, when run\n",
      "backwards, gives another Markov chain — the reverse Markov chain reverse\n",
      "Markov chain— with transition\n",
      "densities eq(yjx)=f(y)q(xjy)=f(x). To see this, ﬁrst observe that f(x) is the long-run\n",
      "proportion of time spent in xfor both the original and reverse Markov chain. Secondly,\n",
      "the “probability ﬂux” from xtoyin the reversed chain must be equal to the probability\n",
      "ﬂux from ytoxin the original chain, meaning f(x)eq(yjx)=f(y)q(xjy), which yields theAppendix C. Probability and Statistics 455\n",
      "stated transition probabilities for the reversed chain. In particular, for a reversible Markov\n",
      "chain we have\n",
      "f(x)q(yjx)=f(y)q(xjy) for all x;y: (C.43)\n",
      "These are the detailed (or local) balance equations . Note that the detailed balance equa-local balance\n",
      "equations tions imply the global balance equations. Hence, if a Markov chain is irreducible and there\n",
      "exists a pdf such that (C.43) holds, then f(x) must be the limiting pdf. In the discrete state\n",
      "space case an additional condition is that the chain must be aperiodic aperiodic , meaning that the\n",
      "return times to the same state cannot always be a multiple of some integer >2.\n",
      "Example C.9 (Random Walk on a Graph) Consider a Markov chain that performs a\n",
      "“random walk” on the graph in Figure C.7, at each step jumping from the current vertex\n",
      "(node) to one of the adjacent vertices, with equal probability. Clearly this Markov chain is\n",
      "reversible. It is also irreducible and aperiodic. Let f(x) denote the limiting probability that\n",
      "the chain is in vertex x. By symmetry, f(1)=f(2)=f(7)=f(8),f(4)=f(5) and f(3)=\n",
      "f(6). Moreover, by the detailed balance equations, f(4)=5=f(1)=3, and f(3)=4=f(1)=3.\n",
      "It follows that f(1)+\u0001\u0001\u0001+f(8)=4f(1)+2\u00025=3f(1)+2\u00024=3f(1)=10f(1)=1, so\n",
      "that f(1)=1=10,f(3)=2=15, and f(4)=1=6.\n",
      "1\n",
      "234\n",
      "567\n",
      "8\n",
      "Figure C.7: The random walk on this graph is reversible.\n",
      "C.11 Statistics\n",
      "Statistics deals with the gathering, summarization, analysis, and interpretation of data. The\n",
      "two main branches of statistics are:\n",
      "1.Classical or frequentist statistics frequentist\n",
      "statistics: Here the observed data \u001cis viewed as the out-\n",
      "come of random data Tdescribed by a probabilistic model — usually the model is\n",
      "speciﬁed up to a (multidimensional) parameter; that is, T\u0018 g(\u0001j\u0012) for some\u0012. The\n",
      "statistical inference is then purely concerned with the model and in particular with\n",
      "the parameter \u0012. For example, on the basis of the data one may wish to\n",
      "(a) estimate the parameter,\n",
      "(b) perform statistical tests on the parameter, or456 C.12. Estimation\n",
      "(c) validate the model.\n",
      "2.Bayesian statistics Bayesian\n",
      "statistics: In this approach we average over all possible values of the\n",
      "parameter\u0012using a user-speciﬁed weight function g(\u0012) and obtain the model\n",
      "T\u0018R\n",
      "g(\u0001j\u0012)g(\u0012) d\u0012. For practical computations, this means that we can treat \u0012as a\n",
      "random variable with pdf g(\u0012). Bayes’ formula g(\u0012j\u001c)/g(\u001cj\u0012)g(\u0012) is used to learn +47\n",
      "\u0012based on the observed data \u001c.\n",
      "Example C.10 (Iid Sample) The most fundamental statistical model is where the data\n",
      "T=X1;:::; Xnis such that the random variables X1;:::; Xnare assumed to be independent\n",
      "and identically distributed:\n",
      "X1;:::; Xniid\u0018Dist;\n",
      "according to some known or unknown distribution Dist. An iid sample is often called a\n",
      "random sample random sample in the statistics literature. Note that the word “sample” can refer to both a\n",
      "collection of random variables and to a single random variable. It should be clear from the\n",
      "context which meaning is being used.\n",
      "Often our guess or model for the true distribution is speciﬁed up to an unknown para-\n",
      "meter\u0012, with\u00122\u0002. The most common model is:\n",
      "X1;:::; Xniid\u0018N(\u0016;\u001b2);\n",
      "in which case \u0012=(\u0016;\u001b2) and \u0002 =R\u0002R+.\n",
      "C.12 Estimation\n",
      "Suppose the model g(\u0001j\u0012) for the dataTis completely speciﬁed up to an unknown para-\n",
      "meter vector \u0012. The aim is to estimate \u0012on the basis of the observed data \u001conly (an altern-\n",
      "ative goal could be to estimate \u0011= (\u0012) for some vector-valued function  ). Speciﬁcally,\n",
      "the goal is to ﬁnd an estimator T=T(T) that is close to the unknown \u0012. The correspond-estimatoring outcome t=T(\u001c) is the estimate of\u0012. The bias of an estimator Tof\u0012is deﬁned asestimate\n",
      "bias ET\u0000\u0012. An estimator Tof\u0012is said to be unbiased ifE\u0012T=\u0012. We often write b\u0012for both\n",
      "an estimator and estimate of \u0012. The mean squared error (MSE) of a real-valued estimatormean squared\n",
      "error Tis deﬁned as\n",
      "MSE =E\u0012(T\u0000\u0012)2:\n",
      "An estimator T1is said to be more e\u000ecient efficient than an estimator T2if the MSE of T1is smaller\n",
      "than the MSE of T2. The MSE can be written as the sum\n",
      "MSE =(E\u0012T\u0000\u0012)2+Var\u0012T:\n",
      "The ﬁrst term measures the unbiasedness and the second is the variance of the estimator.\n",
      "In particular, for an unbiased estimator the MSE of an estimator is simply equal to its\n",
      "variance.\n",
      "For simulation purposes it is often important to include the running time of the estim-\n",
      "ator in e \u000eciency comparisons. One way to compare two unbiased estimators T1andT2is\n",
      "to compare their relative time variance products relative time\n",
      "variance\n",
      "products,\n",
      "riVarTi\n",
      "(ETi)2;i=1;2; (C.44)Appendix C. Probability and Statistics 457\n",
      "where r1andr2are the times required to calculate the estimators T1andT2, respectively.\n",
      "In this scheme, T1is considered more e \u000ecient than T2if its relative time variance product\n",
      "is smaller. We discuss next two systematic approaches for constructing sound estimators.\n",
      "C.12.1 Method of Moments\n",
      "Suppose x1;:::; xnare outcomes from an iid sample X1;:::; Xn\u0018iidg(xj\u0012), where\u0012=\n",
      "[\u00121;:::;\u0012 k]>is unknown. The moments of the sampling distribution can be easily estim-\n",
      "ated. Namely, if X\u0018g(xj\u0012), then the r-th moment of X, that is\u0016r(\u0012)=E\u0012Xr(assuming\n",
      "it exists), can be estimated through the sample r-th moment :1\n",
      "nPn\n",
      "i=1xr\n",
      "i:Themethod of mo-sample r-th\n",
      "moment ments involves choosing the estimate b\u0012of\u0012such that each of the ﬁrst ksample and true\n",
      "method of\n",
      "momentsmoments are matched:\n",
      "1\n",
      "nnX\n",
      "i=1xr\n",
      "i=\u0016r(b\u0012);r=1;2;:::; k:\n",
      "In general, this set of equations is nonlinear and so its solution often has to be found\n",
      "numerically.\n",
      "Example C.11 (Sample Mean and Sample Variance) Suppose the data is given by\n",
      "T=fX1;:::; Xng, where thefXigform an iid sample from a general distribution with mean\n",
      "\u0016and variance \u001b2<1. Matching the ﬁrst two moments gives the set of equations\n",
      "1\n",
      "nnX\n",
      "i=1xi=\u0016;\n",
      "1\n",
      "nnX\n",
      "i=1x2\n",
      "i=\u00162+\u001b2:\n",
      "The method of moments estimates for \u0016and\u001b2are therefore the sample mean sample mean\n",
      "b\u0016=x=1\n",
      "nnX\n",
      "i=1xi; (C.45)\n",
      "and\n",
      "c\u001b2=1\n",
      "nnX\n",
      "i=1x2\n",
      "i\u0000(x)2=1\n",
      "nnX\n",
      "i=1(xi\u0000x)2: (C.46)\n",
      "The corresponding estimator for \u0016,X, is unbiased. However, the estimator for \u001b2is biased:\n",
      "Ec\u001b2=\u001b2(n\u00001)=n. An unbiased estimator is the sample variance sample variance\n",
      "S2=c\u001b2n\n",
      "n\u00001=1\n",
      "n\u00001nX\n",
      "i=1(Xi\u0000X)2:\n",
      "Its square root, S=p\n",
      "S2, is called the sample standard deviation sample\n",
      "standard\n",
      "deviation.\n",
      "Example C.12 (Sample Covariance Matrix) The method of moments can also be\n",
      "used to estimate the covariance matrix of a random vector. In particular, let the X1;:::; Xn458 C.12. Estimation\n",
      "be iid copies of a d-dimensional random vector Xwith mean vector \u0016and covari-\n",
      "ance matrix \u0006. We assume n>d. The moment estimator for \u0016is, as in the d=1 case,\n",
      "X=(X1+\u0001\u0001\u0001+Xn)=n. As the covariance matrix can be written (see (C.15)) as +432\n",
      "\u0006=E(X\u0000\u0016)(X\u0000\u0016)>;\n",
      "the method of moments yields the estimator\n",
      "b\u0006=1\n",
      "nnX\n",
      "i=1(Xi\u0000X)(Xi\u0000X)>: (C.47)\n",
      "Similar to the one-dimensional case ( d=1), replacing the factor 1 =nwith 1=(n\u00001) gives\n",
      "an unbiased estimator, called the sample covariance matrix sample\n",
      "covariance\n",
      "matrix.\n",
      "C.12.2 Maximum Likelihood Method\n",
      "The concept of likelihood is central in statistics. It describes in a precise way the informa-\n",
      "tion about model parameters that is contained in the observed data.\n",
      "LetTbe a (random) data object that is modeled as a draw from the pdf g(\u001cj\u0012) (dis-\n",
      "crete or continuous) with parameter vector \u00122\u0002. Let\u001cbe an outcome ofT. The function\n",
      "L(\u0012j\u001c) :=g(\u001cj\u0012);\u00122\u0002, is called the likelihood function of\u0012, based on\u001c. The (nat-likelihood\n",
      "function ural) logarithm of the likelihood function is called the log-likelihood function and is often\n",
      "log-likelihood\n",
      "functiondenoted by a lower case l.\n",
      "Note that L(\u0012j\u001c) and g(\u001cj\u0012) have the same formula, but the ﬁrst is viewed as a\n",
      "function of\u0012for ﬁxed\u001c, where the second is viewed as a function of \u001cfor ﬁxed\u0012.\n",
      "The concept of likelihood is particularly useful when Tis modeled as an iid sample\n",
      "fX1;:::; Xngfrom some pdf ˚ g. In that case, the likelihood of the data \u001c=fx1;:::; xng, as a\n",
      "function of\u0012, is given by the product\n",
      "L(\u0012j\u001c)=nY\n",
      "i=1˚g(xij\u0012): (C.48)\n",
      "Let\u001cbe an observation from T\u0018 g(\u001cj\u0012), and suppose that g(\u001cj\u0012) takes its largest\n",
      "value at\u0012=b\u0012. In a way this b\u0012is our best estimate for \u0012, as it maximizes the probability\n",
      "(density) for the observation \u001c. It is called the maximum likelihood estimate (MLE) of\u0012.\n",
      "Note that b\u0012=b\u0012(\u001c) is a function of \u001c. The corresponding random variable, also denoted b\u0012is\n",
      "themaximum likelihood estimator maximum\n",
      "likelihood\n",
      "estimator(also abbreviated as MLE).\n",
      "Maximization of L(\u0012j\u001c) as a function of \u0012is equivalent (when searching for the max-\n",
      "imizer) to maximizing the log-likelihood l(\u0012j\u001c), as the natural logarithm is an increasing\n",
      "function. This is often easier, especially when Tis an iid sample from some sampling\n",
      "distribution. For example, for Lof the form (C.48), we have\n",
      "l(\u0012j\u001c)=nX\n",
      "i=1ln ˚g(xij\u0012):Appendix C. Probability and Statistics 459\n",
      "Ifl(\u0012j\u001c) is a di \u000berentiable function with respect to \u0012and the maximum is attained in the\n",
      "interior of\u0002,and there exists a unique maximum point , then we can ﬁnd the MLE of \u0012by\n",
      "solving the equations\n",
      "@\n",
      "@\u0012il(\u0012j\u001c)=0;i=1;:::; d:\n",
      "Example C.13 (Bernoulli Random Sample) Suppose we have data \u001cn=fx1;:::; xng\n",
      "and assume the model X1;:::; Xn\u0018iidBer(\u0012). Then, the likelihood function is given by\n",
      "L(\u0012j\u001c)=nY\n",
      "i=1\u0012xi(1\u0000\u0012)1\u0000xi=\u0012s(1\u0000\u0012)n\u0000s;0<\u0012< 1; (C.49)\n",
      "where s:=x1+\u0001\u0001\u0001+xn=:nx. The log-likelihood is l(\u0012)=sln\u0012+(n\u0000s) ln(1\u0000\u0012). Through\n",
      "di\u000berentiation with respect to \u0012, we ﬁnd the derivative\n",
      "s\n",
      "\u0012\u0000n\u0000s\n",
      "1\u0000\u0012=s\n",
      "\u0012(1\u0000\u0012)\u0000n\n",
      "1\u0000\u0012: (C.50)\n",
      "Solving l0(\u0012)=0 gives the ML estimate b\u0012=xand ML estimator b\u0012=X.\n",
      "C.13 Conﬁdence Intervals\n",
      "An essential part in any estimation procedure is to provide an assessment of the accuracy\n",
      "of the estimate. Indeed, without information on its accuracy the estimate itself would be\n",
      "meaningless. Conﬁdence intervals (also called interval estimates interval\n",
      "estimates) provide a precise way of\n",
      "describing the uncertainty in the estimate.\n",
      "LetX1;:::; Xnbe random variables with a joint distribution depending on a parameter\n",
      "\u00122\u0002. Let T1<T2be statistics; that is, Ti=Ti(X1;:::; Xn),i=1;2 are functions of the\n",
      "data, but not of \u0012.\n",
      "1. The random interval ( T1;T2) is called a stochastic conﬁdence interval stochastic\n",
      "confidence\n",
      "intervalfor\u0012with\n",
      "conﬁdence 1\u0000\u000bif\n",
      "P\u0012[T1<\u0012< T2]>1\u0000\u000bfor all\u00122\u0002: (C.51)\n",
      "2. If t1andt2are the observed values of T1andT2, then the interval ( t1;t2) is called the\n",
      "(numerical) conﬁdence interval (numerical )\n",
      "confidence\n",
      "intervalfor\u0012with conﬁdence 1 \u0000\u000bfor every\u00122\u0002.\n",
      "3. If the right-hand side of (C.51) is merely a heuristic estimate or approximation of\n",
      "the true probability, then the resulting interval is called an approximate conﬁdence\n",
      "interval .\n",
      "4. The probability P\u0012[T1< \u0012 < T2] is called the coverage probability coverage\n",
      "probability. For a 1\u0000\u000b\n",
      "conﬁdence interval, it must be at least 1 \u0000\u000b.\n",
      "For multidimensional parameters \u00122Rdthe stochastic conﬁdence interval is replaced\n",
      "with a stochastic conﬁdence region confidence\n",
      "regionC\u001aRdsuch thatP\u0012[\u00122C]>1\u0000\u000bfor all\u0012.460 C.14. Hypothesis Testing\n",
      "Example C.14 (Approximate Conﬁdence Interval for the Mean) Let X1;X2;:::; Xn\n",
      "be an iid sample from a distribution with mean \u0016and variance \u001b2<1(both assumed\n",
      "to be unknown). By the central limit theorem and the law of large numbers, +449\n",
      "T=X\u0000\u0016\n",
      "S=pnapprox:\u0018N(0;1);\n",
      "for large n, where Sis the sample standard deviation. Rearranging the approximate equality\n",
      "P[jTj6z1\u0000\u000b=2]\u00191\u0000\u000b, where z1\u0000\u000b=2is the 1\u0000\u000b=2 quantile of the standard normal\n",
      "distribution, yields\n",
      "P\"\n",
      "X\u0000z1\u0000\u000b=2Spn6\u00166X+z1\u0000\u000b=2Spn#\n",
      "\u00191\u0000\u000b;\n",
      "so that  \n",
      "X\u0000z1\u0000\u000b=2Spn;X+z1\u0000\u000b=2Spn!\n",
      ";abbreviated as X\u0006z1\u0000\u000b=2Spn; (C.52)\n",
      "is an approximate stochastic (1 \u0000\u000b) conﬁdence interval for \u0016.\n",
      "Since (C.52) is an asymptotic result only, care should be taken when applying it to\n",
      "cases where the sample size is small or moderate and the sampling distribution is heavily\n",
      "skewed.\n",
      "C.14 Hypothesis Testing\n",
      "Suppose the model for the data Tis described by a family of probability distributions that\n",
      "depend on a parameter \u00122\u0002. The aim of hypothesis testing is to decide, on the basis ofhypothesis\n",
      "testing the observed data \u001c, which of two competing hypotheses holds true; these being the null\n",
      "hypothesis ,H0:\u00122\u00020, and the alternative hypothesis ,H1:\u00122\u00021.null hypothesis\n",
      "alternative\n",
      "hypothesisIn classical statistics the null hypothesis and alternative hypothesis do not play equival-\n",
      "ent roles. H0contains the “status quo” statement and is only rejected if the observed data\n",
      "are very unlikely to have happened under H0.\n",
      "The decision whether to accept or reject H0is dependent on the outcome of a test\n",
      "statistic test statistic T=T(T). For simplicity, we discuss only the one-dimensional case T\u0011T. Two\n",
      "(related) types of decision rules are generally used:\n",
      "1.Decision rule 1 :Reject H 0if T falls in the critical region .\n",
      "Here the critical region critical region is any appropriately chosen region in R. In practice a critical\n",
      "region is one of the following:\n",
      "left one-sided : (\u00001;c],\n",
      "right one-sided : [c;1),\n",
      "two-sided : (\u00001;c1][[c2;1).\n",
      "For example, for a right one-sided test, H0is rejected if the outcome of the test\n",
      "statistic is too large. The endpoints c,c1, and c2of the critical regions are called\n",
      "critical values critical values .Appendix C. Probability and Statistics 461\n",
      "2.Decision rule 2 :Reject H 0if the P-value is smaller than some signiﬁcance level \u000b.\n",
      "TheP-value P-value is the probability that, under H0, the (random) test statistic takes a value\n",
      "as extreme as or more extreme than the one observed. In particular, if tis the observed\n",
      "outcome of the test statistic T, then\n",
      "left one-sided test :P:=PH0[T6t],\n",
      "right one-sided :P:=PH0[T>t],\n",
      "two-sided :P:=minf2PH0[T6t];2PH0[T>t]g.\n",
      "The smaller the P-value, the greater the strength of the evidence against H0provided\n",
      "by the data. As a rule of thumb:\n",
      "P<0:10 suggestive evidence,\n",
      "P<0:05 reasonable evidence,\n",
      "P<0:01 strong evidence.\n",
      "Whether the ﬁrst or the second decision rule is used, one can make two types of errors,\n",
      "as depicted in Table C.4.\n",
      "Table C.4: Type I and II errors in hypothesis testing.\n",
      "True statement\n",
      "Decision H0is true H1is true\n",
      "Accept H 0Correct Type II Error\n",
      "Reject H 0Type I Error Correct\n",
      "The choice of the test statistic and the corresponding critical region involves a multiob-\n",
      "jective optimization criterion, whereby both the probabilities of a type I and type II error\n",
      "should, ideally, be chosen as small as possible. Unfortunately, these probabilities compete\n",
      "with each other. For example, if the critical region is made larger (smaller), the probability\n",
      "of a type II error is reduced (increased), but at the same time the probability of a type I\n",
      "error is increased (reduced).\n",
      "Since the type I error is considered more serious, Neyman and Pearson [93] suggested\n",
      "the following approach: choose the critical region such that the probability of a type II error\n",
      "is as small as possible, while keeping the probability of a type I error below a predetermined\n",
      "small signiﬁcance level significance\n",
      "level\u000b.\n",
      "Remark C.3 (Equivalence of Decision Rules) Note that decision rule 1 and 2 are\n",
      "equivalent in the following sense:\n",
      "Reject H0ifTfalls in the critical region, at signiﬁcance level \u000b.\n",
      ",\n",
      "Reject H0if the P-value is6signiﬁcance level \u000b.462 C.14. Hypothesis Testing\n",
      "In other words, the P-value of the test is the smallest level of signiﬁcance that would lead\n",
      "to the rejection of H0.\n",
      "In general, a statistical test involves the following steps:\n",
      "1. Formulate an appropriate statistical model for the data.\n",
      "2. Give the null ( H0) and alternative ( H1) hypotheses in terms of the parameters\n",
      "of the model.\n",
      "3. Determine the test statistic (a function of the data only).\n",
      "4. Determine the (approximate) distribution of the test statistic under H0.\n",
      "5. Calculate the outcome of the test statistic.\n",
      "6. Calculate the P-value orthe critical region, given a preselected signiﬁcance\n",
      "level\u000b.\n",
      "7. Accept or reject H0.\n",
      "The actual choice of an appropriate test statistic is akin to selecting a good estimator\n",
      "for the unknown parameter \u0012. The test statistic should summarize the information about \u0012\n",
      "and make it possible to distinguish between the alternative hypotheses.\n",
      "Example C.15 (Hypothesis Testing) We are given outcomes x1;:::; xmandy1;:::; yn\n",
      "of two simulation studies obtained via independent runs, with m=100 and n=50. The\n",
      "sample means and standard deviations are x=1:3,sX=0:1 and y=1:5,sY=0:3. Thus,\n",
      "thefxigare outcomes of iid random variables fXig, thefyigare outcomes of iid random\n",
      "variablesfYig, and thefXigandfYigare independent. We wish to assess whether the expect-\n",
      "ations\u0016X=EXiand\u0016Y=EYiare the same or not. Going through the 7 steps above, we\n",
      "have:\n",
      "1. The model is already speciﬁed above.\n",
      "2.H0:\u0016X\u0000\u0016Y=0 versus H1:\u0016X\u0000\u0016Y,0.\n",
      "3. For similar reasons as in Example C.14, take\n",
      "T=X\u0000Yq\n",
      "S2\n",
      "X=m+S2\n",
      "Y=n:\n",
      "4. By the central limit theorem, the statistic Thas, under H0, approximately a standard\n",
      "normal distribution (assuming the variances are ﬁnite).\n",
      "5. The outcome of Tist=(x\u0000y)=q\n",
      "s2\n",
      "X=m+s2\n",
      "Y=n\u0019\u00004:59.\n",
      "6. As this is a two-sided test, the P-value is 2 PH0[T6\u00004:59]\u00194\u000110\u00006.\n",
      "7. Because the P-value is extremely small, there is overwhelming evidence that the two\n",
      "expectations are not the same.Appendix C. Probability and Statistics 463\n",
      "Further Reading\n",
      "Accessible treatises on probability and stochastic processes include [27, 26, 39, 54, 101].\n",
      "Kallenberg’s book [61] provides a complete graduate-level overview of the foundations of\n",
      "modern probability. Details on the convergence of probability measures and limit theorems\n",
      "can be found in [11]. For an accessible introduction to mathematical statistics with simple\n",
      "applications see, for example, [69, 74, 124]. For a more detailed overview of statistical\n",
      "inference, see [10, 25]. A standard reference for classical (frequentist) statistical inference\n",
      "is [78].464APPENDIXD\n",
      "PYTHON PRIMER\n",
      "Python has become the programming language of choice for many researchers and\n",
      "practitioners in data science and machine learning. This appendix gives a brief intro-\n",
      "duction to the language. As the language is under constant development and each year\n",
      "many new packages are being released, we do not pretend to be exhaustive in this in-\n",
      "troduction. Instead, we hope to provide enough information for novices to get started\n",
      "with this beautiful and carefully thought-out language.\n",
      "D.1 Getting Started\n",
      "The main website for Python is\n",
      "https://www.python.org/ ,\n",
      "where you will ﬁnd documentation, a tutorial, beginners’ guides, software examples, and\n",
      "so on. It is important to note that there are two incompatible “branches” of Python, called\n",
      "Python 3 and Python 2. Further development of the language will involve only Python 3,\n",
      "and in this appendix (and indeed the rest of the book) we only consider Python 3. As there\n",
      "are many interdependent packages that are frequently used with a Python installation, it\n",
      "is convenient to install a distribution — for instance, the Anaconda Anaconda Python distribution,\n",
      "available from\n",
      "https://www.anaconda.com/ .\n",
      "The Anaconda installer automatically installs the most important packages and also\n",
      "provides a convenient interactive development environment (IDE), called Spyder .\n",
      "Use the Anaconda Navigator to launch Spyder ,Jupyter notebook , install and update\n",
      "packages, or open a command-line terminal.\n",
      "To get started1, try out the Python statements in the input boxes that follow. You can\n",
      "either type these statements at the IPython command prompt or run them as (very short)\n",
      "1We assume that you have installed all the necessary ﬁles and have launched Spyder .\n",
      "465466 D.1. Getting Started\n",
      "Python programs. The output for these two modes of input can di \u000ber slightly. For ex-\n",
      "ample, typing a variable name in the console causes its contents to be automatically printed,\n",
      "whereas in a Python program this must be done explicitly by calling the print function.\n",
      "Selecting (highlighting) several program lines in Spyder and then pressing function key2\n",
      "F9is equivalent to executing these lines one by one in the console.\n",
      "In Python, data is represented as an object object or relation between objects (see also Sec-\n",
      "tion D.2). Basic data types are numeric types (including integers, booleans, and ﬂoats),\n",
      "sequence types (including strings, tuples, and lists), sets, and mappings (currently, diction-\n",
      "aries are the only built-in mapping type).\n",
      "Strings are sequences of characters, enclosed by single or double quotes. We can print\n",
      "strings via the print function.\n",
      "print(\"Hello World!\")\n",
      "Hello World!\n",
      "For pretty-printing output, Python strings can be formatted using the format function. The\n",
      "bracket syntax {i}provides a placeholder for the i-th variable to be printed, with 0 being\n",
      "the ﬁrst index. Individual variables can be formatted separately and as desired; formatting\n",
      "syntax is discussed in more detail in Section D.9. +477\n",
      "print(\"Name:{1} (height {2} m, age {0})\".format(111,\"Bilbo\" ,0.84))\n",
      "Name:Bilbo (height 0.84 m, age 111)\n",
      "Lists can contain di \u000berent types of objects, and are created using square brackets as in the\n",
      "following example:\n",
      "x = [1, 'string ',\"another string\"] # Quote type is not important\n",
      "[1, 'string ','another string ']\n",
      "Elements in lists are indexed starting from 0, and are mutable mutable (can be changed):\n",
      "x = [1,2]\n",
      "x[0] = 2 # Note that the first index is 0\n",
      "x\n",
      "[2,2]\n",
      "In contrast, tuples (with round brackets) are immutable immutable (cannot be changed). Strings are\n",
      "immutable as well.\n",
      "x = (1,2)\n",
      "x[0] = 2\n",
      "TypeError: 'tuple 'object does not support item assignment\n",
      "Lists can be accessed via the slice slice notation [start:end] . It is important to note that end\n",
      "is the index of the ﬁrst element that will notbe selected, and that the ﬁrst element has index\n",
      "0. To gain familiarity with the slice notation, execute each of the following lines.\n",
      "2This may depend on the keyboard and operating system.Appendix D. Python Primer 467\n",
      "a = [2, 3, 5, 7, 11, 13, 17, 19, 23]\n",
      "a[1:4] # Elements with index from 1 to 3\n",
      "a[:4] # All elements with index less than 4\n",
      "a[3:] # All elements with index 3 or more\n",
      "a[-2:] # The last two elements\n",
      "[3, 5, 7]\n",
      "[2, 3, 5, 7]\n",
      "[7, 11, 13, 17, 19, 23]\n",
      "[19, 23]\n",
      "Anoperator operator is a programming language construct that performs an action on one or more\n",
      "operands. The action of an operator in Python depends on the type of the operand(s). For\n",
      "example, operators such as +,\u0003,\u0000, and % that are arithmetic operators when the operands\n",
      "are of a numeric type, can have di \u000berent meanings for objects of non-numeric type (such\n",
      "as strings).\n",
      "'hello '+'world '# String concatenation\n",
      "'helloworld '\n",
      "'hello '* 2 # String repetition\n",
      "'hellohello '\n",
      "[1,2] * 2 # List repetition\n",
      "[1, 2, 1, 2]\n",
      "15 % 4 # Remainder of 15/4\n",
      "3\n",
      "Some common Python operators are given in Table D.1. +469\n",
      "D.2 Python Objects\n",
      "As mentioned in the previous section, data in Python is represented by objects or relations\n",
      "between objects. We recall that basic data types included strings and numeric types (such\n",
      "as integers, booleans, and ﬂoats).\n",
      "As Python is an object-oriented programming language, functions are objects too\n",
      "(everything is an object!). Each object has an identity (unique to each object and immutable\n",
      "— that is, cannot be changed — once created), a type (which determines which operations\n",
      "can be applied to the object, and is considered immutable), and a value (which is either\n",
      "mutable or immutable). The unique identity assigned to an object obj can be found by\n",
      "calling id, as in id(obj) .\n",
      "Each object has a list of attributes attributes , and each attribute is a reference to another object.\n",
      "The function dir applied to an object returns the list of attributes. For example, a string\n",
      "object has many useful attributes, as we shall shortly see. Functions are objects with the\n",
      "__call__ attribute.468 D.3. Types and Operators\n",
      "A class (see Section D.8) can be thought of as a template for creating a custom type of\n",
      "object.\n",
      "s = \"hello\"\n",
      "d = dir(s)\n",
      "print(d,flush=True) # Print the list in \"flushed\" format\n",
      "['__add__ ','__class__ ','__contains__ ','__delattr__ ','__dir__ ',\n",
      "... (many left out) ... 'replace ','rfind ',\n",
      "'rindex ','rjust ','rpartition ','rsplit ','rstrip ','split ',\n",
      "'splitlines ','startswith ','strip ','swapcase ','title ',\n",
      "'translate ','upper ','zfill ']\n",
      "Any attribute attr of an object objcan be accessed via the dot notation dot notation :obj.attr . To\n",
      "ﬁnd more information about any object use the help function.\n",
      "s = \"hello\"\n",
      "help(s.replace)\n",
      "replace(...) method of builtins.str instance\n",
      "S.replace(old, new[, count]) -> str\n",
      "Return a copy of S with all occurrences of substring\n",
      "old replaced by new. If the optional argument count is\n",
      "given , only the first count occurrences are replaced.\n",
      "This shows that the attribute replace is in fact a function. An attribute that is a function is\n",
      "called a method method . We can use the replace method to create a new string from the old one\n",
      "by changing certain characters.\n",
      "s = 'hello '\n",
      "s1 = s.replace( 'e','a')\n",
      "print(s1)\n",
      "hallo\n",
      "In many Python editors, pressing the TAB key, as in objectname.<TAB> , will bring\n",
      "up a list of possible attributes via the editor’s autocompletion feature.\n",
      "D.3 Types and Operators\n",
      "Each object has a type type . Three basic data types in Python are str (for string), int (for\n",
      "integers), and float (for ﬂoating point numbers). The function type returns the type of\n",
      "an object.\n",
      "t1 = type([1,2,3])\n",
      "t2 = type((1,2,3))\n",
      "t3 = type({1,2,3})\n",
      "print(t1,t2,t3)Appendix D. Python Primer 469\n",
      "<class 'list '> <class 'tuple '> <class 'set'>\n",
      "Theassignment assignment operator, =, assigns an object to a variable; e.g., x = 12 . An expression\n",
      "is a combination of values, operators, and variables that yields another value or variable.\n",
      "Variable names are case sensitive and can only contain letters, numbers, and under-\n",
      "scores. They must start with either a letter or underscore. Note that reserved words\n",
      "such as True andFalse are case sensitive as well.\n",
      "Python is a dynamically typed language, and the type of a variable at a particular point\n",
      "during program execution is determined by its most recent object assignment. That is, the\n",
      "type of a variable does not need to be explicitly declared from the outset (as is the case in\n",
      "C or Java), but instead the type of the variable is determined by the object that is currently\n",
      "assigned to it.\n",
      "It is important to understand that a variable in Python is a reference reference to an object —\n",
      "think of it as a label on a shoe box. Even though the label is a simple entity, the contents\n",
      "of the shoe box (the object to which the variable refers) can be arbitrarily complex. Instead\n",
      "of moving the contents of one shoe box to another, it is much simpler to merely move the\n",
      "label.\n",
      "x = [1,2]\n",
      "y = x # y refers to the same object as x\n",
      "print(id(x) == id(y)) # check that the object id 's are the same\n",
      "y[0] = 100 # change the contents of the list that y refers to\n",
      "print(x)\n",
      "True\n",
      "[100,2]\n",
      "x = [1,2]\n",
      "y = x # y refers to the same object as x\n",
      "y = [100,2] # now y refers to a different object\n",
      "print(id(x) == id(y))\n",
      "print(x)\n",
      "False\n",
      "[1,2]\n",
      "Table D.1 shows a selection of Python operators for numerical and logical variables.\n",
      "Table D.1: Common numerical (left) and logical (right) operators.\n",
      "+ addition ~ binary NOT\n",
      "- subtraction & binary AND\n",
      "* multiplication ^ binary XOR\n",
      "** power | binary OR\n",
      "/ division == equal to\n",
      "// integer division != not equal to\n",
      "% modulus470 D.4. Functions and Methods\n",
      "Several of the numerical operators can be combined with an assignment operator, as in\n",
      "x += 1 to mean x = x + 1 . Operators such as +and*can be deﬁned for other data types\n",
      "as well, where they take on a di \u000berent meaning. This is called operator overloading , an\n",
      "example of which is the use of <List> * <Integer> for list repetition as we saw earlier.\n",
      "D.4 Functions and Methods\n",
      "Functions make it easier to divide a complex program into simpler parts. To create a\n",
      "function function , use the following syntax:\n",
      "def <function name>(<parameter_list>):\n",
      "<statements>\n",
      "A function takes a list of input variables that are references to objects. Inside the func-\n",
      "tion, a number of statements are executed which may modify the objects, but not the ref-\n",
      "erence itself. In addition, the function may return an output object (or will return the value\n",
      "None if not explicitly instructed to return output). Think again of the shoe box analogy. The\n",
      "input variables of a function are labels of shoe boxes, and the objects to which they refer\n",
      "are the contents of the shoe boxes. The following program highlights some of the subtleties\n",
      "of variables and objects in Python.\n",
      "Note that the statements within a function must be indented. This is Python’s way to\n",
      "deﬁne where a function begins and ends.\n",
      "x = [1,2,3]\n",
      "def change_list(y):\n",
      "y.append(100) # Append an element to the list referenced by y\n",
      "y[0]=0 # Modify the first element of the same list\n",
      "y = [2,3,4] # The local y now refers to a different list\n",
      "# The list to which y first referred does not change\n",
      "return sum(y)\n",
      "print(change_list(x))\n",
      "print(x)\n",
      "9\n",
      "[0, 2, 3, 100]\n",
      "Variables that are deﬁned inside a function only have local scope ; that is, they are\n",
      "recognized only within that function. This allows the same variable name to be used in\n",
      "di\u000berent functions without creating a conﬂict. If any variable is used within a function,\n",
      "Python ﬁrst checks if the variable has local scope. If this is not the case (the variable has\n",
      "not been deﬁned inside the function), then Python searches for that variable outside the\n",
      "function (the global scope). The following program illustrates several important points.Appendix D. Python Primer 471\n",
      "from numpy import array , square , sqrt\n",
      "x = array([1.2,2.3,4.5])\n",
      "def stat(x):\n",
      "n = len(x) #the length of x\n",
      "meanx = sum(x)/n\n",
      "stdx = sqrt(sum(square(x - meanx))/n)\n",
      "return [meanx ,stdx]\n",
      "print(stat(x))\n",
      "[2.6666666666666665 , 1.3719410418171119]\n",
      "1. Basic math functions such as sqrt are unknown to the standard Python interpreter\n",
      "and need to be imported. More on this in Section D.5 below.\n",
      "2. As was already mentioned, indentation is crucial. It shows where the function begins\n",
      "and ends.\n",
      "3. No semicolons3are needed to end lines, but the ﬁrst line of the function deﬁnition\n",
      "(here line 5) must end with a colon (:).\n",
      "4. Lists are not arrays (vectors of numbers), and vector operations cannot be performed\n",
      "on lists. However, the numpy module is designed speciﬁcally with e \u000ecient vec-\n",
      "tor/matrix operations in mind. On the second code line, we deﬁne xas a vector\n",
      "(ndarray ) object. Functions such as square ,sum, and sqrt are then applied to\n",
      "such arrays. Note that we used the default Python functions lenandsum. More on\n",
      "numpy in Section D.10.\n",
      "5. Running the program with stat(x) instead of print(stat(x)) in line 11 will not\n",
      "show any output in the console.\n",
      "To display the complete list of built-in functions, type (using double underscores)\n",
      "dir(__builtin__) .\n",
      "D.5 Modules\n",
      "A Python module module is a programming construct that is useful for organizing code into\n",
      "manageable parts. To each module with name module_name is associated a Python ﬁle\n",
      "module_name.py containing any number of deﬁnitions, e.g., of functions, classes, and\n",
      "variables, as well as executable statements. Modules can be imported into other programs\n",
      "using the syntax: import <module_name> as <alias_name> , where <alias_name>\n",
      "is a shorthand name for the module.\n",
      "3Semicolons can be used to put multiple commands on a single line.472 D.5. Modules\n",
      "When imported into another Python ﬁle, the module name is treated as a namespace namespace ,\n",
      "providing a naming system where each object has its unique name. For example, di \u000berent\n",
      "modules mod1 andmod2 can have di \u000berent sumfunctions, but they can be distinguished by\n",
      "preﬁxing the function name with the module name via the dot notation, as in mod1.sum and\n",
      "mod2.sum . For example, the following code uses the sqrt function of the numpy module.\n",
      "import numpy as np\n",
      "np.sqrt(2)\n",
      "1.4142135623730951\n",
      "A Python package is simply a directory of Python modules; that is, a collection of\n",
      "modules with additional startup information (some of which may be found in its __path__\n",
      "attribute). Python’s built-in module is called __builtins__ . Of the great many useful\n",
      "Python modules, Table D.2 gives a few.\n",
      "Table D.2: A few useful Python modules /packages.\n",
      "datetime Module for manipulating dates and times.\n",
      "matplotlib MATLABTM-type plotting package\n",
      "numpy Fundamental package for scientiﬁc computing, including random\n",
      "number generation and linear algebra tools. Deﬁnes the ubiquitous\n",
      "ndarray class.\n",
      "os Python interface to the operating system.\n",
      "pandas Fundamental module for data analysis. Deﬁnes the powerful\n",
      "DataFrame class.\n",
      "pytorch Machine learning library that supports GPU computation.\n",
      "scipy Ecosystem for mathematics, science, and engineering, containing\n",
      "many tools for numerical computing, including those for integration,\n",
      "solving di \u000berential equations, and optimization.\n",
      "requests Library for performing HTTP requests and interfacing with the web.\n",
      "seaborn Package for statistical data visualization.\n",
      "sklearn Easy to use machine learning library.\n",
      "statsmodels Package for the analysis of statistical models.\n",
      "Thenumpy package contains various subpackages, such as random ,linalg , and fft.\n",
      "More details are given in Section D.10.\n",
      "When using Spyder , press Ctrl+I in front of any object, to display its help ﬁle in a\n",
      "separate window.\n",
      "As we have already seen, it is also possible to import only speciﬁc functions from a\n",
      "module using the syntax: from <module_name> import <fnc1, fnc2, ...> .\n",
      "from numpy import sqrt , cos\n",
      "sqrt(2)\n",
      "cos(1)Appendix D. Python Primer 473\n",
      "1.4142135623730951\n",
      "0.54030230586813965\n",
      "This avoids the tedious preﬁxing of functions via the (alias) of the module name. However,\n",
      "for large programs it is good practice to always use the preﬁx /alias name construction, to\n",
      "be able to clearly ascertain precisely which module a function being used belongs to.\n",
      "D.6 Flow Control\n",
      "Flow control in Python is similar to that of many programming languages, with conditional\n",
      "statements as well as while andforloops. The syntax for if-then-else ﬂow control is\n",
      "as follows.\n",
      "if <condition1>:\n",
      "<statements>\n",
      "elif <condition2>:\n",
      "<statements>\n",
      "else:\n",
      "<statements>\n",
      "Here, <condition1> and<condition2> are logical conditions that are either True or\n",
      "False ; logical conditions often involve comparison operators (such as ==, >, <=, != ).\n",
      "In the example above, there is one elif part, which allows for an “else if” conditional\n",
      "statement. In general, there can be more than one elif part, or it can be omitted. The else\n",
      "part can also be omitted. The colons are essential, as are the indentations.\n",
      "Thewhile andforloops have the following syntax.\n",
      "while <condition>:\n",
      "<statements>\n",
      "for <variable> in <collection>:\n",
      "<statements>\n",
      "Above, <collection> is an iterable object (see Section D.7 below). For further con-\n",
      "trol in forandwhile loops, one can use a break statement to exit the current loop, and\n",
      "thecontinue statement to continue with the next iteration of the loop, while abandoning\n",
      "any remaining statements in the current iteration. Here is an example.\n",
      "import numpy as np\n",
      "ans = 'y'\n",
      "while ans != 'n':\n",
      "outcome = np.random.randint(1,6+1)\n",
      "if outcome == 6:\n",
      "print(\"Hooray a 6!\")\n",
      "break\n",
      "else:\n",
      "print(\"Bad luck , a\", outcome)\n",
      "ans = input(\"Again? (y/n) \")474 D.7. Iteration\n",
      "D.7 Iteration\n",
      "Iterating over a sequence of objects, such as used in a for loop, is a common operation.\n",
      "To better understand how iteration works, we consider the following code.\n",
      "s = \"Hello\"\n",
      "for c in s:\n",
      "print(c, '*', end= ' ')\n",
      "H * e * l * l * o *\n",
      "A string is an example of a Python object that can be iterated. One of the methods of a\n",
      "string object is __iter__ . Any object that has such a method is called an iterable iterable . Calling\n",
      "this method creates an iterator iterator — an object that returns the next element in the sequence\n",
      "to be iterated. This is done via the method __next__ .\n",
      "s = \"Hello\"\n",
      "t = s.__iter__() # t is now an iterator. Same as iter(s)\n",
      "print(t.__next__() ) # same as next(t)\n",
      "print(t.__next__() )\n",
      "print(t.__next__() )\n",
      "H\n",
      "e\n",
      "l\n",
      "The inbuilt functions next and iter simply call these corresponding double-\n",
      "underscore functions of an object. When executing a for loop, the sequence /collection\n",
      "over which to iterate must be an iterable. During the execution of the forloop, an iterator\n",
      "is created and the next function is executed until there is no next element. An iterator is\n",
      "also an iterable, so can be used in a forloop as well. Lists, tuples, and strings are so-called\n",
      "sequence sequence objects and are iterables, where the elements are iterated by their index.\n",
      "The most common iterator in Python is the range range iterator, which allows iteration over\n",
      "a range of indices. Note that range returns a range object, not a list.\n",
      "for i in range(4,20):\n",
      "print(i, end= ' ')\n",
      "print(range(4,20))\n",
      "4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n",
      "range(4,20)\n",
      "Similar to Python’s slice operator [ i:j], the iterator range (i;j) ranges from itoj,\n",
      "not including the index j.\n",
      "Two other common iterables are sets and dictionaries. Python sets sets are, as in mathem-\n",
      "atics, unordered collections of unique objects. Sets are deﬁned with curly brackets fg, as\n",
      "opposed to round brackets ( ) for tuples, and square brackets [ ] for lists. Unlike lists, sets do\n",
      "not have duplicate elements. Many of the usual set operations are implemented in Python,\n",
      "including the union A | B and intersection A & B .Appendix D. Python Primer 475\n",
      "A = {3, 2, 2, 4}\n",
      "B = {4, 3, 1}\n",
      "C = A & B\n",
      "for i in A:\n",
      "print(i)\n",
      "print(C)\n",
      "2\n",
      "3\n",
      "4\n",
      "{3, 4}\n",
      "A useful way to construct lists is by list comprehension list\n",
      "comprehension; that is, by expressions of the\n",
      "form\n",
      "<expression> for <element> in <list> if <condition>\n",
      "For sets a similar construction holds. In this way, lists and sets can be deﬁned using very\n",
      "similar syntax as in mathematics. Compare, for example, the mathematical deﬁnition of\n",
      "the sets A:=f3;2;4;2g=f2;3;4g(no order and no duplication of elements) and B:=fx2:\n",
      "x2Agwith the Python code below.\n",
      "setA = {3, 2, 4, 2}\n",
      "setB = {x**2 for x in setA}\n",
      "print(setB)\n",
      "listA = [3, 2, 4, 2]\n",
      "listB = [x**2 for x in listA]\n",
      "print(listB)\n",
      "{16, 9, 4}\n",
      "[9, 4, 16, 4]\n",
      "Adictionary dictionary is a set-like data structure, containing one or more key:value pairs en-\n",
      "closed in curly brackets. The keys are often of the same type, but do not have to be; the\n",
      "same holds for the values. Here is a simple example, storing the ages of Lord of the Rings\n",
      "characters in a dictionary.\n",
      "DICT = { 'Gimly ': 140, 'Frodo ':51, 'Aragorn ': 88}\n",
      "for key in DICT:\n",
      "print(key, DICT[key])\n",
      "Gimly 140\n",
      "Frodo 51\n",
      "Aragorn 88\n",
      "D.8 Classes\n",
      "Recall that objects are of fundamental importance in Python — indeed, data types and\n",
      "functions are all objects. A class class is an object type, and writing a class deﬁnition can be\n",
      "thought of as creating a template for a new type of object. Each class contains a number\n",
      "of attributes, including a number of inbuilt methods. The basic syntax for the creation of a\n",
      "class is:476 D.8. Classes\n",
      "class <class_name>:\n",
      "def __init__(self):\n",
      "<statements>\n",
      "<statements>\n",
      "The main inbuilt method is __init__ , which creates an instance instance of a class object.\n",
      "For example, str is a class object (string class), but s = str( 'Hello ')or simply\n",
      "s = 'Hello ', creates an instance, s, of the strclass. Instance attributes are created dur-\n",
      "ing initialization and their values may be di \u000berent for di \u000berent instances. In contrast, the\n",
      "values of class attributes are the same for every instance. The variable self in the initializ-\n",
      "ation method refers to the current instance that is being created. Here is a simple example,\n",
      "explaining how attributes are assigned.\n",
      "class shire_person:\n",
      "def __init__(self ,name): # initialization method\n",
      "self.name = name # instance attribute\n",
      "self.age = 0 # instance attribute\n",
      "address = 'The Shire ' # class attribute\n",
      "print(dir(shire_person)[1:5], '...',dir(shire_person)[-2:])\n",
      "# list of class attributes\n",
      "p1 = shire_person( 'Sam') # create an instance\n",
      "p2 = shire_person( 'Frodo ') # create another instance\n",
      "print(p1.__dict__) # list of instance attributes\n",
      "p2.race = 'Hobbit ' # add another attribute to instance p2\n",
      "p2.age = 33 # change instance attribute\n",
      "print(p2.__dict__)\n",
      "print(getattr(p1, 'address ')) # content of p1 's class attribute\n",
      "['__delattr__ ','__dict__ ','__dir__ ','__doc__ '] ...\n",
      "['__weakref__ ','address ']\n",
      "{'name ':'Sam','age': 0}\n",
      "{'name ':'Frodo ','age': 33, 'race ':'Hobbit '}\n",
      "The Shire\n",
      "It is good practice to create all the attributes of the class object in the __init__ method,\n",
      "but, as seen in the example above, attributes can be created and assigned everywhere, even\n",
      "outside the class deﬁnition. More generally, attributes can be added to any object that has\n",
      "a__dict__ .\n",
      "An “empty” class can be created via\n",
      "class <class_name>:\n",
      "pass\n",
      "Python classes can be derived from a parent class by inheritance inheritance , via the following\n",
      "syntax.\n",
      "class <class_name>(<parent_class_name>):\n",
      "<statements>Appendix D. Python Primer 477\n",
      "The derived class (initially) inherits all of the attributes of the parent class.\n",
      "As an example, the class shire_person below inherits the attributes name ,age, and\n",
      "address from its parent class person . This is done using the super function, used here\n",
      "to refer to the parent class person without naming it explicitly. When creating a new\n",
      "object of type shire_person , the __init__ method of the parent class is invoked, and\n",
      "an additional instance attribute Shire_address is created. The dirfunction conﬁrms that\n",
      "Shire_address is an attribute only of shire_person instances.\n",
      "class person:\n",
      "def __init__(self ,name):\n",
      "self.name = name\n",
      "self.age = 0\n",
      "self.address= ' '\n",
      "class shire_person(person):\n",
      "def __init__(self ,name):\n",
      "super().__init__(name)\n",
      "self.Shire_address = 'Bag End '\n",
      "p1 = shire_person(\"Frodo\")\n",
      "p2 = person(\"Gandalf\")\n",
      "print(dir(p1)[:1],dir(p1)[-3:] )\n",
      "print(dir(p2)[:1],dir(p2)[-3:] )\n",
      "['Shire_address '] ['address ','age','name ']\n",
      "['__class__ '] ['address ','age','name ']\n",
      "D.9 Files\n",
      "To write to or read from a ﬁle, a ﬁle ﬁrst needs to be opened. The open function in Python\n",
      "creates a ﬁle object that is iterable, and thus can be processed in a sequential manner in a\n",
      "fororwhile loop. Here is a simple example.\n",
      "fout = open( 'output.txt ','w')\n",
      "for i in range(0,41):\n",
      "if i%10 == 0:\n",
      "fout.write( '{:3d}\\n '.format(i))\n",
      "fout.close()\n",
      "The ﬁrst argument of open is the name of the ﬁle. The second argument speciﬁes\n",
      "if the ﬁle is opened for reading ( 'r'), writing ( 'w'), appending ( 'a'), and so on. See\n",
      "help(open) . Files are written in text mode by default, but it is also possible to write in\n",
      "binary mode. The above program creates a ﬁle output.txt with 5 lines, containing the\n",
      "strings 0, 10, . . . , 40. Note that if we had written fout.write(i) in the fourth line of the\n",
      "code above, an error message would be produced, as the variable iis an integer, and not a\n",
      "string. Recall that the expression string.format() is Python’s way to specify the format\n",
      "of the output string.\n",
      "The formatting syntax {:3d} indicates that the output should be constrained to a spe-\n",
      "ciﬁc width of three characters, each of which is a decimal value. As mentioned in the478 D.9. Files\n",
      "introduction, bracket syntax {i}provides a placeholder for the i-th variable to be printed,\n",
      "with 0 being the ﬁrst index. The format for the output is further speciﬁed by {i:format} ,\n",
      "where format is typically4of the form:\n",
      "[width][.precision][type]\n",
      "In this speciﬁcation:\n",
      "width speciﬁes the minimum width of output;\n",
      "precision speciﬁes the number of digits to be displayed after the decimal point for\n",
      "a ﬂoating point values of type f, or the number of digits before andafter the decimal\n",
      "point for a ﬂoating point values of type g;\n",
      "type speciﬁes the type of output. The most common types are sfor strings, dfor\n",
      "integers, bfor binary numbers, ffor ﬂoating point numbers (ﬂoats) in ﬁxed-point\n",
      "notation, gfor ﬂoats in general notation, efor ﬂoats in scientiﬁc notation.\n",
      "The following illustrates some behavior of formatting on numbers.\n",
      "'{:5d} '.format(123)\n",
      "'{:.4e} '.format (1234567890)\n",
      "'{:.2f} '.format (1234567890)\n",
      "'{:.2f} '.format (2.718281828)\n",
      "'{:.3f} '.format (2.718281828)\n",
      "'{:.3g} '.format (2.718281828)\n",
      "'{:.3e} '.format (2.718281828)\n",
      "'{0:3.3f}; {2:.4e}; '.format(123.456789, 0.00123456789)\n",
      "'123'\n",
      "'1.2346e+09 '\n",
      "'1234567890.00 '\n",
      "'2.72 '\n",
      "'2.718 '\n",
      "'2.72 '\n",
      "'2.718e+00 '\n",
      "'123.457; 1.2346e-03; '\n",
      "The following code reads the text ﬁle output.txt line by line, and prints the output\n",
      "on the screen. To remove the newline \\ncharacter, we have used the strip method for\n",
      "strings, which removes any whitespace from the start and end of a string.\n",
      "fin = open( 'output.txt ','r')\n",
      "for line in fin:\n",
      "line = line.strip() # strips a newline character\n",
      "print(line)\n",
      "fin.close()\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "4More formatting options are possible.Appendix D. Python Primer 479\n",
      "When dealing with ﬁle input and output it is important to always close ﬁles. Files that\n",
      "remain open, e.g., when a program ﬁnishes unexpectedly due to a programming error, can\n",
      "cause considerable system problems. For this reason it is recommended to open ﬁles via\n",
      "context management . The syntax is as follows.\n",
      "with open( 'output.txt ','w') as f:\n",
      "f.write( 'Hi there! ')\n",
      "Context management ensures that a ﬁle is correctly closed even when the program is\n",
      "terminated prematurely. An example is given in the next program, which outputs the most-\n",
      "frequent words in Dicken’s A Tale of Two Cities , which can be downloaded from the book’s\n",
      "GitHub site as ataleof2cities.txt .\n",
      "Note that in the next program, the ﬁle ataleof2cities.txt must be placed in the cur-\n",
      "rent working directory. The current working directory can be determined via import os\n",
      "followed by cwd = os.getcwd() .\n",
      "numline = 0\n",
      "DICT = {}\n",
      "with open( 'ataleof2cities.txt ', encoding=\"utf8\") as fin:\n",
      "for line in fin:\n",
      "words = line.split()\n",
      "for w in words:\n",
      "if w not in DICT:\n",
      "DICT[w] = 1\n",
      "else:\n",
      "DICT[w] +=1\n",
      "numline += 1\n",
      "sd = sorted(DICT ,key=DICT.get,reverse=True) #sort the dictionary\n",
      "print(\"Number of unique words: {}\\n\".format(len(DICT)))\n",
      "print(\"Ten most frequent words:\\n\")\n",
      "print(\"{:8} {}\".format(\"word\", \"count\"))\n",
      "print(15* '-')\n",
      "for i in range(0,10):\n",
      "print(\"{:8} {}\".format(sd[i], DICT[sd[i]]))\n",
      "Number of unique words: 19091\n",
      "Ten most frequent words:\n",
      "word count\n",
      "---------------\n",
      "the 7348\n",
      "and 4679\n",
      "of 3949\n",
      "to 3387\n",
      "a 2768\n",
      "in 2390\n",
      "his 1911\n",
      "was 1672\n",
      "that 1650\n",
      "I 1444480 D.10. NumPy\n",
      "D.10 NumPy\n",
      "The package NumPy (module name numpy ) provides the building blocks for scientiﬁc\n",
      "computing in Python. It contains all the standard mathematical functions, such as sin,\n",
      "cos,tan, etc., as well as e \u000ecient functions for random number generation, linear algebra,\n",
      "and statistical computation.\n",
      "import numpy as np #import the package\n",
      "x = np.cos(1)\n",
      "data = [1,2,3,4,5]\n",
      "y = np.mean(data)\n",
      "z = np.std(data)\n",
      "print( 'cos(1) = {0:1.8f} mean = {1} std = {2} '.format(x,y,z))\n",
      "cos(1) = 0.54030231 mean = 3.0 std = 1.4142135623730951\n",
      "D.10.1 Creating and Shaping Arrays\n",
      "The fundamental data type in numpy is the ndarray . This data type allows for fast matrix\n",
      "operations via highly optimized numerical libraries such as LAPACK and BLAS; this in\n",
      "contrast to (nested) lists. As such, numpy is often essential when dealing with large amounts\n",
      "of quantitative data.\n",
      "ndarray objects can be created in various ways. The following code creates a 2 \u00023\u00022\n",
      "array of zeros. Think of it as a 3-dimensional matrix or two stacked 3 \u00022 matrices.\n",
      "A = np.zeros([2,3,2]) # 2 by 3 by 2 array of zeros\n",
      "print(A)\n",
      "print(A.shape) # number of rows and columns\n",
      "print(type(A)) # A is an ndarray\n",
      "[[[ 0. 0.]\n",
      "[ 0. 0.]\n",
      "[ 0. 0.]]\n",
      "[[ 0. 0.]\n",
      "[ 0. 0.]\n",
      "[ 0. 0.]]]\n",
      "(2, 3, 2)\n",
      "<class 'numpy.ndarray '>\n",
      "We will be mostly working with 2D arrays; that is, ndarrays that represent ordinary\n",
      "matrices. We can also use the range method and lists to create ndarrays via the array\n",
      "method. Note that arange isnumpy ’s version of range , with the di \u000berence that arange\n",
      "returns an ndarray object.\n",
      "a = np.array(range(4)) # equivalent to np.arange(4)\n",
      "b = np.array([0,1,2,3])\n",
      "C = np.array([[1,2,3],[3,2,1]])\n",
      "print(a, '\\n', b, '\\n', C)\n",
      "[0 1 2 3]\n",
      "[0 1 2 3]Appendix D. Python Primer 481\n",
      "[[1 2 3]\n",
      "[3 2 1]]\n",
      "The dimension of an ndarray can be obtained via its shape method, which returns a\n",
      "tuple. Arrays can be reshaped via the reshape method. This does not change the current\n",
      "ndarray object. To make the change permanent, a new instance needs to be created.\n",
      "a = np.array(range(9)) #a is an ndarray of shape (9,)\n",
      "print(a.shape)\n",
      "A = a.reshape(3,3) #A is an ndarray of shape (3,3)\n",
      "print(a)\n",
      "print(A)\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "(9,)\n",
      "[[0, 1, 2]\n",
      "[3, 4, 5]\n",
      "[6, 7, 8]]\n",
      "One shape dimension for reshape can be speciﬁed as \u00001. The dimension is then\n",
      "inferred from the other dimension(s).\n",
      "The'T'attribute of an ndarray gives its transpose. Note that the transpose of a “vector”\n",
      "with shape ( n;) is the same vector. To distinguish between column and row vectors, reshape\n",
      "such a vector to an n\u00021 and 1\u0002narray, respectively.\n",
      "a = np.arange(3) #1D array (vector) of shape (3,)\n",
      "print(a)\n",
      "print(a.shape)\n",
      "b = a.reshape(-1,1) # 3x1 array (matrix) of shape (3,1)\n",
      "print(b)\n",
      "print(b.T)\n",
      "A = np.arange(9).reshape(3,3)\n",
      "print(A.T)\n",
      "[0 1 2]\n",
      "(3,)\n",
      "[[0]\n",
      "[1]\n",
      "[2]]\n",
      "[[0 1 2]]\n",
      "[[0 3 6]\n",
      "[1 4 7]\n",
      "[2 5 8]]\n",
      "Two useful methods of joining arrays are hstack andvstack , where the arrays are\n",
      "joined horizontally and vertically, respectively.\n",
      "A = np.ones((3,3))\n",
      "B = np.zeros((3,2))\n",
      "C = np.hstack((A,B))\n",
      "print(C)482 D.10. NumPy\n",
      "[[ 1. 1. 1. 0. 0.]\n",
      "[ 1. 1. 1. 0. 0.]\n",
      "[ 1. 1. 1. 0. 0.]]\n",
      "D.10.2 Slicing\n",
      "Arrays can be sliced similarly to Python lists. If an array has several dimensions, a slice for\n",
      "each dimension needs to be speciﬁed. Recall that Python indexing starts at '0'and ends\n",
      "at'len(obj)-1 '. The following program illustrates various slicing operations.\n",
      "A = np.array(range(9)).reshape(3,3)\n",
      "print(A)\n",
      "print(A[0]) # first row\n",
      "print(A[:,1]) # second column\n",
      "print(A[0,1]) # element in first row and second column\n",
      "print(A[0:1,1:2]) # (1,1) ndarray containing A[0,1] = 1\n",
      "print(A[1:,-1]) # elements in 2nd and 3rd rows , and last column\n",
      "[[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]]\n",
      "[0 1 2]\n",
      "[1 4 7]\n",
      "1\n",
      "[[1]]\n",
      "[5 8]\n",
      "Note that ndarrays are mutable objects, so that elements can be modiﬁed directly, without\n",
      "having to create a new object.\n",
      "A[1:,1] = [0,0] # change two elements in the matrix A above\n",
      "print(A)\n",
      "[[0, 1, 2]\n",
      "[3, 0, 5]\n",
      "[6, 0, 8]]\n",
      "D.10.3 Array Operations\n",
      "Basic mathematical operators and functions act element-wise onndarray objects.\n",
      "x = np.array([[2,4],[6,8]])\n",
      "y = np.array([[1,1],[2,2]])\n",
      "print(x+y)\n",
      "[[ 3, 5]\n",
      "[ 8, 10]]\n",
      "print(np.divide(x,y)) # same as x/y\n",
      "[[ 2. 4.]\n",
      "[ 3. 4.]]Appendix D. Python Primer 483\n",
      "print(np.sqrt(x))\n",
      "[[1.41421356 2. ]\n",
      "[2.44948974 2.82842712]]\n",
      "In order to compute matrix multiplications and compute inner products of vectors,\n",
      "numpy ’sdot function can be used, either as a method of an ndarray instance or as a\n",
      "method of np.\n",
      "print(np.dot(x,y))\n",
      "[[10, 10]\n",
      "[22, 22]]\n",
      "print(x.dot(x)) # same as np.dot(x,x)\n",
      "[[28, 40]\n",
      "[60, 88]]\n",
      "Since version 3.5 of Python, it is possible to multiply two ndarray s using the @\n",
      "operator @operator (which implements the np.matmul method). For matrices, this is similar to using\n",
      "thedotmethod. For higher-dimensional arrays the two methods behave di \u000berently.\n",
      "print(x @ y)\n",
      "[[10 10]\n",
      "[22 22]]\n",
      "NumPy allows arithmetic operations on arrays of di \u000berent shapes (dimensions). Spe-\n",
      "ciﬁcally, suppose two arrays have dimensions ( m1;m2;:::; mp) and ( n1;n2;:::; np), respect-\n",
      "ively. The arrays or shapes are said to be aligned aligned if for all i=1;:::; pit holds that\n",
      "mi=ni, or\n",
      "minfmi;nig=1, or\n",
      "either miorni, or both are missing.\n",
      "For example, shapes (1 ;2;3) and (4;2;1) are aligned, as are (2 ;;) and (1;2;3). However,\n",
      "(2;2;2) and (1;2;3) are not aligned. NumPy “duplicates” the array elements across the\n",
      "smaller dimension to match the larger dimension. This process is called broadcasting broadcasting and\n",
      "is carried out without actually making copies, thus providing e \u000ecient memory use. Below\n",
      "are some examples.\n",
      "import numpy as np\n",
      "A= np.arange(4).reshape(2,2) # (2,2) array\n",
      "x1 = np.array([40,500]) # (2,) array\n",
      "x2 = x1.reshape(2,1) # (2,1) array\n",
      "print(A + x1) # shapes (2,2) and (2,)\n",
      "print(A * x2) # shapes (2,2) and (2,1)484 D.10. NumPy\n",
      "[[ 40 501]\n",
      "[ 42 503]]\n",
      "[[ 0 40]\n",
      "[1000 1500]]\n",
      "Note that above x1is duplicated row-wise and x2column-wise. Broadcasting also applies\n",
      "to the matrix-wise operator @, as illustrated below. Here, the matrix bis duplicated across\n",
      "the third dimension resulting in the two matrix multiplications\n",
      "\"0 1\n",
      "2 3#\"0 1\n",
      "2 3#\n",
      "and\"4 5\n",
      "6 7#\"0 1\n",
      "2 3#\n",
      ":\n",
      "B = np.arange(8).reshape(2,2,2)\n",
      "b = np.arange(4).reshape(2,2)\n",
      "print(B@b)\n",
      "[[[ 2 3]\n",
      "[ 6 11]]\n",
      "[[10 19]\n",
      "[14 27]]]\n",
      "Functions such as sum,mean , and stdcan also be executed as methods of an ndarray\n",
      "instance. The argument axis can be passed to specify along which dimension the function\n",
      "is applied. By default axis=None .\n",
      "a = np.array(range(4)).reshape(2,2)\n",
      "print(a.sum(axis=0)) #summing over rows gives column totals\n",
      "[2, 4]\n",
      "D.10.4 Random Numbers\n",
      "One of the sub-modules in numpy israndom . It contains many functions for random vari-\n",
      "able generation.\n",
      "import numpy as np\n",
      "np.random.seed(123) # set the seed for the random number generator\n",
      "x = np.random.random() # uniform (0,1)\n",
      "y = np.random.randint(5,9) # discrete uniform 5,...,8\n",
      "z = np.random.randn(4) # array of four standard normals\n",
      "print(x,y, '\\n',z)\n",
      "0.6964691855978616 7\n",
      "[ 1.77399501 -0.66475792 -0.07351368 1.81403277]\n",
      "For more information on random variable generation in numpy , see\n",
      "https://docs.scipy.org/doc/numpy/reference/random/index.html .Appendix D. Python Primer 485\n",
      "D.11 Matplotlib\n",
      "The main Python graphics library for 2D and 3D plotting is matplotlib , and its subpack-\n",
      "agepyplot contains a collection of functions that make plotting in Python similar to that\n",
      "in M ATLAB .\n",
      "D.11.1 Creating a Basic Plot\n",
      "The code below illustrates various possibilities for creating plots. The style and color of\n",
      "lines and markers can be changed, as well as the font size of the labels. Figure D.1 shows\n",
      "the result.\n",
      "sqrtplot.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "x = np.arange(0, 10, 0.1)\n",
      "u = np.arange(0,10)\n",
      "y = np.sqrt(x)\n",
      "v = u/3\n",
      "plt.figure(figsize = [4,2]) # size of plot in inches\n",
      "plt.plot(x,y, 'g--') # plot green dashed line\n",
      "plt.plot(u,v, 'r.') # plot red dots\n",
      "plt.xlabel( 'x')\n",
      "plt.ylabel( 'y')\n",
      "plt.tight_layout()\n",
      "plt.savefig( 'sqrtplot.pdf ',format= 'pdf') # saving as pdf\n",
      "plt.show() # both plots will now be drawn\n",
      "0\n",
      " 2\n",
      " 4\n",
      " 6\n",
      " 8\n",
      " 10\n",
      "x\n",
      "0\n",
      "1\n",
      "2\n",
      "3y\n",
      "Figure D.1: A simple plot created using pyplot.\n",
      "The library matplotlib also allows the creation of subplots. The scatterplot and histogram\n",
      "in Figure D.2 have been produced using the code below. When creating a histogram there\n",
      "are several optional arguments that a \u000bect the layout of the graph. The number of bins is\n",
      "determined by the parameter bins (the default is 10). Scatterplots also take a number of\n",
      "parameters, such as a string cwhich determines the color of the dots, and alpha which\n",
      "a\u000bects the transparency of the dots.486 D.11. Matplotlib\n",
      "histscat.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "x = np.random.randn(1000)\n",
      "u = np.random.randn(100)\n",
      "v = np.random.randn(100)\n",
      "plt.subplot(121) # first subplot\n",
      "plt.hist(x,bins=25, facecolor= 'b')\n",
      "plt.xlabel( 'X Variable ')\n",
      "plt.ylabel( 'Counts ')\n",
      "plt.subplot(122) # second subplot\n",
      "plt.scatter(u,v,c= 'b', alpha=0.5)\n",
      "plt.show()\n",
      "2\n",
      " 0\n",
      " 2\n",
      "X Variable\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120Counts\n",
      "2\n",
      " 0\n",
      " 2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "Figure D.2: A histogram and scatterplot.\n",
      "One can also create three-dimensional plots as illustrated below.\n",
      "surf3dscat.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "def npdf(x,y):\n",
      "return np.exp(-0.5*(pow(x,2)+pow(y,2)))/np.sqrt(2*np.pi)\n",
      "x, y = np.random.randn(100), np.random.randn(100)\n",
      "z = npdf(x,y)\n",
      "xgrid , ygrid = np.linspace(-3,3,100), np.linspace(-3,3,100)\n",
      "Xarray , Yarray = np.meshgrid(xgrid ,ygrid)Appendix D. Python Primer 487\n",
      "Zarray = npdf(Xarray ,Yarray)\n",
      "fig = plt.figure(figsize=plt.figaspect(0.4))\n",
      "ax1 = fig.add_subplot(121, projection= '3d')\n",
      "ax1.scatter(x,y,z, c= 'g')\n",
      "ax1.set_xlabel( '$x$')\n",
      "ax1.set_ylabel( '$y$')\n",
      "ax1.set_zlabel( '$f(x,y)$ ')\n",
      "ax2 = fig.add_subplot(122, projection= '3d')\n",
      "ax2.plot_surface(Xarray ,Yarray ,Zarray ,cmap= 'viridis ',\n",
      "edgecolor= 'none ')\n",
      "ax2.set_xlabel( '$x$')\n",
      "ax2.set_ylabel( '$y$')\n",
      "ax2.set_zlabel( '$f(x,y)$ ')\n",
      "plt.show()\n",
      "x210123y\n",
      "21012f(x,y)\n",
      "0.00.10.20.30.4\n",
      "x3210123y\n",
      "3210123f(x,y)\n",
      "0.050.100.150.200.250.300.35\n",
      "Figure D.3: Three-dimensional scatter- and surface plots.\n",
      "D.12 Pandas\n",
      "The Python package Pandas (module name pandas ) provides various tools and data struc-\n",
      "tures for data analytics, including the fundamental DataFrame class.\n",
      "For the code in this section we assume that pandas has been imported via\n",
      "import pandas as pd .\n",
      "D.12.1 Series and DataFrame\n",
      "The two main data structures in pandas areSeries andDataFrame . ASeries object can\n",
      "be thought of as a combination of a dictionary and an 1-dimensional ndarray . The syntax488 D.12. Pandas\n",
      "for creating a Series object is\n",
      "series = pd.Series(<data>, index=[ 'index '])\n",
      "Here, <data> some 1-dimensional data structure, such as a 1-dimensional ndarray , a list,\n",
      "or a dictionary, and index is a list of names of the same length as <data> . When <data>\n",
      "is a dictionary, the index is created from the keys of the dictionary. When <data> is an\n",
      "ndarray andindex is omitted, the default index will be [0, ..., len(data)-1] .\n",
      "DICT = { 'one':1, 'two':2, 'three ':3, 'four ':4}\n",
      "print(pd.Series(DICT))\n",
      "one 1\n",
      "two 2\n",
      "three 3\n",
      "four 4\n",
      "dtype: int64\n",
      "years = [ '2000 ','2001 ','2002 ']\n",
      "cost = [2.34, 2.89, 3.01]\n",
      "print(pd.Series(cost ,index = years , name = 'MySeries ')) #name it\n",
      "2000 2.34\n",
      "2001 2.89\n",
      "2002 3.01\n",
      "Name: MySeries , dtype: float64\n",
      "The most commonly-used data structure in pandas is the two-dimensional DataFrame ,\n",
      "which can be thought of as pandas ’ implementation of a spreadsheet or as a diction-\n",
      "ary in which each “key” of the dictionary corresponds to a column name and the dic-\n",
      "tionary “value” is the data in that column. To create a DataFrame one can use the\n",
      "pandas DataFrame method, which has three main arguments: data, index (row labels),\n",
      "and columns (column labels).\n",
      "DataFrame(<data>, index=[ '<row_name> '], columns=[ '<column_name> '])\n",
      "If the index is not speciﬁed, the default index is [0, ..., len(data)-1] . Data can\n",
      "also be read directly from a CSV or Excel ﬁle, as is done in Section 1.1. If a dictionary is +1\n",
      "used to create the data frame (as below), the dictionary keys are used as the column names.\n",
      "DICT = { 'numbers ':[1,2,3,4], 'squared ':[1,4,9,16] }\n",
      "df = pd.DataFrame(DICT , index = list( 'abcd '))\n",
      "print(df)\n",
      "numbers squared\n",
      "a 1 1\n",
      "b 2 4\n",
      "c 3 9\n",
      "d 4 16Appendix D. Python Primer 489\n",
      "D.12.2 Manipulating Data Frames\n",
      "Often data encoded in DataFrame orSeries objects need to be extracted, altered, or com-\n",
      "bined. Getting, setting, and deleting columns works in a similar manner as for dictionaries.\n",
      "The following code illustrates various operations.\n",
      "ages = [6,3,5,6,5,8,0,3]\n",
      "d={'Gender ':['M','F']*4, 'Age': ages}\n",
      "df1 = pd.DataFrame(d)\n",
      "df1.at[0, 'Age']= 60 # change an element\n",
      "df1.at[1, 'Gender '] = 'Female '# change another element\n",
      "df2 = df1.drop( 'Age',1) # drop a column\n",
      "df3 = df2.copy(); # create a separate copy of df2\n",
      "df3[ 'Age'] = ages # add the original column\n",
      "dfcomb = pd.concat([df1,df2,df3],axis=1) # combine the three dfs\n",
      "print(dfcomb)\n",
      "Gender Age Gender Gender Age\n",
      "0 M 60 M M 6\n",
      "1 Female 3 Female Female 3\n",
      "2 M 5 M M 5\n",
      "3 F 6 F F 6\n",
      "4 M 5 M M 5\n",
      "5 F 8 F F 8\n",
      "6 M 0 M M 0\n",
      "7 F 3 F F 3\n",
      "Note that the above DataFrame object has two Age columns. The expression\n",
      "dfcomb['Age'] will return a DataFrame with both these columns.\n",
      "Table D.3: Useful pandas methods for data manipulation.\n",
      "agg Aggregate the data using one or more functions.\n",
      "apply Apply a function to a column or row.\n",
      "astype Change the data type of a variable.\n",
      "concat Concatenate data objects.\n",
      "replace Find and replace values.\n",
      "read_csv Read a CSV ﬁle into a DataFrame.\n",
      "sort_values Sort by values along rows or columns.\n",
      "stack Stack a DataFrame.\n",
      "to_excel Write a DataFrame to an Excel ﬁle.\n",
      "It is important to correctly specify the data type of a variable before embarking on\n",
      "data summarization and visualization tasks, as Python may treat di \u000berent types of objects\n",
      "in dissimilar ways. Common data types for entries in a DataFrame object are float,\n",
      "category, datetime, bool , and int. A generic object type is object .\n",
      "d={'Gender ':['M','F','F']*4, 'Age': [6,3,5,6,5,8,0,3,6,6,7,7]}\n",
      "df=pd.DataFrame(d)\n",
      "print(df.dtypes)\n",
      "df['Gender '] = df[ 'Gender '].astype( 'category ') #change the type\n",
      "print(df.dtypes)490 D.12. Pandas\n",
      "Gender object\n",
      "Age int64\n",
      "dtype: object\n",
      "Gender category\n",
      "Age int64\n",
      "dtype: object\n",
      "D.12.3 Extracting Information\n",
      "Extracting statistical information from a DataFrame object is facilitated by a large col-\n",
      "lection of methods (functions) in pandas . Table D.4 gives a selection of data inspection\n",
      "methods. See Chapter 1 for their practical use. The code below provides several examples +1\n",
      "of useful methods. The apply method allows one to apply general functions to columns\n",
      "or rows of a DataFrame. These operations do not change the data. The locmethod allows\n",
      "for accessing elements (or ranges) in a data frame and acts similar to the slicing operation\n",
      "for lists and arrays, with the di \u000berence that the “stop” value is included , as illustrated in\n",
      "the code below.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "ages = [6,3,5,6,5,8,0,3]\n",
      "np.random.seed(123)\n",
      "df = pd.DataFrame(np.random.randn(3,4), index = list( 'abc'),\n",
      "columns = list( 'ABCD '))\n",
      "print(df)\n",
      "df1 = df.loc[\"b\":\"c\",\"B\":\"C\"] # create a partial data frame\n",
      "print(df1)\n",
      "meanA = df[ 'A'].mean() # mean of 'A'column\n",
      "print( 'mean of column A = {} '.format(meanA))\n",
      "expA = df[ 'A'].apply(np.exp) # exp of all elements in 'A'column\n",
      "print(expA)\n",
      "A B C D\n",
      "a -1.085631 0.997345 0.282978 -1.506295\n",
      "b -0.578600 1.651437 -2.426679 -0.428913\n",
      "c 1.265936 -0.866740 -0.678886 -0.094709\n",
      "B C\n",
      "b 1.651437 -2.426679\n",
      "c -0.866740 -0.678886\n",
      "mean of column A = -0.13276486552118785\n",
      "a 0.337689\n",
      "b 0.560683\n",
      "c 3.546412\n",
      "Name: A, dtype: float64\n",
      "Thegroupby method of a DataFrame object is useful for summarizing and displaying\n",
      "the data in manipulated ways. It groups data according to one or more speciﬁed columns,\n",
      "such that methods such as count andmean can be applied to the grouped data.Appendix D. Python Primer 491\n",
      "Table D.4: Useful pandas methods for data inspection.\n",
      "columns Column names.\n",
      "count Counts number of non-NA cells.\n",
      "crosstab Cross-tabulate two or more categories.\n",
      "describe Summary statistics.\n",
      "dtypes Data types for each column.\n",
      "head Display the top rows of a DataFrame.\n",
      "groupby Group data by column(s).\n",
      "info Display information about the DataFrame.\n",
      "loc Access a group or rows or columns.\n",
      "mean Column /row mean.\n",
      "plot Plot of columns.\n",
      "std Column /row standard deviation.\n",
      "sum Returns column /row sum.\n",
      "tail Display the bottom rows of a DataFrame.\n",
      "value_counts Counts of di \u000berent non-null values.\n",
      "var Variance.\n",
      "df = pd.DataFrame({ 'W':['a','a','b','a','a','b'],\n",
      "'X':np.random.rand(6),\n",
      "'Y':['c','d','d','d','c','c'],'Z':np.random.rand(6)})\n",
      "print(df)\n",
      "W X Y Z\n",
      "0 a 0.993329 c 0.641084\n",
      "1 a 0.925746 d 0.428412\n",
      "2 b 0.266772 d 0.460665\n",
      "3 a 0.201974 d 0.261879\n",
      "4 a 0.529505 c 0.503112\n",
      "5 b 0.006231 c 0.849683\n",
      "print(df.groupby( 'W').mean())\n",
      "X Z\n",
      "W\n",
      "a 0.662639 0.458622\n",
      "b 0.136502 0.655174\n",
      "print(df.groupby([ 'W','Y']).mean())\n",
      "X Z\n",
      "W Y\n",
      "a c 0.761417 0.572098\n",
      "d 0.563860 0.345145\n",
      "b c 0.006231 0.849683\n",
      "d 0.266772 0.460665\n",
      "To allow for multiple functions to be calculated at once, the aggmethod can be used.\n",
      "It can take a list, dictionary, or string of functions.492 D.13. Scikit-learn\n",
      "print(df.groupby( 'W').agg([sum,np.mean]))\n",
      "X Z\n",
      "sum mean sum mean\n",
      "W\n",
      "a 2.650555 0.662639 1.834487 0.458622\n",
      "b 0.273003 0.136502 1.310348 0.655174\n",
      "D.12.4 Plotting\n",
      "Theplot method of a DataFrame makes plots of a DataFrame using Matplotlib. Di \u000berent\n",
      "types of plot can be accessed via the kind = 'str'construction, where str is one of\n",
      "line (default), bar,hist ,box,kde, and several more. Finer control, such as modifying\n",
      "the font, is obtained by using matplotlib directly. The following code produces the line\n",
      "and box plots in Figure D.4.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "df = pd.DataFrame({ 'normal ':np.random.randn(100),\n",
      "'Uniform ':np.random.uniform(0,1,100)})\n",
      "font = { 'family ':'serif ','size ' : 14} #set font\n",
      "matplotlib.rc( 'font ', **font) # change font\n",
      "df.plot() # line plot (default)\n",
      "df.plot(kind = 'box') # box plot\n",
      "matplotlib.pyplot.show() #render plots\n",
      "0\n",
      " 20\n",
      " 40\n",
      " 60\n",
      " 80\n",
      " 100\n",
      "2\n",
      "0\n",
      "2\n",
      "Normal\n",
      "Uniform\n",
      "Normal\n",
      " Uniform\n",
      "2\n",
      "0\n",
      "2\n",
      "Figure D.4: A line and box plot using the plot method of DataFrame .\n",
      "D.13 Scikit-learn\n",
      "Scikit-learn is an open-source machine learning and data science library for Python. The\n",
      "library includes a range of algorithms relating to the chapters in this book. It is widely\n",
      "used due to its simplicity and its breadth. The module name is sklearn . Below is a brief\n",
      "introduction into modeling the data with sklearn . The full documentation can be found\n",
      "atAppendix D. Python Primer 493\n",
      "https://scikit-learn.org/ .\n",
      "D.13.1 Partitioning the Data\n",
      "Randomly partitioning the data in order to test the model may be achieved easily with\n",
      "sklearn ’s function train_test_split . For example, suppose that the training data is\n",
      "described by the matrix Xof explanatory variables and the vector yof responses. Then the\n",
      "following code splits the data set into training and testing sets, with the testing set being\n",
      "half of the total set.\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train , X_test , y_train , y_test = train_test_split(X, y,\n",
      "test_size = 0.5)\n",
      "As an example, the following code generates a synthetic data set and splits it into\n",
      "equally-sized training and test sets.\n",
      "syndat.py\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "np.random.seed(1234)\n",
      "X=np.pi*(2*np.random.random(size=(400,2))-1)\n",
      "y=(np.cos(X[:,0])*np.sin(X[:,1]) >=0)\n",
      "X_train , X_test , y_train , y_test = train_test_split(X, y,\n",
      "test_size=0.5)\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot (111)\n",
      "ax.scatter(X_train[y_train==0,0],X_train[y_train==0,1], c= 'g',\n",
      "marker= 'o',alpha=0.5)\n",
      "ax.scatter(X_train[y_train==1,0],X_train[y_train==1,1], c= 'b',\n",
      "marker= 'o',alpha=0.5)\n",
      "ax.scatter(X_test[y_test==0,0],X_test[y_test==0,1], c= 'g',\n",
      "marker= 's',alpha=0.5)\n",
      "ax.scatter(X_test[y_test==1,0],X_test[y_test==1,1], c= 'b',\n",
      "marker= 's',alpha=0.5)\n",
      "plt.savefig( 'sklearntraintest.pdf ',format= 'pdf')\n",
      "plt.show()\n",
      "D.13.2 Standardization\n",
      "In some instances it may be necessary to standardize the data. This may be done in\n",
      "sklearn with scaling methods such as MinMaxScaler orStandardScaler . Scaling may\n",
      "improve the convergence of gradient-based estimators and is useful when visualizing data\n",
      "on vastly di \u000berent scales. For example, suppose that Xis our explanatory data (e.g., stored\n",
      "as anumpy array), and we wish to standardize such that each value lies between 0 and 1.494 D.13. Scikit-learn\n",
      "3\n",
      " 2\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Figure D.5: Example training (circles) and test (squares) set for two class classiﬁcation.\n",
      "Explanatory variables are the ( x;y) coordinates, classes are zero (green) or one (blue).\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
      "x_scaled = min_max_scaler.fit_transform(X)\n",
      "# equivalent to:\n",
      "x_scaled = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "D.13.3 Fitting and Prediction\n",
      "Once the data has been partitioned and standardized if necessary, the data may be ﬁtted to\n",
      "a statistical model, e.g., a classiﬁcation or regression model. For example, continuing with\n",
      "our data from above, the following ﬁts a model to the data and predicts the responses for\n",
      "the test set.\n",
      "from sklearn.someSubpackage import someClassifier\n",
      "clf = someClassifier() # choose appropriate classifier\n",
      "clf.fit(X_train , y_train) # fit the data\n",
      "y_prediction = clf.predict(X_test) # predict\n",
      "Speciﬁc classiﬁers for logistic regression, naïve Bayes, linear and quadratic discrimin-\n",
      "ant analysis, K-nearest neighbors, and support vector machines are given in Section 7.8.\n",
      "+279\n",
      "D.13.4 Testing the Model\n",
      "Once the model has made its prediction we may test its e \u000bectiveness, using relevant met-\n",
      "rics. For example, for classiﬁcation we may wish to produce the confusion matrix for theAppendix D. Python Primer 495\n",
      "test data. The following code does this for the data shown in Figure D.5, using a support\n",
      "vector machine classiﬁer.\n",
      "from sklearn import svm\n",
      "clf = svm.SVC(kernel = 'rbf')\n",
      "clf.fit(X_train , y_train)\n",
      "y_prediction = clf.predict(X_test)\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(confusion_matrix(y_test , y_prediction))\n",
      "[[102 12]\n",
      "[ 1 85]]\n",
      "D.14 System Calls, URL Access, and Speed-Up\n",
      "Operating system commands (whether in Windows, MacOS, or Linux) for creating dir-\n",
      "ectories, copying or removing ﬁles, or executing programs from the system shell can be\n",
      "issued from within Python by using the package os. Another useful package is requests\n",
      "which enables direct downloads of ﬁles and webpages from URLs. The following Python\n",
      "script uses both. It also illustrates a simple example of exception handling in Python.\n",
      "misc.py\n",
      "import os\n",
      "import requests\n",
      "for c in \"123456\":\n",
      "try: # if it does not yet exist\n",
      "os.mkdir(\"MyDir\"+ c) # make a directory\n",
      "except: # otherwise\n",
      "pass # do nothing\n",
      "uname = \"https://github.com/DSML -book/Programs/tree/master/\n",
      "Appendices/Python Primer/\"\n",
      "fname = \"ataleof2cities.txt\"\n",
      "r = requests.get(uname + fname)\n",
      "print(r.text)\n",
      "open( 'MyDir1/ato2c.txt ','wb').write(r.content) #write to a file\n",
      "# bytes mode is important here\n",
      "The package numba can signiﬁcantly speed up calculations via smart compilation. First\n",
      "run the following code.\n",
      "jitex.py\n",
      "import timeit\n",
      "import numpy as np\n",
      "from numba import jit\n",
      "n = 10**8\n",
      "#@jit\n",
      "def myfun(s,n):\n",
      "for i in range(1,n):496 D.14. System Calls, URL Access, and Speed-Up\n",
      "s = s+ 1/i\n",
      "return s\n",
      "start = timeit.time.clock()\n",
      "print(\"Euler 's constant is approximately {:9.8f}\".format(\n",
      "myfun(0,n) - np.log(n)))\n",
      "end = timeit.time.clock()\n",
      "print(\"elapsed time: {:3.2f} seconds\".format(end-start))\n",
      "Euler 's constant is approximately 0.57721566\n",
      "elapsed time: 5.72 seconds\n",
      "Now remove the # character before the @ character in the code above, in order to\n",
      "activate the “just in time” compiler. This gives a 15-fold speedup:\n",
      "Euler 's constant is approximately 0.57721566\n",
      "elapsed time: 0.39 seconds\n",
      "Further Reading\n",
      "To learn Python, we recommend [82] and [110]. However, as Python is constantly evolving,\n",
      "the most up-to-date references will be available from the Internet.BIBLIOGRAPHY\n",
      "[1] S. C. Ahalt, A. K. Krishnamurthy, P. Chen, and D. E. Melton. Competitive learning\n",
      "algorithms for vector quantization. Neural Networks , 3:277–290, 1990.\n",
      "[2] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on\n",
      "Automatic Control , 19(6):716–723, 1974.\n",
      "[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Math-\n",
      "ematical Society , 68:337–404, 1950.\n",
      "[4] D. Arthur and S. Vassilvitskii. K-means ++: The advantages of careful seeding.\n",
      "InProceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Al-\n",
      "gorithms , pages 1027–1035, Philadelphia, 2007. Society for Industrial and Applied\n",
      "Mathematics.\n",
      "[5] S. Asmussen and P. W. Glynn. Stochastic Simulation: Algorithms and Analysis .\n",
      "Springer, New York, 2007.\n",
      "[6] R. G. Bartle. The Elements of Integration and Lebesgue Measure . John Wiley &\n",
      "Sons, Hoboken, 1995.\n",
      "[7] D. Bates and D. Watts. Nonlinear Regression Analysis and Its Applications . John\n",
      "Wiley & Sons, Hoboken, 1988.\n",
      "[8] J. O. Berger. Statistical Decision Theory and Bayesian Analysis . Springer, New\n",
      "York, second edition, 1985.\n",
      "[9] J. Bezdek. Pattern Recognition with Fuzzy Objective Function Algorithms . Plenum\n",
      "Press, New York, 1981.\n",
      "[10] P. J. Bickel and K. A. Doksum. Mathematical Statistics , volume I. Pearson Prentice\n",
      "Hall, Upper Saddle River, second edition, 2007.\n",
      "497498 Bibliography\n",
      "[11] P. Billingsley. Probability and Measure . John Wiley & Sons, New York, third\n",
      "edition, 1995.\n",
      "[12] C. M. Bishop. Pattern Recognition and Machine Learning . Springer, New York,\n",
      "2006.\n",
      "[13] P. T. Boggs and R. H. Byrd. Adaptive, limited-memory BFGS algorithms for un-\n",
      "constrained optimization. SIAM Journal on Optimization , 29(2):1282–1299, 2019.\n",
      "[14] Z. I. Botev, J. F. Grotowski, and D. P. Kroese. Kernel density estimation via di \u000bu-\n",
      "sion. Annals of Statistics , 38(5):2916–2957, 2010.\n",
      "[15] Z. I. Botev and D. P. Kroese. Global likelihood optimization via the cross-entropy\n",
      "method, with an application to mixture models. In R. G. Ingalls, M. D. Rossetti,\n",
      "J. S. Smith, and B. A. Peters, editors, Proceedings of the 2004 Winter Simulation\n",
      "Conference , pages 529–535, Washington, DC, December 2004.\n",
      "[16] Z. I. Botev, D. P. Kroese, R. Y . Rubinstein, and P. L’Ecuyer. The cross-entropy\n",
      "method for optimization. In V . Govindaraju and C.R. Rao, editors, Machine Learn-\n",
      "ing: Theory and Applications , volume 31 of Handbook of Statistics , pages 35–59.\n",
      "Elsevier, 2013.\n",
      "[17] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and\n",
      "statistical learning via the alternating direction method of multipliers. Foundations\n",
      "and Trends in Machine Learning , 3:1–122, 2010.\n",
      "[18] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press,\n",
      "Cambridge, 2004. Seventh printing with corrections, 2009.\n",
      "[19] R. A. Boyles. On the convergence of the EM algorithm. Journal of the Royal\n",
      "Statistical Society, Series B , 45(1):47–50, 1983.\n",
      "[20] L. Breiman. Classiﬁcation and Regression Trees . CRC Press, Boca Raton, 1987.\n",
      "[21] L. Breiman. Bagging predictors. Machine Learning , 24(2):123–140, 1996.\n",
      "[22] L. Breiman. Heuristics of instability and stabilization in model selection. Annals of\n",
      "Statistics , 24(6):2350–2383, 12 1996.\n",
      "[23] L. Breiman. Random forests. Machine Learning , 45(1):5–32, 2001.\n",
      "[24] F. Cao, D.-Z. Du, B. Gao, P.-J. Wan, and P. M. Pardalos. Minimax problems in\n",
      "combinatorial optimization. In D.-Z. Du and P. M. Pardalos, editors, Minimax and\n",
      "Applications , pages 269–292. Kluwer, Dordrecht, 1995.\n",
      "[25] G. Casella and R. L. Berger. Statistical Inference . Duxbury Press, Paciﬁc Grove,\n",
      "second edition, 2001.\n",
      "[26] K. L. Chung. A Course in Probability Theory . Academic Press, New York, second\n",
      "edition, 1974.Bibliography 499\n",
      "[27] E. Cinlar. Introduction to Stochastic Processes . Prentice Hall, Englewood Cli \u000bs,\n",
      "1975.\n",
      "[28] T. M. Cover and J. A. Thomas. Elements of Information Theory . John Wiley &\n",
      "Sons, New York, 1991.\n",
      "[29] J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. Reorthogonalization and\n",
      "stable algorithms for updating the Gram-Schmidt QR factorization. Mathematics of\n",
      "Computation , 30(136):772–795, 1976.\n",
      "[30] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y . Rubinstein. A tutorial on the\n",
      "cross-entropy method. Annals of Operations Research , 134(1):19–67, 2005.\n",
      "[31] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-\n",
      "plete data via the EM algorithm. Journal of the Royal Statistical Society , 39(1):1 –\n",
      "38, 1977.\n",
      "[32] L. Devroye. Non-Uniform Random Variate Generation . Springer, New York, 1986.\n",
      "[33] N. R. Draper and H. Smith. Applied Regression Analysis . John Wiley & Sons, New\n",
      "York, third edition, 1998.\n",
      "[34] Q. Duan and D. P. Kroese. Splitting for optimization. Computers &Operations\n",
      "Research , 73:119–131, 2016.\n",
      "[35] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation . John Wiley & Sons,\n",
      "New York, 2001.\n",
      "[36] B. Efron and T. J. Hastie. Computer Age Statistical Inference: Algorithms, Evidence,\n",
      "and Data Science . Cambridge University Press, Cambridge, 2016.\n",
      "[37] B. Efron and R. Tibshirani. An Introduction to the Bootstrap . Chapman & Hall,\n",
      "New York, 1994.\n",
      "[38] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters ,\n",
      "27(8):861–874, June 2006.\n",
      "[39] W. Feller. An Introduction to Probability Theory and Its Applications , volume I.\n",
      "John Wiley & Sons, Hoboken, second edition, 1970.\n",
      "[40] J. C. Ferreira and V . A. Menegatto. Eigenvalues of integral operators deﬁned by\n",
      "smooth positive deﬁnite kernels. Integral Equations and Operator Theory , 64:61–\n",
      "81, 2009.\n",
      "[41] N. I. Fisher and P. K. Sen, editors. The Collected Works of Wassily Hoe \u000bding.\n",
      "Springer, New York, 1994.\n",
      "[42] G. S. Fishman. Monte Carlo: Concepts, Algorithms and Applications . Springer,\n",
      "New York, 1996.\n",
      "[43] R. Fletcher. Practical Methods of Optimization . John Wiley & Sons, New York,\n",
      "1987.500 Bibliography\n",
      "[44] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning\n",
      "and an application to boosting. J. Comput. Syst. Sci. , 55(1):119–139, 1997.\n",
      "[45] J. H. Friedman. Greedy function approximation: A gradient boosting machine. An-\n",
      "nals of Statistics , 29:1189–1232, 2000.\n",
      "[46] A. Gelman. Bayesian Data Analysis . Chapman & Hall, New York, second edition,\n",
      "2004.\n",
      "[47] A. Gelman and J. Hall. Data Analysis Using Regression and Multilevel /Hierarchical\n",
      "Models . Cambridge University Press, Cambridge, 2006.\n",
      "[48] S. Geman and D. Geman. Stochastic relaxation, Gibbs distribution and the Bayesian\n",
      "restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-\n",
      "gence , 6(6):721–741, 1984.\n",
      "[49] J. E. Gentle. Random Number Generation and Monte Carlo Methods . Springer,\n",
      "New York, second edition, 2003.\n",
      "[50] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in\n",
      "Practice . Chapman & Hall, New York, 1996.\n",
      "[51] P. Glasserman. Monte Carlo Methods in Financial Engineering . Springer, New\n",
      "York, 2004.\n",
      "[52] G. H. Golub and C. F. Van Loan. Matrix Computations . Johns Hopkins University\n",
      "Press, Baltimore, fourth edition, 2013.\n",
      "[53] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press, Cambridge,\n",
      "2016.\n",
      "[54] G. R. Grimmett and D. R. Stirzaker. Probability and Random Processes . Oxford\n",
      "University Press, third edition, 2001.\n",
      "[55] T. J. Hastie, R. J. Tibshirani, and J. H. Friedman. The Elements of Statistical Learn-\n",
      "ing: Data mining, Inference, and Prediction . Springer, New York, 2009.\n",
      "[56] T. J. Hastie, R. J. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity:\n",
      "The Lasso and Generalizations . CRC Press, Boca Raton, 2015.\n",
      "[57] J.-B. Hiriart-Urruty and C. Lemarèchal. Fundamentals of Convex Analysis .\n",
      "Springer, New York, 2001.\n",
      "[58] W. Hock and K. Schittkowski. Test Examples for Nonlinear Programming Codes .\n",
      "Springer, New York, 1981.\n",
      "[59] J. E. Kelley, Jr. The cutting-plane method for solving convex programs. Journal of\n",
      "the Society for Industrial and Applied Mathematics , 8(4):703–712, 1960.\n",
      "[60] A. K. Jain. Fundamentals of Digital Image Processing . Prentice Hall, Englewood\n",
      "Cli\u000bs, 1989.Bibliography 501\n",
      "[61] O. Kallenberg. Foundations of Modern Probability . Springer, New York, second\n",
      "edition, 2002.\n",
      "[62] A. Karalic. Linear regression in regression tree leaves. In Proceedings of ECAI-92 ,\n",
      "pages 440–441, Hoboken, 1992. John Wiley & Sons.\n",
      "[63] C. Kaynak. Methods of combining multiple classiﬁers and their applications to\n",
      "handwritten digit recognition. Master’s thesis, Institute of Graduate Studies in Sci-\n",
      "ence and Engineering, Bogazici University, 1995.\n",
      "[64] T. Keilath and A. H. Sayed, editors. Fast Reliable Algorithms for Matrices with\n",
      "Structure . SIAM, Pennsylvania, 1999.\n",
      "[65] C. Nussbaumer Knaﬂic. Storytelling with Data: A Data Visualization Guide for\n",
      "Business Professionals . John Wiley & Sons, Hoboken, 2015.\n",
      "[66] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-\n",
      "niques - Adaptive Computation and Machine Learning . The MIT Press, Cambridge,\n",
      "2009.\n",
      "[67] A. N. Kolmogorov and S. V . Fomin. Elements of the Theory of Functions and\n",
      "Functional Analysis . Dover Publications, Mineola, 1999.\n",
      "[68] D. P. Kroese, T. Brereton, T. Taimre, and Z. I. Botev. Why the Monte Carlo method\n",
      "is so important today. Wiley Interdisciplinary Reviews: Computational Statistics ,\n",
      "6(6):386–392, 2014.\n",
      "[69] D. P. Kroese and J. C. C. Chan. Statistical Modeling and Computation . Springer,\n",
      "2014.\n",
      "[70] D. P. Kroese, S. Porotsky, and R. Y . Rubinstein. The cross-entropy method for\n",
      "continuous multi-extremal optimization. Methodology and Computing in Applied\n",
      "Probability , 8(3):383–407, 2006.\n",
      "[71] D. P. Kroese, T. Taimre, and Z. I. Botev. Handbook of Monte Carlo Methods . John\n",
      "Wiley & Sons, New York, 2011.\n",
      "[72] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms\n",
      "and Applications . Springer, New York, second edition, 2003.\n",
      "[73] P. Lafaye de Micheaux, R. Drouilhet, and B. Liquet. The R Software: Fundamentals\n",
      "of Programming and Statistical Analysis . Springer, New York, 2014.\n",
      "[74] R. J. Larsen and M. L. Marx. An Introduction to Mathematical Statistics and Its\n",
      "Applications . Prentice Hall, New York, third edition, 2001.\n",
      "[75] A. M. Law and W. D. Kelton. Simulation Modeling and Analysis . McGraw-Hill,\n",
      "New York, third edition, 2000.\n",
      "[76] P. L’Ecuyer. A uniﬁed view of IPA, SF, and LR gradient estimation techniques.\n",
      "Management Science , 36:1364–1383, 1990.502 Bibliography\n",
      "[77] P. L’Ecuyer. Good parameters and implementations for combined multiple recursive\n",
      "random number generators. Operations Research , 47(1):159 – 164, 1999.\n",
      "[78] E. L. Lehmann and G. Casella. Theory of Point Estimation . Springer, New York,\n",
      "second edition, 1998.\n",
      "[79] T. G. Lewis and W. H. Payne. Generalized feedback shift register pseudorandom\n",
      "number algorithm. Journal of the ACM , 20(3):456–468, 1973.\n",
      "[80] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data . John Wiley\n",
      "& Sons, Hoboken, second edition, 2002.\n",
      "[81] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale\n",
      "optimization. Mathematical Programming , 45(1-3):503–528, 1989.\n",
      "[82] M. Lutz. Learning Python . O’Reilly, ﬁfth edition, 2013.\n",
      "[83] M. Matsumoto and T. Nishimura. Mersenne twister: A 623-dimensionally\n",
      "equidistributed uniform pseudo-random number generator. ACM Transactions on\n",
      "Modeling and Computer Simulation , 8(1):3–30, 1998.\n",
      "[84] W. McKinney. Python for Data Analysis . O’Reilly Media, Inc., second edition,\n",
      "2017.\n",
      "[85] G. J. McLachlan and T. Krishnan. The EM Algorithm and Extensions . John Wiley\n",
      "& Sons, Hoboken, second edition, 2008.\n",
      "[86] G. J. McLachlan and D. Peel. Finite Mixture Models . John Wiley & Sons, New\n",
      "York, 2000.\n",
      "[87] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller.\n",
      "Equations of state calculations by fast computing machines. Journal of Chemical\n",
      "Physics , 21(6):1087–1092, 1953.\n",
      "[88] C. A. Micchelli, Y . Xu, and H. Zhang. Universal kernels. Journal of Machine\n",
      "Learning Research , 7:2651–2667, 2006.\n",
      "[89] Z. Michalewicz. Genetic Algorithms +Data Structures =Evolution Programs .\n",
      "Springer, New York, third edition, 1996.\n",
      "[90] J. F. Monahan. Numerical Methods of Statistics . Cambridge University Press, Lon-\n",
      "don, 2010.\n",
      "[91] T. A. Mroz. The sensitivity of an empirical model of married women’s hours of\n",
      "work to economic and statistical assumptions. Econometrica , 55(4):765–799, 1987.\n",
      "[92] K. P. Murphy. Machine Learning: A Probabilistic Perspective . The MIT Press,\n",
      "Cambridge, 2012.\n",
      "[93] J. Neyman and E. Pearson. On the problem of the most e \u000ecient tests of statistical\n",
      "hypotheses. Philosophical Transactions of the Royal Society of London, Series A ,\n",
      "231:289–337, 1933.Bibliography 503\n",
      "[94] M. A. Nielsen. Neural Networks and Deep Learning , volume 25. Determination\n",
      "Press, 2015.\n",
      "[95] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook. Technical University of\n",
      "Denmark , 2008.\n",
      "[96] J. R. Quinlan. Learning with continuous classes. In A. Adams and L. Sterling,\n",
      "editors, Proceedings AI’92 , pages 343–348, Singapore, 1992. World Scientiﬁc.\n",
      "[97] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning .\n",
      "MIT Press, Cambridge, 2006.\n",
      "[98] B. D. Ripley. Stochastic Simulation . John Wiley & Sons, New York, 1987.\n",
      "[99] C. P. Robert and G. Casella. Monte Carlo Statistical Methods . Springer, New York,\n",
      "second edition, 2004.\n",
      "[100] S. M. Ross. Simulation . Academic Press, New York, third edition, 2002.\n",
      "[101] S. M. Ross. A First Course in Probability . Prentice Hall, Englewood Cli \u000bs, seventh\n",
      "edition, 2005.\n",
      "[102] R. Y . Rubinstein. The cross-entropy method for combinatorial and continuous op-\n",
      "timization. Methodology and Computing in Applied Probability , 2:127–190, 1999.\n",
      "[103] R. Y . Rubinstein and D. P. Kroese. The Cross-Entropy Method: A Uniﬁed Approach\n",
      "to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning .\n",
      "Springer, New York, 2004.\n",
      "[104] R. Y . Rubinstein and D. P. Kroese. Simulation and the Monte Carlo Method . John\n",
      "Wiley & Sons, New York, third edition, 2017.\n",
      "[105] S. Ruder. An overview of gradient descent optimization algorithms. arXiv:\n",
      "1609.04747 , 2016.\n",
      "[106] W. Rudin. Functional Analysis . McGraw–Hill, Singapore, second edition, 1991.\n",
      "[107] D. Salomon. Data Compression: The Complete Reference . Springer, New York,\n",
      "2000.\n",
      "[108] G. A. F. Seber and A. J. Lee. Linear Regression Analysis . John Wiley & Sons,\n",
      "Hoboken, second edition, 2003.\n",
      "[109] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From The-\n",
      "ory to Algorithms . Cambridge University Press, Cambridge, 2014.\n",
      "[110] Z. A. Shaw. Learning Python 3 the Hard Way . Addison–Wesley, Boston, 2017.\n",
      "[111] Y . Shen, S. Kiatsupaibul, Z. B. Zabinsky, and R. L. Smith. An analytically de-\n",
      "rived cooling schedule for simulated annealing. Journal of Global Optimization ,\n",
      "38(2):333–365, 2007.504 Bibliography\n",
      "[112] N. Z. Shor. Minimization Methods for Non-di \u000berentiable Functions . Springer, Ber-\n",
      "lin, 1985.\n",
      "[113] B. W. Silverman. Density Estimation for Statistics and Data Analysis . Chapman &\n",
      "Hall, New York, 1986.\n",
      "[114] J. S. Simono \u000b.Smoothing Methods in Statistics . Springer, New York, 2012.\n",
      "[115] I. Steinwart and A. Christmann. Support Vector Machines . Springer, New York,\n",
      "2008.\n",
      "[116] G. Strang. Introduction to Linear Algebra . Wellesley–Cambridge Press, Cambridge,\n",
      "ﬁfth edition, 2016.\n",
      "[117] G. Strang. Linear Algebra and Learning from Data . Wellesley–Cambridge Press,\n",
      "Cambridge, 2019.\n",
      "[118] W. N. Street, W. H. Wolberg, and O. L. Mangasarian. Nuclear feature extraction for\n",
      "breast tumor diagnosis. In IS&T/SPIE 1993 International Symposium on Electronic\n",
      "Imaging: Science and Technology, San Jose, CA , pages 861–870, 1993.\n",
      "[119] V . M. Tikhomirov. On the representation of continuous functions of several variables\n",
      "as superpositions of continuous functions of one variable and addition. In Selected\n",
      "Works of A. N. Kolmogorov , pages 383–387. Springer, Berlin, 1991.\n",
      "[120] S. van Buuren. Flexible Imputation of Missing Data . CRC Press, Boca Raton,\n",
      "second edition, 2018.\n",
      "[121] V . N. Vapnik. The Nature of Statistical Learning Theory . Springer, New York, 1995.\n",
      "[122] V . N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative fre-\n",
      "quencies of events to their probabilities. Theory of Probability and Its Applications ,\n",
      "16(2):264–280, 1971.\n",
      "[123] G. Wahba. Spline Models for Observational Data . SIAM, Philadelphia, 1990.\n",
      "[124] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference . Springer,\n",
      "2010.\n",
      "[125] A. Webb. Statistical Pattern Recognition . Arnold, London, 1999.\n",
      "[126] H. Wendland. Scattered Data Approximation . Cambridge University Press, Cam-\n",
      "bridge, 2005.\n",
      "[127] D. Williams. Probability with Martingales . Cambridge University Press, Cam-\n",
      "bridge, 1991.\n",
      "[128] C. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of\n",
      "Statistics , 11(1):95–103, 1983.INDEX\n",
      "A\n",
      "acceptance probability, 78–80, 97\n",
      "acceptance–rejection method, 73, 78\n",
      "accuracy (classiﬁcation–), 256\n",
      "activation function, 204, 327\n",
      "AdaBoost, 319–322\n",
      "AdaGrad, 341\n",
      "Adam method, 341, 348\n",
      "adjoint operation, 363\n",
      "a\u000ene transformation, 407, 437\n",
      "agglomerative clustering, 147\n",
      "Akaike information criterion, 126, 176,\n",
      "177\n",
      "algebraic multiplicity, 365\n",
      "aligned arrays (Python), 483\n",
      "almost sure convergence, 442\n",
      "alternating direction method of\n",
      "multipliers, 220, 418\n",
      "alternative hypothesis, 460\n",
      "anaconda (Python), 465\n",
      "analysis of variance (ANOV A), 183, 184,\n",
      "195, 208\n",
      "annealing schedule, 97\n",
      "approximation error, 32–34, 184\n",
      "approximation–estimation tradeo \u000b, 32,\n",
      "41, 325\n",
      "Armijo inexact line search, 411\n",
      "assignment operator (Python), 469\n",
      "attributes (Python), 467auxiliary variable methods, 128\n",
      "axioms of Kolmogorov, 423\n",
      "B\n",
      "back-propagation, 333\n",
      "backward elimination, 201\n",
      "backward substitution, 372\n",
      "bagged estimator, 308\n",
      "bagging, 307, 309, 312\n",
      "balance equations (Markov chains), 78,\n",
      "79, 454\n",
      "bandwidth, 131, 134, 226\n",
      "barplot, 9\n",
      "barrier function, 419\n",
      "Barzilai–Borwein formulas, 336, 415\n",
      "basis\n",
      "of a vector space, 357\n",
      "orthogonal –, 363\n",
      "Bayes\n",
      "empirical, 242\n",
      "error rate, 254\n",
      "factor, 57\n",
      "naïve –, 260\n",
      "optimal decision rule, 260\n",
      "Bayes’ rule, 47, 48, 430, 456\n",
      "Bayesian information criterion, 54\n",
      "Bayesian statistics, 47, 49, 456\n",
      "Bernoulli distribution, 427, 459\n",
      "Bessel distribution, 164, 227\n",
      "505506 Index\n",
      "beta distribution, 52, 427\n",
      "bias of an estimator, 456\n",
      "bias vector (deep learning), 328\n",
      "bias–variance tradeo \u000b, 35, 307\n",
      "binomial distribution, 427\n",
      "Boltzmann distribution, 96\n",
      "bootstrap aggregation, seebagging\n",
      "bootstrap method, 88, 308\n",
      "bounded mapping, 391\n",
      "boxplot, 10, 14\n",
      "broadcasting (Python), 483\n",
      "Broyden’s family, 413\n",
      "Broyden–Fletcher–Goldfarb–Shanno\n",
      "(BFGS) updating, 269, 340, 413\n",
      "burn-in period, 78\n",
      "C\n",
      "categorical variable, 3, 177, 178, 191,\n",
      "192, 253, 301\n",
      "Cauchy sequence, 246, 387\n",
      "Cauchy–Schwarz inequality, 223, 247,\n",
      "391, 414\n",
      "central di \u000berence estimator, 106\n",
      "central limit theorem, 449, 460\n",
      "multivariate, 450\n",
      "centroid, 144\n",
      "chain rule for di \u000berentiation, 403\n",
      "characteristic function, 225, 227, 247,\n",
      "394, 443\n",
      "characteristic polynomial, 365\n",
      "Chebyshev’s inequality, 446\n",
      "chi-squared distribution, 438, 441\n",
      "Cholesky decomposition, 70, 154, 248,\n",
      "266, 375\n",
      "circulant matrix, 383, 395\n",
      "class (Python), 475\n",
      "classiﬁcation, 20, 253–288\n",
      "hierarchical, 258\n",
      "multilabel, 258\n",
      "classiﬁer, 21, 253\n",
      "coe\u000ecient of determination, 181, 195\n",
      "adjusted, 181\n",
      "coe\u000ecient proﬁles, 221\n",
      "combinatorial optimization, 404\n",
      "comma separated values (CSV), 2\n",
      "common random numbers, 106, 119complete Hilbert space, 224, 386\n",
      "complete vector space, 216\n",
      "complete convergence, 445\n",
      "complete-data\n",
      "likelihood, 128\n",
      "log-likelihood, 138\n",
      "composition of functions, 402\n",
      "concave function, 406, 409\n",
      "conditional\n",
      "distribution, 433\n",
      "expectation, 433\n",
      "pdf, 74, 433\n",
      "probability, 430\n",
      "conﬁdence interval, 85, 89, 94, 186, 459\n",
      "Bayesian, 51\n",
      "bootstrap, 89\n",
      "conﬁdence region, 459\n",
      "confusion matrix, 255, 256\n",
      "constrained optimization, 405\n",
      "context management (Python), 479\n",
      "continuous mapping, 391\n",
      "continuous optimization, 404\n",
      "control variable, 92\n",
      "convergence\n",
      "almost sure, 442\n",
      "inLpnorm, 444\n",
      "in distribution, 442\n",
      "in probability, 441\n",
      "sure, 441\n",
      "convex\n",
      "function, 62, 220, 405\n",
      "program, 407–410\n",
      "set, 42, 405\n",
      "convolution, 382, 394\n",
      "convolution neural network, 331\n",
      "Cook’s distance, 212\n",
      "cooling factor, 97\n",
      "correlation coe \u000ecient, 71, 431\n",
      "cost-complexity\n",
      "measure, 305\n",
      "pruning, 305\n",
      "countable sample space, 424\n",
      "covariance, 431\n",
      "matrix, 45, 70, 432–434, 437, 438\n",
      "properties, 431\n",
      "coverage probability, 459Index 507\n",
      "credible\n",
      "interval, 51\n",
      "region, 51\n",
      "critical\n",
      "region, 460\n",
      "value, 460\n",
      "cross tabulate, 7\n",
      "cross-entropy\n",
      "method, 100, 110\n",
      "risk, 53, 122, 125\n",
      "in-sample, 176\n",
      "training loss, 123\n",
      "cross-validation, 37, 38\n",
      "leave-one-out, 40, 173\n",
      "linear model, 174\n",
      "crude Monte Carlo, 85\n",
      "cubic spline, 237\n",
      "cumulative distribution function (cdf),\n",
      "72, 425\n",
      "joint, 429\n",
      "cycle, 81\n",
      "D\n",
      "Davidon–Fletcher–Powell updating, 354,\n",
      "414\n",
      "decision tree, 290\n",
      "deep learning, 332\n",
      "degrees of freedom, 181\n",
      "dendrogram, 147\n",
      "density, 387\n",
      "dependent variable, 168\n",
      "derivatives\n",
      "multidimensional, 400\n",
      "partial, 399\n",
      "design matrix, 179\n",
      "detailed balance equations, 455\n",
      "determinant of a matrix, 359\n",
      "diagonal matrix, 359\n",
      "diagonalizable, 366\n",
      "dictionary (Python), 475\n",
      "digamma function, 127, 162\n",
      "dimension, 357\n",
      "direct sum, 217\n",
      "directional derivative, 406\n",
      "discrete\n",
      "distribution, 425Fourier transform, 394\n",
      "optimization, 404\n",
      "probability space, 424\n",
      "sample space, 424\n",
      "uniform distribution, 427\n",
      "discriminant analysis, 261\n",
      "distribution\n",
      "Bernoulli, 427\n",
      "Bessel, 227\n",
      "beta, 52, 427\n",
      "binomial, 427\n",
      "chi-squared, 438, 441\n",
      "discrete, 425\n",
      "discrete uniform, 427\n",
      "exponential, 427\n",
      "extreme value, 114\n",
      "F, 441\n",
      "gamma, 427\n",
      "Gaussian, seenormal\n",
      "geometric, 427\n",
      "inverse-gamma, 50, 83\n",
      "joint, 429\n",
      "multivariate normal, 45, 437\n",
      "noncentral\u001f2, 439\n",
      "normal, 44, 427, 436\n",
      "Pareto, 427\n",
      "Poisson, 427\n",
      "probability, 424, 429\n",
      "Student’s t, 441\n",
      "uniform, 427\n",
      "Weibull, 427\n",
      "divisive clustering, 147\n",
      "dot notation (Python), 468\n",
      "dual optimization problem, 409–410\n",
      "E\n",
      "early stopping, 49, 251\n",
      "e\u000eciency\n",
      "of estimators, 456\n",
      "of acceptance–rejection, 72\n",
      "eigen-decomposition, 366\n",
      "eigenvalue, 365\n",
      "eigenvector, 365\n",
      "elementary event, 424\n",
      "elite sample, 100\n",
      "empirical508 Index\n",
      "Bayes, 242\n",
      "cdf, 11, 76\n",
      "distribution, 131\n",
      "entropy impurity, 294\n",
      "epoch (deep learning), 351\n",
      "equilikely principle, 424\n",
      "ergodic Markov chain, 454\n",
      "error of the ﬁrst and second kind, 461\n",
      "estimate, 456\n",
      "estimator, 456\n",
      "bias of, 456\n",
      "control variable, 92\n",
      "e\u000eciency of, 456\n",
      "unbiased, 456\n",
      "Euclidean norm, 362\n",
      "evaluation functional, 223, 246\n",
      "event, 423\n",
      "elementary, 424\n",
      "independent, 430\n",
      "exact match ratio, 258\n",
      "exchangeable variables, 40\n",
      "expectation, 428\n",
      "conditional, 433\n",
      "properties, 431, 433\n",
      "vector, 45, 432, 434, 437\n",
      "expectation–maximization (EM)\n",
      "algorithm, 128, 137, 209\n",
      "expected generalization risk, 24\n",
      "expected optimism, 36\n",
      "explanatory variable, 22, 168\n",
      "exponential distribution, 427\n",
      "extreme value distribution, 114\n",
      "F\n",
      "factor, 3, 178\n",
      "false negative, 256\n",
      "false positive, 256\n",
      "fast Fourier transform, 396\n",
      "F\fscore, 257\n",
      "Fdistribution, 183, 197, 426, 441\n",
      "feasible region, 405\n",
      "feature, 1, 20\n",
      "importance, 313\n",
      "map, 189, 216, 225, 231, 244, 276\n",
      "feed-forward network, 328\n",
      "feedback shift register, 69ﬁnite di \u000berence method, 107, 113\n",
      "ﬁnite-dimensional distributions, 429\n",
      "Fisher information matrix, 124\n",
      "Fisher’s scoring method, 127\n",
      "folds (cross-validation), 38\n",
      "forward selection, 200\n",
      "forward substitution, 372\n",
      "Fourier expansion, 388\n",
      "Fourier transform, 393\n",
      "discrete, 394\n",
      "frequentist statistics, 455\n",
      "full rank matrix, 28\n",
      "function (Python), 470\n",
      "function space, 386\n",
      "function, Ck, 405\n",
      "functional, 391\n",
      "functions of random variables, 433\n",
      "G\n",
      "gamma\n",
      "distribution, 427\n",
      "function, 426\n",
      "Gauss–Markov inequality, 59\n",
      "Gauss–Newton search direction, 416\n",
      "Gaussian distribution, seenormal\n",
      "distribution\n",
      "Gaussian kernel, 226\n",
      "Gaussian kernel density estimate, 131\n",
      "Gaussian process, 71, 239\n",
      "Gaussian rule of thumb, 134\n",
      "generalization risk, 23, 86\n",
      "generalized inverse-gamma distribution,\n",
      "163\n",
      "generalized linear model, 204\n",
      "geometric cooling, 97\n",
      "geometric distribution, 427\n",
      "geometric multiplicity, 365\n",
      "Gibbs pdf, 97\n",
      "Gibbs sampler, 81, 83, 84\n",
      "random, 82\n",
      "random order, 82\n",
      "reversible, 82\n",
      "Gini impurity, 294\n",
      "global balance equations, 454\n",
      "global minimizer, 404\n",
      "gradient, 399, 405Index 509\n",
      "boosting, 318\n",
      "descent, 414\n",
      "Gram matrix, 218, 222, 272\n",
      "Gram–Schmidt procedure, 377\n",
      "H\n",
      "Hamming distance, 142\n",
      "Hermite polynomials, 391\n",
      "Hermitian matrix, 364, 367\n",
      "Hessian matrix, 124, 400, 405, 406\n",
      "hidden layer, 327\n",
      "hierarchical classiﬁcation, 258\n",
      "Hilbert matrix, 33\n",
      "inverse, 60\n",
      "Hilbert space, 215, 387\n",
      "isomorphism, 247\n",
      "hinge loss, 271\n",
      "histogram, 10\n",
      "Hoe\u000bding’s inequality, 62\n",
      "homotopy paths, 221\n",
      "hyperparameters, 50, 241\n",
      "hypothesis testing, 460\n",
      "I\n",
      "immutable (Python), 466\n",
      "importance sampling, 93–96\n",
      "improper prior, 50, 83\n",
      "in-sample risk, 35\n",
      "incremental e \u000bects, 179\n",
      "independence\n",
      "of event, 430\n",
      "of random variables, 431\n",
      "independence sampler, 79\n",
      "independent and identically distributed\n",
      "(iid), 431, 448, 456\n",
      "indicator, 11\n",
      "indicator feature, 178\n",
      "indicator loss, 253\n",
      "inﬁnitesimal perturbation analysis, 113\n",
      "information matrix equality, 124\n",
      "inheritance (Python), 476\n",
      "initial distribution (Markov chain), 454\n",
      "inner product, 362\n",
      "instance (Python), 476\n",
      "integration\n",
      "Monte Carlo, 86\n",
      "interaction, 179, 193interior-point method, 420\n",
      "interval estimate, seeconﬁdence interval\n",
      "inverse\n",
      "discrete Fourier transform, 395\n",
      "Fourier transform, 393\n",
      "matrix, 372\n",
      "inverse-gamma distribution, 50, 83\n",
      "inverse-transform method, 72\n",
      "irreducible risk, 32\n",
      "iterable (Python), 474\n",
      "iterative reweighted least squares, 213,\n",
      "351\n",
      "iterator (Python), 474\n",
      "J\n",
      "Jacobi\n",
      "matrix of, 411, 435\n",
      "Jensen’s inequality, 62\n",
      "joint\n",
      "cdf, 429\n",
      "pdf, 429\n",
      "jointly normal, seemultivariate normal\n",
      "jointly normal distribution, see\n",
      "multivariate normal distribution\n",
      "K\n",
      "Karush–Kuhn–Tucker (KKT) conditions,\n",
      "409, 410\n",
      "kernel density estimation, 131, 135, 226,\n",
      "331\n",
      "kernel trick, 232\n",
      "Kiefer–Wolfowitz algorithm, 107\n",
      "K-nearest neighbors method, 270\n",
      "Kolmogorov axioms, 423\n",
      "Kullback–Leibler divergence, 42, 100,\n",
      "128, 352\n",
      "L\n",
      "Lagrange\n",
      "dual program, 409\n",
      "function, 408\n",
      "method, 408–409\n",
      "multiplier, 408\n",
      "Lagrangian, 408, 418\n",
      "penalty, 418\n",
      "Laguerre polynomials, 390\n",
      "Lance–Williams update, 149510 Index\n",
      "Laplace’s approximation, 452\n",
      "lasso (regression), 220\n",
      "latent variable methods, seeauxiliary\n",
      "variable methods\n",
      "law of large numbers, 67, 448, 460\n",
      "law of total probability, 430\n",
      "learner, 22, 168\n",
      "learning rate, 336, 411\n",
      "least-squares\n",
      "iterative reweighted, 213\n",
      "nonlinear, 190, 337, 416\n",
      "ordinary, 27, 46, 171, 191, 211, 380\n",
      "regularized, 172, 236, 378\n",
      "leave-one-out cross-validation, 40, 173\n",
      "left pseudo-inverse, 362\n",
      "left-eigenvector, 367\n",
      "Legendre polynomials, 389\n",
      "length preserving transformation, 363\n",
      "length of a vector, 362\n",
      "level set, 103\n",
      "Levenberg–Marquardt search direction,\n",
      "417\n",
      "leverage, 173\n",
      "Levinson–Durbin, 71, 384\n",
      "likelihood, 42, 48, 123, 458\n",
      "complete-data, 128\n",
      "log-, 136, 458\n",
      "optimization, 137\n",
      "ratio, 93\n",
      "limited memory BFGS, 338\n",
      "limiting pdf, 454\n",
      "limiting pdf (Markov chain), 454\n",
      "line search, 411\n",
      "linear\n",
      "discriminant function, 262\n",
      "kernel, 225, 273\n",
      "mapping, 391\n",
      "model, 43, 211\n",
      "program, 408\n",
      "subspace, 364\n",
      "transformation, 358, 433\n",
      "linearly independent, 357\n",
      "link function, 204\n",
      "linkage, 148\n",
      "matrix, 150\n",
      "list comprehension (Python), 475local balance equations, seedetailed\n",
      "balance equations\n",
      "local minimizer, 404\n",
      "local/global minimum, 404\n",
      "log-likelihood, 458\n",
      "log-odds ratio, 268\n",
      "logarithmic e \u000eciency, 117\n",
      "logistic distribution, 204\n",
      "logistic regression, 204\n",
      "long-run average reward, 89\n",
      "loss function, 20\n",
      "loss matrix, 255\n",
      "M\n",
      "M-estimator, 450\n",
      "Manhattan distance, 142\n",
      "marginal distribution, 429, 438\n",
      "Markov chain, 74, 78, 80, 83, 453\n",
      "ergodic, 454\n",
      "reversible, 454\n",
      "simulation of, 75\n",
      "Markov chain Monte Carlo, 78\n",
      "Markov property, 74, 453\n",
      "Matérn kernel, 227\n",
      "matplotlib (Python), 485–487\n",
      "matrix, 358\n",
      "blockwise inverse, 372\n",
      "covariance, 70, 438\n",
      "determinant, 359\n",
      "diagonal —, 359\n",
      "inverse, 359\n",
      "of Jacobi, 400, 411, 416, 435\n",
      "pseudo-inverse, 362\n",
      "sparse, 381\n",
      "Toeplitz, 381\n",
      "trace, 359\n",
      "transpose, 359\n",
      "matrix multiplication (Python), 483\n",
      "max-cut problem, 151\n",
      "maximum a posteriori, 52\n",
      "maximum distance, 142\n",
      "maximum likelihood estimation, 42, 46,\n",
      "100, 127, 136, 137, 458\n",
      "mean integrated squared error, 133\n",
      "mean squared error, 32, 88, 456\n",
      "measure, 387Index 511\n",
      "Mersenne twister, 69\n",
      "method (Python), 468\n",
      "method of moments, 457\n",
      "Metropolis–Hastings algorithm, 78, 81\n",
      "minibatch, 337\n",
      "minimax\n",
      "equality, 410\n",
      "problem, 410\n",
      "minimization, 413\n",
      "minimizer, 404\n",
      "minimum\n",
      "global, 404\n",
      "local, 404\n",
      "misclassiﬁcation error, 255\n",
      "misclassiﬁcation impurity, 294\n",
      "mixture density, 135\n",
      "model, 40\n",
      "evidence, 54\n",
      "linear, 211\n",
      "matrix, 43, 170, 174\n",
      "multiple linear regression, 169\n",
      "normal linear, 174, 182, 183, 440\n",
      "regression, 191\n",
      "response surface, 189\n",
      "simple linear regression, 187\n",
      "modiﬁed Bessel function of the second\n",
      "kind, 163, 227\n",
      "module (Python), 471\n",
      "modulo 2 generators, 69\n",
      "modulus, 69\n",
      "moment\n",
      "generating function, 429, 438\n",
      "sample-, 457\n",
      "momentum method, 342\n",
      "Monte Carlo\n",
      "integration, 86\n",
      "sampling, 68–85\n",
      "simulation, 67\n",
      "Moore–Penrose pseudo-inverse, 362\n",
      "multi-logit, 268\n",
      "multi-output linear regression, 213\n",
      "nonlinear, 330\n",
      "multilabel classiﬁcation, 258\n",
      "multiple linear regression, 169\n",
      "multiple-recursive generator, 69\n",
      "multiplierLagrange, 408\n",
      "multivariate\n",
      "central limit theorem, 450\n",
      "normal distribution, 44–46, 437\n",
      "mutable (Python), 466\n",
      "N\n",
      "naïve Bayes, 260\n",
      "namespace (Python), 472\n",
      "nested models, 58, 180\n",
      "network architecture, 331\n",
      "network depth, 329\n",
      "network width, 329\n",
      "neural networks, 325\n",
      "Newton’s method, 127, 205, 213, 338,\n",
      "411\n",
      "— for root-ﬁnding, 411\n",
      "quasi —, 338\n",
      "Neyman–Pearson approach, 461\n",
      "noisy optimization, 105\n",
      "nominal distribution, 93\n",
      "noncentral\u001f2distribution, 439\n",
      "norm, 386, 391\n",
      "normal distribution, 45, 427, 436, 437\n",
      "normal equations, 28\n",
      "normal linear model, 46, 174, 182, 183,\n",
      "440\n",
      "normal matrix, 367\n",
      "normal method (bootstrap), 89\n",
      "normal model, 44\n",
      "Bayesian, 49, 50, 83\n",
      "normal updating (cross-entropy), 101\n",
      "null hypothesis, 460\n",
      "null space, 365\n",
      "O\n",
      "object (Python), 466\n",
      "objective function, 404, 405, 409, 417\n",
      "Occam’s razor, 173\n",
      "operator, 391\n",
      "operator (Python), 467\n",
      "optimal decision boundary, 272\n",
      "optimization\n",
      "combinatorial, 404\n",
      "constrained, 405\n",
      "continuous, 404\n",
      "unconstrained, 405512 Index\n",
      "ordinary least-squares, 27\n",
      "orthogonal\n",
      "basis, 363\n",
      "complement, 364\n",
      "matrix, 363, 384\n",
      "polynomial, 390\n",
      "projection, 364\n",
      "vector, 362\n",
      "orthonormal, 363\n",
      "basis, 388\n",
      "system, 387\n",
      "out-of-bag, 309\n",
      "overﬁtting, 23, 35, 141, 172, 216, 237,\n",
      "291, 295, 302, 316\n",
      "overloading (Python), 470\n",
      "P\n",
      "p-norm, 220, 410\n",
      "P-value, 195, 461\n",
      "pandas (Python), 2, 487–492\n",
      "Pareto distribution, 427\n",
      "Parseval’s formula, 394\n",
      "partial derivative, 399\n",
      "partition, 430\n",
      "peaks function, 233\n",
      "Pearson’s height data, 207\n",
      "penalty function, 417, 421\n",
      "exact, 418\n",
      "percentile, 7\n",
      "percentile method (bootstrap), 89, 91\n",
      "permutation matrix, 370\n",
      "Plancherel’s theorem, 394\n",
      "PLU decomposition, 370\n",
      "pointwise squared bias, 35\n",
      "pointwise variance, 35\n",
      "Poisson distribution, 427\n",
      "polynomial kernel, 230\n",
      "polynomial regression model, 26\n",
      "positive deﬁnite\n",
      "matrix, 405\n",
      "positive semideﬁnite\n",
      "function, 223\n",
      "matrix, 369, 406\n",
      "posterior\n",
      "pdf, 48\n",
      "predictive density, 49precision, 257\n",
      "predicted residual, 173\n",
      "— sum of squares (PRESS), 173\n",
      "prediction function, 20\n",
      "prediction interval, 186\n",
      "predictive mean, 240\n",
      "predictor, 168\n",
      "primal optimization problem, 409\n",
      "principal axes, 154\n",
      "principal component analysis (PCA),\n",
      "153, 155\n",
      "principal components, 154\n",
      "prior\n",
      "improper, 83\n",
      "pdf, 48\n",
      "predictive density, 49\n",
      "uninformative, 49\n",
      "probability\n",
      "density function (pdf), 426\n",
      "density function (pdf), joint, 429\n",
      "distribution, 424, 429\n",
      "mass function, 426\n",
      "measure, 423\n",
      "space, 424\n",
      "product rule, 74, 430, 454\n",
      "projected subgradient method, 106\n",
      "projection matrix, 27, 173, 211, 267,\n",
      "364, 440\n",
      "projection pursuit, 351\n",
      "proposal (MCMC), 78\n",
      "pseudo-inverse, 28, 211, 362, 380\n",
      "Pythagoras’ theorem, 180, 181, 183, 232,\n",
      "363\n",
      "Q\n",
      "quadratic discriminant function, 262\n",
      "quadratic program, 408\n",
      "qualitative variable, 3\n",
      "quantile, 51, 85\n",
      "quantile–quantile plot, 199\n",
      "quantitative variable, 3\n",
      "quartile, 7\n",
      "quasi-Newton method, 338, 413\n",
      "quasi-random point set, 233\n",
      "quotient rule for di \u000berentiation, 160Index 513\n",
      "R\n",
      "radial basis function (rbf) kernel, 226,\n",
      "278\n",
      "random\n",
      "experiment, 423\n",
      "number generator, 68\n",
      "numbers (Python), 484\n",
      "sample\n",
      "see iid sample, 456\n",
      "variable, 424\n",
      "vector, 429, 433\n",
      "covariance of, 432\n",
      "expectation of, 432\n",
      "walk sampler, 80\n",
      "range (Python), 474\n",
      "rank, 28, 358\n",
      "rarity parameter (cross-entropy), 100\n",
      "ratio estimator, 89\n",
      "read_csv (Python), 2\n",
      "recall, 257\n",
      "reference (Python), 469\n",
      "regional prediction functions, 290\n",
      "regression, 20, 167\n",
      "function, 21\n",
      "line, 169\n",
      "model, 191\n",
      "simple linear, 181\n",
      "regularization, 216, 217\n",
      "paths, 221\n",
      "regularization parameter, 217\n",
      "regularizer, 217\n",
      "relative error (estimated), 85\n",
      "relative time variance product, 456\n",
      "renewal reward process, 89\n",
      "representational capacity, 325\n",
      "representer of evaluation, 223\n",
      "reproducing kernel Hilbert space\n",
      "(RKHS), 223\n",
      "reproducing property, 223\n",
      "resampling, 76, 88\n",
      "residual squared error, 171\n",
      "residual sum of squares, 171\n",
      "residuals, 171, 173\n",
      "response surface model, 189\n",
      "response variable, 20, 168\n",
      "reverse Markov chain, 454reversibility, 454\n",
      "reversible Gibbs sampler, 82\n",
      "ridge regression, 216, 217\n",
      "Riemann–Lebesgue lemma, 393\n",
      "right pseudo-inverse, 362\n",
      "risk, 20, 167\n",
      "Robbins–Monro algorithm, 106\n",
      "root ﬁnding, 410\n",
      "R2,seecoe\u000ecient of determination\n",
      "S\n",
      "saddle point, 405\n",
      "problem, 410\n",
      "sample\n",
      "mean, 7, 85, 457\n",
      "median, 7\n",
      "quantile, 7\n",
      "range, 8\n",
      "space, 423\n",
      "countable, 424\n",
      "discrete, 424\n",
      "standard deviation, 8, 457\n",
      "variance, 8, 89, 457\n",
      "saturation, 334\n",
      "Savage–Dickey density ratio, 58\n",
      "scale-mixture, 164\n",
      "scatterplot, 13\n",
      "scikit-learn (Python), 492–495\n",
      "score function, 42, 123\n",
      "method, 113\n",
      "secant condition, 413\n",
      "semi-simple matrix, 366\n",
      "sequence object (Python), 474\n",
      "set (Python), 474\n",
      "shear operation, 361\n",
      "Sherman–Morrison\n",
      "formula, 174, 248, 373\n",
      "recursion, 374, 375, 416\n",
      "signiﬁcance level, 461\n",
      "simple linear regression, 169, 187\n",
      "simulated annealing, 96, 97\n",
      "sinc kernel, 226\n",
      "singular value, 379, 380\n",
      "singular value decomposition, 154, 378\n",
      "slack variable, 419\n",
      "Slater’s condition, 410514 Index\n",
      "slice (Python), 3, 466\n",
      "smoothing parameter, 100\n",
      "softmax function, 269, 330\n",
      "source vectors, 143\n",
      "sparse matrix, 381\n",
      "speciﬁcity, 257\n",
      "spectral representation, 379\n",
      "sphere the data, 266\n",
      "splitting for continuous optimization,\n",
      "103\n",
      "splitting rule, 291\n",
      "squared-error loss, 167\n",
      "standard basis, 358\n",
      "standard deviation, 428\n",
      "sample-, 457\n",
      "standard error (estimated), 85\n",
      "standard normal distribution, 436\n",
      "standardization, 437\n",
      "stationary point, 405\n",
      "statistical (estimation) error, 32, 95\n",
      "statistical test\n",
      "one-sided –, 460\n",
      "two-sided –, 460\n",
      "statistics\n",
      "Bayesian, 456\n",
      "frequentist, 455\n",
      "steepest descent, 332, 414\n",
      ", 316size parameter \n",
      "stochastic approximation, 106, 337\n",
      "stochastic conﬁdence interval, 459\n",
      "stochastic counterpart, 107\n",
      "stochastic gradient descent, 337, 351\n",
      "stochastic process, 429\n",
      "strict feasibility, 410\n",
      "strong duality, 410\n",
      "Student’s tdistribution, 183, 426, 441\n",
      "multivariate, 162, 164, 227\n",
      "studentized residual, 212\n",
      "stumps, 321\n",
      "subgradient, 406\n",
      "subgradient method, 106\n",
      "sum rule, 424\n",
      "supervised learning, 22\n",
      "support vectors, 273\n",
      "Sylvester equation, 381\n",
      "systematic Gibbs sampler, 82T\n",
      "tables\n",
      "counts, 6\n",
      "frequency, 6\n",
      "margins, 7\n",
      "target distribution, 78\n",
      "Taylor’s theorem\n",
      "multidimensional, 402\n",
      "test\n",
      "loss, 24\n",
      "sample, 24\n",
      "statistic, 460\n",
      "theta KDE, 134\n",
      "time-homogeneous, 454\n",
      "Tobit regression, 209\n",
      "Toeplitz matrix, 381\n",
      "total sum of squares, 181\n",
      "tower property of expectation, 433\n",
      "trace of a matrix, 359\n",
      "training loss, 23\n",
      "training set, 21\n",
      "transformation\n",
      "of random variables, 433, 435\n",
      "rule, 95, 434\n",
      "transition\n",
      "density, 74, 454\n",
      "graph, 75\n",
      "transpose of a matrix, 358, 359\n",
      "tree branch, 303\n",
      "true negative, 256\n",
      "true positive, 256\n",
      "trust region, 411\n",
      "type (Python), 468\n",
      "type I and type II errors, 461\n",
      "U\n",
      "unbiased, 59\n",
      "unbiased estimator, 456\n",
      "unconstrained optimization, 405\n",
      "uniform distribution, 427\n",
      "union bound, 424\n",
      "unitary matrix, 364\n",
      "universal approximation property, 227\n",
      "unsupervised learning, 22\n",
      "V\n",
      "validation set, 25, 305Index 515\n",
      "Vandermonde matrix, 29, 395\n",
      "Vapnik–Chernovenkis bound, 62\n",
      "variance, 428, 432\n",
      "properties, 431\n",
      "sample, 89, 457\n",
      "sample-, 457\n",
      "vector quantization, 143\n",
      "vector space, 357\n",
      "basis, 357\n",
      "dimension, 357V oronoi tessellation, 143\n",
      "W\n",
      "weak derivative, 113\n",
      "weak duality, 409\n",
      "weak learners, 315\n",
      "Weibull distribution, 427\n",
      "weight matrix (deep learning), 328\n",
      "Wolfe dual program, 409\n",
      "Woodbury identity, 249, 353, 373, 401\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for page in pdfreader.pages:\n",
    "    raw_text += page.extract_text()\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into character\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "text = splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1693"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings \n",
    "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x721efbee7100>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(ChatGoogleGenerativeAI(google_api_key=gemini_key, model='gemini-pro'), chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give the detailed Summary About chapter Deep Learning.\"\n",
    "docs = document_search.similarity_search(query)\n",
    "answer = chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Chapter: Deep Learning**\\n\\n**Introduction**\\n\\n* Deep learning refers to machine learning models with multiple layers of abstraction that can learn complex patterns and representations from data.\\n\\n**Artificial Neural Networks (ANNs)**\\n\\n* ANNs are the foundation of deep learning, mimicking the structure and function of the human brain.\\n* They consist of interconnected layers of nodes (neurons) that process and transform data.\\n* Each layer learns specific features from the input data, building up to complex representations at higher layers.\\n\\n**Convolutional Neural Networks (CNNs)**\\n\\n* CNNs are specialized ANNs designed for processing spatial data, such as images.\\n* They use convolutional operations to extract features from the input data, preserving spatial information.\\n* CNNs are widely used in image recognition, object detection, and facial recognition tasks.\\n\\n**Recurrent Neural Networks (RNNs)**\\n\\n* RNNs are ANNs that can process sequential data, such as text or time series.\\n* They maintain an internal state that is updated with each input, enabling them to capture temporal dependencies.\\n* RNNs are commonly used in natural language processing, speech recognition, and machine translation.\\n\\n**Autoencoders**\\n\\n* Autoencoders are ANNs that can learn to compress and reconstruct data.\\n* They consist of an encoder that compresses the input into a lower-dimensional representation and a decoder that reconstructs the original input.\\n* Autoencoders are used for dimensionality reduction, feature extraction, and data generation.\\n\\n**Generative Adversarial Networks (GANs)**\\n\\n* GANs are a type of deep learning model that can generate new data that resembles the training data.\\n* They consist of two networks: a generator that creates new data and a discriminator that evaluates the generated data.\\n* GANs are used for image generation, text generation, and other creative applications.\\n\\n**Training Deep Learning Models**\\n\\n* Deep learning models are trained using large amounts of data and specialized optimization algorithms.\\n* Training involves iteratively adjusting the model's parameters to minimize the loss function, which measures the model's performance.\\n* Regularization techniques are used to prevent overfitting and improve generalization.\\n\\n**Applications of Deep Learning**\\n\\n* Image recognition and object detection\\n* Natural language processing\\n* Speech recognition and synthesis\\n* Machine translation\\n* Medical diagnosis and drug discovery\\n* Robotics and autonomous vehicles\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
